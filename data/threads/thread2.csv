"The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",0
"The FlowNet proved that optical flow estimation can be approached as a learning challenge. However, traditional methods still set the standard for flow quality, particularly on small displacements and real-world data, where FlowNet falls short of variational methods. This study enhances the concept of end-to-end learning of optical flow and achieves significant success. The improvements in quality and speed stem from three key contributions: firstly, we emphasize the importance of the schedule of presenting data during training. Secondly, we develop a stacked architecture that incorporates warping of the second image with intermediate optical flow. Thirdly, we tackle small displacements by introducing a sub-network specializing in small motions. FlowNet 2.0 is only slightly slower than the original but reduces estimation error by over 50%. It performs on par with state-of-the-art methods while running at interactive frame rates. Additionally, we present faster variants that enable optical flow computation at up to 140fps with accuracy matching the original FlowNet.",1
"This paper proposes a novel MAP inference framework for Markov Random Field (MRF) in parallel computing environments. The inference framework, dubbed Swarm Fusion, is a natural generalization of the Fusion Move method. Every thread (in a case of multi-threading environments) maintains and updates a solution. At each iteration, a thread can generate arbitrary number of solution proposals and take arbitrary number of concurrent solutions from the other threads to perform multi-way fusion in updating its solution. The framework is general, making popular existing inference techniques such as alpha-expansion, fusion move, parallel alpha-expansion, and hierarchical fusion, its special cases. We have evaluated the effectiveness of our approach against competing methods on three problems of varying difficulties, in particular, the stereo, the optical flow, and the layered depthmap estimation problems.",0
"In this paper, a new MAP inference framework for Markov Random Field (MRF) is proposed for use in parallel computing environments. Known as Swarm Fusion, this framework is an extension of the Fusion Move method and is applicable to multi-threading environments where each thread updates and maintains a solution. At each iteration, the threads can generate multiple solution proposals and incorporate concurrent solutions from other threads to update their own solution. This framework is versatile and includes existing inference techniques such as alpha-expansion, fusion move, parallel alpha-expansion, and hierarchical fusion. The effectiveness of Swarm Fusion was evaluated against other methods on three different problems: stereo, optical flow, and layered depthmap estimation.",1
"Surveillance video parsing, which segments the video frames into several labels, e.g., face, pants, left-leg, has wide applications. However,pixel-wisely annotating all frames is tedious and inefficient. In this paper, we develop a Single frame Video Parsing (SVP) method which requires only one labeled frame per video in training stage. To parse one particular frame, the video segment preceding the frame is jointly considered. SVP (1) roughly parses the frames within the video segment, (2) estimates the optical flow between frames and (3) fuses the rough parsing results warped by optical flow to produce the refined parsing result. The three components of SVP, namely frame parsing, optical flow estimation and temporal fusion are integrated in an end-to-end manner. Experimental results on two surveillance video datasets show the superiority of SVP over state-of-the-arts.",0
"The utilization of surveillance video parsing, which categorizes video frames into various labels such as face, pants, and left-leg, has broad applications. However, the process of annotating every frame pixel-by-pixel is time-consuming and inefficient. This research paper introduces the Single Frame Video Parsing (SVP) approach, which only necessitates one labeled frame per video during the training phase. To parse a particular frame, SVP takes into account the video segment that precedes it. SVP employs three components: 1) rough parsing of frames within the video segment, 2) estimation of optical flow between frames, and 3) fusion of the rough parsing results, warped by optical flow, to produce a refined parsing result. The three SVP components, i.e., frame parsing, optical flow estimation, and temporal fusion, are integrated in an end-to-end manner. Experimental results from two surveillance video datasets demonstrate that SVP outperforms state-of-the-art methods.",1
"We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small (< 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning.",0
"Our approach to computing optical flow involves a combination of classical spatial-pyramid formulation and deep learning. The method estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level using the current flow estimate and computing an update to the flow. Instead of minimizing an objective function at each pyramid level, we train one deep network per level to compute the flow update. Our Spatial Pyramid Network (SPyNet) is much simpler and smaller than FlowNet, making it more efficient and suitable for embedded applications. Convolutional approach applied to pairs of warped images is appropriate since the flow at each pyramid level is small. Unlike FlowNet, our learned convolution filters resemble classical spatio-temporal filters, providing insight into the method and how to improve it. Our results outperform FlowNet on most standard benchmarks, indicating a novel direction of combining classical flow methods with deep learning.",1
"Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB-2011. We also show that our method can generalize to improve hand-crafted descriptors (e.g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.",0
"The ability to identify visual similarities among local features is crucial in numerous computer vision applications. Although defining features with a larger contextual scope enhances their discriminative power, it may also compromise the spatial precision of these features. To address this issue, we present AutoScaler, a scale-attention network that balances the trade-off between contextual scale and spatial accuracy in visual correspondence tasks. Our network comprises a feature network that shares weights to calculate multi-scale feature maps and an attention network that optimally combines them in the scale space. As a result, our network can adjust the receptive field sizes according to different input scales. We train the entire network end-to-end in a siamese framework for visual correspondence tasks, and our approach outperforms state-of-the-art methods on challenging benchmarks for optical flow and semantic matching, such as Sintel, KITTI, and CUB-2011. We also demonstrate that our method can improve hand-crafted descriptors (e.g., Daisy) for general visual correspondence tasks. Furthermore, our attention network generates visually interpretable scale attention maps.",1
"In this paper, two simple principal component regression methods for estimating the optical flow between frames of video sequences according to a pel-recursive manner are introduced. These are easy alternatives to dealing with mixtures of motion vectors in addition to the lack of prior information on spatial-temporal statistics (although they are supposed to be normal in a local sense). The 2D motion vector estimation approaches take into consideration simple image properties and are used to harmonize regularized least square estimates. Their main advantage is that no knowledge of the noise distribution is necessary, although there is an underlying assumption of localized smoothness. Preliminary experiments indicate that this approach provides robust estimates of the optical flow.",0
"This paper presents two uncomplicated techniques for principal component regression that can be used to estimate the optical flow between video sequence frames in a pel-recursive manner. These methods provide a straightforward solution for dealing with motion vector mixtures and the absence of prior knowledge on spatial-temporal statistics, which should normally be local. By taking simple image properties into account, these 2D motion vector estimation approaches can effectively harmonize regularized least square estimates. The key benefit is that no understanding of the noise distribution is required, although it is assumed that the smoothness is localized. Preliminary experiments suggest that this method produces reliable optical flow estimates.",1
"The computation of 2-D optical flow by means of regularized pel-recursive algorithms raises a host of issues, which include the treatment of outliers, motion discontinuities and occlusion among other problems. We propose a new approach which allows us to deal with these issues within a common framework. Our approach is based on the use of a technique called Generalized Cross-Validation to estimate the best regularization scheme for a given pixel. In our model, the regularization parameter is a matrix whose entries can account for diverse sources of error. The estimation of the motion vectors takes into consideration local properties of the image following a spatially adaptive approach where each moving pixel is supposed to have its own regularization matrix. Preliminary experiments indicate that this approach provides robust estimates of the optical flow.",0
"A plethora of challenges arise when using regularized pel-recursive algorithms to compute 2-D optical flow, such as handling outliers, motion discontinuities, and occlusion. To address these challenges, we present a novel approach that tackles these issues within a unified framework. Our methodology employs the Generalized Cross-Validation technique to identify the optimal regularization scheme for a given pixel. Our regularization parameter matrix accounts for a range of error sources. Our motion vector estimation method incorporates local image characteristics and adopts a spatially adaptive approach where each moving pixel has its own regularization matrix. Our preliminary experiments suggest that this approach yields robust optical flow estimates.",1
"The pel-recursive computation of 2-D optical flow has been extensively studied in computer vision to estimate motion from image sequences, but it still raises a wealth of issues, such as the treatment of outliers, motion discontinuities and occlusion. It relies on spatio-temporal brightness variations due to motion. Our proposed adaptive regularized approach deals with these issues within a common framework. It relies on the use of a data-driven technique called Mixed Norm (MN) to estimate the best motion vector for a given pixel. In our model, various types of noise can be handled, representing different sources of error. The motion vector estimation takes into consideration local image properties and it results from the minimization of a mixed norm functional with a regularization parameter depending on the kurtosis. This parameter determines the relative importance of the fourth norm and makes the functional convex. The main advantage of the developed procedure is that no knowledge of the noise distribution is necessary. Experiments indicate that this approach provides robust estimates of the optical flow.",0
"The computation of 2-D optical flow using pel-recursive method has been extensively researched in computer vision to estimate motion from image sequences. However, it still poses several challenges including the handling of outliers, motion discontinuities, and occlusion. This technique relies on spatio-temporal brightness changes that result from motion. Our proposed approach is designed to tackle these issues using an adaptive regularized framework. The approach utilizes a data-driven technique referred to as Mixed Norm (MN) to estimate the best motion vector for a given pixel. Our model can handle various types of noise that represent different sources of error. The motion vector estimation considers local image properties and is achieved through the minimization of a mixed norm functional where the regularization parameter depends on the kurtosis. This parameter determines the importance of the fourth norm and makes the functional convex. The major advantage of our approach is that it does not require any knowledge of the noise distribution. Experiments have shown that the proposed method provides robust estimates of optical flow.",1
"This article describes the implementation of the joint motion estimation and image reconstruction framework presented by Burger, Dirks and Sch\""onlieb and extends this framework to large-scale motion between consecutive image frames. The variational framework uses displacements between consecutive frames based on the optical flow approach to improve the image reconstruction quality on the one hand and the motion estimation quality on the other. The energy functional consists of a data-fidelity term with a general operator that connects the input sequence to the solution, it has a total variation term for the image sequence and is connected to the underlying flow using an optical flow term. Additional spatial regularity for the flow is modeled by a total variation regularizer for both components of the flow. The numerical minimization is performed in an alternating manner using primal-dual techniques. The resulting schemes are presented as pseudo-code together with a short numerical evaluation.",0
"In this article, the joint motion estimation and image reconstruction framework proposed by Burger, Dirks, and Sch\""onlieb is discussed and extended to accommodate large-scale motion between consecutive image frames. The framework employs the optical flow approach to determine displacements between frames, which enhances both image reconstruction and motion estimation quality. The energy functional comprises a data-fidelity term, a total variation term for the image sequence, and an optical flow term that connects it to the underlying flow. Moreover, a total variation regularizer is incorporated to establish additional spatial regularity for the flow's two components. The numerical minimization is performed using primal-dual techniques in an alternating manner. The resulting algorithms are presented as pseudo-code, along with a concise numerical evaluation.",1
"Conventional approaches to image de-fencing use multiple adjacent frames for segmentation of fences in the reference image and are limited to restoring images of static scenes only. In this paper, we propose a de-fencing algorithm for images of dynamic scenes using an occlusion-aware optical flow method. We divide the problem of image de-fencing into the tasks of automated fence segmentation from a single image, motion estimation under known occlusions and fusion of data from multiple frames of a captured video of the scene. Specifically, we use a pre-trained convolutional neural network to segment fence pixels from a single image. The knowledge of spatial locations of fences is used to subsequently estimate optical flow in the occluded frames of the video for the final data fusion step. We cast the fence removal problem in an optimization framework by modeling the formation of the degraded observations. The inverse problem is solved using fast iterative shrinkage thresholding algorithm (FISTA). Experimental results show the effectiveness of proposed algorithm.",0
"The usual methods for removing fences from images involve using several contiguous frames to identify fence segments in the reference image. However, their applicability is restricted to restoring static scene images. This study presents a de-fencing technique that works on images of dynamic scenes by utilizing an occlusion-aware optical flow approach. We break down the image de-fencing problem into three tasks: automated fence segmentation from a single image, motion estimation under known occlusions, and combining data from multiple frames of a recorded video of the scene. Specifically, we employ a pre-trained convolutional neural network to identify fence pixels from a single image. We use the information about the spatial locations of fences to estimate optical flow in the occluded frames of the video for the final data fusion step. We address the fence removal issue by constructing a distorted observation model in an optimization framework. The inverse problem is then solved using the fast iterative shrinkage thresholding algorithm (FISTA). According to experimental findings, the proposed technique is effective.",1
"Fine-scale short-term cloud motion prediction is needed for several applications, including solar energy generation and satellite communications. In tropical regions such as Singapore, clouds are mostly formed by convection; they are very localized, and evolve quickly. We capture hemispherical images of the sky at regular intervals of time using ground-based cameras. They provide a high resolution and localized cloud images. We use two successive frames to compute optical flow and predict the future location of clouds. We achieve good prediction accuracy for a lead time of up to 5 minutes.",0
"Accurate prediction of short-term cloud movement at a fine scale is crucial in various fields, such as solar energy production and satellite communication. In Singapore's tropical areas, clouds are primarily formed through convection, are highly concentrated, and undergo rapid evolution. To obtain high-resolution images of localized clouds, we take hemispherical sky images at regular intervals using ground-based cameras. By calculating optical flow from two consecutive frames, we can forecast the future cloud location and attain a respectable prediction accuracy of up to 5 minutes.",1
"In the absence of pedestrian crossing lights, finding a safe moment to cross the road is often hazardous and challenging, especially for people with visual impairments. We present a reliable low-cost solution, an Android device attached to a traffic sign or lighting pole near the crossing, indicating whether it is safe to cross the road. The indication can be by sound, display, vibration, and various communication modalities provided by the Android device. The integral system camera is aimed at approaching traffic. Optical flow is computed from the incoming video stream, and projected onto an influx map, automatically acquired during a brief training period. The crossing safety is determined based on a 1-dimensional temporal signal derived from the projection. We implemented the complete system on a Samsung Galaxy K-Zoom Android smartphone, and obtained real-time operation. The system achieves promising experimental results, providing pedestrians with sufficiently early warning of approaching vehicles. The system can serve as a stand-alone safety device, that can be installed where pedestrian crossing lights are ruled out. Requiring no dedicated infrastructure, it can be powered by a solar panel and remotely maintained via the cellular network.",0
"When pedestrian crossing lights are absent, crossing the road can be dangerous and difficult, especially for those with visual impairments. Our solution is a cost-effective option that involves attaching an Android device to a traffic sign or lighting pole near the crossing, which will indicate whether it is safe to cross. This indication can be given through sound, display, vibration, or other communication methods offered by the Android device. The system's camera is aimed at approaching traffic, and optical flow is computed from the incoming video stream, creating an influx map that is automatically acquired during a brief training period. The safety of the crossing is determined based on a 1-dimensional temporal signal derived from the projection. We implemented the complete system on a Samsung Galaxy K-Zoom Android smartphone, achieving real-time operation. Our system provides pedestrians with early warning of approaching vehicles and can function as a stand-alone safety device that can be installed where pedestrian crossing lights are not available. Additionally, it requires no special infrastructure and can be powered by a solar panel and remotely maintained via the cellular network.",1
"Convex relaxations of nonconvex multilabel problems have been demonstrated to produce superior (provably optimal or near-optimal) solutions to a variety of classical computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. The key idea is that we approximate the dataterm of the vectorial labeling problem in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for the fractional solutions of the relaxed convex problem. In numerous experiments on large-displacement optical flow estimation and on color image denoising we demonstrate that the computed solutions have superior quality while requiring much lower memory and runtime.",0
"Although convex relaxations of nonconvex multilabel problems have been proven to deliver optimal or nearly optimal solutions to computer vision problems, they have limited practical applicability due to the need for finely discretized label spaces, resulting in significant memory and runtime requirements. To address this issue, we propose a sublabel accurate convex relaxation for vectorial multilabel problems that approximates the dataterm of the labeling problem in a piecewise convex manner, which improves the accuracy of the cost function and provides a more meaningful interpretation of the fractional solutions of the relaxed convex problem. Through experiments on large-displacement optical flow estimation and color image denoising, we demonstrate that our method produces higher-quality solutions while requiring significantly less memory and runtime.",1
"Egocentric, or first-person vision which became popular in recent years with an emerge in wearable technology, is different than exocentric (third-person) vision in some distinguishable ways, one of which being that the camera wearer is generally not visible in the video frames. Recent work has been done on action and object recognition in egocentric videos, as well as work on biometric extraction from first-person videos. Height estimation can be a useful feature for both soft-biometrics and object tracking. Here, we propose a method of estimating the height of an egocentric camera without any calibration or reference points. We used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation. Here, we introduce a framework inspired by two stream networks comprising of two Convolutional Neural Networks, one based on spatial information, and one based on information given by optical flow in a frame. Given an egocentric video as an input to the framework, our model yields a height estimate as an output. We also incorporate late fusion to learn a combination of temporal and spatial cues. Comparing our model with other methods we used as baselines, we achieve height estimates for videos with a Mean Average Error of 14.04 cm over a range of 103 cm of data, and classification accuracy for relative height (tall, medium or short) up to 93.75% where chance level is 33%.",0
"In recent years, egocentric vision, also known as first-person vision, has gained popularity with the rise of wearable technology. It differs from exocentric (third-person) vision in that the camera wearer is usually not visible in the video frames. There has been recent research on recognizing actions and objects in egocentric videos, as well as extracting biometric information. Estimating height can be useful for soft-biometrics and object tracking, and we propose a method for estimating the height of an egocentric camera without calibration or reference points. Our approach combines traditional computer vision techniques and deep learning to identify visual cues that result in the best height estimates. We introduce a framework inspired by two stream networks, using one Convolutional Neural Network for spatial information and another for optical flow in a frame. Our model provides a height estimate as output when given an egocentric video as input, and we incorporate late fusion to learn a combination of temporal and spatial cues. Comparing with other methods, we achieve a Mean Average Error of 14.04 cm over a range of 103 cm, and a classification accuracy for relative height (tall, medium or short) up to 93.75%, surpassing chance level of 33%.",1
"We propose a large displacement optical flow method that introduces a new strategy to compute a good local minimum of any optical flow energy functional. The method requires a given set of discrete matches, which can be extremely sparse, and an energy functional which locally guides the interpolation from those matches. In particular, the matches are used to guide a structured coordinate-descent of the energy functional around these keypoints. It results in a two-step minimization method at the finest scale which is very robust to the inevitable outliers of the sparse matcher and able to capture large displacements of small objects. Its benefits over other variational methods that also rely on a set of sparse matches are its robustness against very few matches, high levels of noise and outliers. We validate our proposal using several optical flow variational models. The results consistently outperform the coarse-to-fine approaches and achieve good qualitative and quantitative performance on the standard optical flow benchmarks.",0
"Our proposed method for large displacement optical flow involves a novel approach to finding a good local minimum for any optical flow energy functional. The method utilizes a set of discrete matches, which may be sparse, and an energy functional that guides interpolation from those matches. The matches are used to guide a structured coordinate-descent of the energy functional around the key points. This results in a two-step minimization method at the finest scale that is highly resilient to the inevitable outliers of the sparse matcher and can capture large displacements of small objects. Our method has advantages over other variational methods that also rely on a set of sparse matches, including robustness against very few matches, high levels of noise, and outliers. We tested our proposal using various optical flow variational models and achieved consistently superior results compared to coarse-to-fine approaches, with excellent qualitative and quantitative performance on standard optical flow benchmarks.",1
"Human actions are comprised of a sequence of poses. This makes videos of humans a rich and dense source of human poses. We propose an unsupervised method to learn pose features from videos that exploits a signal which is complementary to appearance and can be used as supervision: motion. The key idea is that humans go through poses in a predictable manner while performing actions. Hence, given two poses, it should be possible to model the motion that caused the change between them. We represent each of the poses as a feature in a CNN (Appearance ConvNet) and generate a motion encoding from optical flow maps using a separate CNN (Motion ConvNet). The data for this task is automatically generated allowing us to train without human supervision. We demonstrate the strength of the learned representation by finetuning the trained model for Pose Estimation on the FLIC dataset, for static image action recognition on PASCAL and for action recognition in videos on UCF101 and HMDB51.",0
"Videos of humans are a valuable resource for observing human poses, as human actions consist of a series of poses. Therefore, we propose an unsupervised method to learn pose features from videos, which takes advantage of a signal complementary to appearance: motion. The underlying concept is that human poses occur in a predictable manner during actions, so it is possible to model the motion that causes changes between two poses. We use a CNN to represent each pose as a feature (Appearance ConvNet) and generate a motion encoding from optical flow maps using another CNN (Motion ConvNet). The data for this task is automatically generated, allowing us to train without human supervision. To demonstrate the effectiveness of the learned representation, we finetune the trained model for Pose Estimation on the FLIC dataset, static image action recognition on PASCAL, and action recognition in videos on UCF101 and HMDB51.",1
"This work advocates Eulerian motion representation learning over the current standard Lagrangian optical flow model. Eulerian motion is well captured by using phase, as obtained by decomposing the image through a complex-steerable pyramid. We discuss the gain of Eulerian motion in a set of practical use cases: (i) action recognition, (ii) motion prediction in static images, (iii) motion transfer in static images and, (iv) motion transfer in video. For each task we motivate the phase-based direction and provide a possible approach.",0
"The focus of this work is on promoting the use of Eulerian motion representation learning, which is a more effective approach than the standard Lagrangian optical flow model currently in use. To accurately capture Eulerian motion, a complex-steerable pyramid is employed to decompose the image and obtain phase information. Our research explores the benefits of using Eulerian motion in various practical applications, such as action recognition, motion prediction in static images, motion transfer in static images, and motion transfer in video. We provide detailed explanations for each task and suggest a potential direction using phase-based methods.",1
"The video and action classification have extremely evolved by deep neural networks specially with two stream CNN using RGB and optical flow as inputs and they present outstanding performance in terms of video analysis. One of the shortcoming of these methods is handling motion information extraction which is done out side of the CNNs and relatively time consuming also on GPUs. So proposing end-to-end methods which are exploring to learn motion representation, like 3D-CNN can achieve faster and accurate performance. We present some novel deep CNNs using 3D architecture to model actions and motion representation in an efficient way to be accurate and also as fast as real-time. Our new networks learn distinctive models to combine deep motion features into appearance model via learning optical flow features inside the network.",0
"Deep neural networks, particularly two stream CNNs that use RGB and optical flow inputs, have made significant advancements in video and action classification, delivering outstanding performance for video analysis. However, these methods have a drawback when it comes to extracting motion information, which is done outside of the CNNs and can be time-consuming on GPUs. To address this issue, end-to-end methods that explore motion representation, such as 3D-CNNs, have been proposed. Our research focuses on developing novel deep CNNs that adopt a 3D architecture to model actions and motion representation efficiently, delivering fast and accurate performance. Our networks learn distinctive models to combine deep motion features with appearance models by incorporating optical flow features within the network.",1
"We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We present one direct application of the proposed framework in weakly-supervised semantic segmentation of videos through label propagation using optical flow.",0
"A novel spatio-temporal video autoencoder is introduced in this study, which combines a traditional spatial image autoencoder with an innovative nested temporal autoencoder. The temporal encoder consists of convolutional long short-term memory (LSTM) cells, which form a differentiable visual memory capable of integrating changes over time. The focus is on motion changes, and a robust optical flow prediction module is utilized as a temporal decoder, along with an image sampler serving as a built-in feedback loop. The entire architecture is end-to-end differentiable, with the system receiving a video frame as input at each time step. Based on the current observation and LSTM memory state, the system predicts the optical flow as a dense transformation map and applies it to the current frame to generate the next frame. By minimizing the reconstruction error between the predicted next frame and the corresponding ground truth next frame, the whole system is trained to extract motion estimation features without any supervision. Furthermore, the proposed framework is directly applied to weakly-supervised semantic segmentation of videos through label propagation using optical flow.",1
"Information of time differentiation is extremely important cue for a motion representation. We have applied first-order differential velocity from a positional information, moreover we believe that second-order differential acceleration is also a significant feature in a motion representation. However, an acceleration image based on a typical optical flow includes motion noises. We have not employed the acceleration image because the noises are too strong to catch an effective motion feature in an image sequence. On one hand, the recent convolutional neural networks (CNN) are robust against input noises. In this paper, we employ acceleration-stream in addition to the spatial- and temporal-stream based on the two-stream CNN. We clearly show the effectiveness of adding the acceleration stream to the two-stream CNN.",0
"The ability to distinguish time is a crucial aspect for depicting motion. Our approach involves utilizing first-order differential velocity based on positional information, as well as second-order differential acceleration as a significant feature for motion representation. However, images of acceleration derived from typical optical flow contain motion distortions. We have refrained from implementing acceleration images due to the overwhelming amount of noise that hinders the ability to capture an effective motion feature in a sequence of images. On the other hand, convolutional neural networks (CNN) have demonstrated a strong tolerance to input noise. Therefore, we have introduced an acceleration-stream in addition to the spatial- and temporal-stream based on the two-stream CNN. Our findings clearly demonstrate the effectiveness of incorporating the acceleration stream into the two-stream CNN.",1
"We tackle the problem of estimating optical flow from a monocular camera in the context of autonomous driving. We build on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. We propose to estimate the traffic participants using instance-level segmentation. For each traffic participant, we use the epipolar constraints that govern each independent motion for faster and more accurate estimation. Our second contribution is a new convolutional net that learns to perform flow matching, and is able to estimate the uncertainty of its matches. This is a core element of our flow estimation pipeline. We demonstrate the effectiveness of our approach in the challenging KITTI 2015 flow benchmark, and show that our approach outperforms published approaches by a large margin.",0
"Our focus is on the estimation of optical flow from a monocular camera for autonomous driving. We take into account that the scene usually has a static background and only a few traffic participants that move rigidly in 3D. To estimate the traffic participants, we suggest instance-level segmentation. We apply epipolar constraints to each participant's independent motion for quicker and more accurate estimation. Additionally, we present a novel convolutional net that can learn to match flows and estimate the uncertainty of these matches, which is a crucial aspect of our flow estimation pipeline. Our approach outperforms published methods by a significant margin, as demonstrated on the KITTI 2015 flow benchmark.",1
"Recently, convolutional networks (convnets) have proven useful for predicting optical flow. Much of this success is predicated on the availability of large datasets that require expensive and involved data acquisition and laborious la- beling. To bypass these challenges, we propose an unsuper- vised approach (i.e., without leveraging groundtruth flow) to train a convnet end-to-end for predicting optical flow be- tween two images. We use a loss function that combines a data term that measures photometric constancy over time with a spatial term that models the expected variation of flow across the image. Together these losses form a proxy measure for losses based on the groundtruth flow. Empiri- cally, we show that a strong convnet baseline trained with the proposed unsupervised approach outperforms the same network trained with supervision on the KITTI dataset.",0
"Convolutional networks (convnets) have recently demonstrated their effectiveness in predicting optical flow. However, this success has relied heavily on the availability of large datasets, which are costly and labor-intensive to acquire and label. To overcome these challenges, we propose an unsupervised approach for training a convnet end-to-end to predict optical flow between two images, without relying on groundtruth flow. Our approach employs a loss function that combines a data term for measuring photometric constancy over time with a spatial term for modeling flow variation across the image. These losses serve as a proxy measure for losses based on groundtruth flow. Our empirical results demonstrate that our proposed unsupervised approach yields better performance than the same network trained with supervision on the KITTI dataset.",1
"This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections. First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information. Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability. This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities. Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking.",0
"The present study proposes a new method for data association in pedestrian tracking, which involves a two-stage learning process to match pairs of detections. The first stage trains a Siamese CNN to learn spatio-temporal descriptors from the input image patches, which include both pixel values and optical flow information. In the second stage, contextual features such as the position and size of the input patches are used in conjunction with the CNN output to generate the final matching probability using a gradient boosting classifier. The effectiveness of this approach is demonstrated by testing it on a linear programming-based multi-person tracker, which shows that our learned matching probabilities can outperform more complex models. Our method achieves state-of-the-art results in multiple people tracking on publicly available sequences.",1
"Event cameras or neuromorphic cameras mimic the human perception system as they measure the per-pixel intensity change rather than the actual intensity level. In contrast to traditional cameras, such cameras capture new information about the scene at MHz frequency in the form of sparse events. The high temporal resolution comes at the cost of losing the familiar per-pixel intensity information. In this work we propose a variational model that accurately models the behaviour of event cameras, enabling reconstruction of intensity images with arbitrary frame rate in real-time. Our method is formulated on a per-event-basis, where we explicitly incorporate information about the asynchronous nature of events via an event manifold induced by the relative timestamps of events. In our experiments we verify that solving the variational model on the manifold produces high-quality images without explicitly estimating optical flow.",0
"Neuromorphic or event cameras imitate the human perception system by measuring per-pixel intensity change rather than intensity level, resulting in sparse events captured at MHz frequency. Although these cameras sacrifice per-pixel intensity information, they provide high temporal resolution. Our study introduces a variational model that accurately mimics the behavior of event cameras, allowing for the real-time reconstruction of intensity images with arbitrary frame rates. Our approach incorporates information about the asynchronous nature of events via an event manifold, induced by the relative timestamps of events. Our experiments demonstrate that our method produces high-quality images without the need for explicit optical flow estimation.",1
"In this work, we propose an approach to the spatiotemporal localisation (detection) and classification of multiple concurrent actions within temporally untrimmed videos. Our framework is composed of three stages. In stage 1, appearance and motion detection networks are employed to localise and score actions from colour images and optical flow. In stage 2, the appearance network detections are boosted by combining them with the motion detection scores, in proportion to their respective spatial overlap. In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called action tubes, are constructed by solving two energy maximisation problems via dynamic programming. While in the first pass, action paths spanning the whole video are built by linking detection boxes over time using their class-specific scores and their spatial overlap, in the second pass, temporal trimming is performed by ensuring label consistency for all constituting detection boxes. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results across the board and significantly increasing detection speed at test time. We achieve a huge leap forward in action detection performance and report a 20% and 11% gain in mAP (mean average precision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the state-of-the-art.",0
"Our work introduces a method for detecting and categorizing multiple actions occurring concurrently in videos that are not trimmed for timing. The framework consists of three stages. Firstly, we use networks for detecting appearance and motion to score and identify actions from color images and optical flow. Secondly, we combine the scores from both networks to enhance the appearance network's detections, in line with the spatial overlap with the motion score. Thirdly, we construct action tubes, which are sequences of detection boxes representing a single action instance, through dynamic programming. We create action paths spanning the entire video in the first pass by linking detection boxes over time using class-specific scores and spatial overlap. In the second pass, we ensure label consistency for all detection boxes by performing temporal trimming. We demonstrate our algorithm's performance on challenging datasets, such as UCF101, J-HMDB-21, and LIRIS-HARL, and achieve new state-of-the-art results while significantly increasing the detection speed at test time. We report a 20% and 11% increase in mAP (mean average precision) on UCF-101 and J-HMDB-21 datasets, respectively, compared to the current state-of-the-art, indicating a significant improvement in action detection performance.",1
"The importance and demands of visual scene understanding have been steadily increasing along with the active development of autonomous systems. Consequently, there has been a large amount of research dedicated to semantic segmentation and dense motion estimation. In this paper, we propose a method for jointly estimating optical flow and temporally consistent semantic segmentation, which closely connects these two problem domains and leverages each other. Semantic segmentation provides information on plausible physical motion to its associated pixels, and accurate pixel-level temporal correspondences enhance the accuracy of semantic segmentation in the temporal domain. We demonstrate the benefits of our approach on the KITTI benchmark, where we observe performance gains for flow and segmentation. We achieve state-of-the-art optical flow results, and outperform all published algorithms by a large margin on challenging, but crucial dynamic objects.",0
"As autonomous systems continue to evolve, the significance and requirements of understanding visual scenes have grown. As a result, a considerable amount of research has been conducted on semantic segmentation and dense motion estimation. Our paper proposes a method that jointly estimates optical flow and temporally consistent semantic segmentation, connecting these two domains and leveraging each other. Semantic segmentation provides plausible physical motion information to associated pixels, while accurate pixel-level temporal correspondences improve semantic segmentation accuracy. We demonstrate the effectiveness of our approach on the KITTI benchmark, where we observed improvements in flow and segmentation performance. Our method achieves state-of-the-art optical flow results and outperforms all previously published algorithms by a significant margin on challenging dynamic objects, which are crucial.",1
"In this paper we present a dense ground truth dataset of nonrigidly deforming real-world scenes. Our dataset contains both long and short video sequences, and enables the quantitatively evaluation for RGB based tracking and registration methods. To construct ground truth for the RGB sequences, we simultaneously capture Near-Infrared (NIR) image sequences where dense markers - visible only in NIR - represent ground truth positions. This allows for comparison with automatically tracked RGB positions and the formation of error metrics. Most previous datasets containing nonrigidly deforming sequences are based on synthetic data. Our capture protocol enables us to acquire real-world deforming objects with realistic photometric effects - such as blur and illumination change - as well as occlusion and complex deformations. A public evaluation website is constructed to allow for ranking of RGB image based optical flow and other dense tracking algorithms, with various statistical measures. Furthermore, we present an RGB-NIR multispectral optical flow model allowing for energy optimization by adoptively combining featured information from both the RGB and the complementary NIR channels. In our experiments we evaluate eight existing RGB based optical flow methods on our new dataset. We also evaluate our hybrid optical flow algorithm by comparing to two existing multispectral approaches, as well as varying our input channels across RGB, NIR and RGB-NIR.",0
"This paper introduces a comprehensive dataset of nonrigidly deforming real-world scenes, consisting of both long and short video sequences. The dataset allows for the quantitative assessment of RGB-based tracking and registration methods. To create ground truth for the RGB sequences, we simultaneously captured Near-Infrared (NIR) image sequences with dense markers visible only in NIR, which represent ground truth positions. This facilitates comparison with automatically tracked RGB positions and error metric formation. Unlike previous datasets based on synthetic data, our capture protocol enables us to acquire real-world deforming objects with realistic photometric effects, occlusion, and complex deformations. We constructed a public evaluation website to rank RGB image-based optical flow and other dense tracking algorithms, with various statistical measures. Additionally, we present an RGB-NIR multispectral optical flow model that combines featured information from both the RGB and the complementary NIR channels. We evaluated eight existing RGB-based optical flow methods on our new dataset, as well as our hybrid optical flow algorithm by comparing it to two existing multispectral approaches and varying our input channels across RGB, NIR, and RGB-NIR.",1
"Representing videos by densely extracted local space-time features has recently become a popular approach for analysing actions. In this paper, we tackle the problem of categorising human actions by devising Bag of Words (BoW) models based on covariance matrices of spatio-temporal features, with the features formed from histograms of optical flow. Since covariance matrices form a special type of Riemannian manifold, the space of Symmetric Positive Definite (SPD) matrices, non-Euclidean geometry should be taken into account while discriminating between covariance matrices. To this end, we propose to embed SPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW approach to its Riemannian version. The proposed BoW approach takes into account the manifold geometry of SPD matrices during the generation of the codebook and histograms. Experiments on challenging human action datasets show that the proposed method obtains notable improvements in discrimination accuracy, in comparison to several state-of-the-art methods.",0
"The representation of videos through densely extracted local space-time features has become a popular method for action analysis. This paper addresses the problem of categorizing human actions by creating Bag of Words (BoW) models that are based on covariance matrices of spatio-temporal features. These features are formed from histograms of optical flow. Since covariance matrices belong to the Riemannian manifold, Symmetric Positive Definite (SPD) matrices, non-Euclidean geometry must be considered when differentiating between covariance matrices. To address this, we propose embedding SPD manifolds into Euclidean spaces via a diffeomorphism and expanding the BoW approach to its Riemannian version. The proposed BoW approach takes into account the manifold geometry of SPD matrices during the codebook and histogram generation. Experiments on difficult human action datasets demonstrate that the proposed method shows significant improvements in discrimination accuracy compared to several state-of-the-art methods.",1
"Saliency maps are used to understand human attention and visual fixation. However, while very well established for static images, there is no general agreement on how to compute a saliency map of dynamic scenes. In this paper we propose a mathematically rigorous approach to this prob- lem, including static saliency maps of each video frame for the calculation of the optical flow. Taking into account static saliency maps for calculating the optical flow allows for overcoming the aperture problem. Our ap- proach is able to explain human fixation behavior in situations which pose challenges to standard approaches, such as when a fixated object disappears behind an occlusion and reappears after several frames. In addition, we quantitatively compare our model against alternative solutions using a large eye tracking data set. Together, our results suggest that assessing optical flow information across a series of saliency maps gives a highly accurate and useful account of human overt attention in dynamic scenes.",0
"The purpose of saliency maps is to comprehend human attention and visual fixation. Although they are well-established for still images, there is no consensus on how to create a saliency map for moving scenes. This paper proposes a rigorous mathematical approach to this problem, which involves generating static saliency maps for each video frame to calculate optical flow. Incorporating static saliency maps when computing optical flow helps to solve the aperture problem. Our method can explain human fixation behavior in challenging scenarios, such as when a fixated object disappears behind an obstruction and reappears later. Furthermore, we compare our model against other solutions using a vast eye tracking dataset. Our findings indicate that evaluating optical flow information through a sequence of saliency maps provides a highly accurate and valuable account of human overt attention in dynamic scenes.",1
"In this paper, we introduce an end-to-end framework for video analysis focused towards practical scenarios built on theoretical foundations from sparse representation, including a novel descriptor for general purpose video analysis. In our approach, we compute kinematic features from optical flow and first and second-order derivatives of intensities to represent motion and appearance respectively. These features are then used to construct covariance matrices which capture joint statistics of both low-level motion and appearance features extracted from a video. Using an over-complete dictionary of the covariance based descriptors built from labeled training samples, we formulate low-level event recognition as a sparse linear approximation problem. Within this, we pose the sparse decomposition of a covariance matrix, which also conforms to the space of semi-positive definite matrices, as a determinant maximization problem. Also since covariance matrices lie on non-linear Riemannian manifolds, we compare our former approach with a sparse linear approximation alternative that is suitable for equivalent vector spaces of covariance matrices. This is done by searching for the best projection of the query data on a dictionary using an Orthogonal Matching pursuit algorithm. We show the applicability of our video descriptor in two different application domains - namely low-level event recognition in unconstrained scenarios and gesture recognition using one shot learning. Our experiments provide promising insights in large scale video analysis.",0
"The purpose of this paper is to present an entire framework for analyzing videos in practical situations, which is based on theoretical foundations from sparse representation. We introduce a new descriptor for general-purpose video analysis that utilizes kinematic features from optical flow and first and second-order derivatives of intensities to represent motion and appearance. These features are used to create covariance matrices that capture joint statistics of both low-level motion and appearance features extracted from a video. We formulate low-level event recognition as a sparse linear approximation problem using an over-complete dictionary of the covariance-based descriptors built from labeled training samples. To achieve this, we pose the sparse decomposition of a covariance matrix as a determinant maximization problem, which conforms to the space of semi-positive definite matrices. We also compare our approach with a sparse linear approximation alternative that is suitable for equivalent vector spaces of covariance matrices, using an Orthogonal Matching pursuit algorithm. We demonstrate the effectiveness of our video descriptor in two different application domains: low-level event recognition in unconstrained scenarios and gesture recognition using one shot learning. Our experiments provide valuable insights into large-scale video analysis.",1
"We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos. We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet. Our method outperforms the state of the art (including top ConvNet methods) by a large margin on two standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.",0
"Our proposal is a ConvNet pose estimator that is tailored to the individual's appearance, resulting in improved pose estimation in lengthy videos. Our contributions include: (i) generating additional annotations using image-based matching and dense optical flow with a few high-precision pose annotations from a generic ConvNet pose estimator; (ii) developing an occlusion aware self-evaluation model to distinguish high-quality annotations from erroneous ones; and (iii) demonstrating that these high-quality annotations can be utilized to fine-tune a ConvNet pose estimator, personalizing it to identify key discriminative features of the individual's appearance. The personalized ConvNet outperforms the original generic ConvNet and state-of-the-art methods on two benchmarks and a new challenging YouTube video dataset. Moreover, training on the automatically generated annotations can enhance the performance of a generic ConvNet on other benchmarks.",1
"This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80x60 pixels).",0
"This research aims to identify individuals in video footage by analyzing their walking pattern, known as gait. While traditional methods rely on binary silhouettes to identify gait signatures, this study explores the use of convolutional neural networks (CNN) to learn high-level descriptors from low-level motion features, such as optical flow components. The proposed CNN architecture is thoroughly evaluated on the challenging TUM-GAID dataset, and the results demonstrate that using spatio-temporal cuboids of optical flow as input data for CNN can achieve state-of-the-art results on the gait task with an image resolution eight times lower than previously reported (i.e. 80x60 pixels).",1
"Optical strain is an extension of optical flow that is capable of quantifying subtle changes on faces and representing the minute facial motion intensities at the pixel level. This is computationally essential for the relatively new field of spontaneous micro-expression, where subtle expressions can be technically challenging to pinpoint. In this paper, we present a novel method for detecting and recognizing micro-expressions by utilizing facial optical strain magnitudes to construct optical strain features and optical strain weighted features. The two sets of features are then concatenated to form the resultant feature histogram. Experiments were performed on the CASME II and SMIC databases. We demonstrate on both databases, the usefulness of optical strain information and more importantly, that our best approaches are able to outperform the original baseline results for both detection and recognition tasks. A comparison of the proposed method with other existing spatio-temporal feature extraction approaches is also presented.",0
"The concept of optical strain expands on optical flow to measure subtle changes in facial features and represent small facial movements at a pixel level. This is significant in the field of spontaneous micro-expression, where identifying these expressions can be difficult. Our study introduces a new technique for recognizing micro-expressions by utilizing facial optical strain measurements to create optical strain features and optical strain weighted features. These features are combined to create a feature histogram. We conducted experiments on the CASME II and SMIC databases, demonstrating the effectiveness of optical strain information in improving detection and recognition tasks beyond the baseline results. Additionally, we compare our method to other spatio-temporal feature extraction approaches.",1
"In this paper, we tackle the problem of temporally consistent boundary detection and hierarchical segmentation in videos. While finding the best high-level reasoning of region assignments in videos is the focus of much recent research, temporal consistency in boundary detection has so far only rarely been tackled. We argue that temporally consistent boundaries are a key component to temporally consistent region assignment. The proposed method is based on the point-wise mutual information (PMI) of spatio-temporal voxels. Temporal consistency is established by an evaluation of PMI-based point affinities in the spectral domain over space and time. Thus, the proposed method is independent of any optical flow computation or previously learned motion models. The proposed low-level video segmentation method outperforms the learning-based state of the art in terms of standard region metrics.",0
"The article addresses the issue of consistent boundary detection and hierarchical segmentation in videos. While there has been much research on identifying high-level region assignments in videos, little attention has been paid to temporal consistency in boundary detection. The authors argue that consistent boundaries are a crucial aspect of region assignment. Their proposed approach relies on PMI analysis of spatio-temporal voxels to establish temporal consistency through point affinities evaluated in the spectral domain over space and time. Unlike existing methods that rely on optical flow computation or motion models, their model is independent. The proposed low-level video segmentation technique outperforms existing learning-based methods in standard region metrics.",1
"In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.",0
"In a traditional convolutional layer, the filters that are learned remain static after training. However, we have created a new framework called the Dynamic Filter Network, which generates filters dynamically based on the input. This architecture is highly flexible due to its adaptive nature, but does not require an excessive increase in the number of model parameters. It can learn a wide range of filtering operations, including local spatial transformations, selective blurring or sharpening, and adaptive feature extraction. Multiple layers can also be combined in a recurrent architecture. Our experiments with video and stereo prediction show that this approach achieves state-of-the-art performance on the moving MNIST dataset using a much smaller model. We have also discovered that the network learns flow information by examining unlabelled training data, suggesting that it can pretrain networks for various supervised tasks in an unsupervised manner, such as optical flow and depth estimation.",1
"Compared to other applications in computer vision, convolutional neural networks have under-performed on pedestrian detection. A breakthrough was made very recently by using sophisticated deep CNN models, with a number of hand-crafted features, or explicit occlusion handling mechanism. In this work, we show that by re-using the convolutional feature maps (CFMs) of a deep convolutional neural network (DCNN) model as image features to train an ensemble of boosted decision models, we are able to achieve the best reported accuracy without using specially designed learning algorithms. We empirically identify and disclose important implementation details. We also show that pixel labelling may be simply combined with a detector to boost the detection performance. By adding complementary hand-crafted features such as optical flow, the DCNN based detector can be further improved. We set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from $11.7\%$ to $8.9\%$, a relative improvement of $24\%$. We also achieve a comparable result to the state-of-the-art approaches on the KITTI dataset.",0
"Convolutional neural networks have not performed well in pedestrian detection compared to other computer vision applications. However, a recent breakthrough was made by using deep CNN models with hand-crafted features or explicit occlusion handling mechanisms. This study demonstrates that using convolutional feature maps (CFMs) from a DCNN model as image features to train an ensemble of boosted decision models can achieve the best accuracy without requiring specially designed learning algorithms. Important implementation details are identified and disclosed. Additionally, combining pixel labeling with a detector can improve detection performance. The DCNN-based detector can be further improved by adding complementary hand-crafted features such as optical flow. A new record was set on the Caltech pedestrian dataset, with the log-average miss rate lowered from 11.7% to 8.9%, a relative improvement of 24%. Comparable results were also achieved on the KITTI dataset compared to state-of-the-art approaches.",1
"A recent paper by Gatys et al. describes a method for rendering an image in the style of another image. First, they use convolutional neural network features to build a statistical model for the style of an image. Then they create a new image with the content of one image but the style statistics of another image. Here, we extend this method to render a movie in a given artistic style. The naive solution that independently renders each frame produces poor results because the features of the style move substantially from one frame to the next. The other naive method that initializes the optimization for the next frame using the rendered version of the previous frame also produces poor results because the features of the texture stay fixed relative to the frame of the movie instead of moving with objects in the scene. The main contribution of this paper is to use optical flow to initialize the style transfer optimization so that the texture features move with the objects in the video. Finally, we suggest a method to incorporate optical flow explicitly into the cost function.",0
"Gatys et al. recently published a paper outlining a technique to generate an image in the style of another image. They used convolutional neural network features to create a statistical model for the image's style and then combined it with the content of a different image to produce a new image. In this study, we build upon this method to apply an artistic style to a movie. However, rendering each frame independently or initializing the optimization of the next frame with the previous one yields poor results. This is because the style features change significantly between frames, and the texture features remain fixed relative to the movie's frame. Thus, we introduce optical flow to initialize the style transfer optimization, enabling the texture features to move with the objects in the video. Additionally, we propose a way to explicitly integrate optical flow into the cost function.",1
"Manual spatio-temporal annotation of human action in videos is laborious, requires several annotators and contains human biases. In this paper, we present a weakly supervised approach to automatically obtain spatio-temporal annotations of an actor in action videos. We first obtain a large number of action proposals in each video. To capture a few most representative action proposals in each video and evade processing thousands of them, we rank them using optical flow and saliency in a 3D-MRF based framework and select a few proposals using MAP based proposal subset selection method. We demonstrate that this ranking preserves the high quality action proposals. Several such proposals are generated for each video of the same action. Our next challenge is to iteratively select one proposal from each video so that all proposals are globally consistent. We formulate this as Generalized Maximum Clique Graph problem using shape, global and fine grained similarity of proposals across the videos. The output of our method is the most action representative proposals from each video. Our method can also annotate multiple instances of the same action in a video. We have validated our approach on three challenging action datasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promising results compared to several baseline methods. Moreover, on UCF Sports, we demonstrate that action classifiers trained on these automatically obtained spatio-temporal annotations have comparable performance to the classifiers trained on ground truth annotation.",0
"This paper introduces a method for obtaining spatio-temporal annotations of human actions in videos through weakly supervised learning, which eliminates the need for manual annotation that is laborious and prone to human biases. The proposed approach involves generating multiple action proposals for each video, ranking them based on optical flow and saliency, and selecting a few representative proposals using a MAP-based subset selection method. The challenge is to ensure global consistency among the selected proposals from different videos, which is addressed by formulating it as a generalized maximum clique graph problem based on shape, global, and fine-grained similarity. The proposed method can annotate multiple instances of the same action in a video and has been validated on three challenging action datasets, including UCF Sport, sub-JHMDB, and THUMOS'13, with promising results compared to several baseline methods. The classifiers trained on the automatically obtained spatio-temporal annotations have demonstrated comparable performance to the classifiers trained on ground truth annotations.",1
"Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.",0
"The act of smiling can reveal the mental state of a person, whether it is genuine or fake. Although it typically indicates happiness, it can also signify other emotions, such as pity. This study seeks to differentiate between authentic and contrived smiles by examining the overall facial movement and subtle changes in features like the eyes and lips. The approach involves tracking facial markers and using dense optical flow to analyze the micro and macro expressions. The end goal is to automatically classify smiles as either spontaneous or posed using support vector machines (SVM). The experiment yielded promising results, outperforming other comparable methods when applied to a large dataset.",1
"Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.",0
"Usually, modern computer vision algorithms necessitate a costly data procurement process and meticulous human labeling. However, our approach capitalizes on the recent advancements in computer graphics to produce completely labeled, dynamic, and realistic substitute virtual worlds. We present an effective cloning technique that transfers the real world to the virtual one. To verify our technique, we create and publicly release a video dataset named Virtual KITTI. The dataset is automatically labeled, providing precise ground truth for object detection, tracking, scene, and instance segmentation, depth, and optical flow. Our research also indicates that pre-training deep learning algorithms on virtual data enhances their performance, and they behave similarly in both virtual and real worlds. Virtual worlds allow the measurement of the effect of different weather and imaging conditions on recognition performance, which may significantly impact tracking deep models' otherwise high-performance.",1
"This paper deals with a challenging, frequently encountered, yet not properly investigated problem in two-frame optical flow estimation. That is, the input frames are compounds of two imaging layers -- one desired background layer of the scene, and one distracting, possibly moving layer due to transparency or reflection. In this situation, the conventional brightness constancy constraint -- the cornerstone of most existing optical flow methods -- will no longer be valid. In this paper, we propose a robust solution to this problem. The proposed method performs both optical flow estimation, and image layer separation. It exploits a generalized double-layer brightness consistency constraint connecting these two tasks, and utilizes the priors for both of them. Experiments on both synthetic data and real images have confirmed the efficacy of the proposed method. To the best of our knowledge, this is the first attempt towards handling generic optical flow fields of two-frame images containing transparency or reflection.",0
"This article addresses a complex problem that is frequently encountered in two-frame optical flow estimation, but has not been thoroughly investigated. The problem involves input frames that consist of two imaging layers - a desired background layer of the scene and a distracting, potentially moving layer caused by transparency or reflection. This renders the conventional brightness constancy constraint, which is the foundation of most existing optical flow methods, invalid. To tackle this issue, the authors propose a robust solution that performs optical flow estimation and image layer separation simultaneously. They utilize a generalized double-layer brightness consistency constraint that connects these two tasks and incorporates priors for both. The effectiveness of the proposed approach is validated through experiments on synthetic data and real images. This is the first attempt to handle generic optical flow fields of two-frame images that contain transparency or reflection, to the best of the authors' knowledge.",1
"Optical flow estimation is a widely known problem in computer vision introduced by Gibson, J.J(1950) to describe the visual perception of human by stimulus objects. Estimation of optical flow model can be achieved by solving for the motion vectors from region of interest in the the different timeline. In this paper, we assumed slightly uniform change of velocity between two nearby frames, and solve the optical flow problem by traditional method, Lucas-Kanade(1981). This method performs minimization of errors between template and target frame warped back onto the template. Solving minimization steps requires optimization methods which have diverse convergence rate and error. We explored first and second order optimization methods, and compare their results with Gauss-Newton method in Lucas-Kanade. We generated 105 videos with 10,500 frames by synthetic objects, and 10 videos with 1,000 frames from real world footage. Our experimental results could be used as tuning parameters for Lucas-Kanade method.",0
"The issue of optical flow estimation is a well-known problem within computer vision that was first introduced by Gibson, J.J(1950) to explain how humans perceive visual stimuli. The estimation of an optical flow model can be achieved by determining the motion vectors from a region of interest in various timelines. This paper assumes a slightly uniform velocity change between two nearby frames and utilizes the traditional Lucas-Kanade(1981) method to solve the optical flow problem. This method minimizes errors between the target frame and the template warped back onto the template. To solve the minimization steps, optimization methods are required, which have different convergence rates and errors. This study explores first and second order optimization methods and compares their results with the Gauss-Newton method in Lucas-Kanade. 105 videos with 10,500 frames of synthetic objects and 10 videos with 1,000 frames of real-world footage were generated for experimentation. The results provide tuning parameters for the Lucas-Kanade method.",1
"The deep two-stream architecture exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.",0
"The two-stream architecture achieved exceptional results in recognizing action from video, but the computation of optical flow was the most time-consuming step and prevented the approach from being real-time. To improve the architecture's speed, this study replaced optical flow with motion vector, which can be obtained directly from compressed videos without additional computation. However, motion vector lacks fine structures and contains noisy and inaccurate motion patterns, leading to a reduction in recognition performance. The researchers discovered that optical flow and motion vector are inherently correlated, and transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly improve the latter's performance. They introduced three strategies for this: initialization transfer, supervision transfer, and a combination of the two. Experimental results demonstrated that their method achieved comparable recognition performance to the state-of-the-art, while processing 390.7 frames per second, which is 27 times faster than the original two-stream method.",1
"Motion blur can adversely affect a number of vision tasks, hence it is generally considered a nuisance. We instead treat motion blur as a useful signal that allows to compute the motion of objects from a single image. Drawing on the success of joint segmentation and parametric motion models in the context of optical flow estimation, we propose a parametric object motion model combined with a segmentation mask to exploit localized, non-uniform motion blur. Our parametric image formation model is differentiable w.r.t. the motion parameters, which enables us to generalize marginal-likelihood techniques from uniform blind deblurring to localized, non-uniform blur. A two-stage pipeline, first in derivative space and then in image space, allows to estimate both parametric object motion as well as a motion segmentation from a single image alone. Our experiments demonstrate its ability to cope with very challenging cases of object motion blur.",0
"Although motion blur is typically seen as a hindrance due to its negative impact on visual tasks, we propose a new approach that considers it a valuable indicator for determining object motion in a single image. By incorporating a segmentation mask and a parametric object motion model, we can effectively utilize localized, non-uniform motion blur. Our parametric image formation model can be differentiated with respect to motion parameters, allowing us to apply marginal-likelihood techniques for non-uniform blur as well as uniform blind deblurring. Our two-stage pipeline, utilizing both derivative and image space, can estimate both parametric object motion and motion segmentation from a single image. In experiments, our method successfully handles difficult cases of object motion blur.",1
We present a global optimization approach to optical flow estimation. The approach optimizes a classical optical flow objective over the full space of mappings between discrete grids. No descriptor matching is used. The highly regular structure of the space of mappings enables optimizations that reduce the computational complexity of the algorithm's inner loop from quadratic to linear and support efficient matching of tens of thousands of nodes to tens of thousands of displacements. We show that one-shot global optimization of a classical Horn-Schunck-type objective over regular grids at a single resolution is sufficient to initialize continuous interpolation and achieve state-of-the-art performance on challenging modern benchmarks.,0
"Our study introduces a global optimization technique for estimating optical flow on a worldwide scale. The technique concentrates on optimizing a classical optical flow goal across the entire range of mappings between discrete grids, without employing descriptor matching. The space of mappings' highly consistent structure permits optimizations that reduce the inner loop's computational complexity of the algorithm from quadratic to linear. This approach facilitates the efficient matching of tens of thousands of nodes to tens of thousands of displacements. Our findings demonstrate that a one-shot global optimization of a classical Horn-Schunck-style objective over regular grids at a single resolution is adequate to initialize continuous interpolation and attain cutting-edge performance on challenging contemporary benchmarks.",1
"Existing optical flow methods make generic, spatially homogeneous, assumptions about the spatial structure of the flow. In reality, optical flow varies across an image depending on object class. Simply put, different objects move differently. Here we exploit recent advances in static semantic scene segmentation to segment the image into objects of different types. We define different models of image motion in these regions depending on the type of object. For example, we model the motion on roads with homographies, vegetation with spatially smooth flow, and independently moving objects like cars and planes with affine motion plus deviations. We then pose the flow estimation problem using a novel formulation of localized layers, which addresses limitations of traditional layered models for dealing with complex scene motion. Our semantic flow method achieves the lowest error of any published monocular method in the KITTI-2015 flow benchmark and produces qualitatively better flow and segmentation than recent top methods on a wide range of natural videos.",0
"Optical flow methods currently in use assume a uniform flow structure throughout an image, which is not accurate since the flow varies depending on the object class. Different objects move differently. To address this issue, we utilize advancements in static semantic scene segmentation to segment objects of different types in an image. We create different models of image motion for each region based on the type of object, such as using homographies for roads, spatially smooth flow for vegetation, and affine motion plus deviations for independently moving objects like cars and planes. To overcome the limitations of traditional layered models when dealing with complex scene motion, we introduce a new formulation of localized layers to pose the flow estimation problem. Our semantic flow method outperforms other published monocular methods in the KITTI-2015 flow benchmark and produces superior flow and segmentation results compared to recent top methods in various natural videos.",1
"Optical flow is typically estimated by minimizing a ""data cost"" and an optional regularizer. While there has been much work on different regularizers many modern algorithms still use a data cost that is not very different from the ones used over 30 years ago: a robust version of brightness constancy or gradient constancy. In this paper we leverage the recent availability of ground-truth optical flow databases in order to learn a data cost. Specifically we take a generative approach in which the data cost models the distribution of noise after warping an image according to the flow and we measure the ""goodness"" of a data cost by how well it matches the true distribution of flow warp error. Consistent with current practice, we find that robust versions of gradient constancy are better models than simple brightness constancy but a learned GMM that models the density of patches of warp error gives a much better fit than any existing assumption of constancy. This significant advantage of the GMM is due to an explicit modeling of the spatial structure of warp errors, a feature which is missing from almost all existing data costs in optical flow. Finally, we show how a good density model of warp error patches can be used for optical flow estimation on whole images. We replace the data cost by the expected patch log-likelihood (EPLL), and show how this cost can be optimized iteratively using an additional step of denoising the warp error image. The results of our experiments are promising and show that patch models with higher likelihood lead to better optical flow estimation.",0
"The common method to estimate optical flow involves minimizing a data cost and an optional regularizer. Despite various regularizers being developed, many modern algorithms still rely on a robust version of brightness or gradient constancy as the data cost, similar to those used decades ago. This paper takes advantage of recent ground-truth optical flow databases to learn a data cost using a generative approach. The data cost models the noise distribution after warping an image and is evaluated based on how well it matches the true distribution of flow warp error. While robust gradient constancy is a better model than simple brightness constancy, a learned Gaussian mixture model (GMM) that considers the density of patches of warp error provides a much better fit than any existing constancy assumption. The GMM's advantage lies in its explicit modeling of the spatial structure of warp errors, a feature missing from most data costs in optical flow. The paper demonstrates how a good density model of warp error patches can be used for optical flow estimation on whole images by replacing the data cost with the expected patch log-likelihood (EPLL). The EPLL cost can be optimized iteratively by denoising the warp error image. The experiments suggest that patch models with higher likelihood lead to better optical flow estimation.",1
"We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks.",0
"Our proposal involves a novel optical flow computation pipeline that utilizes Deep Learning methods. Our approach involves using a Siamese CNN to compute image descriptors independently and concurrently. The descriptors acquired are subsequently compared efficiently using the L2 norm, eliminating the need for network processing of patch pairs. Our method's triumph is attributed to a pioneering loss function that calculates higher moments of the loss distributions for each training batch. When paired with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, our approach delivers top-notch performance on the most demanding and competitive optical flow benchmarks.",1
"The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level.",0
"Despite the challenges posed by multiple objects, complex background geometry, and observer motion, humans possess an exceptional ability to detect and segment moving objects with great speed and accuracy, even when objects are camouflaged. However, current advances in motion segmentation still fall short of human capabilities. In this study, we present a novel likelihood function based on the angle and magnitude of optical flow, which maximizes information about object motions. We also introduce innovative initialization techniques to develop a superior motion segmentation algorithm that outperforms existing methods by a considerable margin. To evaluate our method, we compare it to five state-of-the-art techniques using two established benchmarks and a new data set of camouflaged animals to push the boundaries of motion segmentation research.",1
"Non-rigid video interpolation is a common computer vision task. In this paper we present an optical flow approach which adopts a Laplacian Cotangent Mesh constraint to enhance the local smoothness. Similar to Li et al., our approach adopts a mesh to the image with a resolution up to one vertex per pixel and uses angle constraints to ensure sensible local deformations between image pairs. The Laplacian Mesh constraints are expressed wholly inside the optical flow optimization, and can be applied in a straightforward manner to a wide range of image tracking and registration problems. We evaluate our approach by testing on several benchmark datasets, including the Middlebury and Garg et al. datasets. In addition, we show application of our method for constructing 3D Morphable Facial Models from dynamic 3D data.",0
"The task of non-rigid video interpolation is frequently tackled in computer vision. This study introduces an optical flow technique that incorporates a Laplacian Cotangent Mesh constraint to improve local smoothness. Our approach follows a similar path to Li et al. and uses a mesh to map the image with a resolution of one vertex per pixel. Angle constraints are utilized to ensure reasonable local deformations between image pairs. The Laplacian Mesh constraints are integrated entirely within the optical flow optimization and can be easily applied to a variety of image tracking and registration problems. To assess our method, we conduct tests on various benchmark datasets, such as the Middlebury and Garg et al. datasets. Moreover, we demonstrate how our approach can be utilized for developing 3D Morphable Facial Models from dynamic 3D data.",1
"The proposed method uses live image footage which, based on calculations of pixel motion, decides whether or not an object is in the blind-spot. If found, the driver is notified by a sensory light or noise built into the vehicle's CPU. The new technology incorporates optical vectors and flow fields rather than expensive radar-waves, creating cheaper detection systems that retain the needed accuracy while adapting to the current processor speeds.",0
"The method being suggested utilizes live footage to determine whether an object is located in the blind-spot by calculating pixel motion. Upon detection, the vehicle's CPU alerts the driver through either a sensory light or noise. Unlike costly radar-waves, this novel technology incorporates optical vectors and flow fields, resulting in less expensive detection systems that maintain the required precision and can adapt to current processor speeds.",1
This technical report describes an improved image mosaicking algorithm. It is based on Jain's logarithmic search algorithm [Jain 1981] which is coupled to the method of Kourogi (1999} for matching images in a video sequence. Logarithmic search has a better invariance against illumination changes than the original optical-flow-based method of Kourogi.,0
"An enhanced image mosaicking algorithm is presented in this technical report, which utilizes Jain's logarithmic search algorithm [Jain 1981] in conjunction with Kourogi's (1999) approach for matching images in a video sequence. The logarithmic search algorithm offers superior invariance against illumination changes compared to Kourogi's original optical-flow-based method.",1
"State-of-the-art video deblurring methods cannot handle blurry videos recorded in dynamic scenes, since they are built under a strong assumption that the captured scenes are static. Contrary to the existing methods, we propose a video deblurring algorithm that can deal with general blurs inherent in dynamic scenes. To handle general and locally varying blurs caused by various sources, such as moving objects, camera shake, depth variation, and defocus, we estimate pixel-wise non-uniform blur kernels. We infer bidirectional optical flows to handle motion blurs, and also estimate Gaussian blur maps to remove optical blur from defocus in our new blur model. Therefore, we propose a single energy model that jointly estimates optical flows, defocus blur maps and latent frames. We also provide a framework and efficient solvers to minimize the proposed energy model. By optimizing the energy model, we achieve significant improvements in removing general blurs, estimating optical flows, and extending depth-of-field in blurry frames. Moreover, in this work, to evaluate the performance of non-uniform deblurring methods objectively, we have constructed a new realistic dataset with ground truths. In addition, extensive experimental on publicly available challenging video data demonstrate that the proposed method produces qualitatively superior performance than the state-of-the-art methods which often fail in either deblurring or optical flow estimation.",0
"Current video deblurring techniques are limited in their ability to handle blurry videos captured in dynamic scenes due to their reliance on a static scene assumption. In contrast to these approaches, we propose a novel video deblurring algorithm that can effectively address general blurs present in dynamic scenes. Our approach involves the estimation of pixel-wise non-uniform blur kernels to handle general and locally varying blurs caused by moving objects, camera shake, depth variation, and defocus. Bidirectional optical flows are inferred to handle motion blurs, while Gaussian blur maps are estimated to remove optical blur from defocus in our new blur model. Our approach utilizes a single energy model that jointly estimates optical flows, defocus blur maps, and latent frames, and we provide a framework and efficient solvers to minimize the proposed energy model. By optimizing the energy model, our method achieves significant improvements in removing general blurs, estimating optical flows, and extending depth-of-field in blurry frames. Additionally, we have created a new realistic dataset with ground truths to evaluate the performance of non-uniform deblurring methods objectively. Extensive experiments on publicly available challenging video data demonstrate that our proposed method outperforms state-of-the-art methods that often fail in either deblurring or optical flow estimation.",1
"Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical.   We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: 1) inverse search for patch correspondences; 2) dense displacement field creation through patch aggregation along multiple scales; 3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews in 2001.   DIS is competitive on standard optical flow benchmarks with large displacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching the temporal resolution of human's biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for visual applications.",0
"Optical flow extraction research has traditionally prioritized accuracy over time complexity. However, for real-life visual applications like tracking, activity detection, and recognition, time complexity is a critical factor. Our proposed solution achieves both low time complexity and competitive accuracy for the computation of dense optical flow. The method involves three parts: 1) inverse search for patch correspondences; 2) dense displacement field creation through patch aggregation across multiple scales; and 3) variational refinement. Our method, called Dense Inverse Search-based (DIS), efficiently searches for correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews in 2001. DIS is competitive with standard optical flow benchmarks with large displacements and runs at a speed of 300-600Hz on a single CPU core. This makes DIS orders of magnitude faster than state-of-the-art methods that achieve the same level of accuracy, making it an ideal choice for visual applications. Additionally, DIS reaches the temporal resolution of the human biological vision system.",1
"It is hard to estimate optical flow given a realworld video sequence with camera shake and other motion blur. In this paper, we first investigate the blur parameterization for video footage using near linear motion elements. we then combine a commercial 3D pose sensor with an RGB camera, in order to film video footage of interest together with the camera motion. We illustrates that this additional camera motion/trajectory channel can be embedded into a hybrid framework by interleaving an iterative blind deconvolution and warping based optical flow scheme. Our method yields improved accuracy within three other state-of-the-art baselines given our proposed ground truth blurry sequences; and several other realworld sequences filmed by our imaging system.",0
"Estimating optical flow in a real-world video sequence with camera shake and motion blur is a challenging task. This paper explores the use of near linear motion elements to parameterize blur in video footage. To capture the camera motion, we combined a commercial 3D pose sensor with an RGB camera. We demonstrate that incorporating this additional camera motion/trajectory channel into a hybrid framework through an iterative blind deconvolution and warping-based optical flow scheme can enhance accuracy. Our proposed method outperforms three other state-of-the-art baselines on our ground truth blurry sequences and various real-world sequences captured by our imaging system.",1
"It is hard to densely track a nonrigid object in long term, which is a fundamental research issue in the computer vision community. This task often relies on estimating pairwise correspondences between images over time where the error is accumulated and leads to a drift issue. In this paper, we introduce a novel optimization framework with an Anchor Patch constraint. It is supposed to significantly reduce overall errors given long sequences containing non-rigidly deformable objects. Our framework can be applied to any dense tracking algorithm, e.g. optical flow. We demonstrate the success of our approach by showing significant error reduction on 6 popular optical flow algorithms applied to a range of real-world nonrigid benchmarks. We also provide quantitative analysis of our approach given synthetic occlusions and image noise.",0
"Tracking nonrigid objects over a long period of time presents a difficult challenge for the computer vision community. One of the key issues is the accumulation of errors that arises from estimating pairwise correspondences between images. This leads to a drift problem that can be particularly problematic for non-rigid objects. To address this, we propose a new optimization framework that uses an Anchor Patch constraint to reduce errors over long sequences. Our framework can be used with any dense tracking algorithm, such as optical flow. We demonstrate the effectiveness of our approach by applying it to six popular optical flow algorithms on a variety of real-world nonrigid benchmarks. We also evaluate our framework using synthetic occlusions and image noise. Our results show that our approach significantly reduces errors in tracking nonrigid objects over long periods of time.",1
"Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",0
"The process of learning how to forecast future images from a video sequence entails creating an internal representation that accurately models image evolution, thus providing insight into its content and dynamics. The prediction of future frames in video sequences holds promise for unsupervised feature learning, as it eliminates the need for complex pixel tracking. Although optical flow has been studied extensively in computer vision, the prediction of future frames has received less attention. However, many vision applications could benefit from knowledge of upcoming frames. In this study, we use a convolutional network to generate future frames based on an input sequence. To address the inherently blurry predictions resulting from the Mean Squared Error (MSE) loss function, we propose three complementary feature learning strategies: a multi-scale architecture, adversarial training, and an image gradient difference loss function. We compare our predictions to published results based on recurrent neural networks in the UCF101 dataset.",1
"This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.",0
"The focus of this paper is on identifying cohesive movements within crowds and exploring their applications in crowd scene comprehension, specifically semantic region detection and recurrent activity mining. By analyzing input motion fields, such as optical flow fields, the paper proposes a novel approach that generates a thermal energy field, which can capture both the motion correlation among particles and the individual particle's motion trends. A two-step clustering process is then employed to identify stable semantic regions from the resulting time-varying coherent motions, which can aid in recognizing pre-defined activities in crowd scenes. Additionally, a cluster-and-merge process is introduced that automatically identifies recurrent activities in crowd scenes by clustering and merging extracted coherent motions. Results from experiments conducted on various videos demonstrate the effectiveness of this approach.",1
"We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.",0
"In challenging real-world scenarios where the observer's rotation and translation are unknown and only two views with a small baseline are available, accurately estimating camera egomotion is a complex task due to the nonconvex cost function of the perspective camera motion equation and the non-Gaussian noise generated by the optical flow estimates and scene non-rigidity. To tackle this problem, we present the expected residual likelihood method (ERL) that determines the confidence weights for noisy optical flow data by analyzing the likelihood distributions of the flow field residuals under various counterfactual model parameters. ERL effectively identifies outliers and retrieves the appropriate confidence weights in many settings. We compare ERL with a recent optimization framework that uses a lifted kernel to formulate the perspective camera motion equation and estimate joint parameter and confidence weights. To prevent local minima, we integrate these methods into a motion estimation pipeline. Our experiments on the challenging KITTI dataset demonstrate that ERL outperforms the lifted kernel method and other baseline monocular egomotion estimation techniques with minimal runtime overhead.",1
"Object segmentation in infant's egocentric videos is a fundamental step in studying how children perceive objects in early stages of development. From the computer vision perspective, object segmentation in such videos pose quite a few challenges because the child's view is unfocused, often with large head movements, effecting in sudden changes in the child's point of view which leads to frequent change in object properties such as size, shape and illumination. In this paper, we develop a semi-automated, domain specific, method to address these concerns and facilitate the object annotation process for cognitive scientists allowing them to select and monitor the object under segmentation. The method starts with an annotation from the user of the desired object and employs graph cut segmentation and optical flow computation to predict the object mask for subsequent video frames automatically. To maintain accuracy, we use domain specific heuristic rules to re-initialize the program with new user input whenever object properties change dramatically. The evaluations demonstrate the high speed and accuracy of the presented method for object segmentation in voluminous egocentric videos. We apply the proposed method to investigate potential patterns in object distribution in child's view at progressive ages.",0
"Studying how infants perceive objects in early stages of development requires fundamental object segmentation in egocentric videos. However, this poses challenges from a computer vision perspective due to the child's unfocused view and frequent changes in object properties such as size, shape, and illumination caused by large head movements and sudden changes in the child's point of view. To address these concerns, we developed a semi-automated, domain-specific method to facilitate object annotation for cognitive scientists. Our method involves user annotation of the desired object, graph cut segmentation, and optical flow computation to predict the object mask for subsequent video frames automatically. We also use domain-specific heuristic rules to re-initialize the program with new user input whenever object properties change dramatically to maintain accuracy. Our evaluations demonstrate the high speed and accuracy of our method for object segmentation in voluminous egocentric videos, which we apply to investigate potential patterns in object distribution in a child's view at progressive ages.",1
"This paper shows how to extract dense optical flow from videos with a convolutional neural network (CNN). The proposed model constitutes a potential building block for deeper architectures to allow using motion without resorting to an external algorithm, \eg for recognition in videos. We derive our network architecture from signal processing principles to provide desired invariances to image contrast, phase and texture. We constrain weights within the network to enforce strict rotation invariance and substantially reduce the number of parameters to learn. We demonstrate end-to-end training on only 8 sequences of the Middlebury dataset, orders of magnitude less than competing CNN-based motion estimation methods, and obtain comparable performance to classical methods on the Middlebury benchmark. Importantly, our method outputs a distributed representation of motion that allows representing multiple, transparent motions, and dynamic textures. Our contributions on network design and rotation invariance offer insights nonspecific to motion estimation.",0
"In this paper, a convolutional neural network (CNN) is presented as a means to extract dense optical flow from videos. The model can be used as a building block for more complex architectures that don't require external algorithms for video recognition. The network architecture is derived from signal processing principles to provide desired invariances to image contrast, phase, and texture. The weights within the network are constrained to enforce strict rotation invariance, which substantially reduces the number of parameters to learn. The end-to-end training is demonstrated on only 8 sequences of the Middlebury dataset, which is much less than other CNN-based motion estimation methods. The performance of the proposed method is comparable to classical methods on the Middlebury benchmark. The method outputs a distributed representation of motion that allows representing multiple, transparent motions, and dynamic textures. The contributions on network design and rotation invariance offer insights that are not specific to motion estimation.",1
"Dense image matching is a fundamental low-level problem in Computer Vision, which has received tremendous attention from both discrete and continuous optimization communities. The goal of this paper is to combine the advantages of discrete and continuous optimization in a coherent framework. We devise a model based on energy minimization, to be optimized by both discrete and continuous algorithms in a consistent way. In the discrete setting, we propose a novel optimization algorithm that can be massively parallelized. In the continuous setting we tackle the problem of non-convex regularizers by a formulation based on differences of convex functions. The resulting hybrid discrete-continuous algorithm can be efficiently accelerated by modern GPUs and we demonstrate its real-time performance for the applications of dense stereo matching and optical flow.",0
"The task of matching dense images is a crucial issue in Computer Vision, and it has captured the attention of both the discrete and continuous optimization communities. The purpose of this article is to merge the benefits of both discrete and continuous optimization within a unified structure. We introduce an energy minimization model that can be optimized consistently by both discrete and continuous algorithms. In the discrete realm, we present an innovative optimization algorithm that can be parallelized massively. In the continuous context, we address the challenge of non-convex regularizers by formulating it according to differences of convex functions. The outcome is a hybrid algorithm that incorporates both discrete and continuous methods and can be efficiently accelerated through modern GPUs. We showcase its real-time performance by applying it to the domains of dense stereo matching and optical flow.",1
"Traditional methods for motion estimation estimate the motion field F between a pair of images as the one that minimizes a predesigned cost function. In this paper, we propose a direct method and train a Convolutional Neural Network (CNN) that when, at test time, is given a pair of images as input it produces a dense motion field F at its output layer. In the absence of large datasets with ground truth motion that would allow classical supervised training, we propose to train the network in an unsupervised manner. The proposed cost function that is optimized during training, is based on the classical optical flow constraint. The latter is differentiable with respect to the motion field and, therefore, allows backpropagation of the error to previous layers of the network. Our method is tested on both synthetic and real image sequences and performs similarly to the state-of-the-art methods.",0
"Conventional techniques for motion estimation typically determine the motion field F between two images by minimizing a predetermined cost function. This study introduces a novel approach wherein a Convolutional Neural Network (CNN) is trained to generate a dense motion field F as output when presented with a pair of images at test time. Due to the absence of extensive datasets with verified motion, we opt for unsupervised training using a cost function based on the optical flow constraint, which is differentiable and facilitates backpropagation of error to earlier network layers. Our method is evaluated on both synthetic and real image sequences and is comparable to existing cutting-edge methods.",1
"An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.",0
"Deep convolutional neural networks are becoming increasingly popular for tackling computer vision and image/video processing challenges, achieving impressive results in areas such as object recognition and detection, semantic segmentation, action recognition, optical flow, and superresolution. However, to implement these advances in embedded and mobile computer vision systems, hardware acceleration is crucial. Our team has developed a new architecture, design, and implementation for an accelerator that outperforms previous work in terms of power, area, and I/O efficiency. We have also conducted the first silicon measurements of this device, which can provide up to 196 GOp/s on a small 3.09 mm^2 of silicon in UMC 65nm technology. Additionally, our device boasts a power efficiency of 803 GOp/s/W and requires significantly less bandwidth, making it the first scalable architecture capable of achieving TOp/s performance.",1
"Sparse representation-based classifiers have shown outstanding accuracy and robustness in image classification tasks even with the presence of intense noise and occlusion. However, it has been discovered that the performance degrades significantly either when test image is not aligned with the dictionary atoms or the dictionary atoms themselves are not aligned with each other, in which cases the sparse linear representation assumption fails. In this paper, having both training and test images misaligned, we introduce a novel sparse coding framework that is able to efficiently adapt the dictionary atoms to the test image via large displacement optical flow. In the proposed algorithm, every dictionary atom is automatically aligned with the input image and the sparse code is then recovered using the adapted dictionary atoms. A corresponding supervised dictionary learning algorithm is also developed for the proposed framework. Experimental results on digit datasets recognition verify the efficacy and robustness of the proposed algorithm.",0
"Sparse representation-based classifiers have proven to be highly accurate and resilient in image classification tasks, even in the presence of strong noise and occlusion. However, their performance deteriorates significantly if the test image is not aligned with the dictionary atoms or if the atoms themselves are misaligned. This leads to a failure of the assumption of sparse linear representation. This paper introduces a novel sparse coding framework that can adapt the dictionary atoms to the test image using large displacement optical flow, even when both training and test images are misaligned. The proposed algorithm aligns every dictionary atom with the input image, and the sparse code is then obtained using the adapted dictionary atoms. A corresponding supervised dictionary learning algorithm is also developed. Experimental results on digit datasets recognition demonstrate the effectiveness and robustness of the proposed algorithm.",1
"Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.",0
"This work aims to predict the motion of a scene and the direction it will move in, using a non-semantic form of action prediction. To achieve this, we have developed a convolutional neural network (CNN) based approach that predicts the future motion of every pixel in an image in terms of optical flow. Our model is trained on tens of thousands of realistic videos and does not require any human labeling, relying solely on the scene's context. Unlike previous approaches, our CNN model makes no assumptions about the scene and can accurately predict future optical flow in various scenarios, outperforming all previous methods by a significant margin.",1
"Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.",0
"Supervised learning has been employed to successfully solve optical flow estimation using convolutional networks. This was made possible through training the FlowNet using a large synthetically generated dataset. In this paper, we expand the application of convolutional networks to include disparity and scene flow estimation. To achieve this, we introduce three synthetic stereo video datasets that offer sufficient realism, variation, and size to train large networks. These datasets enable the first large-scale training and evaluation of scene flow methods. In addition, we present a convolutional network that achieves state-of-the-art disparity estimation in real-time. By combining the flow and disparity estimation networks and training them jointly, we demonstrate the first scene flow estimation using a convolutional network.",1
"In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at https://github.com/s-gupta/fast-rcnn/tree/distillation",0
"The proposed technique in our study involves transferring supervision from images of one modality to another. Our approach utilizes representations learned from a well-labeled modality to train representations for an unlabeled, corresponding modality. This method allows for the creation of comprehensive representations for unmarked modalities, making it ideal for pre-training new modalities with limited labeled data. Our experimental results indicate significant enhancements for cross modal supervision transfers from labeled RGB images to unlabeled depth and optical flow images. Code, data, and pre-trained models can be accessed at https://github.com/s-gupta/fast-rcnn/tree/distillation.",1
"While egocentric video is becoming increasingly popular, browsing it is very difficult. In this paper we present a compact 3D Convolutional Neural Network (CNN) architecture for long-term activity recognition in egocentric videos. Recognizing long-term activities enables us to temporally segment (index) long and unstructured egocentric videos. Existing methods for this task are based on hand tuned features derived from visible objects, location of hands, as well as optical flow.   Given a sparse optical flow volume as input, our CNN classifies the camera wearer's activity. We obtain classification accuracy of 89%, which outperforms the current state-of-the-art by 19%. Additional evaluation is performed on an extended egocentric video dataset, classifying twice the amount of categories than current state-of-the-art. Furthermore, our CNN is able to recognize whether a video is egocentric or not with 99.2% accuracy, up by 24% from current state-of-the-art. To better understand what the network actually learns, we propose a novel visualization of CNN kernels as flow fields.",0
"The popularity of egocentric video is increasing, but it is difficult to browse. This paper presents a compact 3D Convolutional Neural Network architecture for recognizing long-term activities in egocentric videos. This allows for the temporal segmentation of long and unstructured videos. Current methods rely on hand-tuned features such as visible objects, hand location, and optical flow. Our CNN uses a sparse optical flow volume as input and achieves an 89% classification accuracy, outperforming the current state-of-the-art by 19%. We also classify twice the amount of categories and can accurately recognize whether a video is egocentric or not with 99.2% accuracy, up by 24% from current state-of-the-art. Additionally, we propose a new method for visualizing the CNN kernels as flow fields to better understand what the network learns.",1
"Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive pre-processing or post-processing methods.   In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.",0
"In recent years, deep learning has become a popular method for video analysis, particularly in video classification and detection. However, it is widely believed that successful implementation of deep networks in these domains requires time-consuming architecture search, parameter tweaking, and pre/post-processing methods. This paper challenges these beliefs by presenting a deep 3D convolutional architecture that can perform voxel-level prediction without any preprocessing. The same architecture proved successful in three different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The resulting networks are efficient and do not require post-processing, making them a competitive alternative to more computationally expensive methods in video analysis.",1
"Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition.   We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks (CNNs) share the same convolution-pooling network structure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm. Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources.   Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm.",0
"The development of handcrafted local features and learning based features has led to significant advancements in image and video classification research. Although these two architectures were proposed around the same time and have thrived during similar historical periods, they are typically considered as separate approaches. In this study, we highlight their structural similarities and demonstrate how adopting a unified perspective can aid in designing features that strike a balance between efficiency and effectiveness. Specifically, we investigate the challenge of developing efficient video feature learning algorithms for action recognition. By first establishing that locally handcrafted features and Convolutional Neural Networks (CNNs) share a common convolution-pooling network structure, we propose a two-stream Convolutional ISA (ConvISA) that leverages the structure of the leading handcrafted video feature while offering greater modeling capabilities and a cost-effective training algorithm. Our method also accounts for the unique attributes of pixels and optical flow by using custom-designed network structures. Our experimental findings on recognized action benchmarks demonstrate that by focusing on CNN structure rather than end-to-end training methods, we can develop a potent and efficient video feature learning algorithm.",1
"The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow.   To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps; (ii) spatial fusion layers that learn an implicit spatial model; (iii) optical flow is used to align heatmap predictions from neighbouring frames; and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map.   We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion.   The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also Chen & Yuille and Tompson et al. in the high precision region).",0
"This study aims to estimate human poses in videos with the use of multiple frames. A ConvNet architecture is explored, which takes advantage of temporal context by utilizing optical flow to combine information across frames. The proposed network architecture introduces several innovations, such as a deeper network for regressing heatmaps, spatial fusion layers that learn an implicit spatial model, optical flow to align heatmap predictions, and a final parametric pooling layer to combine the aligned heatmaps into a pooled confidence map. The results show that this architecture outperforms other methods, including those that use optical flow solely at the input layers or regress joint coordinates directly. It also surpasses the state of the art by a significant margin on three video pose estimation datasets, including the challenging Poses in the Wild dataset. Additionally, it outperforms other deep methods that do not use a graphical model on the single-image FLIC benchmark, including Chen & Yuille and Tompson et al. in the high precision region.",1
"In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths",0
"Motion segmentation in camera videos is usually achieved through the use of optical flow, which detects the image plane motion of pixels. However, objects at different depths from the camera can display varying optical flows, leading to a depth-dependent segmentation of the scene. Our aim is to create a segmentation algorithm that clusters pixels based on their real-world motion rather than their depth in the scene. To achieve this, we utilize optical flow orientations instead of complete vectors, as they are independent of object depth under camera translation. Our probabilistic model estimates the number of observed independent motions, resulting in a labeling consistent with real-world motion in the scene. Our system correctly identifies static objects as one segment, even if they exist at varying depths. We utilize color features and information from previous frames to correct any errors resulting from orientation-based segmentation. We have tested our system on over thirty videos from different benchmarks, and it proves to be particularly robust on complex background scenes with objects at varying depths.",1
"We present a deeply integrated method of exploiting low-cost gyroscopes to improve general purpose feature tracking. Most previous methods use gyroscopes to initialize and bound the search for features. In contrast, we use them to regularize the tracking energy function so that they can directly assist in the tracking of ambiguous and poor-quality features. We demonstrate that our simple technique offers significant improvements in performance over conventional template-based tracking methods, and is in fact competitive with more complex and computationally expensive state-of-the-art trackers, but at a fraction of the computational cost. Additionally, we show that the practice of initializing template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using gyro-predicted optical flow offers no advantage over using a careful optical-only initialization method, suggesting that some deeper level of integration, like the method we propose, is needed in order to realize a genuine improvement in tracking performance from these inertial sensors.",0
"Our approach involves utilizing low-cost gyroscopes in a comprehensive manner to enhance feature tracking. Unlike previous methods that use gyroscopes to limit the search for features, we incorporate them into the tracking energy function to aid in the tracking of ambiguous and low-quality features. Our straightforward technique exhibits significant performance enhancements compared to traditional template-based tracking techniques and is on par with more complex and expensive state-of-the-art trackers, at a fraction of the computational cost. Furthermore, our research indicates that employing gyro-predicted optical flow to initialize template-based feature trackers, such as KLT, provides no advantage over a meticulous optical-only initialization approach. Therefore, our integrated method is necessary for achieving genuine improvements in tracking performance utilizing these inertial sensors.",1
"Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this paper we present a dense correspondence field approach that is much less outlier prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach is conceptually novel as it does not require explicit regularization, smoothing (like median filtering) or a new data term, but solely our novel purely data based search strategy that finds most inliers (even for small objects), while it effectively avoids finding outliers. Moreover, we present novel enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury.",0
"Current optical flow algorithms that use large displacement commonly rely on sparse descriptor matching or dense approximate nearest neighbor fields for initialization. While the latter provides denser results, it is prone to outliers as it aims to find visually similar correspondences instead of optical flow. In this study, we introduce a new approach that utilizes a dense correspondence field which is less susceptible to outliers and more suitable for optical flow estimation. Our method does not require explicit regularization, smoothing or a new data term, but rather a novel data-based search strategy that efficiently identifies inliers while avoiding outliers. Additionally, we present new enhancements for outlier filtering. Our results demonstrate that our approach outperforms current state-of-the-art descriptor matching techniques for large displacement optical flow, as validated by our initialization of EpicFlow on MPI-Sintel, KITTI and Middlebury datasets.",1
"We introduce a novel matching algorithm, called DeepMatching, to compute dense correspondences between images. DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. The proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images. We evaluate the performance of DeepMatching, in comparison with state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al 2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013) datasets. DeepMatching outperforms the state-of-the-art algorithms and shows excellent results in particular for repetitive textures.We also propose a method for estimating optical flow, called DeepFlow, by integrating DeepMatching in the large displacement optical flow (LDOF) approach of Brox and Malik (2011). Compared to existing matching algorithms, additional robustness to large displacements and complex motion is obtained thanks to our matching approach. DeepFlow obtains competitive performance on public benchmarks for optical flow estimation.",0
"Our paper presents a new algorithm called DeepMatching, which is designed to calculate dense correspondences between images. This algorithm uses a hierarchical, multi-layer correlational architecture inspired by deep convolutional methods and is effective in dealing with non-rigid deformations and repetitive textures. We conducted evaluations of DeepMatching's performance against other state-of-the-art matching algorithms using three different datasets: Mikolajczyk, MPI-Sintel, and Kitti. Our results show that DeepMatching outperforms these other algorithms, especially in cases of repetitive textures. Furthermore, we propose a method called DeepFlow for estimating optical flow by integrating DeepMatching with Brox and Malik's large displacement optical flow approach. Our matching algorithm improves the robustness of optical flow estimation for large displacements and complex motion. Overall, DeepFlow performs competitively on public benchmarks for optical flow estimation.",1
"We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.",0
"The Encoder-Recurrent-Decoder (ERD) model is suggested for identifying and projecting human body pose in motion capture and videos. This recurrent neural network integrates nonlinear encoder and decoder networks before and after the recurrent layers. The ERD model is evaluated in motion capture generation, body pose labeling, and body pose forecasting tasks. The model is capable of handling motion capture training data across multiple subjects and activity domains and generates novel motions without drifting for long periods. In human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. In video pose forecasting, ERD predicts body joint displacements over a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs broaden the scope of previous Long Short Term Memory (LSTM) models in the literature by learning representations and their dynamics simultaneously. The research shows that such representation learning plays a critical role in both labeling and prediction in space-time, which is a unique feature of the spatio-temporal visual domain compared to 1D text, speech or handwriting, where direct combinations with recurrent units have shown excellent results using straightforward hard coded representations.",1
"Computational neuroscience studies that have examined human visual system through functional magnetic resonance imaging (fMRI) have identified a model where the mammalian brain pursues two distinct pathways (for recognition of biological movement tasks). In the brain, dorsal stream analyzes the information of motion (optical flow), which is the fast features, and ventral stream (form pathway) analyzes form information (through active basis model based incremental slow feature analysis ) as slow features. The proposed approach suggests the motion perception of the human visual system composes of fast and slow feature interactions that identifies biological movements. Form features in the visual system biologically follows the application of active basis model with incremental slow feature analysis for the extraction of the slowest form features of human objects movements in the ventral stream. Applying incremental slow feature analysis provides an opportunity to use the action prototypes. To extract the slowest features episodic observation is required but the fast features updates the processing of motion information in every frames. Experimental results have shown promising accuracy for the proposed model and good performance with two datasets (KTH and Weizmann).",0
"Through the use of functional magnetic resonance imaging (fMRI), computational neuroscience studies have explored the human visual system and discovered a model that highlights two distinct pathways dedicated to recognizing biological movement tasks. The dorsal stream of the brain analyzes fast optical flow motion information, while the ventral stream (form pathway) examines form information through active basis model-based incremental slow feature analysis. This approach emphasizes the importance of fast and slow feature interactions in identifying biological movements. The ventral stream utilizes incremental slow feature analysis to extract the slowest form features of human object movements, allowing for the application of action prototypes. Episodic observation is necessary to extract slow features, while fast features update the motion information processing in every frame. The model has shown promising accuracy and excellent performance with the KTH and Weizmann datasets.",1
"A technique for the enhancement of point targets in clutter is described. The local 3-D spectrum at each pixel is estimated recursively. An optical flow-field for the textured background is then generated using the 3-D autocorrelation function and the local velocity estimates are used to apply high-pass velocity-selective spatiotemporal filters, with finite impulse responses (FIRs), to subtract the background clutter signal, leaving the foreground target signal, plus noise. Parallel software implementations using a multicore central processing unit (CPU) and a graphical processing unit (GPU) are investigated.",0
"The article explains a method to improve the detection of point targets in a noisy environment. By recursively estimating the 3-D spectrum at each pixel, an optical flow-field is created for the textured background. The local velocity estimates obtained from this process are then utilized to apply high-pass velocity-selective spatiotemporal filters with finite impulse responses (FIRs) to remove the background clutter signal. This leaves behind the foreground target signal, along with some noise. The study looks into implementing this technique on both multicore central processing units (CPUs) and graphical processing units (GPUs) in parallel.",1
"Two complementary approaches have been extensively used in signal and image processing leading to novel results, the sparse representation methodology and the variational strategy. Recently, a new sparsity based model has been proposed, the cosparse analysis framework, which may potentially help in bridging sparse approximation based methods to the traditional total-variation minimization. Based on this, we introduce a sparsity based framework for solving overparameterized variational problems. The latter has been used to improve the estimation of optical flow and also for general denoising of signals and images. However, the recovery of the space varying parameters involved was not adequately addressed by traditional variational methods. We first demonstrate the efficiency of the new framework for one dimensional signals in recovering a piecewise linear and polynomial function. Then, we illustrate how the new technique can be used for denoising and segmentation of images.",0
"Signal and image processing have extensively utilized two approaches, namely the sparse representation methodology and the variational strategy, leading to innovative outcomes. A new model, the cosparse analysis framework, has recently been proposed, which has the potential to connect sparse approximation-based techniques with traditional total-variation minimization. Consequently, we introduce a sparsity-based framework to solve overparameterized variational problems, which has been employed to enhance optical flow estimation and noise reduction of signals and images. Nonetheless, the recovery of the space-varying parameters in traditional variational methods was insufficiently addressed. Initially, we exhibit the effectiveness of the new framework in recovering a piecewise linear and polynomial function for one-dimensional signals. Subsequently, we demonstrate how the new approach can be implemented for image denoising and segmentation.",1
"We consider a variational method to solve the optical flow problem with varying illumination. We apply an adaptive control of the regularization parameter which allows us to preserve the edges and fine features of the computed flow. To reduce the complexity of the estimation for high resolution images and the time of computations, we implement a multi-level parallel approach based on the domain decomposition with the Schwarz overlapping method. The second level of parallelism uses the massively parallel solver MUMPS. We perform some numerical simulations to show the efficiency of our approach and to validate it on classical and real-world image sequences.",0
"Our approach to solving the optical flow problem with changing illumination involves utilizing a variational method. By employing an adaptive control of the regularization parameter, we are able to maintain the integrity of the flow's edges and finer details. To simplify the estimation process for high-resolution images and expedite computation time, we utilize a multi-level parallel approach that employs the Schwarz overlapping method for domain decomposition. We also incorporate the MUMPS massively parallel solver for an additional level of parallelism. To demonstrate the effectiveness of our approach and validate it with real-world image sequences, we conduct numerical simulations.",1
"Honey bees use optical flow to avoid obstacles effectively. In this research work similar methodology was tested on a simulated mobile robot. Simulation framework was based on VRML and Simulink in a 3D world. Optical flow vectors were calculated from a video scene captured by a virtual camera which was used as inputs to a fuzzy logic controller. Fuzzy logic controller decided the locomotion of the robot. Different fuzzy logic rules were evaluated. The robot was able to navigate through complex static and dynamic environments effectively, avoiding obstacles on its path.",0
"The utilization of optical flow by honey bees to proficiently steer clear of obstacles was explored in this study. The same approach was put to the test on a simulated mobile robot using a simulation framework created on VRML and Simulink, within a 3D realm. The optical flow vectors were deduced from a video scene snapped by a virtual camera, which served as inputs for a fuzzy logic controller that determined the robot's movement. Several fuzzy logic rules were assessed, and the robot successfully navigated through intricate static and dynamic environments, evading any obstructions in its path.",1
"Spatial multiplexing cameras (SMCs) acquire a (typically static) scene through a series of coded projections using a spatial light modulator (e.g., a digital micro-mirror device) and a few optical sensors. This approach finds use in imaging applications where full-frame sensors are either too expensive (e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC systems reconstruct static scenes using techniques from compressive sensing (CS). For videos, however, existing acquisition and recovery methods deliver poor quality. In this paper, we propose the CS multi-scale video (CS-MUVI) sensing and recovery framework for high-quality video acquisition and recovery using SMCs. Our framework features novel sensing matrices that enable the efficient computation of a low-resolution video preview, while enabling high-resolution video recovery using convex optimization. To further improve the quality of the reconstructed videos, we extract optical-flow estimates from the low-resolution previews and impose them as constraints in the recovery procedure. We demonstrate the efficacy of our CS-MUVI framework for a host of synthetic and real measured SMC video data, and we show that high-quality videos can be recovered at roughly $60\times$ compression.",0
"Spatial multiplexing cameras (SMCs) are utilized to capture a static scene by projecting coded images through a spatial light modulator and a few optical sensors. This approach is preferred in imaging applications where full-frame sensors are either expensive or unavailable, such as for short-wave infrared wavelengths. Although existing SMC systems utilize compressive sensing techniques to reconstruct static scenes, these methods deliver low-quality videos. In this study, we introduce the CS multi-scale video (CS-MUVI) framework that utilizes novel sensing matrices to compute a low-resolution video preview efficiently. The framework allows for high-resolution video recovery using convex optimization and optical-flow estimates as constraints. We demonstrate the effectiveness of our framework using synthetic and real SMC video data, and we show that high-quality videos can be recovered at roughly $60\times$ compression.",1
"In this work, we introduce a deep-structured conditional random field (DS-CRF) model for the purpose of state-based object silhouette tracking. The proposed DS-CRF model consists of a series of state layers, where each state layer spatially characterizes the object silhouette at a particular point in time. The interactions between adjacent state layers are established by inter-layer connectivity dynamically determined based on inter-frame optical flow. By incorporate both spatial and temporal context in a dynamic fashion within such a deep-structured probabilistic graphical model, the proposed DS-CRF model allows us to develop a framework that can accurately and efficiently track object silhouettes that can change greatly over time, as well as under different situations such as occlusion and multiple targets within the scene. Experiment results using video surveillance datasets containing different scenarios such as occlusion and multiple targets showed that the proposed DS-CRF approach provides strong object silhouette tracking performance when compared to baseline methods such as mean-shift tracking, as well as state-of-the-art methods such as context tracking and boosted particle filtering.",0
"This study presents a novel deep-structured conditional random field (DS-CRF) model that tracks state-based object silhouettes. The DS-CRF model comprises state layers that describe the object silhouette at specific points in time. Interactions between adjacent state layers are established based on inter-frame optical flow. By integrating spatial and temporal context dynamically within the model, the proposed DS-CRF enables precise and efficient tracking of object silhouettes that may change significantly over time or under different situations such as occlusion and multiple targets. The experimental results demonstrate that the DS-CRF approach outperforms baseline methods like mean-shift tracking and state-of-the-art methods such as context tracking and boosted particle filtering in video surveillance datasets with various scenarios like occlusion and multiple targets.",1
"Several state-of-the-art video deblurring methods are based on a strong assumption that the captured scenes are static. These methods fail to deblur blurry videos in dynamic scenes. We propose a video deblurring method to deal with general blurs inherent in dynamic scenes, contrary to other methods. To handle locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene, we approximate pixel-wise kernel with bidirectional optical flows. Therefore, we propose a single energy model that simultaneously estimates optical flows and latent frames to solve our deblurring problem. We also provide a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Extensive experimental results demonstrate the superiority of the proposed method in real and challenging videos that state-of-the-art methods fail in either deblurring or optical flow estimation.",0
"Numerous modern video deblurring techniques rely on the assumption that the captured scenes are stationary, leading to their inability to deblur videos in dynamic environments. Conversely, we present a novel video deblurring approach that can handle general blurs present in dynamic scenes. Our method tackles various blurs, including those caused by camera shake, moving objects, and depth variations, by approximating the pixel-wise kernel using bidirectional optical flows. We propose a single energy model that simultaneously estimates optical flows and latent frames to solve the deblurring issue, along with an efficient framework to optimize the energy model. By minimizing the energy function, we obtain considerable improvements in removing blurs and estimating accurate optical flows in blurry frames. Our experimental results demonstrate that our technique outperforms state-of-the-art methods in real-world and challenging video scenarios where they fail to either deblur or estimate optical flow.",1
"Crowd flow segmentation is an important step in many video surveillance tasks. In this work, we propose an algorithm for segmenting flows in H.264 compressed videos in a completely unsupervised manner. Our algorithm works on motion vectors which can be obtained by partially decoding the compressed video without extracting any additional features. Our approach is based on modelling the motion vector field as a Conditional Random Field (CRF) and obtaining oriented motion segments by finding the optimal labelling which minimises the global energy of CRF. These oriented motion segments are recursively merged based on gradient across their boundaries to obtain the final flow segments. This work in compressed domain can be easily extended to pixel domain by substituting motion vectors with motion based features like optical flow. The proposed algorithm is experimentally evaluated on a standard crowd flow dataset and its superior performance in both accuracy and computational time are demonstrated through quantitative results.",0
"The segmentation of crowd flow is a crucial aspect of various video surveillance tasks. Our study presents an unsupervised algorithm for segmenting flows in H.264 compressed videos. The algorithm relies on motion vectors, which can be retrieved by partially decoding the compressed video, without the need for additional features. We adopt a Conditional Random Field (CRF) approach to model the motion vector field and obtain oriented motion segments by selecting the optimal labelling, which minimizes the global energy of the CRF. The oriented motion segments are merged recursively based on the gradient across their boundaries, resulting in the final flow segments. This compressed domain work can be extended to the pixel domain by replacing motion vectors with optical flow-based features. Our algorithm's effectiveness is evaluated on a standard crowd flow dataset, and we demonstrate its superior performance in accuracy and computational time through quantitative results.",1
"We propose a novel approach for optical flow estimation , targeted at large displacements with significant oc-clusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries -- two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.",0
"Our proposed method for estimating optical flow is innovative and addresses the challenge of significant occlusions and large displacements. The approach involves two steps: first, we use edge-preserving interpolation to achieve dense matching from a sparse set of matches. Second, we initiate variational energy minimization using the dense matches. The interpolation method relies on an edge-aware geodesic distance which is designed to handle common and difficult issues such as occlusions and motion boundaries. We also suggest an approximation scheme for the geodesic distance to enable fast computation without sacrificing performance. After the dense interpolation, we carry out standard one-level variational energy minimization to obtain the final flow estimation. Our approach, named Edge-Preserving Interpolation of Correspondences (EpicFlow), is both fast and robust to large displacements. It significantly outperforms existing methods on MPI-Sintel and performs comparably on Kitti and Middlebury datasets.",1
"In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.",0
"The aim of this paper is to introduce a novel feature representation for first-person videos that can effectively capture both the overall scene dynamics (egomotion) as well as the significant local motion observed in these videos, which is crucial for activity recognition. The proposed representation framework is based on time series pooling and is designed to abstract short-term/long-term changes in feature descriptor elements. This involves monitoring how descriptor values change over time and summarizing them to represent motion in the activity video. The framework can handle any per-frame feature descriptors, including conventional motion descriptors such as histogram of optical flows (HOF) and appearance descriptors from convolutional neural networks (CNN). Experimental results show that the proposed approach outperforms previous feature representations like bag-of-visual-words and improved Fisher vector (IFV) using the same underlying feature descriptors. Furthermore, the proposed feature representation outperforms existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features developed for 3rd-person videos when it comes to handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.",1
"Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations.   Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",0
"Recently, convolutional neural networks (CNNs) have demonstrated great success in various computer vision assignments, particularly in recognition-related tasks. However, CNNs have not been as effective in optical flow estimation. This research aims to create appropriate CNNs that can tackle the optical flow estimation task through supervised learning. Two architectures are proposed and compared: a generic architecture and a second one that includes a layer that correlates feature vectors at different image locations. Due to an insufficient amount of existing ground truth data sets to train a CNN, a synthetic Flying Chairs dataset is generated. Results show that networks trained on this artificial dataset can still generalize well to current datasets like Sintel and KITTI, achieving competitive accuracy at frame rates ranging from 5 to 10 fps.",1
"In this work, we have developed a robust lane detection and departure warning technique. Our system is based on single camera sensor. For lane detection a modified Inverse Perspective Mapping using only a few extrinsic camera parameters and illuminant Invariant techniques is used. Lane markings are represented using a combination of 2nd and 4th order steerable filters, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space, and illuminant invariant representation. Lanes are assumed to be cubic curves and fitted using robust RANSAC. This method can reliably detect lanes of the road and its boundary. This method has been experimented in Indian road conditions under different challenging situations and the result obtained were very good. For lane departure angle an optical flow based method were used.",0
"Our research has produced a dependable system for detecting lanes and issuing departure warnings. Our approach utilizes a single camera sensor and employs a modified Inverse Perspective Mapping technique to detect lanes. We use a combination of 2nd and 4th order steerable filters to represent lane markings, which are robust to shadowing. Our system can remove the effects of shadowing and extra sunlight using the Lab color space and illuminant invariant representation. We assume that lanes are cubic curves and fit them using robust RANSAC. Our method has been tested in various challenging situations on Indian roads, and we achieved promising results. Additionally, we applied an optical flow-based approach to measure lane departure angle.",1
"This paper introduces a state-of-the-art video representation and applies it to efficient action recognition and detection. We first propose to improve the popular dense trajectory features by explicit camera motion estimation. More specifically, we extract feature point matches between frames using SURF descriptors and dense optical flow. The matches are used to estimate a homography with RANSAC. To improve the robustness of homography estimation, a human detector is employed to remove outlier matches from the human body as human motion is not constrained by the camera. Trajectories consistent with the homography are considered as due to camera motion, and thus removed. We also use the homography to cancel out camera motion from the optical flow. This results in significant improvement on motion-based HOF and MBH descriptors. We further explore the recent Fisher vector as an alternative feature encoding approach to the standard bag-of-words histogram, and consider different ways to include spatial layout information in these encodings. We present a large and varied set of evaluations, considering (i) classification of short basic actions on six datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that our improved trajectory features significantly outperform previous dense trajectories, and that Fisher vectors are superior to bag-of-words encodings for video recognition tasks. In all three tasks, we show substantial improvements over the state-of-the-art results.",0
"This article presents a modern approach to video representation, which is applied to efficient action recognition and detection. Initially, the paper proposes enhancing dense trajectory features using explicit camera motion estimation. This is achieved by extracting feature point matches between frames using SURF descriptors and dense optical flow, and estimating a homography with RANSAC. To enhance the robustness of homography estimation, a human detector is utilized to remove outlier matches from the human body, as human motion is not restricted by the camera. Trajectories that correspond to the homography are considered as camera motion and, therefore, eliminated. Furthermore, the homography is employed to eliminate camera motion from the optical flow, resulting in significant improvement on motion-based HOF and MBH descriptors. Additionally, the paper explores the Fisher vector as an alternative feature encoding approach to the usual bag-of-words histogram and considers different ways to include spatial layout information in these encodings. The article presents a comprehensive set of evaluations, including classification of short basic actions on six datasets, localization of such actions in feature-length movies, and large-scale recognition of complex events. The improved trajectory features significantly outperform previous dense trajectories, and Fisher vectors are superior to bag-of-words encodings for video recognition tasks. In all three tasks, the paper shows substantial improvements over the state-of-the-art results.",1
"We present a method for determining surface flows from solar images based upon optical flow techniques. We apply the method to sets of images obtained by a variety of solar imagers to assess its performance. The {\tt opflow3d} procedure is shown to extract accurate velocity estimates when provided perfect test data and quickly generates results consistent with completely distinct methods when applied on global scales. We also validate it in detail by comparing it to an established method when applied to high-resolution datasets and find that it provides comparable results without the need to tune, filter or otherwise preprocess the images before its application.",0
"A technique utilizing optical flow methods is presented for identifying surface flows from solar images. The method's performance is evaluated by applying it to image sets captured by various solar imagers. If provided with ideal test data, the {\tt opflow3d} procedure produces precise velocity estimates and generates results that are consistent with other methods on a global scale. We also thoroughly validate the method by comparing it to an established method on high-resolution datasets. The results demonstrate that it produces similar outcomes without requiring image tuning, filtering, or pre-processing.",1
"Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).",0
"The use of Convolutional neural networks (CNNs) has been widespread for identifying images, with outstanding outcomes in recognition, detection, segmentation, and retrieval. In this study, we present and evaluate multiple deep neural network structures that integrate image data across a video over more extended time frames than before. We introduce two methods that can handle full-length videos. The first approach explores various convolutional temporal feature pooling structures, examining the design choices that need to be made when adapting a CNN to this task. The second method models the video as an ordered sequence of frames, utilizing a recurrent neural network that uses Long Short-Term Memory (LSTM) cells connected to the output of the underlying CNN. Our top-performing networks exhibit significant improvements in performance compared to previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).",1
"In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.",0
"Recently, the computer vision community has established centralized benchmarks to assess the performance of various tasks, such as generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite the possible drawbacks of these benchmarks, they have proven to be immensely useful in advancing the state of the art in their respective domains. However, there has been little effort to standardize quantitative benchmarks for multiple target tracking. The PETS dataset is one of the few exceptions, primarily designed for surveillance applications, but its usage is often inconsistent due to the use of different data subsets, training methods, and evaluation scripts. This article presents our work on developing a new benchmark for multiple object tracking that addresses these issues. We explore the challenges of creating such a framework, gather existing and new data, collect state-of-the-art methods to be tested on the datasets, and establish a unified evaluation system. With MOTChallenge, we strive to establish a uniform evaluation framework for more meaningful quantification of multi-target tracking.",1
"Videos contain very rich semantic information. Traditional hand-crafted features are known to be inadequate in analyzing complex video semantics. Inspired by the huge success of the deep learning methods in analyzing image, audio and text data, significant efforts are recently being devoted to the design of deep nets for video analytics. Among the many practical needs, classifying videos (or video clips) based on their major semantic categories (e.g., ""skiing"") is useful in many applications. In this paper, we conduct an in-depth study to investigate important implementation options that may affect the performance of deep nets on video classification. Our evaluations are conducted on top of a recent two-stream convolutional neural network (CNN) pipeline, which uses both static frames and motion optical flows, and has demonstrated competitive performance against the state-of-the-art methods. In order to gain insights and to arrive at a practical guideline, many important options are studied, including network architectures, model fusion, learning parameters and the final prediction methods. Based on the evaluations, very competitive results are attained on two popular video classification benchmarks. We hope that the discussions and conclusions from this work can help researchers in related fields to quickly set up a good basis for further investigations along this very promising direction.",0
"The vast amount of semantic information found in videos cannot be effectively analyzed using traditional hand-crafted features. To overcome this, there has been a recent focus on designing deep nets for video analytics inspired by the success of deep learning methods in analyzing image, audio, and text data. Classifying videos by their semantic categories, such as ""skiing,"" has practical applications. In this study, we conducted an in-depth analysis of implementation options that may impact the performance of deep nets on video classification. Our evaluations were conducted on a two-stream convolutional neural network pipeline that uses static frames and motion optical flows and has shown competitive performance. We examined network architectures, model fusion, learning parameters, and final prediction methods to gain insights and develop practical guidelines. With these evaluations, we achieved very competitive results on two popular video classification benchmarks. Our hope is that this work will provide researchers in related fields with a solid foundation for further investigations in this promising direction.",1
"Edge preserving filters preserve the edges and its information while blurring an image. In other words they are used to smooth an image, while reducing the edge blurring effects across the edge like halos, phantom etc. They are nonlinear in nature. Examples are bilateral filter, anisotropic diffusion filter, guided filter, trilateral filter etc. Hence these family of filters are very useful in reducing the noise in an image making it very demanding in computer vision and computational photography applications like denoising, video abstraction, demosaicing, optical-flow estimation, stereo matching, tone mapping, style transfer, relighting etc. This paper provides a concrete introduction to edge preserving filters starting from the heat diffusion equation in olden to recent eras, an overview of its numerous applications, as well as mathematical analysis, various efficient and optimized ways of implementation and their interrelationships, keeping focus on preserving the boundaries, spikes and canyons in presence of noise. Furthermore it provides a realistic notion for efficient implementation with a research scope for hardware realization for further acceleration.",0
"When an image needs to be smoothed, but its edges and details must be preserved, edge preserving filters are used. These filters, such as the bilateral filter, anisotropic diffusion filter, guided filter, and trilateral filter, are non-linear and can reduce blurring effects like halos and phantoms. This makes them highly valuable in computer vision and computational photography applications, including denoising, video abstraction, and tone mapping. To provide a comprehensive introduction to edge preserving filters, this paper explores their history, applications, mathematical analysis, and efficient implementation methods. The focus is on preserving boundaries, spikes, and canyons in the presence of noise, and the paper also proposes hardware acceleration as a future research direction.",1
"In this paper we propose a novel approach to multi-action recognition that performs joint segmentation and classification. This approach models each action using a Gaussian mixture using robust low-dimensional action features. Segmentation is achieved by performing classification on overlapping temporal windows, which are then merged to produce the final result. This approach is considerably less complicated than previous methods which use dynamic programming or computationally expensive hidden Markov models (HMMs). Initial experiments on a stitched version of the KTH dataset show that the proposed approach achieves an accuracy of 78.3%, outperforming a recent HMM-based approach which obtained 71.2%.",0
"Our paper introduces a new method for recognizing multiple actions that involves both segmentation and classification. We employ a Gaussian mixture to model each action, utilizing resilient, low-dimensional features. By classifying overlapping temporal windows, we achieve segmentation, which is then consolidated for the ultimate outcome. This approach is simpler than previous techniques, which utilize dynamic programming or computationally intensive hidden Markov models (HMMs). Preliminary tests on a combined version of the KTH dataset demonstrate that our approach attains a 78.3% accuracy, surpassing a recent HMM-based approach that achieved 71.2%.",1
"The matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels. This approach works under assumption, that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically. However, this might not be the case, if we try to match pixels over a large interval of time.   In this paper we propose a method, which learns the matching function, that automatically finds the space of allowed changes in visual appearance, such as due to the motion blur, chromatic distortions, different colour calibration or seasonal changes. Furthermore, it automatically learns the importance of matching scores of contextual features at different relative locations and scales. Proposed classifier gives reliable estimations of pixel disparities already without any form of regularization.   We evaluated our method on two standard problems - stereo matching on KITTI outdoor dataset, optical flow on Sintel data set, and on newly introduced TimeLapse change detection dataset. Our algorithm obtained very promising results comparable to the state-of-the-art.",0
"Traditionally, the matching function for stereo reconstruction or optical flow problems has been based on the distance between the features of matched pixels. This approach assumes that the visual appearance of pixels in two stereo cameras or consecutive video frames does not change significantly. However, this is not always the case when matching pixels over a large time interval. In this study, we propose a method that learns the matching function to identify the range of allowed changes in visual appearance caused by factors such as motion blur, chromatic distortions, colour calibration, or seasonal changes. Our classifier also learns the importance of matching contextual features at different relative locations and scales, providing reliable pixel disparity estimations without regularization. We tested our method on three datasets, including KITTI outdoor stereo matching, Sintel optical flow, and TimeLapse change detection, and achieved very promising results comparable to the state-of-the-art.",1
"In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pre-training on surrogate tasks.   We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.",0
"The focus of this study is on the utilization of convolutional neural networks (convnets) for detecting pedestrians. Although convnets have been successful in various applications, they have historically been less effective than other pedestrian detection methods. Our approach omits explicit modeling of the problem within the network, such as parts or occlusion modeling, and instead demonstrates that competitive performance can be achieved without elaborate features. We examine small and large convnets, their architectural choices, parameters, and the effect of various training data, including pre-training on surrogate tasks. We present the best convnet detectors for the Caltech and KITTI datasets and achieve top performance in both the Caltech1x and Caltech10x training conditions. Using additional data during training, our most robust convnet model performs comparably to detectors that use extra data (optical flow) during testing.",1
"This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low-level features in combination with a boosted decision forest. Based on this observation we propose a unifying framework and experimentally explore different filter families. We report extensive results enabling a systematic analysis.   Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets, while using only HOG+LUV as low-level features. When adding optical flow features we further improve detection quality and report the best known results on the Caltech dataset, reaching 93% recall at 1 FPPI.",0
"The paper begins by noting that numerous exceptional pedestrian detectors can be simulated by incorporating a boosted decision forest with an intermediate layer that filters low-level features. With this in mind, a unified framework is proposed, and diverse filter families are experimentally investigated, with comprehensive results that enable a thorough analysis. By utilizing filtered channel features with only HOG+LUV as low-level features, we achieve optimal performance on the challenging KITTI and Caltech datasets. Additionally, by introducing optical flow features, detection quality is further enhanced, resulting in the best-known results on the Caltech dataset, with a 93% recall at 1 FPPI.",1
"A 3-D spatiotemporal prediction-error filter (PEF), is used to enhance foreground/background contrast in (real and simulated) sensor image sequences. Relative velocity is utilized to extract point-targets that would otherwise be indistinguishable on spatial frequency alone. An optical-flow field is generated using local estimates of the 3-D autocorrelation function via the application of the fast Fourier transform (FFT) and inverse FFT. Velocity estimates are then used to tune in a background-whitening PEF that is matched to the motion and texture of the local background. Finite-impulse-response (FIR) filters are designed and implemented in the frequency domain. An analytical expression for the frequency response of velocity-tuned FIR filters, of odd or even dimension, with an arbitrary delay in each dimension, is derived.",0
"To enhance contrast between foreground and background in sensor image sequences, a 3-D spatiotemporal prediction-error filter (PEF) is employed, which is applicable to both real and simulated data. To distinguish point-targets that may appear indistinguishable based on spatial frequency alone, the relative velocity is utilized. By applying the fast Fourier transform (FFT) and inverse FFT to local estimates of the 3-D autocorrelation function, an optical-flow field is generated. This flow field is then used to adjust a background-whitening PEF that matches the motion and texture of the local background. To accomplish this, finite-impulse-response (FIR) filters are designed and implemented in the frequency domain. An analytical expression is derived for the frequency response of velocity-tuned FIR filters with an arbitrary delay in each dimension, of either odd or even dimension.",1
"We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.   Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both.   Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",0
"Our study delves into the architectures of deep Convolutional Networks (ConvNets) that are discriminatively trained for video action recognition. The task at hand involves acquiring the complementary information on appearance from still frames and motion between frames. Additionally, our goal is to generalize the best performing hand-crafted features through a data-driven learning framework. We present three contributions. Firstly, we propose a two-stream ConvNet architecture that incorporates both spatial and temporal networks. Secondly, we demonstrate that a ConvNet that is trained on multi-frame dense optical flow can achieve excellent performance despite limited training data. Lastly, we show that multi-task learning applied to two distinct action classification datasets can augment the amount of training data and enhance performance on both. Our architecture is trained and assessed on the standard benchmarks of UCF-101 and HMDB-51 for video actions, where it competes with the state of the art. It also significantly surpasses previous attempts to use deep nets for video classification.",1
"In this paper, we describe a simple strategy for mitigating variability in temporal data series by shifting focus onto long-term, frequency domain features that are less susceptible to variability. We apply this method to the human action recognition task and demonstrate how working in the frequency domain can yield good recognition features for commonly used optical flow and articulated pose features, which are highly sensitive to small differences in motion, viewpoint, dynamic backgrounds, occlusion and other sources of variability. We show how these frequency-based features can be used in combination with a simple forest classifier to achieve good and robust results on the popular KTH Actions dataset.",0
"This paper outlines an uncomplicated approach for reducing variability in temporal data series by emphasizing long-term, frequency domain traits that are less vulnerable to variability. The method is employed in the human action recognition task, and the study demonstrates that operating in the frequency domain can produce reliable recognition features for frequently used optical flow and articulated pose features, which are susceptible to minor variations in motion, perspective, dynamic backgrounds, occlusion, and other sources of variability. The study illustrates how these frequency-based features can be integrated with a straightforward forest classifier to achieve reliable and stable outcomes on the widely used KTH Actions dataset.",1
"In this paper we present a number of methods (manual, semi-automatic and automatic) for tracking individual targets in high density crowd scenes where thousand of people are gathered. The necessary data about the motion of individuals and a lot of other physical information can be extracted from consecutive image sequences in different ways, including optical flow and block motion estimation. One of the famous methods for tracking moving objects is the block matching method. This way to estimate subject motion requires the specification of a comparison window which determines the scale of the estimate. In this work we present a real-time method for pedestrian recognition and tracking in sequences of high resolution images obtained by a stationary (high definition) camera located in different places on the Haram mosque in Mecca. The objective is to estimate pedestrian velocities as a function of the local density.The resulting data of tracking moving pedestrians based on video sequences are presented in the following section. Through the evaluated system the spatio-temporal coordinates of each pedestrian during the Tawaf ritual are established. The pilgrim velocities as function of the local densities in the Mataf area (Haram Mosque Mecca) are illustrated and very precisely documented.",0
"This paper outlines various methods (manual, semi-automatic, and automatic) for tracking individual targets in crowded scenes with high density, where thousands of people are present. To extract motion data and other physical information, optical flow and block motion estimation techniques are used. The block matching method is a popular approach for tracking moving objects, which involves selecting a comparison window to determine the estimate's scale. In this study, a real-time pedestrian recognition and tracking method is presented, using high-resolution images captured by a stationary camera placed at different locations in the Haram Mosque in Mecca. The aim is to estimate pedestrian velocities based on local density. The resulting data from tracking moving pedestrians in video sequences are presented in the following section, establishing the spatio-temporal coordinates of each pedestrian during the Tawaf ritual. The pilgrim velocities are illustrated and accurately documented as a function of local densities in the Mataf area of the Haram Mosque in Mecca.",1
"Handling all together large displacements, motion details and occlusions remains an open issue for reliable computation of optical flow in a video sequence. We propose a two-step aggregation paradigm to address this problem. The idea is to supply local motion candidates at every pixel in a first step, and then to combine them to determine the global optical flow field in a second step. We exploit local parametric estimations combined with patch correspondences and we experimentally demonstrate that they are sufficient to produce highly accurate motion candidates. The aggregation step is designed as the discrete optimization of a global regularized energy. The occlusion map is estimated jointly with the flow field throughout the two steps. We propose a generic exemplar-based approach for occlusion filling with motion vectors. We achieve state-of-the-art results in computer vision benchmarks, with particularly significant improvements in the case of large displacements and occlusions.",0
"The reliable computation of optical flow in a video sequence faces challenges in handling large displacements, motion details, and occlusions. To tackle this problem, we present a two-step aggregation approach. In the first step, we provide local motion candidates at each pixel, followed by the combination of these candidates in the second step to determine the global optical flow field. We utilize local parametric estimations and patch correspondences to produce accurate motion candidates. The aggregation step involves optimizing a global regularized energy. Our approach estimates the occlusion map jointly with the flow field throughout the two steps. Additionally, we propose an exemplar-based method for filling occlusions with motion vectors. Our method achieves state-of-the-art results in computer vision benchmarks, particularly in scenarios with large displacements and occlusions.",1
The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the \textit{ChaLearn Gesture Dataset}. We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos --- to remove all the unimportant frames from videos. We present two methods that use combination of HOG-HOF descriptors together with variants of Dynamic Time Warping technique. Both methods outperform other published methods and help narrow down the gap between human performance and algorithms on this task. The code has been made publicly available in the MLOSS repository.,0
"The aim of this document is to depict the development of gesture recognition systems that utilize one-shot-learning techniques on the \textit{ChaLearn Gesture Dataset}. These systems incorporate RGB and depth images, combining appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) to facilitate parallel temporal segmentation and recognition. The Quadratic-Chi distance family is implemented to gauge differences between histograms, capturing cross-bin relationships. We introduce a novel algorithm for video trimming, which discards insignificant frames from videos. We present two methods that employ a combination of HOG-HOF descriptors along with variants of Dynamic Time Warping technique. These methods exceed the performance of other existing methods and help to bridge the gap between human performance and algorithms in this domain. The code is publicly available in the MLOSS repository.",1
"This paper proposes combining spatio-temporal appearance (STA) descriptors with optical flow for human action recognition. The STA descriptors are local histogram-based descriptors of space-time, suitable for building a partial representation of arbitrary spatio-temporal phenomena. Because of the possibility of iterative refinement, they are interesting in the context of online human action recognition. We investigate the use of dense optical flow as the image function of the STA descriptor for human action recognition, using two different algorithms for computing the flow: the Farneb\""ack algorithm and the TVL1 algorithm. We provide a detailed analysis of the influencing optical flow algorithm parameters on the produced optical flow fields. An extensive experimental validation of optical flow-based STA descriptors in human action recognition is performed on the KTH human action dataset. The encouraging experimental results suggest the potential of our approach in online human action recognition.",0
"The aim of this study is to propose a new approach to human action recognition by combining spatio-temporal appearance (STA) descriptors with optical flow. These descriptors are based on local histograms of space-time, which can provide a partial representation of any spatio-temporal phenomenon. As these descriptors can be refined iteratively, they are particularly useful for real-time human action recognition. In this research, we explore the use of dense optical flow as the image function of the STA descriptor, using two different algorithms: the Farneb\""ack algorithm and the TVL1 algorithm. We also analyze the impact of various optical flow algorithm parameters on the resulting optical flow fields. To evaluate the effectiveness of our approach, we conduct extensive experiments on the KTH human action dataset. The results of our experiments are encouraging and demonstrate the potential of using optical flow-based STA descriptors for real-time human action recognition.",1
This paper describes a technique of real time head gesture recognition system. The method includes Gaussian mixture model (GMM) accompanied by optical flow algorithm which provided us the required information regarding head movement. The proposed model can be implemented in various control system. We are also presenting the result and implementation of both mentioned method.,0
"In this paper, a method for recognizing head gestures in real time is presented. This approach utilizes the Gaussian mixture model (GMM) in conjunction with an optical flow algorithm to gather necessary data on head movements. The model has the potential to be integrated into multiple control systems. Additionally, the outcomes and applications of both methods are demonstrated.",1
"Crowd monitoring and analysis in mass events are highly important technologies to support the security of attending persons. Proposed methods based on terrestrial or airborne image/video data often fail in achieving sufficiently accurate results to guarantee a robust service. We present a novel framework for estimating human count, density and motion from video data based on custom tailored object detection techniques, a regression based density estimate and a total variation based optical flow extraction. From the gathered features we present a detailed accuracy analysis versus ground truth measurements. In addition, all information is projected into world coordinates to enable a direct integration with existing geo-information systems. The resulting human counts demonstrate a mean error of 4% to 9% and thus represent a most efficient measure that can be robustly applied in security critical services.",0
"The monitoring and analysis of crowds at large events is crucial for ensuring the safety of attendees. However, traditional methods that rely on images or videos from the ground or air often fall short in providing accurate results. To address this issue, we have developed a new approach that uses customized object detection techniques, regression-based density estimates, and total variation-based optical flow extraction to estimate human count, density, and motion from video data. The accuracy of our method was measured against ground truth measurements, and the resulting data was projected onto world coordinates for integration with existing geo-information systems. Our results showed a mean error of 4% to 9%, making this approach highly effective and reliable for security-critical services.",1
"A robust and efficient anomaly detection technique is proposed, capable of dealing with crowded scenes where traditional tracking based approaches tend to fail. Initial foreground segmentation of the input frames confines the analysis to foreground objects and effectively ignores irrelevant background dynamics. Input frames are split into non-overlapping cells, followed by extracting features based on motion, size and texture from each cell. Each feature type is independently analysed for the presence of an anomaly. Unlike most methods, a refined estimate of object motion is achieved by computing the optical flow of only the foreground pixels. The motion and size features are modelled by an approximated version of kernel density estimation, which is computationally efficient even for large training datasets. Texture features are modelled by an adaptively grown codebook, with the number of entries in the codebook selected in an online fashion. Experiments on the recently published UCSD Anomaly Detection dataset show that the proposed method obtains considerably better results than three recent approaches: MPPCA, social force, and mixture of dynamic textures (MDT). The proposed method is also several orders of magnitude faster than MDT, the next best performing method.",0
"This article introduces a reliable and effective technique for detecting anomalies that can handle crowded scenes where traditional tracking-based methods often struggle. To achieve this, the input frames are first segmented to focus solely on foreground objects while ignoring background dynamics. The frames are then divided into separate cells, and motion, size, and texture features are extracted from each cell independently. Anomalies are identified by analyzing each feature type individually. A refined estimation of object motion is achieved by computing the optical flow of only the foreground pixels, which is an improvement over existing methods. The motion and size features are modeled using a computationally efficient approximation of kernel density estimation, even for large training datasets. Texture features are modeled using an adaptively grown codebook, with the number of entries in the codebook selected in an online fashion. The proposed method outperforms recent approaches, including MPPCA, social force, and mixture of dynamic textures (MDT), and is significantly faster than MDT. The results demonstrate the effectiveness of the proposed technique, especially in crowded scenes.",1
"This paper describes and provides an initial solution to a novel video editing task, i.e., video de-fencing. It targets automatic restoration of the video clips that are corrupted by fence-like occlusions during capture. Our key observation lies in the visual parallax between fences and background scenes, which is caused by the fact that the former are typically closer to the camera. Unlike in traditional image inpainting, fence-occluded pixels in the videos tend to appear later in the temporal dimension and are therefore recoverable via optimized pixel selection from relevant frames. To eventually produce fence-free videos, major challenges include cross-frame sub-pixel image alignment under diverse scene depth, and ""correct"" pixel selection that is robust to dominating fence pixels. Several novel tools are developed in this paper, including soft fence detection, weighted truncated optical flow method and robust temporal median filter. The proposed algorithm is validated on several real-world video clips with fences.",0
"In this paper, a new video editing task called video de-fencing is introduced and an initial solution is presented. The objective is to automatically restore video clips that have been corrupted by fence occlusions during capture. The key observation is the visual parallax between fences and background scenes, as fences are typically closer to the camera. Unlike traditional image inpainting, fence-occluded pixels in videos can be recovered by optimizing pixel selection from relevant frames. Challenges include cross-frame sub-pixel image alignment and robust pixel selection. The paper introduces several novel tools, such as soft fence detection, a weighted truncated optical flow method, and a robust temporal median filter. The proposed algorithm is tested on real-world video clips with fences. The ultimate goal is to produce fence-free videos.",1
"Recognizing group activities is challenging due to the difficulties in isolating individual entities, finding the respective roles played by the individuals and representing the complex interactions among the participants. Individual actions and group activities in videos can be represented in a common framework as they share the following common feature: both are composed of a set of low-level features describing motions, e.g., optical flow for each pixel or a trajectory for each feature point, according to a set of composition constraints in both temporal and spatial dimensions. In this paper, we present a unified model to assess the similarity between two given individual or group activities. Our approach avoids explicit extraction of individual actors, identifying and representing the inter-person interactions. With the proposed approach, retrieval from a video database can be performed through Query-by-Example; and activities can be recognized by querying videos containing known activities. The suggested video matching process can be performed in an unsupervised manner. We demonstrate the performance of our approach by recognizing a set of human actions and football plays.",0
"It is difficult to identify group activities since it is challenging to isolate individuals, determine their roles, and represent the complex interactions between them. However, both individual actions and group activities in videos share the common feature of being composed of low-level features that describe motions, such as optical flow or trajectories. We propose a unified model that assesses the similarity between individual and group activities without explicitly extracting individual actors or representing inter-person interactions. Our approach enables retrieval from a video database through Query-by-Example and can recognize activities by querying videos with known activities. The suggested video matching process is unsupervised, and we demonstrate its performance in recognizing human actions and football plays.",1
"In this paper we address the problem of tracking non-rigid objects whose local appearance and motion changes as a function of time. This class of objects includes dynamic textures such as steam, fire, smoke, water, etc., as well as articulated objects such as humans performing various actions. We model the temporal evolution of the object's appearance/motion using a Linear Dynamical System (LDS). We learn such models from sample videos and use them as dynamic templates for tracking objects in novel videos. We pose the problem of tracking a dynamic non-rigid object in the current frame as a maximum a-posteriori estimate of the location of the object and the latent state of the dynamical system, given the current image features and the best estimate of the state in the previous frame. The advantage of our approach is that we can specify a-priori the type of texture to be tracked in the scene by using previously trained models for the dynamics of these textures. Our framework naturally generalizes common tracking methods such as SSD and kernel-based tracking from static templates to dynamic templates. We test our algorithm on synthetic as well as real examples of dynamic textures and show that our simple dynamics-based trackers perform at par if not better than the state-of-the-art. Since our approach is general and applicable to any image feature, we also apply it to the problem of human action tracking and build action-specific optical flow trackers that perform better than the state-of-the-art when tracking a human performing a particular action. Finally, since our approach is generative, we can use a-priori trained trackers for different texture or action classes to simultaneously track and recognize the texture or action in the video.",0
"The focus of our paper is on the challenge of tracking objects that are non-rigid and whose appearance and movement changes over time. This group of objects includes dynamic textures like smoke, fire, water, and steam, as well as articulated objects like humans performing various actions. To address this challenge, we use a Linear Dynamical System (LDS) to model how the object's appearance and motion evolve over time. We train these models using sample videos and apply them as dynamic templates to track objects in new videos. Our approach involves determining the object's location and latent state of the dynamical system in the current frame, based on the best estimation of the previous frame's state and current image features. One advantage of our method is that we can specify the type of texture to track in the scene by using previously trained models. Our approach extends the common tracking methods like SSD and kernel-based tracking from static to dynamic templates. We evaluate our algorithm's performance on synthetic and real-world examples of dynamic textures and show that our simple dynamics-based trackers perform as well, if not better than, state-of-the-art methods. Since our approach is general and applicable to any image feature, we also apply it to track human actions and build action-specific optical flow trackers that outperform state-of-the-art methods when tracking a person performing a particular action. Finally, because our approach is generative, we can use pre-trained trackers for different texture or action classes to simultaneously track and recognize these textures or actions in the video.",1
This paper addresses the problem of correlation estimation in sets of compressed images. We consider a framework where images are represented under the form of linear measurements due to low complexity sensing or security requirements. We assume that the images are correlated through the displacement of visual objects due to motion or viewpoint change and the correlation is effectively represented by optical flow or motion field models. The correlation is estimated in the compressed domain by jointly processing the linear measurements. We first show that the correlated images can be efficiently related using a linear operator. Using this linear relationship we then describe the dependencies between images in the compressed domain. We further cast a regularized optimization problem where the correlation is estimated in order to satisfy both data consistency and motion smoothness objectives with a Graph Cut algorithm. We analyze in detail the correlation estimation performance and quantify the penalty due to image compression. Extensive experiments in stereo and video imaging applications show that our novel solution stays competitive with methods that implement complex image reconstruction steps prior to correlation estimation. We finally use the estimated correlation in a novel joint image reconstruction scheme that is based on an optimization problem with sparsity priors on the reconstructed images. Additional experiments show that our correlation estimation algorithm leads to an effective reconstruction of pairs of images in distributed image coding schemes that outperform independent reconstruction algorithms by 2 to 4 dB.,0
"The issue of estimating correlation in compressed image sets is the focus of this paper. The images are represented as linear measurements due to low complexity sensing or security requirements, and are assumed to have correlation resulting from displacement of visual objects due to motion or change in viewpoint. This correlation is represented by optical flow or motion field models. The correlation is estimated in the compressed domain by jointly processing the linear measurements. A linear operator is used to efficiently relate the correlated images, and the dependencies between images in the compressed domain are described. A regularized optimization problem is formulated to estimate the correlation, satisfying data consistency and motion smoothness objectives with a Graph Cut algorithm. The performance of the correlation estimation is analyzed, and the penalty due to image compression is quantified. The proposed method is competitive with methods that implement complex image reconstruction steps prior to correlation estimation, as demonstrated by extensive experiments in stereo and video imaging applications. The estimated correlation is used in a novel joint image reconstruction scheme with sparsity priors on the reconstructed images. Additional experiments show that the correlation estimation algorithm leads to effective reconstruction of pairs of images in distributed image coding schemes that outperform independent reconstruction algorithms by 2 to 4 dB.",1
"An image articulation manifold (IAM) is the collection of images formed when an object is articulated in front of a camera. IAMs arise in a variety of image processing and computer vision applications, where they provide a natural low-dimensional embedding of the collection of high-dimensional images. To date IAMs have been studied as embedded submanifolds of Euclidean spaces. Unfortunately, their promise has not been realized in practice, because real world imagery typically contains sharp edges that render an IAM non-differentiable and hence non-isometric to the low-dimensional parameter space under the Euclidean metric. As a result, the standard tools from differential geometry, in particular using linear tangent spaces to transport along the IAM, have limited utility. In this paper, we explore a nonlinear transport operator for IAMs based on the optical flow between images and develop new analytical tools reminiscent of those from differential geometry using the idea of optical flow manifolds (OFMs). We define a new metric for IAMs that satisfies certain local isometry conditions, and we show how to use this metric to develop a new tools such as flow fields on IAMs, parallel flow fields, parallel transport, as well as a intuitive notion of curvature. The space of optical flow fields along a path of constant curvature has a natural multi-scale structure via a monoid structure on the space of all flow fields along a path. We also develop lower bounds on approximation errors while approximating non-parallel flow fields by parallel flow fields.",0
"The IAM refers to a set of images that are captured when an object is moved in front of a camera. IAMs are useful in various image processing and computer vision applications as they offer a low-dimensional representation of high-dimensional images. However, previous studies have focused on IAMs as submanifolds of Euclidean spaces, which is not practical for real-world imagery due to sharp edges that make IAMs non-differentiable and non-isometric to the low-dimensional parameter space under the Euclidean metric. This limits the utility of standard tools from differential geometry. In this paper, we propose a nonlinear transport operator for IAMs based on optical flow between images and introduce the concept of optical flow manifolds (OFMs). We define a new metric for IAMs that satisfies local isometry conditions and develop analytical tools such as flow fields on IAMs, parallel flow fields, parallel transport, and a notion of curvature. We also explore the multi-scale structure of the space of optical flow fields along a path of constant curvature using a monoid structure. Finally, we present lower bounds on approximation errors while approximating non-parallel flow fields by parallel flow fields.",1
"An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.",0
"The article presents a novel algorithm that enables pose and motion estimation by utilizing corresponding features in omnidirectional images and a digital terrain map. The previous research focused on developing a similar algorithm for regular cameras. By utilizing a Digital Terrain Map (DTM/DEM) as a global reference, it is possible to retrieve the camera's absolute position and orientation. The algorithm achieves this by formulating a constraint between corresponding features in two consecutive frames using the DTM. This paper extends the constraints to support non-central projection, which is often the case with omnidirectional systems. The utilization of omnidirectional data enhances the algorithm's accuracy and robustness. The study confirms the algorithm's feasibility through experimentation with two types of omnidirectional acquisition systems, namely polydioptric and catadioptric cameras.",1
"The paper deals with the error analysis of a navigation algorithm that uses as input a sequence of images acquired by a moving camera and a Digital Terrain Map (DTM) of the region been imaged by the camera during the motion. The main sources of error are more or less straightforward to identify: camera resolution, structure of the observed terrain and DTM accuracy, field of view and camera trajectory. After characterizing and modeling these error sources in the framework of the CDTM algorithm, a closed form expression for their effect on the pose and motion errors of the camera can be found. The analytic expression provides a priori measurements for the accuracy in terms of the parameters mentioned above.",0
"This paper focuses on analyzing the errors in a navigation algorithm that takes in a sequence of images captured by a moving camera and a Digital Terrain Map (DTM) of the area being photographed during the camera's movement. The primary sources of error are easily identifiable, including camera resolution, observed terrain structure and DTM precision, field of view, and camera trajectory. By characterizing and modeling these error sources within the CDTM algorithm framework, it becomes possible to derive a closed-form expression that shows how these errors affect the camera's pose and motion. This analytical formula provides prior measurements for accuracy in relation to the aforementioned parameters.",1
"This work presents a framework for tracking head movements and capturing the movements of the mouth and both the eyebrows in real-time. We present a head tracker which is a combination of a optical flow and a template based tracker. The estimation of the optical flow head tracker is used as starting point for the template tracker which fine-tunes the head estimation. This approach together with re-updating the optical flow points prevents the head tracker from drifting. This combination together with our switching scheme, makes our tracker very robust against fast movement and motion-blur. We also propose a way to reduce the influence of partial occlusion of the head. In both the optical flow and the template based tracker we identify and exclude occluded points.",0
"In this work, a framework is introduced for real-time tracking of head movements, as well as capturing the movements of the mouth and eyebrows. The head tracker utilizes a combination of optical flow and a template-based approach. The optical flow head tracker is utilized as a starting point for the template tracker, which refines the head estimation. To avoid drifting, the optical flow points are re-updated. This approach, combined with a switching scheme, ensures the tracker's robustness against fast movement and motion-blur. Additionally, a method is proposed for reducing the impact of partial occlusion of the head by identifying and excluding occluded points in both the optical flow and the template-based tracker.",1
"The problem of the generation of an intermediate image between two given images in an image sequence is considered. The problem is formulated as an optimal control problem governed by a transport equation. This approach bears similarities with the Horn \& Schunck method for optical flow calculation but in fact the model is quite different. The images are modelled in $BV$ and an analysis of solutions of transport equations with values in $BV$ is included. Moreover, the existence of optimal controls is proven and necessary conditions are derived. Finally, two algorithms are given and numerical results are compared with existing methods. The new method is competitive with state-of-the-art methods and even outperforms several existing methods.",0
"The focus of this study is on generating an intermediate image in a sequence of images, which involves solving an optimal control problem using a transport equation. Although this method shares similarities with the Horn \& Schunck method for optical flow calculation, the model used is distinct. The images are represented in $BV$, and the solutions of transport equations with values in $BV$ are analyzed. Furthermore, the study proves the existence of optimal controls and derives necessary conditions. Two algorithms are presented, and their numerical results are compared with existing methods. The new approach is not only competitive with state-of-the-art methods, but it also outperforms several of them.",1
"This paper proposes a method of gesture recognition with a focus on important actions for distinguishing similar gestures. The method generates a partial action sequence by using optical flow images, expresses the sequence in the eigenspace, and checks the feature vector sequence by applying an optimum path-searching method of weighted graph to focus the important actions. Also presented are the results of an experiment on the recognition of similar sign language words.",0
"The main focus of this paper is to suggest a technique for identifying significant movements in gesture recognition to differentiate between similar gestures. The approach involves creating a partial sequence of actions using optical flow images, representing the sequence in the eigenspace, and examining the feature vector sequence by utilizing a weighted graph optimum path-searching method to highlight the crucial actions. Additionally, the paper discusses the outcomes of an experiment conducted to recognize comparable sign language words.",1
"In the field of computer vision, a crucial task is the detection of motion (also called optical flow extraction). This operation allows analysis such as 3D reconstruction, feature tracking, time-to-collision and novelty detection among others. Most of the optical flow extraction techniques work within a finite range of speeds. Usually, the range of detection is extended towards higher speeds by combining some multiscale information in a serial architecture. This serial multi-scale approach suffers from the problem of error propagation related to the number of scales used in the algorithm. On the other hand, biological experiments show that human motion perception seems to follow a parallel multiscale scheme. In this work we present a bio-inspired parallel architecture to perform detection of motion, providing a wide range of operation and avoiding error propagation associated with the serial architecture. To test our algorithm, we perform relative error comparisons between both classical and proposed techniques, showing that the parallel architecture is able to achieve motion detection with results similar to the serial approach.",0
"Detecting motion (also known as optical flow extraction) is a critical task in the field of computer vision. This task allows for analysis such as feature tracking, novelty detection, time-to-collision, and 3D reconstruction. Optical flow extraction techniques typically have a finite range of speed detection. However, combining multiscale information in a serial architecture can expand the range of detection towards higher speeds. Unfortunately, this approach is prone to error propagation due to the number of scales used in the algorithm. In contrast, human motion perception appears to adopt a parallel multiscale scheme. This study introduces a bio-inspired parallel architecture for motion detection that provides a wide range of operation while avoiding the error propagation associated with the serial architecture. To evaluate the algorithm's performance, the authors conducted relative error comparisons between the proposed and classical techniques. The results reveal that the parallel architecture achieves motion detection comparable to the serial approach.",1
"In this paper, we propose a global method for estimating the motion of a camera which films a static scene. Our approach is direct, fast and robust, and deals with adjacent frames of a sequence. It is based on a quadratic approximation of the deformation between two images, in the case of a scene with constant depth in the camera coordinate system. This condition is very restrictive but we show that provided translation and depth inverse variations are small enough, the error on optical flow involved by the approximation of depths by a constant is small. In this context, we propose a new model of camera motion, that allows to separate the image deformation in a similarity and a ``purely'' projective application, due to change of optical axis direction. This model leads to a quadratic approximation of image deformation that we estimate with an M-estimator; we can immediatly deduce camera motion parameters.",0
"The objective of this study is to suggest a global technique for determining the movement of a camera that captures a still scene. Our method is prompt, reliable, and straightforward and applies to adjacent frames in a sequence. It is based on a quadratic evaluation of the transformation between two images, assuming a constant depth of the scene with reference to the camera coordinate system. Although this prerequisite is quite limiting, we demonstrate that as long as the changes in depth inverse variations and translation are minimal, the optical flow error incurred by approximating depths with a constant remains insignificant. In light of this, we introduce a new camera motion model, capable of distinguishing the image distortion into a similarity and a ""purely"" projective transformation, caused by the alteration of the optical axis direction. This model results in a quadratic evaluation of image distortion, which we estimate using an M-estimator, enabling us to instantly deduce the motion parameters of the camera.",1
"In this paper, we are interested in the application to video segmentation of the discrete shape optimization problem involving the shape weighted perimeter and an additional term depending on a parameter. Based on recent works and in particular the one of Darbon and Sigelle, we justify the equivalence of the shape optimization problem and a weighted total variation regularization. For solving this problem, we adapt the projection algorithm proposed recently for solving the basic TV regularization problem. Another solution to the shape optimization investigated here is the graph cut technique. Both methods have the advantage to lead to a global minimum. Since we can distinguish moving objects from static elements of a scene by analyzing norm of the optical flow vectors, we choose the optical flow norm as initial data. In order to have the contour as close as possible to an edge in the image, we use a classical edge detector function as the weight of the weighted total variation. This model has been used in one of our former works. We also apply the same methods to a video segmentation model used by Jehan-Besson, Barlaud and Aubert. In this case, only standard perimeter is incorporated in the shape functional. We also propose another way for finding moving objects by using an a contrario detection of objects on the image obtained by solving the Rudin-Osher-Fatemi Total Variation regularization problem.We can notice the segmentation can be associated to a level set in the former methods.",0
"The objective of this paper is to explore the use of discrete shape optimization in video segmentation, which involves the shape weighted perimeter and an additional term dependent on a parameter. Drawing on recent research, including Darbon and Sigelle, we establish the equivalence of the shape optimization problem and weighted total variation regularization. To solve this problem, we apply the projection algorithm, which has been successful in solving basic TV regularization problems. We also investigate the graph cut technique, both methods ensuring a global minimum. For initial data, we use the optical flow norm to distinguish moving objects from static elements. To ensure the contour closely follows the image edge, we adopt a classical edge detector function as the weight of the weighted total variation. This model has been used in our previous work and is also applied to a video segmentation model employed by Jehan-Besson, Barlaud, and Aubert. In this model, only standard perimeter is incorporated into the shape functional. Additionally, we propose a novel approach to finding moving objects through a contrario detection of objects on the image achieved by solving the Rudin-Osher-Fatemi Total Variation regularization problem. We note that the segmentation can be linked to a level set in previous methods.",1
"In this paper, we propose a new type of Actor, named forward-looking Actor or FORK for short, for Actor-Critic algorithms. FORK can be easily integrated into a model-free Actor-Critic algorithm. Our experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK can bring to the state-of-the-art algorithms. A variation of FORK can further solve Bipedal-WalkerHardcore in as few as four hours using a single GPU.",0
"The suggestion in this article is to introduce a fresh form of Actor, which is called the forward-looking Actor (FORK), to be used in Actor-Critic algorithms. This FORK can be effortlessly included in a model-free Actor-Critic algorithm. We carried out experiments on six Box2D and MuJoCo environments, which involve continuous state and action spaces, and found that FORK considerably improves the performance of the top-rated algorithms. Moreover, a variant of FORK can solve Bipedal-WalkerHardcore in just four hours with a single GPU.",1
"Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates a single sample trajectory sequentially and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set and design a robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion. For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts(within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms.",0
"The objective of robust reinforcement learning (RL) is to identify a policy that optimizes performance under the worst-case scenario within a set of uncertain Markov decision processes (MDPs). This paper primarily focuses on model-free robust RL, where the uncertainty set encompasses a misspecified MDP that produces a single sample trajectory sequentially and is presumed unknown. To estimate the unknown uncertainty set, a sample-based approach is employed, and a robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting) are designed for online and incremental implementation. The robust Q-learning algorithm is proven to converge to the optimal robust Q function, while the robust TDC algorithm is shown to converge asymptotically to stationary points. Unlike the previous study by Roy et al. (2017), our algorithms do not require any additional conditions on the discount factor to ensure convergence. The finite-time error bounds of the two algorithms are also discussed, and the robust Q-learning and robust TDC algorithms are demonstrated to converge as quickly as their vanilla counterparts (within a constant factor) through numerical experiments. Our approach can be extended easily to robustify various other algorithms, such as TD, SARSA, and other GTD algorithms.",1
"Temporal-difference learning with gradient correction (TDC) is a two time-scale algorithm for policy evaluation in reinforcement learning. This algorithm was initially proposed with linear function approximation, and was later extended to the one with general smooth function approximation. The asymptotic convergence for the on-policy setting with general smooth function approximation was established in [bhatnagar2009convergent], however, the finite-sample analysis remains unsolved due to challenges in the non-linear and two-time-scale update structure, non-convex objective function and the time-varying projection onto a tangent plane. In this paper, we develop novel techniques to explicitly characterize the finite-sample error bound for the general off-policy setting with i.i.d.\ or Markovian samples, and show that it converges as fast as $\mathcal O(1/\sqrt T)$ (up to a factor of $\mathcal O(\log T)$). Our approach can be applied to a wide range of value-based reinforcement learning algorithms with general smooth function approximation.",0
"TDC is a policy evaluation algorithm for reinforcement learning that utilizes temporal-difference learning and gradient correction. Originally designed for linear function approximation, it was later adapted for general smooth function approximation. While its asymptotic convergence in the on-policy setting with general smooth function approximation was established in [bhatnagar2009convergent], finite-sample analysis remains a challenge due to the non-linear and two-time-scale update structure, non-convex objective function, and time-varying projection onto a tangent plane. In this study, we introduce new techniques to explicitly characterize the finite-sample error bound for the general off-policy setting with i.i.d. or Markovian samples, and demonstrate that it converges as quickly as $\mathcal O(1/\sqrt T)$ (up to a factor of $\mathcal O(\log T)$). Our approach can be applied to a variety of value-based reinforcement learning algorithms that use general smooth function approximation.",1
"We use reinforcement learning to tackle the problem of untangling braids. We experiment with braids with 2 and 3 strands. Two competing players learn to tangle and untangle a braid. We interface the braid untangling problem with the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. The results provide evidence that the more we train the system, the better the untangling player gets at untangling braids. At the same time, our tangling player produces good examples of tangled braids.",0
"The issue of untangling braids is addressed by applying reinforcement learning. Braids consisting of 2 and 3 strands are tested, with two opposing players learning how to tangle and untangle them. The braid untangling challenge is integrated with the OpenAI Gym environment, which is a commonly utilized method of linking agents with reinforcement learning tasks. The findings demonstrate that with more training, the untangling player's ability to untangle braids improves while the tangling player generates well-formed tangled braids.",1
"Millions of battery-powered sensors deployed for monitoring purposes in a multitude of scenarios, e.g., agriculture, smart cities, industry, etc., require energy-efficient solutions to prolong their lifetime. When these sensors observe a phenomenon distributed in space and evolving in time, it is expected that collected observations will be correlated in time and space. In this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling mechanism capable of taking advantage of correlated information. We design our solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The proposed mechanism is capable of determining the frequency with which sensors should transmit their updates, to ensure accurate collection of observations, while simultaneously considering the energy available. To evaluate our scheduling mechanism, we use multiple datasets containing environmental observations obtained in multiple real deployments. The real observations enable us to model the environment with which the mechanism interacts as realistically as possible. We show that our solution can significantly extend the sensors' lifetime. We compare our mechanism to an idealized, all-knowing scheduler to demonstrate that its performance is near-optimal. Additionally, we highlight the unique feature of our design, energy-awareness, by displaying the impact of sensors' energy levels on the frequency of updates.",0
"To prolong the lifetime of battery-powered sensors used for monitoring purposes in various scenarios such as agriculture, smart cities, and industry, energy-efficient solutions are required. These sensors often collect correlated observations in time and space when monitoring a phenomenon. In this study, a scheduling mechanism based on Deep Reinforcement Learning (DRL) using the Deep Deterministic Policy Gradient (DDPG) algorithm is proposed to take advantage of correlated information while considering the available energy. The proposed mechanism determines the frequency of updates required for accurate observation collection and evaluates its performance with multiple datasets obtained from real deployments. The energy-awareness feature of the design is displayed by showing the impact of sensors’ energy levels on the update frequency. The study demonstrates that the proposed mechanism significantly extends the lifetime of sensors and performs near-optimal compared to an idealized, all-knowing scheduler.",1
"Value factorization is a popular and promising approach to scaling up multi-agent reinforcement learning in cooperative settings. However, the theoretical understanding of such methods is limited. In this paper, we formalize a multi-agent fitted Q-iteration framework for analyzing factorized multi-agent Q-learning. Based on this framework, we investigate linear value factorization and reveal that multi-agent Q-learning with this simple decomposition implicitly realizes a powerful counterfactual credit assignment, but may not converge in some settings. Through further analysis, we find that on-policy training or richer joint value function classes can improve its local or global convergence properties, respectively. Finally, to support and extend our theoretical implications to practical realization, we conduct an empirical analysis of state-of-the-art deep multi-agent Q-learning algorithms on didactic examples and a broad set of StarCraft II unit micromanagement tasks.",0
"The method of value factorization is widely used to enhance multi-agent reinforcement learning in cooperative scenarios. Despite its promise, there is a limited understanding of the theoretical aspects of this technique. This study presents a formalized framework of multi-agent fitted Q-iteration to analyze factorized multi-agent Q-learning. By employing this framework, the authors explore linear value factorization and demonstrate that this approach implicitly executes a potent counterfactual credit assignment. However, it may not always converge in certain situations. Further analysis reveals that on-policy training or richer joint value function classes can enhance its local or global convergence properties, respectively. To validate these theoretical implications, the authors undertake an empirical examination of cutting-edge deep multi-agent Q-learning algorithms on a range of StarCraft II unit micromanagement tasks and didactic examples.",1
"Model-Based Reinforcement Learning involves learning a \textit{dynamics model} from data, and then using this model to optimise behaviour, most often with an online \textit{planner}. Much of the recent research along these lines presents a particular set of design choices, involving problem definition, model learning and planning. Given the multiple contributions, it is difficult to evaluate the effects of each. This paper sets out to disambiguate the role of different design choices for learning dynamics models, by comparing their performance to planning with a ground-truth model -- the simulator. First, we collect a rich dataset from the training sequence of a model-free agent on 5 domains of the DeepMind Control Suite. Second, we train feed-forward dynamics models in a supervised fashion, and evaluate planner performance while varying and analysing different model design choices, including ensembling, stochasticity, multi-step training and timestep size. Besides the quantitative analysis, we describe a set of qualitative findings, rules of thumb, and future research directions for planning with learned dynamics models. Videos of the results are available at https://sites.google.com/view/learning-better-models.",0
"The process of Model-Based Reinforcement Learning involves acquiring knowledge about a \textit{dynamics model} through data, and subsequently utilizing this information to enhance behavior, often with the aid of an online \textit{planner}. A significant portion of recent research in this field entails a specific set of design choices, encompassing problem definition, model learning, and planning. Given the diverse contributions, it is challenging to assess the impact of each one. This article endeavors to clarify the role of distinct design choices for acquiring knowledge about dynamics models by comparing their performance to planning using a ground-truth model - the simulator. Firstly, we obtain a comprehensive dataset from the training sequence of a model-free agent across five domains of the DeepMind Control Suite. Secondly, we train feed-forward dynamics models in a supervised manner and evaluate planner performance while varying and analyzing various model design choices, such as ensembling, stochasticity, multi-step training, and timestep size. In addition to quantitative analysis, we present qualitative findings, guidelines, and future research directions for planning using learned dynamics models. Videos of the outcomes can be viewed at https://sites.google.com/view/learning-better-models.",1
"We propose and validate a novel car following model based on deep reinforcement learning. Our model is trained to maximize externally given reward functions for the free and car-following regimes rather than reproducing existing follower trajectories. The parameters of these reward functions such as desired speed, time gap, or accelerations resemble that of traditional models such as the Intelligent Driver Model (IDM) and allow for explicitly implementing different driving styles. Moreover, they partially lift the black-box nature of conventional neural network models. The model is trained on leading speed profiles governed by a truncated Ornstein-Uhlenbeck process reflecting a realistic leader's kinematics.   This allows for arbitrary driving situations and an infinite supply of training data. For various parameterizations of the reward functions, and for a wide variety of artificial and real leader data, the model turned out to be unconditionally string stable, comfortable, and crash-free. String stability has been tested with a platoon of five followers following an artificial and a real leading trajectory. A cross-comparison with the IDM calibrated to the goodness-of-fit of the relative gaps showed a higher reward compared to the traditional model and a better goodness-of-fit.",0
"Our study introduces a fresh car following model that relies on deep reinforcement learning and has been validated. The model aims to maximize externally set reward functions for both the free and car-following scenarios, rather than duplicating existing follower trajectories. The parameters of the reward functions, such as desired speed, accelerations, and time gap, are similar to those of traditional models like the Intelligent Driver Model (IDM) and can be used to apply different driving styles explicitly. This approach also partially eliminates the black-box nature of conventional neural network models. To train the model, we employed truncated Ornstein-Uhlenbeck processes to regulate the leader's kinematics, which allowed for unlimited data and various driving situations. Our experiments with different reward function configurations and artificial and real leader data confirmed that the model is unconditionally comfortable, crash-free, and string stable. String stability was tested by having a group of five followers following both artificial and real leading trajectories. A comparison with the IDM, calibrated to the relative gaps' goodness-of-fit, showed that our model had a higher reward and a better goodness-of-fit.",1
"Sequential decision making in the presence of uncertainty and stochastic dynamics gives rise to distributions over state/action trajectories in reinforcement learning (RL) and optimal control problems. This observation has led to a variety of connections between RL and inference in probabilistic graphical models (PGMs). Here we explore a different dimension to this relationship, examining reinforcement learning using the tools and abstractions of statistical physics. The central object in the statistical physics abstraction is the idea of a partition function $\mathcal{Z}$, and here we construct a partition function from the ensemble of possible trajectories that an agent might take in a Markov decision process. Although value functions and $Q$-functions can be derived from this partition function and interpreted via average energies, the $\mathcal{Z}$-function provides an object with its own Bellman equation that can form the basis of alternative dynamic programming approaches. Moreover, when the MDP dynamics are deterministic, the Bellman equation for $\mathcal{Z}$ is linear, allowing direct solutions that are unavailable for the nonlinear equations associated with traditional value functions. The policies learned via these $\mathcal{Z}$-based Bellman updates are tightly linked to Boltzmann-like policy parameterizations. In addition to sampling actions proportionally to the exponential of the expected cumulative reward as Boltzmann policies would, these policies take entropy into account favoring states from which many outcomes are possible.",0
"Reinforcement learning (RL) and optimal control problems involve making sequential decisions while dealing with uncertainty and stochastic dynamics, resulting in distributions over state/action trajectories. This has led to connections between RL and inference in probabilistic graphical models (PGMs). In this study, we investigate the use of statistical physics to study RL. We introduce the partition function $\mathcal{Z}$, which is constructed from the ensemble of possible trajectories an agent might take in a Markov decision process. While value functions and $Q$-functions can be derived from this partition function through average energies, the $\mathcal{Z}$-function has its own Bellman equation that forms the basis of alternative dynamic programming approaches. When the MDP dynamics are deterministic, the Bellman equation for $\mathcal{Z}$ is linear, allowing for direct solutions that are unavailable for the nonlinear equations associated with traditional value functions. The policies learned via $\mathcal{Z}$-based Bellman updates are closely connected to Boltzmann-like policy parameterizations. These policies not only sample actions proportionally to the exponential of the expected cumulative reward, but also consider entropy, favoring states from which multiple outcomes are possible.",1
"In multi-agent deep reinforcement learning, extracting sufficient and compact information of other agents is critical to attain efficient convergence and scalability of an algorithm. In canonical frameworks, distilling of such information is often done in an implicit and uninterpretable manner, or explicitly with cost functions not able to reflect the relationship between information compression and utility in representation. In this paper, we present Information-Bottleneck-based Other agents' behavior Representation learning for Multi-agent reinforcement learning (IBORM) to explicitly seek low-dimensional mapping encoder through which a compact and informative representation relevant to other agents' behaviors is established. IBORM leverages the information bottleneck principle to compress observation information, while retaining sufficient information relevant to other agents' behaviors used for cooperation decision. Empirical results have demonstrated that IBORM delivers the fastest convergence rate and the best performance of the learned policies, as compared with implicit behavior representation learning and explicit behavior representation learning without explicitly considering information compression and utility.",0
"Extracting concise yet adequate information about other agents is crucial in achieving efficient convergence and scalability in multi-agent deep reinforcement learning. However, in conventional frameworks, distilling such information is often accomplished implicitly and incomprehensibly or explicitly with cost functions that fail to reflect the relationship between information compression and the usefulness of representation. To address this issue, our paper introduces the Information-Bottleneck-based Other agents' behavior Representation learning for Multi-agent reinforcement learning (IBORM) method, which seeks a low-dimensional mapping encoder that establishes a compact and informative representation relevant to other agents' behaviors. IBORM utilizes the information bottleneck principle to compress observation information while retaining adequate information pertinent to other agents' behaviors for cooperation decision-making. Empirical results indicate that IBORM outperforms implicit and explicit behavior representation learning methods that do not explicitly consider information compression and utility in terms of the fastest convergence rate and the best performance of the learned policies.",1
"Reinforcement learning (RL) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. One such class of policies are the semi-parametric H-step lookahead policies, which select actions using trajectory optimization over a dynamics model for a fixed horizon with a terminal value function. In this work, we investigate a novel instantiation of H-step lookahead with a learned model and a terminal value function learned by a model-free off-policy algorithm, named Learning Off-Policy with Online Planning (LOOP). We provide a theoretical analysis of this method, suggesting a tradeoff between model errors and value function errors and empirically demonstrate this tradeoff to be beneficial in deep reinforcement learning. Furthermore, we identify the ""Actor Divergence"" issue in this framework and propose Actor Regularized Control (ARC), a modified trajectory optimization procedure. We evaluate our method on a set of robotic tasks for Offline and Online RL and demonstrate improved performance. We also show the flexibility of LOOP to incorporate safety constraints during deployment with a set of navigation environments. We demonstrate that LOOP is a desirable framework for robotics applications based on its strong performance in various important RL settings. Project video and details can be found at $\href{https://hari-sikchi.github.io/loop}{\text{hari-sikchi.github.io/loop}}$.",0
"The deployment of performant and flexible policies is necessary for reinforcement learning (RL) in low-data and risk-sensitive domains, especially when constraints need to be incorporated during deployment. Semi-parametric H-step lookahead policies, which use trajectory optimization over a dynamics model for a fixed horizon with a terminal value function, are one such class of policies. In this study, we explore a new version of H-step lookahead that employs a model-free off-policy algorithm called Learning Off-Policy with Online Planning (LOOP) to learn a model and a terminal value function. We provide a theoretical analysis of this approach, highlighting a tradeoff between model errors and value function errors, which we show to be advantageous in deep reinforcement learning. We also identify the ""Actor Divergence"" problem in this framework and suggest Actor Regularized Control (ARC) as a modified trajectory optimization procedure. Our method shows improved performance in a range of important RL settings, including a set of robotic tasks for Offline and Online RL, as well as navigation environments where safety constraints are incorporated during deployment. LOOP is a highly desirable framework for robotics applications, and additional information and a project video can be found at $\href{https://hari-sikchi.github.io/loop}{\text{hari-sikchi.github.io/loop}}$.",1
"Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states. The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity. However, in the real world, rewards may move or only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span. To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed. We demonstrate that the FR facilitates the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli.",0
"State representations that enable quick learning and efficient navigation towards rewarding states are beneficial for both animals and artificial agents. The successor representation (SR), which measures expected cumulative state occupancy under a fixed policy, allows for efficient transfer between different reward structures in a Markovian environment and is thought to underlie biological behavior and neural activity. However, real-world rewards may be fleeting or shift location, and agents may prioritize reaching goal states quickly rather than following a set task horizon. In these cases, a more relevant representation would predict when an agent is likely to first reach desired states. To address this, the first-occupancy representation (FR) measures the expected temporal discount to the first time a state is accessed. The FR facilitates efficient path selection, enables provably optimal trajectory planning under certain conditions, and produces similar behavior to animals avoiding threats.",1
"Quantitative trading (QT), which refers to the usage of mathematical models and data-driven techniques in analyzing the financial market, has been a popular topic in both academia and financial industry since 1970s. In the last decade, reinforcement learning (RL) has garnered significant interest in many domains such as robotics and video games, owing to its outstanding ability on solving complex sequential decision making problems. RL's impact is pervasive, recently demonstrating its ability to conquer many challenging QT tasks. It is a flourishing research direction to explore RL techniques' potential on QT tasks. This paper aims at providing a comprehensive survey of research efforts on RL-based methods for QT tasks. More concretely, we devise a taxonomy of RL-based QT models, along with a comprehensive summary of the state of the art. Finally, we discuss current challenges and propose future research directions in this exciting field.",0
"Since the 1970s, Quantitative Trading (QT) has been a popular topic in both academia and the financial industry. It involves using mathematical models and data-driven techniques to analyze the financial market. Over the last decade, Reinforcement Learning (RL) has gained significant interest in various domains such as robotics and video games due to its ability to solve complex sequential decision-making problems. RL has recently demonstrated its potential to conquer many challenging QT tasks, making it a flourishing research direction to explore. This paper aims to provide a comprehensive survey of research on RL-based methods for QT tasks. The paper presents a taxonomy of RL-based QT models and a summary of the state-of-the-art, followed by a discussion of current challenges and proposed future research directions in the field.",1
"Not having access to compact and meaningful representations is known to significantly increase the complexity of reinforcement learning (RL). For this reason, it can be useful to perform state representation learning (SRL) before tackling RL tasks. However, obtaining a good state representation can only be done if a large diversity of transitions is observed, which can require a difficult exploration, especially if the environment is initially reward-free. To solve the problems of exploration and SRL in parallel, we propose a new approach called XSRL (eXploratory State Representation Learning). On one hand, it jointly learns compact state representations and a state transition estimator which is used to remove unexploitable information from the representations. On the other hand, it continuously trains an inverse model, and adds to the prediction error of this model a $k$-step learning progress bonus to form the maximization objective of a discovery policy. This results in a policy that seeks complex transitions from which the trained models can effectively learn. Our experimental results show that the approach leads to efficient exploration in challenging environments with image observations, and to state representations that significantly accelerate learning in RL tasks.",0
"Reinforcement learning (RL) can become more complex without access to concise and meaningful representations. State representation learning (SRL) can be helpful in addressing this issue before undertaking RL tasks. However, achieving a good state representation can be challenging and require extensive exploration, especially in environments without rewards. To address the problems of exploration and SRL simultaneously, we propose a new method called XSRL (eXploratory State Representation Learning). The approach jointly learns compact state representations and a state transition estimator while eliminating unexploitable information from the representations. It also trains an inverse model and incorporates a $k$-step learning progress bonus to form the maximization objective of a discovery policy. This results in a policy that aims to find complex transitions to help the trained models learn effectively. Our experimental findings demonstrate that the proposed method leads to efficient exploration in challenging environments with image observations and accelerates learning in RL tasks through state representations.",1
"Vision-based reinforcement learning (RL) is a promising technique to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to an increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve the sample diversity. Our method enhances the exploration capability of the RL algorithms by taking advantage of the SRL setup. Our experiments show that the presented approach outperforms the baseline for all tested environments. These results are most apparent for environments where the baseline method struggles. Even in simple environments, our method stabilizes the training, reduces the reward variance and boosts sample efficiency.",0
"Vision-based reinforcement learning is a promising method for solving control tasks that involve images as the primary observation. However, current state-of-the-art RL algorithms face challenges in terms of sample efficiency, particularly when using image observations. Consequently, there has been increased interest in incorporating state representation learning (SRL) techniques into the RL pipeline, which has shown to improve sample efficiency and other benefits. However, high-quality and diverse samples are crucial for maximizing the potential of this paradigm and ensuring efficient and effective training. To address this, we propose an approach that leverages the SRL setup to enhance RL algorithms' exploration capability and improve sample diversity. Our experiments demonstrate that our method outperforms the baseline across all tested environments, particularly in challenging ones. Even in simpler environments, our approach stabilizes training, reduces reward variance, and boosts sample efficiency.",1
"Bi-Level Optimization (BLO) is originated from the area of economic game theory and then introduced into the optimization community. BLO is able to handle problems with a hierarchical structure, involving two levels of optimization tasks, where one task is nested inside the other. In machine learning and computer vision fields, despite the different motivations and mechanisms, a lot of complex problems, such as hyper-parameter optimization, multi-task and meta-learning, neural architecture search, adversarial learning and deep reinforcement learning, actually all contain a series of closely related subproblms. In this paper, we first uniformly express these complex learning and vision problems from the perspective of BLO. Then we construct a best-response-based single-level reformulation and establish a unified algorithmic framework to understand and formulate mainstream gradient-based BLO methodologies, covering aspects ranging from fundamental automatic differentiation schemes to various accelerations, simplifications, extensions and their convergence and complexity properties. Last but not least, we discuss the potentials of our unified BLO framework for designing new algorithms and point out some promising directions for future research.",0
"Originally from economic game theory, Bi-Level Optimization (BLO) was later introduced to the optimization community. BLO is capable of handling hierarchical problems with two levels of optimization tasks, where one task is nested inside another. Many complex problems in the fields of machine learning and computer vision, such as hyper-parameter optimization, multi-task and meta-learning, neural architecture search, adversarial learning and deep reinforcement learning, consist of a series of closely related subproblems. In this article, we present a uniform expression of these complex learning and vision problems based on BLO. We then create a best-response-based single-level reformulation and establish a unified algorithmic framework that covers various BLO methodologies, including fundamental automatic differentiation schemes, accelerations, simplifications, extensions, and their convergence and complexity properties. Finally, we discuss the potential of our unified BLO framework for developing new algorithms and point out some promising directions for future research.",1
"We propose a exploration mechanism of policy in Deep Reinforcement Learning, which is exploring more when agent needs, called Add Noise to Noise (AN2N). The core idea is: when the Deep Reinforcement Learning agent is in a state of poor performance in history, it needs to explore more. So we use cumulative rewards to evaluate which past states the agents have not performed well, and use cosine distance to measure whether the current state needs to be explored more. This method shows that the exploration mechanism of the agent's policy is conducive to efficient exploration. We combining the proposed exploration mechanism AN2N with Deep Deterministic Policy Gradient (DDPG), Soft Actor-Critic (SAC) algorithms, and apply it to the field of continuous control tasks, such as halfCheetah, Hopper, and Swimmer, achieving considerable improvement in performance and convergence speed.",0
"We introduce a policy exploration mechanism called Add Noise to Noise (AN2N) in Deep Reinforcement Learning that encourages greater exploration by the agent when necessary. The underlying concept is based on the idea that when the agent is not performing well historically, it needs to explore more. We use cumulative rewards to assess which past states the agents have performed poorly and cosine distance to determine if the current state requires further exploration. Our findings indicate that this policy exploration mechanism is beneficial for efficient exploration. We integrate AN2N with Deep Deterministic Policy Gradient (DDPG) and Soft Actor-Critic (SAC) algorithms and apply it in the domain of continuous control tasks such as halfCheetah, Hopper, and Swimmer, resulting in significant enhancement in performance and convergence speed.",1
"Deep reinforcement learning (RL) algorithms can learn complex policies to optimize agent operation over time. RL algorithms have shown promising results in solving complicated problems in recent years. However, their application on real-world physical systems remains limited. Despite the advancements in RL algorithms, the industries often prefer traditional control strategies. Traditional methods are simple, computationally efficient and easy to adjust. In this paper, we first propose a new Q-learning algorithm for continuous action space, which can bridge the control and RL algorithms and bring us the best of both worlds. Our method can learn complex policies to achieve long-term goals and at the same time it can be easily adjusted to address short-term requirements without retraining. Next, we present an approximation of our algorithm which can be applied to address short-term requirements of any pre-trained RL algorithm. The case studies demonstrate that both our proposed method as well as its practical approximation can achieve short-term and long-term goals without complex reward functions.",0
"Complex policies can be learned by deep reinforcement learning (RL) algorithms to optimize agent operation over time. Although RL algorithms have demonstrated success in solving intricate problems recently, their application on real-world physical systems remains limited. Traditional control strategies are preferred by industries because they are simple, computationally efficient, and easy to adjust. In this paper, we propose a new Q-learning algorithm for continuous action space, which combines the advantages of both control and RL algorithms. Our method can learn complex policies to achieve long-term goals and can be easily adjusted to address short-term requirements without retraining. Furthermore, we present an approximation of our algorithm that can be applied to address short-term requirements of any pre-trained RL algorithm. Our case studies demonstrate that both our proposed method and its practical approximation can achieve short-term and long-term goals without relying on complex reward functions.",1
"The progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents. However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity.",0
"The availability of challenging benchmarks for training agents heavily drives progress in deep reinforcement learning (RL). However, the benchmarks that are widely used by the community are not specifically designed to evaluate the capabilities of RL methods. While there are environments available to assess particular open problems in RL, extending them to richer and more complex environments beyond proof-of-concept results is generally challenging. To address this issue, we introduce MiniHack, a powerful sandbox framework that enables the easy design of novel RL environments. MiniHack provides a one-stop-shop for RL experiments, from small rooms to complex procedurally generated worlds. By utilizing the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack enables the creation of custom RL testbeds that are fast and convenient to use. This sandbox framework allows for easy design of novel environments using either a human-readable description language or a simple Python interface. Additionally, MiniHack offers a variety of RL tasks and baselines, can wrap existing RL benchmarks, and provides ways to add complexity seamlessly.",1
"In complex environments with high dimension, training a reinforcement learning (RL) model from scratch often suffers from lengthy and tedious collection of agent-environment interactions. Instead, leveraging expert demonstration to guide RL agent can boost sample efficiency and improve final convergence. In order to better integrate expert prior with on-policy RL models, we propose a generic framework for Learning from Demonstration (LfD) based on actor-critic algorithms. Technically, we first employ K-Means clustering to evaluate the similarity of sampled exploration with demonstration data. Then we increase the likelihood of actions in similar frames by modifying the gradient update strategy to leverage demonstration. We conduct experiments on 4 standard benchmark environments in Mujoco and 2 self-designed robotic environments. Results show that, under certain condition, our algorithm can improve sample efficiency by 20% ~ 40%. By combining our framework with on-policy algorithms, RL models can accelerate convergence and obtain better final mean episode rewards especially in complex robotic context where interactions are expensive.",0
"When dealing with complex environments that have high dimensionality, training a reinforcement learning (RL) model from scratch can be a time-consuming and laborious process due to the need for extensive collection of agent-environment interactions. Instead, incorporating expert demonstration to guide the RL agent can enhance sample efficiency and improve the final convergence. To improve the integration of expert knowledge with on-policy RL models, we propose a generic Learning from Demonstration (LfD) framework based on actor-critic algorithms. Our approach uses K-Means clustering to evaluate the similarity between sampled exploration and demonstration data, and modifies the gradient update strategy to increase the likelihood of actions in similar frames. We evaluated our framework on four standard benchmark environments in Mujoco and two self-designed robotic environments. Our results indicate that in certain conditions, our algorithm can boost sample efficiency by 20% to 40%. By leveraging our framework with on-policy algorithms, RL models can accelerate convergence and achieve better final mean episode rewards, particularly in complex robotic contexts where interactions are costly.",1
"We study the model-based undiscounted reinforcement learning for partially observable Markov decision processes (POMDPs). The oracle we consider is the optimal policy of the POMDP with a known environment in terms of the average reward over an infinite horizon. We propose a learning algorithm for this problem, building on spectral method-of-moments estimations for hidden Markov models, the belief error control in POMDPs and upper-confidence-bound methods for online learning. We establish a regret bound of $O(T^{2/3}\sqrt{\log T})$ for the proposed learning algorithm where $T$ is the learning horizon. This is, to the best of our knowledge, the first algorithm achieving sublinear regret with respect to our oracle for learning general POMDPs.",0
"Our focus is on model-based undiscounted reinforcement learning for partially observable Markov decision processes (POMDPs). Specifically, we examine the optimal policy of the POMDP, assuming a known environment, with regards to the average reward over an infinite horizon. To address this problem, we present a learning algorithm that utilizes spectral method-of-moments estimations for hidden Markov models, belief error control in POMDPs, and upper-confidence-bound methods for online learning. Our algorithm achieves a regret bound of $O(T^{2/3}\sqrt{\log T})$ in the learning horizon $T$. This is the first algorithm, to our knowledge, that achieves sublinear regret in learning general POMDPs relative to our oracle.",1
"Unsupervised disentangled representation learning is a long-standing problem in computer vision. This work proposes a novel framework for performing image clustering from deep embeddings by combining instance-level contrastive learning with a deep embedding based cluster center predictor. Our approach jointly learns representations and predicts cluster centers in an end-to-end manner. This is accomplished via a three-pronged approach that combines a clustering loss, an instance-wise contrastive loss, and an anchor loss. Our fundamental intuition is that using an ensemble loss that incorporates instance-level features and a clustering procedure focusing on semantic similarity reinforces learning better representations in the latent space. We observe that our method performs exceptionally well on popular vision datasets when evaluated using standard clustering metrics such as Normalized Mutual Information (NMI), in addition to producing geometrically well-separated cluster embeddings as defined by the Euclidean distance. Our framework performs on par with widely accepted clustering methods and outperforms the state-of-the-art contrastive learning method on the CIFAR-10 dataset with an NMI score of 0.772, a 7-8% improvement on the strong baseline.",0
"The issue of unsupervised disentangled representation learning has been a persistent challenge in computer vision. In this study, a fresh approach is presented for image clustering using deep embeddings, which combines instance-level contrastive learning with a deep embedding based cluster center predictor. Our method simultaneously learns representations and predicts cluster centers in an end-to-end manner, through a three-pronged approach that incorporates a clustering loss, an instance-wise contrastive loss, and an anchor loss. Our fundamental idea is that the combination of an ensemble loss that considers instance-level features and a clustering procedure focused on semantic similarity can facilitate better learning of representations in the latent space. Our results demonstrate that our technique performs remarkably well on well-known vision datasets, with standard clustering metrics such as Normalized Mutual Information (NMI), and yields geometrically well-separated cluster embeddings as determined by Euclidean distance. Our framework is on par with widely accepted clustering methods and surpasses the state-of-the-art contrastive learning method on the CIFAR-10 dataset, with an NMI score of 0.772, which is a 7-8% improvement over the strong baseline.",1
"Reinforcement learning methods have recently been very successful at performing complex sequential tasks like playing Atari games, Go and Poker. These algorithms have outperformed humans in several tasks by learning from scratch, using only scalar rewards obtained through interaction with their environment. While there certainly has been considerable independent innovation to produce such results, many core ideas in reinforcement learning are inspired by phenomena in animal learning, psychology and neuroscience. In this paper, we comprehensively review a large number of findings in both neuroscience and psychology that evidence reinforcement learning as a promising candidate for modeling learning and decision making in the brain. In doing so, we construct a mapping between various classes of modern RL algorithms and specific findings in both neurophysiological and behavioral literature. We then discuss the implications of this observed relationship between RL, neuroscience and psychology and its role in advancing research in both AI and brain science.",0
"Recently, reinforcement learning methods have achieved remarkable success in performing complex sequential tasks such as playing Atari games, Go, and Poker. These algorithms have even surpassed human performance by learning from scratch with the help of scalar rewards obtained from their environment. While independent innovation has certainly played a significant role in achieving these results, many fundamental ideas in reinforcement learning are inspired by phenomena observed in animal learning, psychology, and neuroscience. In this study, we conduct a comprehensive review of a multitude of findings in both psychology and neuroscience that demonstrate reinforcement learning as a promising model for learning and decision-making in the brain. By doing so, we establish a connection between various classes of modern RL algorithms and specific discoveries in both neurophysiological and behavioral literature. Finally, we discuss the implications of this observed relationship between RL, neuroscience, and psychology in advancing research in both artificial intelligence and brain science.",1
"Driving safely requires multiple capabilities from human and intelligent agents, such as the generalizability to unseen environments, the decision making in complex multi-agent settings, and the safety awareness of the surrounding traffic. Despite the great success of reinforcement learning, most of the RL research studies each capability separately due to the lack of the integrated interactive environments. In this work, we develop a new driving simulation platform called MetaDrive for the study of generalizable reinforcement learning algorithms. MetaDrive is highly compositional, which can generate an infinite number of diverse driving scenarios from both the procedural generation and the real traffic data replay. Based on MetaDrive, we construct a variety of RL tasks and baselines in both single-agent and multi-agent settings, including benchmarking generalizability across unseen scenes, safe exploration, and learning multi-agent traffic. We open-source this simulator and maintain its development at: https://github.com/decisionforce/metadrive",0
"To drive safely, a range of abilities are required from both humans and intelligent agents. These include the ability to adapt to new environments, navigate complex multi-agent situations, and remain aware of other traffic. However, despite the success of reinforcement learning, most research studies tend to focus on individual capabilities due to a lack of integrated interactive environments. In response to this, we have developed MetaDrive, a new driving simulation platform that allows for the study of generalizable reinforcement learning algorithms. MetaDrive is highly versatile and can generate an unlimited number of driving scenarios using both procedural generation and real traffic data. We have used MetaDrive to create a range of RL tasks and baselines for both single-agent and multi-agent settings, such as benchmarking generalizability, safe exploration, and learning multi-agent traffic. The simulator is open-source and can be accessed at: https://github.com/decisionforce/metadrive",1
"Robots could learn their own state and world representation from perception and experience without supervision. This desirable goal is the main focus of our field of interest, state representation learning (SRL). Indeed, a compact representation of such a state is beneficial to help robots grasp onto their environment for interacting. The properties of this representation have a strong impact on the adaptive capability of the agent. In this article we present an approach based on imitation learning. The idea is to train several policies that share the same representation to reproduce various demonstrations. To do so, we use a multi-head neural network with a shared state representation feeding a task-specific agent. If the demonstrations are diverse, the trained representation will eventually contain the information necessary for all tasks, while discarding irrelevant information. As such, it will potentially become a compact state representation useful for new tasks. We call this approach SRLfD (State Representation Learning from Demonstration). Our experiments confirm that when a controller takes SRLfD-based representations as input, it can achieve better performance than with other representation strategies and promote more efficient reinforcement learning (RL) than with an end-to-end RL strategy.",0
"In our field of interest, state representation learning (SRL), the main goal is for robots to learn their own state and world representation through perception and experience without supervision. This is desirable as a compact representation of the state can help robots interact with their environment. The adaptive capability of the agent is greatly influenced by the properties of this representation. In this article, we propose an approach based on imitation learning. This involves training several policies with a shared state representation to reproduce diverse demonstrations. Our multi-head neural network feeds a task-specific agent with the shared state representation. With SRLfD (State Representation Learning from Demonstration), the trained representation will eventually contain the necessary information for all tasks while discarding irrelevant information. This approach promotes more efficient reinforcement learning (RL) compared to an end-to-end RL strategy and achieves better performance when a controller takes SRLfD-based representations as input.",1
"Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions.",0
"Sparse-reward reinforcement learning poses a challenge for efficiently solving complex, multi-task problems. To achieve sample efficiency in multi-task learning, low-level policies must be reused and shared. To enable automatic task decomposition, we propose step-by-step human demonstrations in the form of natural language instructions and action trajectories. We present a dataset of such demonstrations in a crafting-based grid world and our model features a high-level language generator and low-level policy, conditioned on language. Our results demonstrate that human demonstrations aid in solving the most complex tasks and incorporating natural language enables the model to generalize to unseen tasks in a zero-shot setting and learn quickly from few demonstrations. Generalization is reflected in both the agent's actions and the generated natural language instructions for unseen tasks. Notably, our approach yields interpretable behaviors as the agent can generate a sequence of high-level descriptions of its actions.",1
"Reinforcement learning requires skillful definition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into reinforcement learning is a promising way to improve learning performance. In this paper, a comprehensive human guidance-based reinforcement learning framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the reinforcement learning process is proposed to boost the efficiency and performance of the reinforcement learning algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-arts suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.",0
"Solving optimization and control problems through reinforcement learning requires a skillful definition and significant computational effort, which may hinder its potential. However, incorporating human guidance into the reinforcement learning process can enhance learning performance. In this study, we establish a comprehensive framework for human-guided reinforcement learning. We propose a prioritized experience replay mechanism that adapts to human guidance to improve the efficiency and performance of the algorithm. To ease the burden on human participants, we develop a behavior model based on incremental online learning to mimic human actions. We evaluate the proposed algorithm through two challenging autonomous driving tasks and compare it against state-of-the-art methods. Our experiments demonstrate that our algorithm outperforms other methods in terms of learning efficiency, performance, and robustness.",1
"In cooperative multi-agent reinforcement learning (MARL), where agents only have access to partial observations, efficiently leveraging local information is critical. During long-time observations, agents can build \textit{awareness} for teammates to alleviate the problem of partial observability. However, previous MARL methods usually neglect this kind of utilization of local information. To address this problem, we propose a novel framework, multi-agent \textit{Local INformation Decomposition for Awareness of teammates} (LINDA), with which agents learn to decompose local information and build awareness for each teammate. We model the awareness as stochastic random variables and perform representation learning to ensure the informativeness of awareness representations by maximizing the mutual information between awareness and the actual trajectory of the corresponding agent. LINDA is agnostic to specific algorithms and can be flexibly integrated to different MARL methods. Sufficient experiments show that the proposed framework learns informative awareness from local partial observations for better collaboration and significantly improves the learning performance, especially on challenging tasks.",0
"Effective utilization of local information is crucial in cooperative multi-agent reinforcement learning (MARL) where agents have limited access to observations. The development of awareness amongst teammates can help alleviate partial observability issues during prolonged observations. Nevertheless, prior MARL approaches have largely overlooked this aspect. To tackle this problem, we present a novel framework, namely multi-agent Local INformation Decomposition for Awareness of teammates (LINDA), which enables agents to decompose local information and foster awareness for each teammate. We model awareness as stochastic random variables and leverage representation learning to ensure the informative nature of awareness representations by maximizing the mutual information between awareness and the actual trajectory of the corresponding agent. LINDA is algorithm-agnostic and can be seamlessly integrated into different MARL methods. Our extensive experiments demonstrate that LINDA enables agents to acquire informative awareness from local partial observations, thus enhancing collaboration, and significantly improving learning performance, particularly on challenging tasks.",1
"The rampant adoption of ML methodologies has revealed that models are usually adopted to make decisions without taking into account the uncertainties in their predictions. More critically, they can be vulnerable to adversarial examples. Thus, we believe that developing ML systems that take into account predictive uncertainties and are robust against adversarial examples is a must for critical, real-world tasks. We start with a case study in retailing. We propose a robust implementation of the Nerlove-Arrow model using a Bayesian structural time series model. Its Bayesian nature facilitates incorporating prior information reflecting the manager's views, which can be updated with relevant data. However, this case adopted classical Bayesian techniques, such as the Gibbs sampler. Nowadays, the ML landscape is pervaded with neural networks and this chapter also surveys current developments in this sub-field. Then, we tackle the problem of scaling Bayesian inference to complex models and large data regimes. In the first part, we propose a unifying view of two different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to improved and efficient novel sampling schemes. In the second part, we develop a framework to boost the efficiency of Bayesian inference in probabilistic models by embedding a Markov chain sampler within a variational posterior approximation. After that, we present an alternative perspective on adversarial classification based on adversarial risk analysis, and leveraging the scalable Bayesian approaches from chapter 2. In chapter 4 we turn to reinforcement learning, introducing Threatened Markov Decision Processes, showing the benefits of accounting for adversaries in RL while the agent learns.",0
"The widespread use of ML techniques has highlighted that models are often utilized to make decisions without considering the uncertainties in their forecasts, and they can be vulnerable to adversarial examples. Therefore, it is imperative to create ML systems that are resilient against adversarial examples and take into account predictive uncertainties for crucial real-world tasks. To illustrate this point, we present a case study in retailing and propose a robust implementation of the Nerlove-Arrow model utilizing a Bayesian structural time series model. We explore the current developments in the neural network sub-field and tackle the challenge of scaling Bayesian inference to complex models and large data regimes. We introduce novel sampling schemes and a framework to enhance the efficiency of Bayesian inference in probabilistic models. Additionally, we examine adversarial classification from a different perspective and leverage scalable Bayesian approaches. Finally, we delve into reinforcement learning and introduce Threatened Markov Decision Processes to demonstrate the benefits of accounting for adversaries while the agent learns.",1
"Neural architecture search (NAS) has achieved remarkable results in deep neural network design. Differentiable architecture search converts the search over discrete architectures into a hyperparameter optimization problem which can be solved by gradient descent. However, questions have been raised regarding the effectiveness and generalizability of gradient methods for solving non-convex architecture hyperparameter optimization problems. In this paper, we propose L$^{2}$NAS, which learns to intelligently optimize and update architecture hyperparameters via an actor neural network based on the distribution of high-performing architectures in the search history. We introduce a quantile-driven training procedure which efficiently trains L$^{2}$NAS in an actor-critic framework via continuous-action reinforcement learning. Experiments show that L$^{2}$NAS achieves state-of-the-art results on NAS-Bench-201 benchmark as well as DARTS search space and Once-for-All MobileNetV3 search space. We also show that search policies generated by L$^{2}$NAS are generalizable and transferable across different training datasets with minimal fine-tuning.",0
"Remarkable results in the design of deep neural networks have been achieved by Neural architecture search (NAS). A hyperparameter optimization problem can be solved by gradient descent through the conversion of the search over discrete architectures into a differentiable architecture search. However, the effectiveness and generalizability of gradient methods for solving non-convex architecture hyperparameter optimization problems have been questioned. In this paper, we propose L$^{2}$NAS, which utilizes an actor neural network to intelligently optimize and update architecture hyperparameters based on the distribution of high-performing architectures in the search history. L$^{2}$NAS is trained with a quantile-driven training procedure, which is efficient and utilizes continuous-action reinforcement learning in an actor-critic framework. The experiments demonstrate that L$^{2}$NAS produces state-of-the-art results in both the NAS-Bench-201 benchmark and the DARTS search space and Once-for-All MobileNetV3 search space. Furthermore, the search policies generated by L$^{2}$NAS are transferable and generalizable across various training datasets with minimal fine-tuning.",1
"The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation. We adopt this viewpoint and model the actor and critic interaction as a two-player general-sum game with a leader-follower structure known as a Stackelberg game. Given this abstraction, we propose a meta-framework for Stackelberg actor-critic algorithms where the leader player follows the total derivative of its objective instead of the usual individual gradient. From a theoretical standpoint, we develop a policy gradient theorem for the refined update and provide a local convergence guarantee for the Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an empirical standpoint, we demonstrate via simple examples that the learning dynamics we study mitigate cycling and accelerate convergence compared to the usual gradient dynamics given cost structures induced by actor-critic formulations. Finally, extensive experiments on OpenAI gym environments show that Stackelberg actor-critic algorithms always perform at least as well and often significantly outperform the standard actor-critic algorithm counterparts.",0
"Actor-critic based reinforcement learning algorithms exhibit a hierarchical interaction between the actor and critic that can be interpreted through game theory. To capture this structure, we model the interaction as a two-player general-sum game with a leader-follower structure called a Stackelberg game. Our proposed meta-framework for Stackelberg actor-critic algorithms involves the leader player following the total derivative of its objective instead of the individual gradient. We have developed a policy gradient theorem for the refined update and ensure local convergence to a local Stackelberg equilibrium. Through experiments on OpenAI gym environments, we observe that our learning dynamics mitigate cycling and accelerate convergence compared to the usual gradient dynamics. Additionally, our Stackelberg actor-critic algorithms always perform at least as well, and often significantly outperform, standard actor-critic algorithms.",1
"Integer programs provide a powerful abstraction for representing a wide range of real-world scheduling problems. Despite their ability to model general scheduling problems, solving large-scale integer programs (IP) remains a computational challenge in practice. The incorporation of more complex objectives such as robustness to disruptions further exacerbates the computational challenge. We present NICE (Neural network IP Coefficient Extraction), a novel technique that combines reinforcement learning and integer programming to tackle the problem of robust scheduling. More specifically, NICE uses reinforcement learning to approximately represent complex objectives in an integer programming formulation. We use NICE to determine assignments of pilots to a flight crew schedule so as to reduce the impact of disruptions. We compare NICE with (1) a baseline integer programming formulation that produces a feasible crew schedule, and (2) a robust integer programming formulation that explicitly tries to minimize the impact of disruptions. Our experiments show that, across a variety of scenarios, NICE produces schedules resulting in 33\% to 48\% fewer disruptions than the baseline formulation. Moreover, in more severely constrained scheduling scenarios in which the robust integer program fails to produce a schedule within 90 minutes, NICE is able to build robust schedules in less than 2 seconds on average.",0
"The use of integer programs is a valuable tool for representing scheduling problems in the real world. However, solving large-scale integer programs remains a computational challenge, especially when incorporating more complex objectives such as robustness to disruptions. To address this challenge, we introduce NICE, a technique that combines reinforcement learning and integer programming to tackle the problem of robust scheduling. NICE uses reinforcement learning to create an approximate representation of complex objectives in an integer programming formulation. We apply NICE to solve the problem of assigning pilots to a flight crew schedule, with the goal of reducing the impact of disruptions. Our experiments demonstrate that NICE outperforms both a baseline integer programming formulation and a robust integer programming formulation, resulting in 33\% to 48\% fewer disruptions across a range of scenarios. Additionally, NICE is able to produce robust schedules in under 2 seconds, even in severely constrained scheduling scenarios where the robust integer program fails to produce a schedule within 90 minutes.",1
"This paper provides an empirical evaluation of recently developed exploration algorithms within the Arcade Learning Environment (ALE). We study the use of different reward bonuses that incentives exploration in reinforcement learning. We do so by fixing the learning algorithm used and focusing only on the impact of the different exploration bonuses in the agent's performance. We use Rainbow, the state-of-the-art algorithm for value-based agents, and focus on some of the bonuses proposed in the last few years. We consider the impact these algorithms have on performance within the popular game Montezuma's Revenge which has gathered a lot of interest from the exploration community, across the the set of seven games identified by Bellemare et al. (2016) as challenging for exploration, and easier games where exploration is not an issue. We find that, in our setting, recently developed bonuses do not provide significantly improved performance on Montezuma's Revenge or hard exploration games. We also find that existing bonus-based methods may negatively impact performance on games in which exploration is not an issue and may even perform worse than $\epsilon$-greedy exploration.",0
"This article presents an empirical assessment of exploration algorithms developed for the Arcade Learning Environment (ALE). We examine various reward bonuses that encourage exploration in reinforcement learning, while keeping the learning algorithm constant and concentrating solely on the influence of exploration bonuses on the agent's performance. Our research employs Rainbow, the leading algorithm for value-based agents, and examines some of the bonuses introduced in recent years. We evaluate the impact of these algorithms on the performance of Montezuma's Revenge, a popular game of interest to the exploration community, as well as seven games identified as challenging for exploration by Bellemare et al. (2016), and simpler games where exploration is not a problem. We conclude that the newly developed bonuses do not significantly enhance performance on Montezuma's Revenge or difficult exploration games. Moreover, we discovered that existing bonus-based methods may have a negative effect on games where exploration is not a concern and may even perform worse than $\epsilon$-greedy exploration.",1
"This paper proposes a paradigm shift for affective computing by viewing the affect modeling task as a reinforcement learning process. According to our proposed framework the context (environment) and the actions of an agent define the common representation that interweaves behavior and affect. To realise this framework we build on recent advances in reinforcement learning and use a modified version of the Go-Explore algorithm which has showcased supreme performance in hard exploration tasks. In this initial study, we test our framework in an arcade game by training Go-Explore agents to both play optimally and attempt to mimic human demonstrations of arousal. We vary the degree of importance between optimal play and arousal imitation and create agents that can effectively display a palette of affect and behavioral patterns. Our Go-Explore implementation not only introduces a new paradigm for affect modeling; it empowers believable AI-based game testing by providing agents that can blend and express a multitude of behavioral and affective patterns.",0
"The objective of this paper is to change the traditional approach to affective computing and instead suggest a reinforcement learning process for affect modeling. Our framework proposes that the environment and actions of an agent aid in creating a common representation that blends behavior and affect. We utilize recent developments in reinforcement learning and modify the Go-Explore algorithm to realize our proposed framework. Our study involves training Go-Explore agents to play an arcade game optimally while mimicking human arousal demonstrations. By varying the importance of optimal play and arousal imitation, we create agents that can display a range of affect and behavioral patterns. Our Go-Explore implementation not only introduces a new paradigm for affect modeling but also enables more believable AI-based game testing through the expression of diverse affective and behavioral patterns.",1
"Volt-var control (VVC) is the problem of operating power distribution systems within healthy regimes by controlling actuators in power systems. Existing works have mostly adopted the conventional routine of representing the power systems (a graph with tree topology) as vectors to train deep reinforcement learning (RL) policies. We propose a framework that combines RL with graph neural networks and study the benefits and limitations of graph-based policy in the VVC setting. Our results show that graph-based policies converge to the same rewards asymptotically however at a slower rate when compared to vector representation counterpart. We conduct further analysis on the impact of both observations and actions: on the observation end, we examine the robustness of graph-based policy on two typical data acquisition errors in power systems, namely sensor communication failure and measurement misalignment. On the action end, we show that actuators have various impacts on the system, thus using a graph representation induced by power systems topology may not be the optimal choice. In the end, we conduct a case study to demonstrate that the choice of readout function architecture and graph augmentation can further improve training performance and robustness.",0
"The issue of Volt-var control (VVC) involves the operation of power distribution systems in a healthy manner by manipulating actuators in power systems. Current research mainly employs the conventional method of representing power systems as vectors in a graph with tree topology to train deep reinforcement learning (RL) policies. Our proposed framework combines RL with graph neural networks and explores the advantages and drawbacks of a graph-based policy in the VVC scenario. Our findings reveal that graph-based policies ultimately achieve the same rewards as vector representation policies, although at a slower pace. We further analyze the effects of observations and actions. On the observation front, we test the robustness of graph-based policies against common data acquisition errors in power systems, such as sensor communication failure and measurement misalignment. On the action front, we illustrate that actuators have diverse impacts on the system, making a graph representation based on power systems topology less than ideal. Lastly, we conduct a case study that demonstrates how the choice of readout function architecture and graph augmentation can enhance training performance and robustness.",1
"The framework of deep reinforcement learning (DRL) provides a powerful and widely applicable mathematical formalization for sequential decision-making. In this paper, we start from studying the f-divergence between learning policy and sampling policy and derive a novel DRL framework, termed f-Divergence Reinforcement Learning (FRL). We highlight that the policy evaluation and policy improvement phases are induced by minimizing f-divergence between learning policy and sampling policy, which is distinct from the conventional DRL algorithm objective that maximizes the expected cumulative rewards. Besides, we convert this framework to a saddle-point optimization problem with a specific f function through Fenchel conjugate, which consists of policy evaluation and policy improvement. Then we derive new policy evaluation and policy improvement methods in FRL. Our framework may give new insights for analyzing DRL algorithms. The FRL framework achieves two advantages: (1) policy evaluation and policy improvement processes are derived simultaneously by f-divergence; (2) overestimation issue of value function are alleviated. To evaluate the effectiveness of the FRL framework, we conduct experiments on Atari 2600 video games, which show that our framework matches or surpasses the DRL algorithms we tested.",0
"In this paper, we introduce a novel deep reinforcement learning (DRL) framework called f-Divergence Reinforcement Learning (FRL), which is based on the f-divergence between learning policy and sampling policy. This approach differs from conventional DRL algorithms that maximize expected cumulative rewards. By using Fenchel conjugate, we convert the framework to a saddle-point optimization problem and derive new policy evaluation and policy improvement methods. Our framework has two key advantages: (1) simultaneous derivation of policy evaluation and policy improvement processes through f-divergence; and (2) alleviation of the overestimation issue of value function. We evaluate the effectiveness of FRL on Atari 2600 video games and demonstrate that it matches or surpasses the performance of tested DRL algorithms. This framework provides new insights for analyzing DRL algorithms.",1
"We propose a novel deep reinforcement learning-based approach for 3D object reconstruction from monocular images. Prior works that use mesh representations are template based. Thus, they are limited to the reconstruction of objects that have the same topology as the template. Methods that use volumetric grids as intermediate representations are computationally expensive, which limits their application in real-time scenarios. In this paper, we propose a novel end-to-end method that reconstructs 3D objects of arbitrary topology from a monocular image. It is composed of of (1) a Vertex Generation Network (VGN), which predicts the initial 3D locations of the object's vertices from an input RGB image, (2) a differentiable triangulation layer, which learns in a non-supervised manner, using a novel reinforcement learning algorithm, the best triangulation of the object's vertices, and finally, (3) a hierarchical mesh refinement network that uses graph convolutions to refine the initial mesh. Our key contribution is the learnable triangulation process, which recovers in an unsupervised manner the topology of the input shape. Our experiments on ShapeNet and Pix3D benchmarks show that the proposed method outperforms the state-of-the-art in terms of visual quality, reconstruction accuracy, and computational time.",0
"We present an innovative approach to 3D object reconstruction from monocular images using deep reinforcement learning. Previous works using mesh representations were limited to objects with the same topology as the template, while those using volumetric grids were computationally expensive for real-time applications. Our proposed end-to-end method reconstructs 3D objects of any topology from a monocular image, consisting of a Vertex Generation Network (VGN) for predicting initial 3D locations, a differentiable triangulation layer that learns the best triangulation of vertices via a novel reinforcement learning algorithm, and a hierarchical mesh refinement network that uses graph convolutions to refine the mesh. Our significant contribution is the unsupervised learnable triangulation process that recovers the input shape's topology. Results from ShapeNet and Pix3D benchmarks show that our method outperforms state-of-the-art approaches in terms of visual quality, reconstruction accuracy, and computational time.",1
"An informative measurement is the most efficient way to gain information about an unknown state. We give a first-principles derivation of a general-purpose dynamic programming algorithm that returns a sequence of informative measurements by sequentially maximizing the entropy of possible measurement outcomes. This algorithm can be used by an autonomous agent or robot to decide where best to measure next, planning a path corresponding to an optimal sequence of informative measurements. This algorithm is applicable to states and controls that are continuous or discrete, and agent dynamics that is either stochastic or deterministic; including Markov decision processes. Recent results from approximate dynamic programming and reinforcement learning, including on-line approximations such as rollout and Monte Carlo tree search, allow an agent or robot to solve the measurement task in real-time. The resulting near-optimal solutions include non-myopic paths and measurement sequences that can generally outperform, sometimes substantially, commonly-used greedy heuristics such as maximizing the entropy of each measurement outcome. This is demonstrated for a global search problem, where on-line planning with an extended local search is found to reduce the number of measurements in the search by half.",0
"To obtain information about an unknown state, an informative measurement is the most efficient method. We present a first-principles explanation of a general dynamic programming algorithm that generates a series of informative measurements by maximizing the entropy of likely measurement outcomes in sequence. This technique can be employed by an autonomous agent or robot to determine the best location to measure next and plan a route that corresponds to an optimal sequence of informative measurements. This algorithm is applicable to continuous or discrete states and controls, as well as stochastic or deterministic agent dynamics, such as Markov decision processes. Recent breakthroughs in approximate dynamic programming and reinforcement learning, including on-line approximations such as rollout and Monte Carlo tree search, enable an agent or robot to solve the measurement task in real-time. The resulting solutions are close to optimal and include non-myopic paths and measurement sequences that can outperform commonly used greedy heuristics, such as maximizing the entropy of each measurement outcome. This is demonstrated in a global search problem, where on-line planning with an extended local search reduces the number of measurements in the search by half.",1
"In the Bayesian reinforcement learning (RL) setting, a prior distribution over the unknown problem parameters -- the rewards and transitions -- is assumed, and a policy that optimizes the (posterior) expected return is sought. A common approximation, which has been recently popularized as meta-RL, is to train the agent on a sample of $N$ problem instances from the prior, with the hope that for large enough $N$, good generalization behavior to an unseen test instance will be obtained. In this work, we study generalization in Bayesian RL under the probably approximately correct (PAC) framework, using the method of algorithmic stability. Our main contribution is showing that by adding regularization, the optimal policy becomes stable in an appropriate sense. Most stability results in the literature build on strong convexity of the regularized loss -- an approach that is not suitable for RL as Markov decision processes (MDPs) are not convex. Instead, building on recent results of fast convergence rates for mirror descent in regularized MDPs, we show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability. This result, which may be of independent interest, allows us to study the effect of regularization on generalization in the Bayesian RL setting.",0
"The Bayesian reinforcement learning (RL) approach assumes a prior distribution over the unknown parameters of the problem, such as rewards and transitions, and aims to find a policy that maximizes the expected return. To achieve this, a popular method called meta-RL trains the agent on a sample of N instances from the prior, hoping that it will generalize well to new test cases. In this study, we explore generalization in Bayesian RL using the probably approximately correct (PAC) framework and algorithmic stability. Our main finding is that adding regularization can stabilize the optimal policy, which is not typically possible in RL due to the non-convex nature of Markov decision processes. Despite this, we demonstrate that regularized MDPs satisfy a quadratic growth criterion, leading to stable policies and allowing us to examine the impact of regularization on generalization in Bayesian RL.",1
"Approximation of the value functions in value-based deep reinforcement learning systems induces overestimation bias, resulting in suboptimal policies. We show that when the reinforcement signals received by the agents have a high variance, deep actor-critic approaches that overcome the overestimation bias lead to a substantial underestimation bias. We introduce a parameter-free, novel deep Q-learning variant to reduce this underestimation bias for continuous control. By obtaining fixed weights in computing the critic objective as a linear combination of the approximate critic functions, our Q-value update rule integrates the concepts of Clipped Double Q-learning and Maxmin Q-learning. We test the performance of our improvement on a set of MuJoCo and Box2D continuous control tasks and find that it improves the state-of-the-art and outperforms the baseline algorithms in the majority of the environments.",0
"Value-based deep reinforcement learning systems that use approximation of value functions can lead to suboptimal policies due to overestimation bias. However, when agents receive reinforcement signals with high variance, using deep actor-critic approaches to overcome overestimation bias can result in significant underestimation bias. In this study, we propose a novel deep Q-learning variant that reduces this underestimation bias for continuous control without requiring any parameters. Our approach uses fixed weights in computing the critic objective as a linear combination of the approximate critic functions, integrating Clipped Double Q-learning and Maxmin Q-learning concepts into our Q-value update rule. Our proposed method is evaluated on various MuJoCo and Box2D continuous control tasks and found to outperform baseline algorithms and improve the state-of-the-art in the majority of the environments.",1
"Soft Actor-Critic (SAC) is an off-policy actor-critic reinforcement learning algorithm, essentially based on entropy regularization. SAC trains a policy by maximizing the trade-off between expected return and entropy (randomness in the policy). It has achieved state-of-the-art performance on a range of continuous-control benchmark tasks, outperforming prior on-policy and off-policy methods. SAC works in an off-policy fashion where data are sampled uniformly from past experiences (stored in a buffer) using which parameters of the policy and value function networks are updated. We propose certain crucial modifications for boosting the performance of SAC and make it more sample efficient. In our proposed improved SAC, we firstly introduce a new prioritization scheme for selecting better samples from the experience replay buffer. Secondly we use a mixture of the prioritized off-policy data with the latest on-policy data for training the policy and the value function networks. We compare our approach with the vanilla SAC and some recent variants of SAC and show that our approach outperforms the said algorithmic benchmarks. It is comparatively more stable and sample efficient when tested on a number of continuous control tasks in MuJoCo environments.",0
"Soft Actor-Critic (SAC) is a reinforcement learning algorithm that operates off-policy and is based on entropy regularization. The algorithm maximizes the balance between expected return and policy randomness to train a policy. SAC has performed exceptionally well on continuous-control benchmark tasks, surpassing both on-policy and off-policy methods. SAC's off-policy approach involves uniformly sampling data from past experiences stored in a buffer to update the policy and value function networks. Our proposed modifications aim to improve SAC's performance and sample efficiency. Specifically, we introduce a prioritization scheme for selecting superior samples from the experience replay buffer and use a combination of prioritized off-policy data and the latest on-policy data to train the networks. Our approach outperforms vanilla SAC and other recent variants on various continuous control tasks in MuJoCo environments. Furthermore, it is more stable and sample efficient.",1
"Trip recommender system, which targets at recommending a trip consisting of several ordered Points of Interest (POIs), has long been treated as an important application for many location-based services. Currently, most prior arts generate trips following pre-defined objectives based on constraint programming, which may fail to reflect the complex latent patterns hidden in the human mobility data. And most of these methods are usually difficult to respond in real time when the number of POIs is large. To that end, we propose an Adversarial Neural Trip Recommendation (ANT) framework to tackle the above challenges. First of all, we devise a novel attention-based encoder-decoder trip generator that can learn the correlations among POIs and generate well-designed trips under given constraints. Another novelty of ANT relies on an adversarial learning strategy integrating with reinforcement learning to guide the trip generator to produce high-quality trips. For this purpose, we introduce a discriminator, which distinguishes the generated trips from real-life trips taken by users, to provide reward signals to optimize the generator. Moreover, we devise a novel pre-train schema based on learning from demonstration, which speeds up the convergence to achieve a sufficient-and-efficient training process. Extensive experiments on four real-world datasets validate the effectiveness and efficiency of our proposed ANT framework, which demonstrates that ANT could remarkably outperform the state-of-the-art baselines with short response time.",0
"The recommendation of trips consisting of multiple Points of Interest (POIs) in a specific order is a crucial application for location-based services. However, current methods that generate trips using constraint programming fail to reflect complex latent patterns in human mobility data. Additionally, these techniques may not respond in real-time when the number of POIs is high. To address these challenges, we introduce the Adversarial Neural Trip Recommendation (ANT) framework. This approach employs an attention-based encoder-decoder trip generator to learn POI correlations and produce well-designed trips under given constraints. Furthermore, ANT employs an adversarial learning strategy with reinforcement learning to generate high-quality trips. This is achieved by using a discriminator to differentiate between generated and real-life trips, providing reward signals to optimize the generator. Additionally, we introduce a pre-training schema based on learning from demonstration to speed up convergence and achieve efficient training. Our experiments on four real-world datasets demonstrate that ANT significantly outperforms state-of-the-art baselines with short response times, verifying its effectiveness and efficiency.",1
"Cooperative multi-agent reinforcement learning is a decentralized paradigm in sequential decision making where agents distributed over a network iteratively collaborate with neighbors to maximize global (network-wide) notions of rewards. Exact computations typically involve a complexity that scales exponentially with the number of agents. To address this curse of dimensionality, we design a scalable algorithm based on the Natural Policy Gradient framework that uses local information and only requires agents to communicate with neighbors within a certain range. Under standard assumptions on the spatial decay of correlations for the transition dynamics of the underlying Markov process and the localized learning policy, we show that our algorithm converges to the globally optimal policy with a dimension-free statistical and computational complexity, incurring a localization error that does not depend on the number of agents and converges to zero exponentially fast as a function of the range of communication.",0
"In the field of sequential decision making, cooperative multi-agent reinforcement learning involves agents working together over a network to achieve overall rewards. However, the complexity of exact computations increases exponentially with the number of agents, making it challenging to implement. To overcome this, we have developed a scalable algorithm that utilizes local information and only requires communication between neighboring agents within a specific range. By assuming standard spatial decay of correlations and localized learning policies, our algorithm can converge to the globally optimal policy, with no statistical or computational complexity dependent on the number of agents. Although there may be a localization error, this reduces exponentially as the range of communication increases.",1
"In this paper, to reduce the congestion rate at the city center and increase the quality of experience (QoE) of each user, the framework of long-range autonomous valet parking (LAVP) is presented, where an Electric Autonomous Vehicle (EAV) is deployed in the city, which can pick up, drop off users at their required spots, and then drive to the car park out of city center autonomously. In this framework, we aim to minimize the overall distance of the EAV, while guarantee all users are served, i.e., picking up, and dropping off users at their required spots through optimizing the path planning of the EAV and number of serving time slots. To this end, we first propose a learning based algorithm, which is named as Double-Layer Ant Colony Optimization (DL-ACO) algorithm to solve the above problem in an iterative way. Then, to make the real-time decision, while consider the dynamic environment (i.e., the EAV may pick up and drop off users from different locations), we further present a deep reinforcement learning (DRL) based algorithm, which is known as deep Q network (DQN). The experimental results show that the DL-ACO and DQN-based algorithms both achieve the considerable performance.",0
This paper introduces the Long-Range Autonomous Valet Parking (LAVP) framework to alleviate city center congestion and enhance the quality of user experience (QoE). The framework employs an Electric Autonomous Vehicle (EAV) that can fetch and drop off users at designated locations and park itself outside the city center. The objective of the framework is to minimize the EAV's overall distance while ensuring that all users are served. This is achieved by optimizing the EAV's path planning and the number of serving time slots. The paper proposes a Double-Layer Ant Colony Optimization (DL-ACO) algorithm to solve the problem iteratively and a Deep Q Network (DQN) algorithm for real-time decision making in a dynamic environment. The experimental results demonstrate that both the DL-ACO and DQN-based algorithms perform well.,1
"Offline reinforcement learning (RL) enables learning control policies by utilizing only prior experience, without any online interaction. This can allow robots to acquire generalizable skills from large and diverse datasets, without any costly or unsafe online data collection. Despite recent algorithmic advances in offline RL, applying these methods to real-world problems has proven challenging. Although offline RL methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. In this paper, our aim is to develop a practical workflow for using offline RL analogous to the relatively well-understood workflows for supervised learning problems. To this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. Our workflow is derived from a conceptual understanding of the behavior of conservative offline RL algorithms and cross-validation in supervised learning. We demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards. Explanatory video and additional results can be found at sites.google.com/view/offline-rl-workflow",0
"Learning control policies through offline reinforcement learning (RL) involves utilizing prior experience without the need for online interaction. This approach can provide robots with generalizable skills from diverse and sizable datasets while avoiding costly or hazardous online data collection. Despite recent algorithmic advancements, applying offline RL to real-world problems remains challenging. While offline RL methods can learn from previous data, determining various design choices, from model architecture to algorithm hyperparameters, without online policy evaluation can be ambiguous. In this study, we aim to create a practical workflow for offline RL comparable to supervised learning workflows. We develop metrics and conditions that allow practitioners to track offline training progress and adjust the algorithm and model architecture to improve final performance. Our workflow is based on conservative offline RL algorithm behavior and cross-validation in supervised learning. We demonstrate its effectiveness in generating successful policies without online tuning in simulated robotic learning scenarios, as well as manipulative skill learning tasks with raw image observations and sparse binary rewards on two distinct real robots. Additional results and an explanatory video can be found at sites.google.com/view/offline-rl-workflow.",1
"In many real-world scenarios, the utility of a user is derived from the single execution of a policy. In this case, to apply multi-objective reinforcement learning, the expected utility of the returns must be optimised. Various scenarios exist where a user's preferences over objectives (also known as the utility function) are unknown or difficult to specify. In such scenarios, a set of optimal policies must be learned. However, settings where the expected utility must be maximised have been largely overlooked by the multi-objective reinforcement learning community and, as a consequence, a set of optimal solutions has yet to be defined. In this paper we address this challenge by proposing first-order stochastic dominance as a criterion to build solution sets to maximise expected utility. We also propose a new dominance criterion, known as expected scalarised returns (ESR) dominance, that extends first-order stochastic dominance to allow a set of optimal policies to be learned in practice. We then define a new solution concept called the ESR set, which is a set of policies that are ESR dominant. Finally, we define a new multi-objective distributional tabular reinforcement learning (MOT-DRL) algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.",0
"The usefulness of a user is often based on a single policy in real-life situations. To implement multi-objective reinforcement learning in these cases, the expected utility of the returns must be optimized. There are instances where a user's preferences for objectives, also called the utility function, are unknown or challenging to specify, requiring the learning of a set of optimal policies. However, the multi-objective reinforcement learning community has mostly ignored settings where the expected utility must be maximized, resulting in an undefined set of optimal solutions. To tackle this challenge, we suggest using first-order stochastic dominance as a criterion to construct solution sets that maximize expected utility. We also introduce a new dominance criterion, ESR dominance, which expands first-order stochastic dominance to enable the learning of a practical set of optimal policies. We then define the ESR set, which is a collection of policies that are ESR dominant, as a new solution concept. Lastly, we develop a new MOT-DRL algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.",1
"In value-based deep reinforcement learning methods, approximation of value functions induces overestimation bias and leads to suboptimal policies. We show that in deep actor-critic methods that aim to overcome the overestimation bias, if the reinforcement signals received by the agent have a high variance, a significant underestimation bias arises. To minimize the underestimation, we introduce a parameter-free, novel deep Q-learning variant. Our Q-value update rule combines the notions behind Clipped Double Q-learning and Maxmin Q-learning by computing the critic objective through the nested combination of maximum and minimum operators to bound the approximate value estimates. We evaluate our modification on the suite of several OpenAI Gym continuous control tasks, improving the state-of-the-art in every environment tested.",0
"When using value-based deep reinforcement learning, approximating value functions can cause overestimation bias and result in suboptimal policies. However, deep actor-critic methods attempt to overcome this bias, but if the agent's reinforcement signals have a high variance, it can lead to underestimation bias. To address this issue, we propose a new deep Q-learning approach that does not require any parameters. Our Q-value update rule is a combination of Clipped Double Q-learning and Maxmin Q-learning, using maximum and minimum operators to limit the approximate value estimates. We tested our technique on multiple OpenAI Gym continuous control tasks and achieved better results than the current state-of-the-art in all environments.",1
"Point cloud registration plays a critical role in a multitude of computer vision tasks, such as pose estimation and 3D localization. Recently, a plethora of deep learning methods were formulated that aim to tackle this problem. Most of these approaches find point or feature correspondences, from which the transformations are computed. We give a different perspective and frame the registration problem as a Markov Decision Process. Instead of directly searching for the transformation, the problem becomes one of finding a sequence of translation and rotation actions that is equivalent to this transformation. To this end, we propose an artificial agent trained end-to-end using deep supervised learning. In contrast to conventional reinforcement learning techniques, the observations are sampled i.i.d. and thus no experience replay buffer is required, resulting in a more streamlined training process. Experiments on ModelNet40 show results comparable or superior to the state of the art in the case of clean, noisy and partially visible datasets.",0
"Point cloud registration is crucial for various computer vision tasks, including 3D localization and pose estimation. Recent developments in deep learning have led to several methods for solving this problem. These approaches typically identify point or feature matches to compute transformations. Our approach, however, views the registration issue as a Markov Decision Process. Rather than searching for the transformation directly, we seek a sequence of translation and rotation actions that is equivalent to the transformation. We propose an end-to-end trained artificial agent using deep supervised learning to accomplish this. Unlike traditional reinforcement learning methods, our observations are sampled i.i.d., eliminating the need for an experience replay buffer and simplifying the training process. Our experiments on ModelNet40 demonstrate comparable or better results than state-of-the-art approaches for clean, noisy, and partially visible datasets.",1
"Methods to learn under algorithmic triage have predominantly focused on supervised learning settings where each decision, or prediction, is independent of each other. Under algorithmic triage, a supervised learning model predicts a fraction of the instances and humans predict the remaining ones. In this work, we take a first step towards developing reinforcement learning models that are optimized to operate under algorithmic triage. To this end, we look at the problem through the framework of options and develop a two-stage actor-critic method to learn reinforcement learning models under triage. The first stage performs offline, off-policy training using human data gathered in an environment where the human has operated on their own. The second stage performs on-policy training to account for the impact that switching may have on the human policy, which may be difficult to anticipate from the above human data. Extensive simulation experiments in a synthetic car driving task show that the machine models and the triage policies trained using our two-stage method effectively complement human policies and outperform those provided by several competitive baselines.",0
"Previous approaches to learning in the context of algorithmic triage have mainly focused on supervised learning, where individual decisions or predictions are made independently. In algorithmic triage, a supervised learning model predicts some instances, while humans predict the rest. This research aims to develop reinforcement learning models that are optimized for algorithmic triage. To achieve this goal, we employ a two-stage actor-critic approach based on the options framework. In the first stage, we perform offline, off-policy training using human data collected in an environment where the human is working independently. In the second stage, we conduct on-policy training to account for the impact of switching on the human policy, which is difficult to predict using the above-mentioned human data. Extensive simulation experiments using a synthetic car driving task demonstrate that our two-stage method effectively complements human policies and outperforms several competitive baselines in terms of machine models and triage policies trained.",1
"Previous AutoML pruning works utilized individual layer features to automatically prune filters. We analyze the correlation for two layers from the different blocks which have a short-cut structure. It shows that, in one block, the deeper layer has many redundant filters which can be represented by filters in the former layer. So, it is necessary to take information from other layers into consideration in pruning. In this paper, a novel pruning method, named GraphPruning, is proposed. Any series of the network is viewed as a graph. To automatically aggregate neighboring features for each node, a graph aggregator based on graph convolution networks(GCN) is designed. In the training stage, a PruningNet that is given aggregated node features generates reasonable weights for any size of the sub-network. Subsequently, the best configuration of the Pruned Network is searched by reinforcement learning. Different from previous work, we take the node features from a well-trained graph aggregator instead of the hand-craft features, as the states in reinforcement learning. Compared with other AutoML pruning works, our method has achieved the state-of-the-art under the same conditions on ImageNet-2012.",0
"Prior research on AutoML pruning focused on using individual layer features to automatically prune filters. Our study, however, examines the correlation between two layers from different blocks that have a shortcut structure. Our analysis reveals that the deeper layer in one block has many redundant filters that can be represented by filters in the former layer. Therefore, it is important to consider information from other layers when pruning. This paper proposes a novel pruning method, called GraphPruning, which views any series of the network as a graph. To automatically aggregate neighboring features for each node, we designed a graph aggregator based on graph convolution networks (GCN). In the training stage, a PruningNet generates reasonable weights for any size of the sub-network by utilizing the aggregated node features. Subsequently, we search for the best configuration of the Pruned Network through reinforcement learning. Unlike prior work, our method leverages node features from a well-trained graph aggregator, rather than hand-crafted features, as the states in reinforcement learning. Our experimental results demonstrate that our method outperforms other AutoML pruning approaches under the same conditions on ImageNet-2012.",1
"Guideline-based treatment for sepsis and septic shock is difficult because sepsis is a disparate range of life-threatening organ dysfunctions whose pathophysiology is not fully understood. Early intervention in sepsis is crucial for patient outcome, yet those interventions have adverse effects and are frequently overadministered. Greater personalization is necessary, as no single action is suitable for all patients. We present a novel application of reinforcement learning in which we identify optimal recommendations for sepsis treatment from data, estimate their confidence level, and identify treatment options infrequently observed in training data. Rather than a single recommendation, our method can present several treatment options. We examine learned policies and discover that reinforcement learning is biased against aggressive intervention due to the confounding relationship between mortality and level of treatment received. We mitigate this bias using subspace learning, and develop methodology that can yield more accurate learning policies across healthcare applications.",0
"The treatment of sepsis and septic shock according to guidelines is challenging due to the diverse range of life-threatening organ dysfunctions associated with sepsis, and the incomplete understanding of its pathophysiology. Although early intervention is crucial for successful patient outcome, the interventions available may have negative effects and may be overused. Personalized treatment options are required as there is no one-size-fits-all approach. In this study, we introduce a new approach using reinforcement learning to identify optimal treatment recommendations for sepsis. Our method considers data to estimate the confidence level of the recommendations and also identifies treatment options that are rarely observed in training data. Instead of a single recommendation, our approach provides multiple treatment options. However, we found that our reinforcement learning approach is biased towards less aggressive intervention due to the correlation between mortality and treatment level. To address this, we use subspace learning and develop a methodology that can provide more accurate learning policies for healthcare applications.",1
"Research on exploration in reinforcement learning, as applied to Atari 2600 game-playing, has emphasized tackling difficult exploration problems such as Montezuma's Revenge (Bellemare et al., 2016). Recently, bonus-based exploration methods, which explore by augmenting the environment reward, have reached above-human average performance on such domains. In this paper we reassess popular bonus-based exploration methods within a common evaluation framework. We combine Rainbow (Hessel et al., 2018) with different exploration bonuses and evaluate its performance on Montezuma's Revenge, Bellemare et al.'s set of hard of exploration games with sparse rewards, and the whole Atari 2600 suite. We find that while exploration bonuses lead to higher score on Montezuma's Revenge they do not provide meaningful gains over the simpler $\epsilon$-greedy scheme. In fact, we find that methods that perform best on that game often underperform $\epsilon$-greedy on easy exploration Atari 2600 games. We find that our conclusions remain valid even when hyperparameters are tuned for these easy-exploration games. Finally, we find that none of the methods surveyed benefit from additional training samples (1 billion frames, versus Rainbow's 200 million) on Bellemare et al.'s hard exploration games. Our results suggest that recent gains in Montezuma's Revenge may be better attributed to architecture change, rather than better exploration schemes; and that the real pace of progress in exploration research for Atari 2600 games may have been obfuscated by good results on a single domain.",0
"Exploration in reinforcement learning research has focused on addressing complex exploration challenges like Montezuma's Revenge in Atari 2600 game-playing (Bellemare et al., 2016). Bonus-based exploration techniques have recently been developed, which augment the environment reward to explore, and have achieved above-average performance compared to humans in such domains. This study re-evaluates popular bonus-based exploration methods using a unified assessment framework. Rainbow (Hessel et al., 2018) is combined with various exploration bonuses and tested on Montezuma's Revenge, Bellemare et al.'s challenging exploration games with low rewards, and the entire Atari 2600 suite. The findings reveal that while exploration bonuses result in higher scores on Montezuma's Revenge, they do not offer significant advantages over the simpler $\epsilon$-greedy approach. Interestingly, the best-performing methods on Montezuma's Revenge often underperform $\epsilon$-greedy on straightforward exploration Atari 2600 games. These conclusions remain valid even when hyperparameters are optimized for easy-exploration games. Furthermore, none of the methods benefit from additional training samples on Bellemare et al.'s hard exploration games. The results suggest that recent progress in Montezuma's Revenge can be attributed to architectural changes rather than enhanced exploration techniques. Additionally, the study indicates that the progress in exploration research for Atari 2600 games may have been concealed by the good results on a single domain.",1
"Offline reinforcement learning (RL), also known as batch RL, offers the prospect of policy optimization from large pre-recorded datasets without online environment interaction. It addresses challenges with regard to the cost of data collection and safety, both of which are particularly pertinent to real-world applications of RL. Unfortunately, most off-policy algorithms perform poorly when learning from a fixed dataset. In this paper, we propose a novel offline RL algorithm to learn policies from data using a form of critic-regularized regression (CRR). We find that CRR performs surprisingly well and scales to tasks with high-dimensional state and action spaces -- outperforming several state-of-the-art offline RL algorithms by a significant margin on a wide range of benchmark tasks.",0
"Batch RL, also called offline RL, presents an opportunity to optimize policies using pre-recorded datasets without online interaction with the environment. This approach addresses the issue of data collection costs and safety concerns, which are especially relevant to RL applications in the real world. However, most off-policy algorithms do not perform well when learning from a fixed dataset. This paper introduces a new offline RL algorithm that utilizes critic-regularized regression (CRR) to learn policies from data. The results show that CRR performs exceptionally well and can handle high-dimensional state and action spaces, outperforming several state-of-the-art offline RL algorithms by a significant margin on various benchmark tasks.",1
"Meta reinforcement learning (RL) attempts to discover new RL algorithms automatically from environment interaction. In so-called black-box approaches, the policy and the learning algorithm are jointly represented by a single neural network. These methods are very flexible, but they tend to underperform in terms of generalisation to new, unseen environments. In this paper, we explore the role of symmetries in meta-generalisation. We show that a recent successful meta RL approach that meta-learns an objective for backpropagation-based learning exhibits certain symmetries (specifically the reuse of the learning rule, and invariance to input and output permutations) that are not present in typical black-box meta RL systems. We hypothesise that these symmetries can play an important role in meta-generalisation. Building off recent work in black-box supervised meta learning, we develop a black-box meta RL system that exhibits these same symmetries. We show through careful experimentation that incorporating these symmetries can lead to algorithms with a greater ability to generalise to unseen action & observation spaces, tasks, and environments.",0
"The aim of meta reinforcement learning (RL) is to automatically discover new RL algorithms through interaction with the environment. Black-box approaches use a single neural network to represent the policy and learning algorithm, making them highly adaptable, but they often struggle to generalize to new and unfamiliar environments. This study investigates the significance of symmetries in meta-generalization and proposes that certain symmetries, such as the reuse of the learning rule and invariance to input and output permutations, can enhance meta-generalization. Inspired by recent research in black-box supervised meta learning, the authors develop a black-box meta RL system that incorporates these symmetries and demonstrate through rigorous experimentation that these symmetries can improve the algorithm's ability to generalize to new action and observation spaces, tasks, and environments.",1
"We consider the joint design and control of discrete-time stochastic dynamical systems over a finite time horizon. We formulate the problem as a multi-step optimization problem under uncertainty seeking to identify a system design and a control policy that jointly maximize the expected sum of rewards collected over the time horizon considered. The transition function, the reward function and the policy are all parametrized, assumed known and differentiable with respect to their parameters. We then introduce a deep reinforcement learning algorithm combining policy gradient methods with model-based optimization techniques to solve this problem. In essence, our algorithm iteratively approximates the gradient of the expected return via Monte-Carlo sampling and automatic differentiation and takes projected gradient ascent steps in the space of environment and policy parameters. This algorithm is referred to as Direct Environment and Policy Search (DEPS). We assess the performance of our algorithm in three environments concerned with the design and control of a mass-spring-damper system, a small-scale off-grid power system and a drone, respectively. In addition, our algorithm is benchmarked against a state-of-the-art deep reinforcement learning algorithm used to tackle joint design and control problems. We show that DEPS performs at least as well or better in all three environments, consistently yielding solutions with higher returns in fewer iterations. Finally, solutions produced by our algorithm are also compared with solutions produced by an algorithm that does not jointly optimize environment and policy parameters, highlighting the fact that higher returns can be achieved when joint optimization is performed.",0
"Our focus is on the joint design and control of stochastic dynamical systems during a finite time horizon. Our approach involves creating a multi-step optimization problem that seeks to identify a control policy and system design that jointly maximize the expected sum of rewards collected over the time horizon. The policy, reward function, and transition function are all assumed to be differentiable and parametrized. To solve this, we introduce the Direct Environment and Policy Search (DEPS) algorithm, which combines policy gradient methods with model-based optimization techniques. The algorithm approximates the gradient of the expected return through Monte-Carlo sampling and automatic differentiation and takes projected gradient ascent steps in the space of environment and policy parameters. We evaluate the algorithm's performance in three environments, including a mass-spring-damper system, a small-scale off-grid power system, and a drone, and compare its performance to a state-of-the-art deep reinforcement learning algorithm. Our results show that DEPS performs equally well or better in all three environments, yielding solutions with higher returns in fewer iterations. Finally, we demonstrate that joint optimization of environment and policy parameters leads to higher returns than an algorithm that does not perform joint optimization.",1
"Pruning has been widely used to slim convolutional neural network (CNN) models to achieve a good trade-off between accuracy and model size so that the pruned models become feasible for power-constrained devices such as mobile phones. This process can be automated to avoid the expensive hand-crafted efforts and to explore a large pruning space automatically so that the high-performance pruning policy can be achieved efficiently. Nowadays, reinforcement learning (RL) and Bayesian optimization (BO)-based auto pruners are widely used due to their solid theoretical foundation, universality, and high compressing quality. However, the RL agent suffers from long training times and high variance of results, while the BO agent is time-consuming for high-dimensional design spaces. In this work, we propose an enhanced BO agent to obtain significant acceleration for auto pruning in high-dimensional design spaces. To achieve this, a novel clustering algorithm is proposed to reduce the dimension of the design space to speedup the searching process. Then, a roll-back algorithm is proposed to recover the high-dimensional design space so that higher pruning accuracy can be obtained. We validate our proposed method on ResNet, MobileNet, and VGG models, and our experiments show that the proposed method significantly improves the accuracy of BO when pruning very deep CNN models. Moreover, our method achieves lower variance and shorter time than the RL-based counterpart.",0
"To make convolutional neural network (CNN) models feasible for power-limited devices like mobile phones, pruning has been widely applied to slim them down without sacrificing accuracy. Automated pruning can save the expensive manual efforts and explore a vast pruning space to achieve optimal performance. Reinforcement learning (RL) and Bayesian optimization (BO) are commonly used in auto pruning due to their universality, theoretical foundation, and high compression quality. However, RL agents are time-consuming and produce high variance, while BO agents take a long time for high-dimensional design spaces. To address this, we propose an improved BO agent that accelerates auto pruning in high-dimensional design spaces. Our method uses a novel clustering algorithm to reduce the dimension of the design space and speed up the search process. Then, a roll-back algorithm is employed to recover the high-dimensional design space for higher pruning accuracy. We validate our approach on ResNet, MobileNet, and VGG models and demonstrate significant improvements in accuracy when pruning deep CNN models. Additionally, our method outperforms the RL-based counterpart in terms of variance and time.",1
"Non-stationarity can arise in Reinforcement Learning (RL) even in stationary environments. For example, most RL algorithms collect new data throughout training, using a non-stationary behaviour policy. Due to the transience of this non-stationarity, it is often not explicitly addressed in deep RL and a single neural network is continually updated. However, we find evidence that neural networks exhibit a memory effect where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance. Consequently, to improve generalisation of deep RL agents, we propose Iterated Relearning (ITER). ITER augments standard RL training by repeated knowledge transfer of the current policy into a freshly initialised network, which thereby experiences less non-stationarity during training. Experimentally, we show that ITER improves performance on the challenging generalisation benchmarks ProcGen and Multiroom.",0
"Reinforcement Learning (RL) can experience Non-stationarity even when the environment is stationary. This is because RL algorithms obtain new data during training using a non-stationary behavior policy. Although this non-stationarity is typically ignored in deep RL, it can have a lasting impact on the neural network's latent representation, negatively affecting generalization performance. To counter this effect, we propose Iterated Relearning (ITER), which involves repeated knowledge transfer of the current policy into a newly initialized network. This approach reduces the amount of non-stationarity during training and improves generalization performance on challenging benchmarks such as ProcGen and Multiroom, as demonstrated in our experiments.",1
"Ensemble reinforcement learning (RL) aims to mitigate instability in Q-learning and to learn a robust policy, which introduces multiple value and policy functions. In this paper, we consider finding a novel but simple ensemble Deep RL algorithm to solve the resource consumption issue. Specifically, we consider integrating multiple models into a single model. To this end, we propose the \underline{M}inimalist \underline{E}nsemble \underline{P}olicy \underline{G}radient framework (MEPG), which introduces minimalist ensemble consistent Bellman update. And we find one value network is sufficient in our framework. Moreover, we theoretically show that the policy evaluation phase in the MEPG is mathematically equivalent to a deep Gaussian Process. To verify the effectiveness of the MEPG framework, we conduct experiments on the gym simulator, which show that the MEPG framework matches or outperforms the state-of-the-art ensemble methods and model-free methods without additional computational resource costs.",0
"The goal of ensemble reinforcement learning (RL) is to tackle the instability in Q-learning and develop a robust policy by utilizing multiple value and policy functions. This paper proposes a new and uncomplicated ensemble Deep RL algorithm to address the issue of resource consumption. The approach involves combining multiple models into a single model. To achieve this, the Minimalist Ensemble Policy Gradient framework (MEPG) is introduced, which incorporates a minimalist ensemble consistent Bellman update. The framework requires only one value network, and we demonstrate theoretically that the policy evaluation phase in the MEPG is equivalent to a deep Gaussian Process. To evaluate the effectiveness of the MEPG framework, we conduct experiments on the gym simulator, which demonstrate that it performs as well as or better than current ensemble methods and model-free methods, without the need for additional computational resources.",1
"We present Tianshou, a highly modularized python library for deep reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou provides a flexible, reliable, yet simple implementation of a modular DRL library, and has supported more than 20 classic algorithms succinctly through a unified interface. To facilitate related research and prove Tianshou's reliability, we have released Tianshou's benchmark on MuJoCo environments, covering eight classic algorithms with the state-of-the-art performance. We open-sourced Tianshou at https://github.com/thu-ml/tianshou/, which has received over 3.7k stars and become one of the most popular PyTorch-based DRL libraries.",0
"Tianshou is a Python library for deep reinforcement learning (DRL) that utilizes PyTorch as its backend. It has a highly modularized structure, making it a flexible and reliable implementation of a modular DRL library. With a unified interface, Tianshou supports more than 20 classic algorithms with ease. To demonstrate its dependability and aid further research, we have released Tianshou's benchmark on MuJoCo environments, showcasing its exceptional performance with eight classic algorithms. Tianshou has been open-sourced at https://github.com/thu-ml/tianshou/ and has gained immense popularity, with over 3.7k stars, as one of the most popular PyTorch-based DRL libraries.",1
"The development of autonomous driving has attracted extensive attention in recent years, and it is essential to evaluate the performance of autonomous driving. However, testing on the road is expensive and inefficient. Virtual testing is the primary way to validate and verify self-driving cars, and the basis of virtual testing is to build simulation scenarios. In this paper, we propose a training, testing, and evaluation pipeline for the lane-changing task from the perspective of deep reinforcement learning. First, we design lane change scenarios for training and testing, where the test scenarios include stochastic and deterministic parts. Then, we deploy a set of benchmarks consisting of learning and non-learning approaches. We train several state-of-the-art deep reinforcement learning methods in the designed training scenarios and provide the benchmark metrics evaluation results of the trained models in the test scenarios. The designed lane-changing scenarios and benchmarks are both opened to provide a consistent experimental environment for the lane-changing task.",0
"The advancement of self-driving technology has garnered significant attention lately, and it is crucial to assess its performance. However, on-road testing is not only costly but also inefficient. Therefore, virtual testing is the main approach to validate and verify autonomous vehicles, which relies on creating simulation scenarios. This research presents a pipeline for training, testing, and evaluating the lane-changing task, based on deep reinforcement learning. Initially, we develop lane change scenarios for training and testing, including deterministic and stochastic scenarios. Then, we deploy a set of benchmarks comprising both learning and non-learning approaches. We train several state-of-the-art deep reinforcement learning models in the training scenarios and evaluate the benchmark metrics of the trained models in the test scenarios. The designed lane-changing scenarios and benchmarks are publicly available to ensure a consistent experimental environment.",1
"We consider the problem of efficient blackbox optimization over a large hybrid search space, consisting of a mixture of a high dimensional continuous space and a complex combinatorial space. Such examples arise commonly in evolutionary computation, but also more recently, neuroevolution and architecture search for Reinforcement Learning (RL) policies. In this paper, we introduce ES-ENAS, a simple joint optimization procedure by combining Evolutionary Strategies (ES) and combinatorial optimization techniques in a highly scalable and intuitive way, inspired by the \textit{one-shot} or \textit{supernet} paradigm introduced in Efficient Neural Architecture Search (ENAS). Our main insight is noticing that ES is already a highly distributed algorithm involving hundreds of blackbox evaluations which can not only be used for training neural network weights, but also for feedback to a combinatorial optimizer. Through this relatively simple marriage between two different lines of research, we are able to gain the best of both worlds, and empirically demonstrate our approach by optimizing BBOB functions over hybrid spaces as well as combinatorial neural network architectures via edge pruning and quantization on popular RL benchmarks. Due to the modularity of the algorithm, we also are able incorporate a wide variety of popular techniques ranging from use of different continuous and combinatorial optimizers, as well as constrained optimization.",0
"The focus of our study is on efficiently solving the problem of blackbox optimization in a large hybrid search space that is a mix of a high dimensional continuous space and a complex combinatorial space. This is a common occurrence in evolutionary computation, as well as in neuroevolution and architecture search for Reinforcement Learning policies. Our proposed solution, ES-ENAS, involves a joint optimization procedure that combines Evolutionary Strategies (ES) and combinatorial optimization techniques in an intuitive and scalable way. Our approach is inspired by the \textit{one-shot} or \textit{supernet} paradigm from Efficient Neural Architecture Search (ENAS). By leveraging the distributed nature of ES, which involves numerous blackbox evaluations, we can utilize this for training neural network weights, as well as for feedback to a combinatorial optimizer. This allows us to benefit from the strengths of both ES and combinatorial optimization. We demonstrate the effectiveness of our approach by optimizing BBOB functions over hybrid spaces and combinatorial neural network architectures through edge pruning and quantization on popular RL benchmarks. As our algorithm is modular, we can incorporate various popular techniques such as the use of different continuous and combinatorial optimizers, as well as constrained optimization.",1
"We develop an assisted learning framework for assisting organization-level learners to improve their learning performance with limited and imbalanced data. In particular, learners at the organization level usually have sufficient computation resource, but are subject to stringent collaboration policy and information privacy. Their limited imbalanced data often cause biased inference and sub-optimal decision-making. In our assisted learning framework, an organizational learner purchases assistance service from a service provider and aims to enhance its model performance within a few assistance rounds. We develop effective stochastic training algorithms for assisted deep learning and assisted reinforcement learning. Different from existing distributed algorithms that need to frequently transmit gradients or models, our framework allows the learner to only occasionally share information with the service provider, and still achieve a near-oracle model as if all the data were centralized.",0
"Our framework addresses the challenge of improving the learning performance of organization-level learners who have limited and imbalanced data. These learners have sufficient computation resources but are bound by strict collaboration policies and information privacy concerns. The scarcity of data often results in biased inference and sub-optimal decision-making. To address these issues, we propose an assisted learning framework where an organization-level learner can purchase assistance services from a service provider to enhance their model performance over a few rounds. Our framework employs effective stochastic training algorithms for assisted deep learning and assisted reinforcement learning. Unlike existing distributed algorithms that require frequent gradient and model transmission, our approach only requires occasional sharing of information with the service provider while still achieving near-perfect models as if all the data were centralized.",1
"From assigning computing tasks to servers and advertisements to users, sequential online matching problems arise in a wide variety of domains. The challenge in online matching lies in making irrevocable assignments while there is uncertainty about future inputs. In the theoretical computer science literature, most policies are myopic or greedy in nature. In real-world applications where the matching process is repeated on a regular basis, the underlying data distribution can be leveraged for better decision-making. We present an end-to-end Reinforcement Learning framework for deriving better matching policies based on trial-and-error on historical data. We devise a set of neural network architectures, design feature representations, and empirically evaluate them across two online matching problems: Edge-Weighted Online Bipartite Matching and Online Submodular Bipartite Matching. We show that most of the learning approaches perform significantly better than classical greedy algorithms on four synthetic and real-world datasets. Our code is publicly available at https://github.com/lyeskhalil/CORL.git.",0
"There are various domains in which sequential online matching problems are present, including assigning computing tasks to servers and advertisements to users. The challenge that arises in online matching is making firm assignments without knowledge of future inputs. In the computer science literature, policies for online matching are typically myopic or greedy. However, real-world applications conduct matching regularly, which enables better decision-making by utilizing underlying data distribution. We introduce an end-to-end Reinforcement Learning framework that learns better matching policies based on experimentation with historical data. We create a group of neural network structures, establish feature representations, and evaluate them on two online matching problems: Edge-Weighted Online Bipartite Matching and Online Submodular Bipartite Matching. Our experiments show that the majority of the learning approaches outperform classical greedy algorithms on four synthetic and real-world datasets. Our code is available to the public at https://github.com/lyeskhalil/CORL.git.",1
"A novel reinforcement learning algorithm is introduced for multiarmed restless bandits with average reward, using the paradigms of Q-learning and Whittle index. Specifically, we leverage the structure of the Whittle index policy to reduce the search space of Q-learning, resulting in major computational gains. Rigorous convergence analysis is provided, supported by numerical experiments. The numerical experiments show excellent empirical performance of the proposed scheme.",0
"An innovative reinforcement learning technique has been developed for multiarmed restless bandits that offer an average reward. This approach combines Q-learning and Whittle index paradigms and utilizes the structure of the Whittle index policy to simplify the Q-learning search space. As a result, significant computational advantages are achieved. The convergence analysis is thorough and the numerical experiments demonstrate the exceptional performance of the proposed method.",1
"Exploration is an essential part of reinforcement learning, which restricts the quality of learned policy. Hard-exploration environments are defined by huge state space and sparse rewards. In such conditions, an exhaustive exploration of the environment is often impossible, and the successful training of an agent requires a lot of interaction steps. In this paper, we propose an exploration method called Rollback-Explore (RbExplore), which utilizes the concept of the persistent Markov decision process, in which agents during training can roll back to visited states. We test our algorithm in the hard-exploration Prince of Persia game, without rewards and domain knowledge. At all used levels of the game, our agent outperforms or shows comparable results with state-of-the-art curiosity methods with knowledge-based intrinsic motivation: ICM and RND. An implementation of RbExplore can be found at https://github.com/cds-mipt/RbExplore.",0
"Reinforcement learning necessitates exploration, but this can be limited and hinder the quality of the learned policy in hard-exploration environments, which are characterized by vast state spaces and sparse rewards. Due to the difficulty of exhaustive exploration, successful agent training requires extensive interaction steps. To address this issue, we propose Rollback-Explore (RbExplore), an exploration method that utilizes the concept of persistent Markov decision processes, enabling agents to roll back to previously visited states during training. Our algorithm was tested in the challenging Prince of Persia game, where there were no rewards or domain knowledge. Across all game levels, our agent surpassed or exhibited comparable results to state-of-the-art curiosity methods with knowledge-based intrinsic motivation (ICM and RND). Implementation of RbExplore is available at https://github.com/cds-mipt/RbExplore.",1
"Learning to act in an environment to maximise rewards is among the brain's key functions. This process has often been conceptualised within the framework of reinforcement learning, which has also gained prominence in machine learning and artificial intelligence (AI) as a way to optimise decision-making. A common aspect of both biological and machine reinforcement learning is the reactivation of previously experienced episodes, referred to as replay. Replay is important for memory consolidation in biological neural networks, and is key to stabilising learning in deep neural networks. Here, we review recent developments concerning the functional roles of replay in the fields of neuroscience and AI. Complementary progress suggests how replay might support learning processes, including generalisation and continual learning, affording opportunities to transfer knowledge across the two fields to advance the understanding of biological and artificial learning and memory.",0
"The brain's primary function is to learn how to act in an environment to obtain the maximum rewards. This process is commonly referred to as reinforcement learning and is crucial to decision-making in both biological and artificial intelligence. A fundamental aspect of both is replay, where previously experienced episodes are reactivated to consolidate memory in biological networks and stabilize learning in deep neural networks. In this article, we examine recent advancements in the role of replay in neuroscience and AI. These developments demonstrate how replay can enhance learning processes such as generalization and continual learning, promoting cross-field knowledge transfer to further advance our understanding of biological and artificial learning and memory.",1
"The process of capturing a well-composed photo is difficult and it takes years of experience to master. We propose a novel pipeline for an autonomous agent to automatically capture an aesthetic photograph by navigating within a local region in a scene. Instead of classical optimization over heuristics such as the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess photo quality. A reinforcement learning framework is used to optimize the model with respect to the learned aesthetics metric. We train our model in simulation with indoor scenes, and we demonstrate that our system can capture aesthetic photos in both simulation and real world environments on a ground robot. To our knowledge, this is the first system that can automatically explore an environment to capture an aesthetic photo with respect to a learned aesthetic estimator.",0
"Achieving a well-composed photo is a challenging task that requires years of experience to master. To address this issue, we introduce a new process that allows an autonomous agent to capture aesthetically pleasing photographs by navigating within a specific area of a scene. Instead of relying on traditional rules of composition such as the rule-of-thirds, we utilize a data-driven aesthetics estimator to evaluate the quality of the photo. To optimize the model based on the learned aesthetics metric, we employ a reinforcement learning framework. Through simulation training with indoor scenes, we demonstrate that our system can successfully capture appealing photos in both simulated and real-world environments using a ground robot. This is the first system known to us that can autonomously explore an environment to capture a beautiful photo based on a learned aesthetics estimator.",1
"Reinforcement learning (RL) has shown impressive success in exploring high-dimensional environments to learn complex tasks, but can often exhibit unsafe behaviors and require extensive environment interaction when exploration is unconstrained. A promising strategy for learning in dynamically uncertain environments is requiring that the agent can robustly return to learned safe sets, where task success (and therefore safety) can be guaranteed. While this approach has been successful in low-dimensions, enforcing this constraint in environments with visual observations is exceedingly challenging. We present a novel continuous representation for safe sets by framing it as a binary classification problem in a learned latent space, which flexibly scales to image observations. We then present a new algorithm, Latent Space Safe Sets (LS3), which uses this representation for long-horizon tasks with sparse rewards. We evaluate LS3 on 4 domains, including a challenging sequential pushing task in simulation and a physical cable routing task. We find that LS3 can use prior task successes to restrict exploration and learn more efficiently than prior algorithms while satisfying constraints. See https://tinyurl.com/latent-ss for code and supplementary material.",0
"Reinforcement learning (RL) has proven to be successful in learning complex tasks in high-dimensional environments. However, unconstrained exploration can lead to unsafe behaviors and extensive environment interaction. To overcome this challenge, a promising strategy involves requiring the agent to return to learned safe sets, where task success can be guaranteed. Although this approach has been successful in low-dimensional environments, it is challenging to enforce in visually rich environments. In this study, we propose a novel continuous representation for safe sets using a binary classification problem in a learned latent space, which can scale to image observations. Our algorithm, Latent Space Safe Sets (LS3), utilizes this representation to perform long-horizon tasks with sparse rewards. We evaluate LS3 in four domains, including a challenging sequential pushing task in simulation and a physical cable routing task. Our results demonstrate that LS3 can efficiently learn while satisfying constraints by restricting exploration using prior task successes. Please refer to https://tinyurl.com/latent-ss for code and supplementary material.",1
"Abnormal states in deep reinforcement learning~(RL) are states that are beyond the scope of an RL policy. Such states may make the RL system unsafe and impede its deployment in real scenarios. In this paper, we propose a simple yet effective anomaly detection framework for deep RL algorithms that simultaneously considers random, adversarial and out-of-distribution~(OOD) state outliers. In particular, we attain the class-conditional distributions for each action class under the Gaussian assumption, and rely on these distributions to discriminate between inliers and outliers based on Mahalanobis Distance~(MD) and Robust Mahalanobis Distance. We conduct extensive experiments on Atari games that verify the effectiveness of our detection strategies. To the best of our knowledge, we present the first in-detail study of statistical and adversarial anomaly detection in deep RL algorithms. This simple unified anomaly detection paves the way towards deploying safe RL systems in real-world applications.",0
"The occurrence of abnormal states in deep reinforcement learning (RL) can result in the failure of an RL policy and prevent the safe utilization of the system in practical situations. To address this issue, we propose a straightforward yet efficient framework for detecting anomalies in deep RL algorithms that accounts for random, adversarial, and out-of-distribution (OOD) state outliers. Specifically, we determine the class-conditional distributions for each action class under the assumption of a Gaussian distribution and rely on these distributions to differentiate between inliers and outliers using Mahalanobis Distance (MD) and Robust Mahalanobis Distance. Our detection strategies are extensively tested on Atari games, and the results confirm their effectiveness. To our knowledge, this is the first comprehensive study of statistical and adversarial anomaly detection in deep RL algorithms. This uncomplicated unified anomaly detection method is a step towards implementing secure RL systems in real-world applications.",1
"Hierarchical reinforcement learning has focused on discovering temporally extended actions, such as options, that can provide benefits in problems requiring extensive exploration. One promising approach that learns these options end-to-end is the option-critic (OC) framework. We examine and show in this paper that OC does not decompose a problem into simpler sub-problems, but instead increases the size of the search over policy space with each option considering the entire state space during learning. This issue can result in practical limitations of this method, including sample inefficient learning. To address this problem, we introduce Context-Specific Representation Abstraction for Deep Option Learning (CRADOL), a new framework that considers both temporal abstraction and context-specific representation abstraction to effectively reduce the size of the search over policy space. Specifically, our method learns a factored belief state representation that enables each option to learn a policy over only a subsection of the state space. We test our method against hierarchical, non-hierarchical, and modular recurrent neural network baselines, demonstrating significant sample efficiency improvements in challenging partially observable environments.",0
"The focus of hierarchical reinforcement learning has been on finding extended actions, such as options, that can bring benefits to problems that require extensive exploration. The option-critic (OC) framework is a promising approach that learns these options end-to-end. However, our paper shows that OC does not break down a problem into simpler sub-problems, but instead increases the search space with each option considering the entire state space during learning. This can lead to practical limitations, including inefficient learning. To overcome this issue, we propose a new framework called CRADOL, which combines temporal abstraction and context-specific representation abstraction to reduce the search space. Our method learns a factored belief state representation that allows each option to learn a policy over a subsection of the state space. We compare our approach with hierarchical, non-hierarchical, and modular recurrent neural network baselines and demonstrate significant improvements in sample efficiency in challenging partially observable environments.",1
"Model-free reinforcement learning (RL) is capable of learning control policies for high-dimensional, complex robotic tasks, but tends to be data-inefficient. Model-based RL tends to be more data-efficient but often suffers from learning a high-dimensional model that is good enough for policy improvement. This limits its use to learning simple models for restrictive domains. Optimal control generates solutions without collecting any data, assuming an accurate model of the system and environment is known, which is often true in many control theory applications. However, optimal control cannot be scaled to problems with a high-dimensional state space. In this paper, we propose a novel approach to alleviate data inefficiency of model-free RL in high-dimensional problems by warm-starting the learning process using a lower-dimensional model-based solution. Particularly, we initialize a baseline function for the high-dimensional RL problem via supervision from a lower-dimensional value function, which can be obtained by solving a lower-dimensional problem with a known, approximate model using ""classical"" techniques such as value iteration or optimal control. Therefore, our approach implicitly exploits the model priors from simplified problem space to facilitate the policy learning in high-dimensional RL tasks. We demonstrate our approach on two representative robotic learning tasks and observe significant improvement in policy performance and learning efficiency. We also evaluate our method empirically with a third task.",0
"Although model-free reinforcement learning (RL) can learn control policies for complex, high-dimensional robotic tasks, it is not efficient with data. Model-based RL, on the other hand, is more data-efficient, but it often struggles to learn a high-dimensional model that is good enough for policy improvement. This makes it unsuitable for learning simple models in limited domains. Optimal control can produce solutions without data but requires an accurate model of the system and environment, which is generally only possible in control theory applications. Unfortunately, optimal control cannot be applied to problems with high-dimensional state spaces. In this article, we propose a new method to increase the data efficiency of model-free RL for high-dimensional problems. We achieve this by using a lower-dimensional model-based solution to warm-start the learning process. Specifically, we use a lower-dimensional value function to supervise the creation of a baseline function for the high-dimensional RL problem. The value function is obtained by solving a lower-dimensional problem using classical methods such as optimal control or value iteration. Therefore, our approach makes use of model priors from simpler problem spaces to aid in policy learning in high-dimensional RL tasks. We demonstrate the effectiveness of our method on two typical robotic learning tasks and observe a substantial improvement in policy performance and learning efficiency. We also empirically evaluate our approach on a third task.",1
"We study a finite-horizon restless multi-armed bandit problem with multiple actions, dubbed R(MA)^2B. The state of each arm evolves according to a controlled Markov decision process (MDP), and the reward of pulling an arm depends on both the current state of the corresponding MDP and the action taken. The goal is to sequentially choose actions for arms so as to maximize the expected value of the cumulative rewards collected. Since finding the optimal policy is typically intractable, we propose a computationally appealing index policy which we call Occupancy-Measured-Reward Index Policy. Our policy is well-defined even if the underlying MDPs are not indexable. We prove that it is asymptotically optimal when the activation budget and number of arms are scaled up, while keeping their ratio as a constant. For the case when the system parameters are unknown, we develop a learning algorithm. Our learning algorithm uses the principle of optimism in the face of uncertainty and further uses a generative model in order to fully exploit the structure of Occupancy-Measured-Reward Index Policy. We call it the R(MA)^2B-UCB algorithm. As compared with the existing algorithms, R(MA)^2B-UCB performs close to an offline optimum policy, and also achieves a sub-linear regret with a low computational complexity. Experimental results show that R(MA)^2B-UCB outperforms the existing algorithms in both regret and run time.",0
"We examine R(MA)^2B, a restless multi-armed bandit problem with multiple actions and a finite time horizon. Each arm is associated with a controlled Markov decision process (MDP) that determines its state, and the reward for pulling an arm depends on the current state of the corresponding MDP and the action taken. The objective is to select actions for arms sequentially to maximize the cumulative rewards collected. Due to the intractability of finding the optimal policy, we propose an index policy, called the Occupancy-Measured-Reward Index Policy, which is computationally efficient and applicable to non-indexable MDPs. When the number of arms and activation budget are proportionally increased, we prove that our policy is asymptotically optimal. For situations where system parameters are unknown, we introduce the R(MA)^2B-UCB algorithm, which utilizes the principle of optimism and a generative model to fully exploit the structure of the Occupancy-Measured-Reward Index Policy. Our algorithm achieves a sub-linear regret and low computational complexity, and outperforms existing algorithms in both regret and run time based on experimental results.",1
"Scaling adaptive traffic-signal control involves dealing with combinatorial state and action spaces. Multi-agent reinforcement learning attempts to address this challenge by distributing control to specialized agents. However, specialization hinders generalization and transferability, and the computational graphs underlying neural-networks architectures -- dominating in the multi-agent setting -- do not offer the flexibility to handle an arbitrary number of entities which changes both between road networks, and over time as vehicles traverse the network. We introduce Inductive Graph Reinforcement Learning (IG-RL) based on graph-convolutional networks which adapts to the structure of any road network, to learn detailed representations of traffic-controllers and their surroundings. Our decentralized approach enables learning of a transferable-adaptive-traffic-signal-control policy. After being trained on an arbitrary set of road networks, our model can generalize to new road networks, traffic distributions, and traffic regimes, with no additional training and a constant number of parameters, enabling greater scalability compared to prior methods. Furthermore, our approach can exploit the granularity of available data by capturing the (dynamic) demand at both the lane and the vehicle levels. The proposed method is tested on both road networks and traffic settings never experienced during training. We compare IG-RL to multi-agent reinforcement learning and domain-specific baselines. In both synthetic road networks and in a larger experiment involving the control of the 3,971 traffic signals of Manhattan, we show that different instantiations of IG-RL outperform baselines.",0
"The challenge of scaling adaptive traffic-signal control involves dealing with complex state and action spaces. Multi-agent reinforcement learning has been used to address this challenge by distributing control among specialized agents. However, this approach has limitations in terms of generalization and transferability, and the neural-network architectures dominating in the multi-agent setting cannot handle an arbitrary number of entities that change over time and across different road networks. To overcome these limitations, we propose Inductive Graph Reinforcement Learning (IG-RL) based on graph-convolutional networks that can adapt to the structure of any road network and learn detailed representations of traffic-controllers and their surroundings. Our decentralized approach enables the learning of a transferable-adaptive-traffic-signal-control policy that can be applied to new road networks, traffic distributions, and traffic regimes without additional training. The proposed method can capture the dynamic demand at both the lane and vehicle levels, making it more scalable compared to prior methods. We evaluate IG-RL on both synthetic and real-world road networks and traffic settings, and show that it outperforms other methods.",1
"Mean Field Games (MFGs) can potentially scale multi-agent systems to extremely large populations of agents. Yet, most of the literature assumes a single initial distribution for the agents, which limits the practical applications of MFGs. Machine Learning has the potential to solve a wider diversity of MFG problems thanks to generalizations capacities. We study how to leverage these generalization properties to learn policies enabling a typical agent to behave optimally against any population distribution. In reference to the Master equation in MFGs, we coin the term ``Master policies'' to describe them and we prove that a single Master policy provides a Nash equilibrium, whatever the initial distribution. We propose a method to learn such Master policies. Our approach relies on three ingredients: adding the current population distribution as part of the observation, approximating Master policies with neural networks, and training via Reinforcement Learning and Fictitious Play. We illustrate on numerical examples not only the efficiency of the learned Master policy but also its generalization capabilities beyond the distributions used for training.",0
"The potential scalability of Mean Field Games (MFGs) for large populations of agents is limited by the assumption of a single initial distribution for the agents in most literature. To address this limitation, Machine Learning can be used to generalize MFG problems and enable agents to behave optimally against any population distribution. We introduce the term ""Master policies"" to describe policies learned for MFGs, which provide a Nash equilibrium for any initial distribution. Our proposed method for learning Master policies involves adding the current population distribution as part of the observation, approximating policies with neural networks, and training via Reinforcement Learning and Fictitious Play. Numerical examples illustrate the efficiency and generalization capabilities of the learned Master policy beyond the distributions used for training.",1
"We introduce PowerGym, an open-source reinforcement learning environment for Volt-Var control in power distribution systems. Following OpenAI Gym APIs, PowerGym targets minimizing power loss and voltage violations under physical networked constraints. PowerGym provides four distribution systems (13Bus, 34Bus, 123Bus, and 8500Node) based on IEEE benchmark systems and design variants for various control difficulties. To foster generalization, PowerGym offers a detailed customization guide for users working with their distribution systems. As a demonstration, we examine state-of-the-art reinforcement learning algorithms in PowerGym and validate the environment by studying controller behaviors. The repository is available at \url{https://github.com/siemens/powergym}.",0
"The PowerGym is a reinforcement learning environment for Volt-Var control in power distribution systems that is open-source. It targets the minimization of power loss and voltage violations while adhering to physical networked constraints, similar to OpenAI Gym APIs. The PowerGym provides four distribution systems based on IEEE benchmark systems and design variants for varying degrees of control difficulty: 13Bus, 34Bus, 123Bus, and 8500Node. To enhance generalization, PowerGym offers a comprehensive customization guide for users who are working with their distribution systems. To validate the environment, we examine the behaviors of controllers using state-of-the-art reinforcement learning algorithms in PowerGym. The repository is available at \url{https://github.com/siemens/powergym}.",1
"In this paper we present a risk-averse reinforcement learning (RL) method called Conditional value-at-risk Adversarial Reinforcement Learning (CARL). To the best of our knowledge, CARL is the first game formulation for Conditional Value-at-Risk (CVaR) RL. The game takes place between a policy player and an adversary that perturbs the policy player's state transitions given a finite budget. We prove that, at the maximin equilibrium point, the learned policy is CVaR optimal with a risk tolerance explicitly related to the adversary's budget. We provide a gradient-based training procedure to solve CARL by formulating it as a zero-sum Stackelberg Game, enabling the use of deep reinforcement learning architectures and training algorithms. Finally, we show that solving the CARL game does lead to risk-averse behaviour in a toy grid environment, also confirming that an increased adversary produces increasingly cautious policies.",0
"The paper introduces a novel risk-averse reinforcement learning approach called Conditional value-at-risk Adversarial Reinforcement Learning (CARL). CARL is the first game formulation for Conditional Value-at-Risk (CVaR) RL. It involves a game between a policy player and an adversary, who perturbs the policy player's state transitions within a finite budget. The learned policy is CVaR optimal at the maximin equilibrium point, and the risk tolerance is explicitly connected to the adversary's budget. A gradient-based training procedure is provided to solve CARL as a zero-sum Stackelberg Game, allowing for the use of deep reinforcement learning architectures and training algorithms. Moreover, the study demonstrates that solving the CARL game results in risk-averse behavior in a toy grid environment, indicating that more cautious policies are produced with an increased adversary.",1
"Emergency vehicles (EMVs) play a crucial role in responding to time-critical events such as medical emergencies and fire outbreaks in an urban area. The less time EMVs spend traveling through the traffic, the more likely it would help save people's lives and reduce property loss. To reduce the travel time of EMVs, prior work has used route optimization based on historical traffic-flow data and traffic signal pre-emption based on the optimal route. However, traffic signal pre-emption dynamically changes the traffic flow which, in turn, modifies the optimal route of an EMV. In addition, traffic signal pre-emption practices usually lead to significant disturbances in traffic flow and subsequently increase the travel time for non-EMVs. In this paper, we propose EMVLight, a decentralized reinforcement learning (RL) framework for simultaneous dynamic routing and traffic signal control. EMVLight extends Dijkstra's algorithm to efficiently update the optimal route for the EMVs in real time as it travels through the traffic network. The decentralized RL agents learn network-level cooperative traffic signal phase strategies that not only reduce EMV travel time but also reduce the average travel time of non-EMVs in the network. This benefit has been demonstrated through comprehensive experiments with synthetic and real-world maps. These experiments show that EMVLight outperforms benchmark transportation engineering techniques and existing RL-based signal control methods.",0
"The timely response to medical emergencies and fire outbreaks in urban areas is crucial, and emergency vehicles (EMVs) play a vital role in this. The less time that EMVs spend navigating traffic, the higher the likelihood of saving lives and minimizing property damage. In the past, route optimization based on historical traffic data and traffic signal pre-emption have been utilized to reduce EMV travel time. However, traffic signal pre-emption can create disturbances in traffic flow and result in increased travel times for non-EMVs. Our proposed solution, EMVLight, is a decentralized reinforcement learning framework that simultaneously controls traffic signals and dynamically routes EMVs. EMVLight extends Dijkstra's algorithm to update EMV routes in real-time and uses decentralized RL agents to learn network-level cooperative traffic signal phase strategies that benefit EMVs and non-EMVs alike. Comprehensive experiments with synthetic and real-world maps demonstrate the superiority of EMVLight compared to existing methods.",1
"Multi-task learning ideally allows robots to acquire a diverse repertoire of useful skills. However, many multi-task reinforcement learning efforts assume the robot can collect data from all tasks at all times. In reality, the tasks that the robot learns arrive sequentially, depending on the user and the robot's current environment. In this work, we study a practical sequential multi-task RL problem that is motivated by the practical constraints of physical robotic systems, and derive an approach that effectively leverages the data and policies learned for previous tasks to cumulatively grow the robot's skill-set. In a series of simulated robotic manipulation experiments, our approach requires less than half the samples than learning each task from scratch, while avoiding impractical round-robin data collection. On a Franka Emika Panda robot arm, our approach incrementally learns ten challenging tasks, including bottle capping and block insertion.",0
"The concept of multi-task learning is to equip robots with a variety of beneficial skills. However, numerous attempts at multi-task reinforcement learning assume that robots can gather data from all tasks at all times, which is unrealistic. In actuality, robots learn tasks sequentially, depending on their current surroundings and the user. This study examines a practical sequential multi-task RL problem that reflects the logistical limitations of physical robotic systems. We have developed an approach that effectively uses the data and policies learned for previous tasks to expand the robot's skill-set cumulatively. Through simulated robotic manipulation experiments, our approach requires less than half the samples necessary to learn each task from scratch, and also avoids impractical round-robin data collection. On a Franka Emika Panda robot arm, our approach incrementally learns ten challenging tasks, including bottle capping and block insertion.",1
"We propose using regularization for Multi-Agent Reinforcement Learning rather than learning explicit cooperative structures called {\em Multi-Agent Regularized Q-learning} (MARQ). Many MARL approaches leverage centralized structures in order to exploit global state information or removing communication constraints when the agents act in a decentralized manner. Instead of learning redundant structures which is removed during agent execution, we propose instead to leverage shared experiences of the agents to regularize the individual policies in order to promote structured exploration. We examine several different approaches to how MARQ can either explicitly or implicitly regularize our policies in a multi-agent setting. MARQ aims to address these limitations in the MARL context through applying regularization constraints which can correct bias in off-policy out-of-distribution agent experiences and promote diverse exploration. Our algorithm is evaluated on several benchmark multi-agent environments and we show that MARQ consistently outperforms several baselines and state-of-the-art algorithms; learning in fewer steps and converging to higher returns.",0
"Our proposal for Multi-Agent Reinforcement Learning involves using regularization instead of Multi-Agent Regularized Q-learning (MARQ), which involves learning explicit cooperative structures that can be removed during agent execution. Many MARL approaches rely on centralized structures for exploiting global state information or overcoming communication constraints during decentralized agent actions. Instead of redundant structures, we suggest using shared experiences of agents to regularize individual policies, promoting structured exploration. We explore different ways in which MARQ can explicitly or implicitly regularize policies in a multi-agent setting. Our approach addresses MARL limitations by applying regularization constraints that correct bias in off-policy out-of-distribution agent experiences and encourage diverse exploration. We evaluate our algorithm on various benchmark multi-agent environments and demonstrate that MARQ consistently outperforms baselines and state-of-the-art algorithms, converging to higher returns in fewer steps.",1
"Reinforcement learning has been shown to perform a range of complex tasks through interaction with an environment or collected leveraging experience. However, many of these approaches presume optimal or near optimal experiences or the presence of a consistent environment. In this work we propose dual, advantage-based behavior policy based on counterfactual regret minimization. We demonstrate the flexibility of this approach and how it can be adapted to online contexts where the environment is available to collect experiences and a variety of other contexts. We demonstrate this new algorithm can outperform several strong baseline models in different contexts based on a range of continuous environments. Additional ablations provide insights into how our dual behavior regularized reinforcement learning approach is designed compared with other plausible modifications and demonstrates its ability to generalize.",0
"Interaction with an environment or leveraging experience is how reinforcement learning accomplishes complex tasks. However, some methods assume optimal or nearly optimal experiences or a consistent environment. In this study, we suggest a dual, advantage-based behavior policy that uses counterfactual regret minimization. We show how this approach is adaptable to various contexts, including online environments, where experiences can be collected. We demonstrate that our algorithm outperforms several strong baseline models in different contexts with continuous environments. Additionally, ablations provide insights into how our dual behavior regularized reinforcement learning approach compares to other modifications and how it can generalize.",1
"This paper introduces Greedy UnMix (GUM) for cooperative multi-agent reinforcement learning (MARL). Greedy UnMix aims to avoid scenarios where MARL methods fail due to overestimation of values as part of the large joint state-action space. It aims to address this through a conservative Q-learning approach through restricting the state-marginal in the dataset to avoid unobserved joint state action spaces, whilst concurrently attempting to unmix or simplify the problem space under the centralized training with decentralized execution paradigm. We demonstrate the adherence to Q-function lower bounds in the Q-learning for MARL scenarios, and demonstrate superior performance to existing Q-learning MARL approaches as well as more general MARL algorithms over a set of benchmark MARL tasks, despite its relative simplicity compared with state-of-the-art approaches.",0
"The paper presents Greedy UnMix (GUM), a method for cooperative multi-agent reinforcement learning (MARL) that aims to prevent MARL methods from failing due to value overestimation in the large joint state-action space. GUM achieves this by utilizing a conservative Q-learning approach that restricts the state-marginal in the dataset to avoid unobserved joint state-action spaces, while also simplifying the problem space under the centralized training with decentralized execution paradigm. The study shows that GUM adheres to Q-function lower bounds in Q-learning for MARL scenarios and outperforms existing Q-learning MARL methods and more general MARL algorithms across various benchmark tasks. Despite its relative simplicity compared to state-of-the-art approaches, GUM demonstrates superior performance.",1
"Designing missiles' autopilot controllers has been a complex task, given the extensive flight envelope and the nonlinear flight dynamics. A solution that can excel both in nominal performance and in robustness to uncertainties is still to be found. While Control Theory often debouches into parameters' scheduling procedures, Reinforcement Learning has presented interesting results in ever more complex tasks, going from videogames to robotic tasks with continuous action domains. However, it still lacks clearer insights on how to find adequate reward functions and exploration strategies. To the best of our knowledge, this work is pioneer in proposing Reinforcement Learning as a framework for flight control. In fact, it aims at training a model-free agent that can control the longitudinal flight of a missile, achieving optimal performance and robustness to uncertainties. To that end, under TRPO's methodology, the collected experience is augmented according to HER, stored in a replay buffer and sampled according to its significance. Not only does this work enhance the concept of prioritized experience replay into BPER, but it also reformulates HER, activating them both only when the training progress converges to suboptimal policies, in what is proposed as the SER methodology. Besides, the Reward Engineering process is carefully detailed. The results show that it is possible both to achieve the optimal performance and to improve the agent's robustness to uncertainties (with low damage on nominal performance) by further training it in non-nominal environments, therefore validating the proposed approach and encouraging future research in this field.",0
"The task of designing autopilot controllers for missiles has been challenging due to the complex flight envelope and nonlinear flight dynamics. A solution that can perform well under normal conditions while being robust to uncertainties is yet to be discovered. While Control Theory has often relied on parameters' scheduling procedures, Reinforcement Learning has shown promising results in more complex tasks, from videogames to continuous action domains in robotics. However, there is still a lack of clear understanding on how to determine appropriate reward functions and exploration strategies. This study proposes a pioneering approach to using Reinforcement Learning as a framework for flight control, aiming to train a model-free agent that can control the longitudinal flight of a missile with optimal performance and robustness to uncertainties. The approach utilizes TRPO's methodology to augment collected experience according to HER, stored in a replay buffer and sampled based on significance. The study enhances the prioritized experience replay concept into BPER and reformulates HER, activating them both only when training progress converges to suboptimal policies, known as the SER methodology. The Reward Engineering process is carefully detailed, and the results demonstrate that it is possible to achieve optimal performance while improving the agent's robustness to uncertainties with minimal damage to nominal performance by further training it in non-nominal environments. This validates the proposed approach and encourages future research in this field.",1
"Maintaining the long-term exploration capability of the agent remains one of the critical challenges in deep reinforcement learning. A representative solution is to leverage reward shaping to provide intrinsic rewards for the agent to encourage exploration. However, most existing methods suffer from vanishing intrinsic rewards, which cannot provide sustainable exploration incentives. Moreover, they rely heavily on complex models and additional memory to record learning procedures, resulting in high computational complexity and low robustness. To tackle this problem, entropy-based methods are proposed to evaluate the global exploration performance, encouraging the agent to visit the state space more equitably. However, the sample complexity of estimating the state visitation entropy is prohibitive when handling environments with high-dimensional observations. In this paper, we introduce a novel metric entitled Jain's fairness index (JFI) to replace the entropy regularizer, which solves the exploration problem from a brand new perspective. In sharp contrast to the entropy regularizer, JFI is more computable and robust and can be easily applied generalized into arbitrary tasks. Furthermore, we leverage a variational auto-encoder (VAE) model to capture the life-long novelty of states, which is combined with the global JFI score to form multimodal intrinsic rewards. Finally, extensive simulation results demonstrate that our multimodal reward shaping (MMRS) method can achieve higher performance than other benchmark schemes.",0
"One of the key challenges in deep reinforcement learning is to maintain the agent's long-term exploration capability. A common approach is to use reward shaping to provide intrinsic rewards that encourage exploration. However, many existing methods suffer from short-lived intrinsic rewards, which fail to provide sustained exploration incentives. Additionally, these methods often rely on complex models and additional memory, which results in high computational complexity and low robustness. To address this issue, entropy-based methods have been proposed to evaluate global exploration performance and encourage more equitable state space visits. However, these methods have a high sample complexity when handling high-dimensional observations. In this study, we propose a new metric, Jain's fairness index (JFI), to replace the entropy regularizer and solve the exploration problem from a new perspective. JFI is more computable, robust, and can be applied to arbitrary tasks. We combine a variational auto-encoder (VAE) model with the global JFI score to form multimodal intrinsic rewards that capture the life-long novelty of states. Our extensive simulations demonstrate that our multimodal reward shaping (MMRS) method achieves higher performance than other benchmark schemes.",1
"Plug-and-Play (PnP) is a non-convex optimization framework that combines proximal algorithms, for example, the alternating direction method of multipliers (ADMM), with advanced denoising priors. Over the past few years, great empirical success has been obtained by PnP algorithms, especially for the ones that integrate deep learning-based denoisers. However, a key challenge of PnP approaches is the need for manual parameter tweaking as it is essential to obtain high-quality results across the high discrepancy in imaging conditions and varying scene content. In this work, we present a class of tuning-free PnP proximal algorithms that can determine parameters such as denoising strength, termination time, and other optimization-specific parameters automatically. A core part of our approach is a policy network for automated parameter search which can be effectively learned via a mixture of model-free and model-based deep reinforcement learning strategies. We demonstrate, through rigorous numerical and visual experiments, that the learned policy can customize parameters to different settings, and is often more efficient and effective than existing handcrafted criteria. Moreover, we discuss several practical considerations of PnP denoisers, which together with our learned policy yield state-of-the-art results. This advanced performance is prevalent on both linear and nonlinear exemplar inverse imaging problems, and in particular shows promising results on compressed sensing MRI, sparse-view CT, single-photon imaging, and phase retrieval.",0
"The Plug-and-Play (PnP) framework is an optimization technique that uses proximal algorithms, such as the alternating direction method of multipliers (ADMM), and advanced denoising priors to solve non-convex problems. PnP algorithms, especially those that incorporate deep learning-based denoisers, have achieved great success in recent years. However, a major challenge of PnP approaches is the need for manual parameter adjustment to achieve high-quality results across a wide range of imaging conditions and scene content. This study introduces a class of PnP proximal algorithms that require no manual tuning and can automatically determine parameters such as denoising strength and termination time. The approach leverages a policy network for automated parameter search, which is effectively learned through a combination of model-free and model-based deep reinforcement learning strategies. Rigorous numerical and visual experiments demonstrate that the learned policy can tailor parameters to different settings and is often more efficient and effective than existing handcrafted criteria. The study also discusses practical considerations of PnP denoisers and shows that the learned policy yields state-of-the-art results on both linear and nonlinear exemplar inverse imaging problems, including compressed sensing MRI, sparse-view CT, single-photon imaging, and phase retrieval.",1
"Deep reinforcement learning (RL) is a powerful framework to train decision-making models in complex dynamical environments. However, RL can be slow as it learns through repeated interaction with a simulation of the environment. Accelerating RL requires both algorithmic and engineering innovations. In particular, there are key systems engineering bottlenecks when using RL in complex environments that feature multiple agents or high-dimensional state, observation, or action spaces, for example. We present WarpDrive, a flexible, lightweight, and easy-to-use open-source RL framework that implements end-to-end multi-agent RL on a single GPU (Graphics Processing Unit), building on PyCUDA and PyTorch. Using the extreme parallelization capability of GPUs, WarpDrive enables orders-of-magnitude faster RL compared to common implementations that blend CPU simulations and GPU models. Our design runs simulations and the agents in each simulation in parallel. It eliminates data copying between CPU and GPU. It also uses a single simulation data store on the GPU that is safely updated in-place. Together, this allows the user to run thousands of concurrent multi-agent simulations and train on extremely large batches of experience. For example, WarpDrive yields 2.9 million environment steps/second with 2000 environments and 1000 agents (at least 100x higher throughput compared to a CPU implementation) in a benchmark Tag simulation. WarpDrive provides a lightweight Python interface and environment wrappers to simplify usage and promote flexibility and extensions. As such, WarpDrive provides a framework for building high-throughput RL systems.",0
"Deep reinforcement learning (RL) is a potent methodology for developing decision-making models in intricate, dynamic environments. Nonetheless, RL can be time-consuming, as it involves iteratively engaging with a simulated environment. To expedite RL, there is a need for both algorithmic and engineering advancements, particularly when working with complicated environments featuring multiple agents or high-dimensional state, observation, or action spaces. Here, we introduce WarpDrive, an easy-to-use, flexible, and lightweight open-source RL platform that enables end-to-end multi-agent RL on a single Graphics Processing Unit (GPU). Built on PyCUDA and PyTorch, WarpDrive harnesses the exceptional parallelization capabilities of GPUs, resulting in orders-of-magnitude faster RL than conventional implementations that blend CPU simulations with GPU models. Our innovative design runs simulations and agents in parallel without data copying between CPU and GPU. It utilizes a single simulation data store on the GPU that can be safely updated in-place, paving the way for thousands of concurrent multi-agent simulations and training on vast experience batches. WarpDrive's Python interface and environment wrappers simplify usage, promoting flexibility and extensions, making it a valuable framework for constructing high-throughput RL systems. In a Tag benchmark simulation, WarpDrive produces 2.9 million environment steps/second with 2000 environments and 1000 agents, resulting in at least 100 times higher throughput compared to a CPU implementation.",1
"In real scenarios, state observations that an agent observes may contain measurement errors or adversarial noises, misleading the agent to take suboptimal actions or even collapse while training. In this paper, we study the training robustness of distributional Reinforcement Learning~(RL), a class of state-of-the-art methods that estimate the whole distribution, as opposed to only the expectation, of the total return. Firstly, we propose State-Noisy Markov Decision Process~(SN-MDP) in the tabular case to incorporate both random and adversarial state observation noises, in which the contraction of both expectation-based and distributional Bellman operators is derived. Beyond SN-MDP with the function approximation, we theoretically characterize the bounded gradient norm of histogram-based distributional loss, accounting for the better training robustness of distribution RL. We also provide stricter convergence conditions of the Temporal-Difference~(TD) learning under more flexible state noises, as well as the sensitivity analysis by the leverage of influence function. Finally, extensive experiments on the suite of games show that distributional RL enjoys better training robustness compared with its expectation-based counterpart across various state observation noises.",0
"The observations an agent receives in real-world situations may contain mistakes or be deliberately altered, causing the agent to make suboptimal decisions or fail during training. This study focuses on the training resilience of distributional Reinforcement Learning, which estimates the entire return distribution rather than just the expectation. The authors introduce the State-Noisy Markov Decision Process (SN-MDP) in the tabular case to account for both random and adversarial observation errors, and derive the contraction of both expectation-based and distributional Bellman operators. They also theoretically analyze the gradient norm of histogram-based distributional loss, providing stricter convergence conditions for TD learning and conducting a sensitivity analysis using influence function. Finally, experiments demonstrate that distributional RL is more robust to various state observation errors than its expectation-based counterpart.",1
"Classical global convergence results for first-order methods rely on uniform smoothness and the \L{}ojasiewicz inequality. Motivated by properties of objective functions that arise in machine learning, we propose a non-uniform refinement of these notions, leading to \emph{Non-uniform Smoothness} (NS) and \emph{Non-uniform \L{}ojasiewicz inequality} (N\L{}). The new definitions inspire new geometry-aware first-order methods that are able to converge to global optimality faster than the classical $\Omega(1/t^2)$ lower bounds. To illustrate the power of these geometry-aware methods and their corresponding non-uniform analysis, we consider two important problems in machine learning: policy gradient optimization in reinforcement learning (PG), and generalized linear model training in supervised learning (GLM). For PG, we find that normalizing the gradient ascent method can accelerate convergence to $O(e^{-t})$ while incurring less overhead than existing algorithms. For GLM, we show that geometry-aware normalized gradient descent can also achieve a linear convergence rate, which significantly improves the best known results. We additionally show that the proposed geometry-aware descent methods escape landscape plateaus faster than standard gradient descent. Experimental results are used to illustrate and complement the theoretical findings.",0
"The conventional global convergence outcomes for first-order methods rely on uniform smoothness and the \L{}ojasiewicz inequality. In light of the characteristics of objective functions that arise in machine learning, we suggest a non-uniform refinement of these concepts, resulting in \emph{Non-uniform Smoothness} (NS) and \emph{Non-uniform \L{}ojasiewicz inequality} (N\L{}). These novel definitions prompt new geometry-aware first-order methods that can attain global optimality at a faster rate than the classical $\Omega(1/t^2)$ lower bounds. To demonstrate the effectiveness of these geometry-aware methods and their corresponding non-uniform analysis, we examine two significant machine learning issues: policy gradient optimization in reinforcement learning (PG), and generalized linear model training in supervised learning (GLM). For PG, we determine that normalizing the gradient ascent method can hasten convergence to $O(e^{-t})$ while incurring less overhead than existing algorithms. For GLM, we demonstrate that geometry-aware normalized gradient descent can also achieve a linear convergence rate, which is a notable improvement over the current best-known results. We also establish that the proposed geometry-aware descent methods surpass landscape plateaus more quickly than standard gradient descent. Experimental results are employed to illustrate and supplement the theoretical discoveries.",1
"Reinforcement learning is well-studied under discrete actions. Integer actions setting is popular in the industry yet still challenging due to its high dimensionality. To this end, we study reinforcement learning under integer actions by incorporating the Soft Actor-Critic (SAC) algorithm with an integer reparameterization. Our key observation for integer actions is that their discrete structure can be simplified using their comparability property. Hence, the proposed integer reparameterization does not need one-hot encoding and is of low dimensionality. Experiments show that the proposed SAC under integer actions is as good as the continuous action version on robot control tasks and outperforms Proximal Policy Optimization on power distribution systems control tasks.",0
"Reinforcement learning has been extensively researched with regard to discrete actions. Although integer actions are popular in industry settings, they remain challenging due to their high dimensionality. Our study aims to address this issue by incorporating the Soft Actor-Critic (SAC) algorithm and an integer reparameterization into reinforcement learning under integer actions. We have observed that the comparability property of integer actions simplifies their discrete structure, thus eliminating the need for one-hot encoding and reducing dimensionality. Our experiments demonstrate that the proposed SAC algorithm performs on par with the continuous action version in robot control tasks and surpasses Proximal Policy Optimization in power distribution systems control tasks.",1
"The challenge of mapping indoor environments is addressed. Typical heuristic algorithms for solving the motion planning problem are frontier-based methods, that are especially effective when the environment is completely unknown. However, in cases where prior statistical data on the environment's architectonic features is available, such algorithms can be far from optimal. Furthermore, their calculation time may increase substantially as more areas are exposed. In this paper we propose two means by which to overcome these shortcomings. One is the use of deep reinforcement learning to train the motion planner. The second is the inclusion of a pre-trained generative deep neural network, acting as a map predictor. Each one helps to improve the decision making through use of the learned structural statistics of the environment, and both, being realized as neural networks, ensure a constant calculation time. We show that combining the two methods can shorten the mapping time, compared to frontier-based motion planning, by up to 75%.",0
"This paper addresses the challenge of mapping indoor environments. Frontier-based methods are commonly used to solve the motion planning problem, especially when the environment is unknown. However, these methods may not be optimal when prior statistical data on the environment's architecture is available, and their calculation time may increase significantly as more areas are uncovered. To address these issues, this paper proposes two approaches. The first involves using deep reinforcement learning to train the motion planner, while the second involves incorporating a pre-trained generative deep neural network as a map predictor. Both approaches utilize learned structural statistics of the environment and have a constant calculation time since they are realized as neural networks. The combination of these two methods can reduce mapping time by up to 75% compared to frontier-based motion planning.",1
"We propose two policy gradient algorithms for solving the problem of control in an off-policy reinforcement learning (RL) context. Both algorithms incorporate a smoothed functional (SF) based gradient estimation scheme. The first algorithm is a straightforward combination of importance sampling-based off-policy evaluation with SF-based gradient estimation. The second algorithm, inspired by the stochastic variance-reduced gradient (SVRG) algorithm, incorporates variance reduction in the update iteration. For both algorithms, we derive non-asymptotic bounds that establish convergence to an approximate stationary point. From these results, we infer that the first algorithm converges at a rate that is comparable to the well-known REINFORCE algorithm in an off-policy RL context, while the second algorithm exhibits an improved rate of convergence.",0
"For the problem of control in off-policy reinforcement learning (RL), we suggest two policy gradient algorithms. These algorithms use a gradient estimation scheme based on a smoothed functional (SF). The first algorithm combines importance sampling-based off-policy evaluation with SF-based gradient estimation, while the second algorithm is influenced by the stochastic variance-reduced gradient (SVRG) algorithm and incorporates variance reduction in the update iteration. We establish non-asymptotic bounds for both algorithms, which confirm convergence to an approximate stationary point. Our findings indicate that the first algorithm converges at a rate similar to the REINFORCE algorithm in an off-policy RL context, whereas the second algorithm has a better rate of convergence.",1
"A World Model is a generative model used to simulate an environment. World Models have proven capable of learning spatial and temporal representations of Reinforcement Learning environments. In some cases, a World Model offers an agent the opportunity to learn entirely inside of its own dream environment. In this work we explore improving the generalization capabilities from dream environments to real environments (Dream2Real). We present a general approach to improve a controller's ability to transfer from a neural network dream environment to reality at little additional cost. These improvements are gained by drawing on inspiration from Domain Randomization, where the basic idea is to randomize as much of a simulator as possible without fundamentally changing the task at hand. Generally, Domain Randomization assumes access to a pre-built simulator with configurable parameters but oftentimes this is not available. By training the World Model using dropout, the dream environment is capable of creating a nearly infinite number of different dream environments. Previous use cases of dropout either do not use dropout at inference time or averages the predictions generated by multiple sampled masks (Monte-Carlo Dropout). Dropout's Dream Land leverages each unique mask to create a diverse set of dream environments. Our experimental results show that Dropout's Dream Land is an effective technique to bridge the reality gap between dream environments and reality. Furthermore, we additionally perform an extensive set of ablation studies.",0
"A World Model is a model that can generate an environment for simulation purposes. It has proved successful in learning spatial and temporal representations of Reinforcement Learning environments. Sometimes, an agent can learn solely from its own dream environment using a World Model. In this study, we investigate ways of enhancing the generalization ability of dream environments to real environments (Dream2Real). We present a universal approach for improving a controller's ability to transfer from a neural network dream environment to reality at a minimal expense. We achieve this by drawing inspiration from Domain Randomization, which involves randomizing as much of a simulator as possible without altering the primary task. In most cases, Domain Randomization assumes access to a pre-built simulator with adjustable parameters, but this is not always feasible. By training the World Model with dropout, the dream environment can create a vast array of different dream environments. Previous uses of dropout either do not use dropout during inference or average the predictions generated by multiple sampled masks (Monte-Carlo Dropout). Dropout's Dream Land uses each unique mask to create a varied set of dream environments. Our experimental findings show that Dropout's Dream Land is an effective technique for bridging the reality gap between dream environments and reality. Additionally, we perform an extensive set of ablation studies.",1
"In recommender systems (RecSys) and real-time bidding (RTB) for online advertisements, we often try to optimize sequential decision making using bandit and reinforcement learning (RL) techniques. In these applications, offline reinforcement learning (offline RL) and off-policy evaluation (OPE) are beneficial because they enable safe policy optimization using only logged data without any risky online interaction. In this position paper, we explore the potential of using simulation to accelerate practical research of offline RL and OPE, particularly in RecSys and RTB. Specifically, we discuss how simulation can help us conduct empirical research of offline RL and OPE. We take a position to argue that we should effectively use simulations in the empirical research of offline RL and OPE. To refute the counterclaim that experiments using only real-world data are preferable, we first point out the underlying risks and reproducibility issue in real-world experiments. Then, we describe how these issues can be addressed by using simulations. Moreover, we show how to incorporate the benefits of both real-world and simulation-based experiments to defend our position. Finally, we also present an open challenge to further facilitate practical research of offline RL and OPE in RecSys and RTB, with respect to public simulation platforms. As a possible solution for the issue, we show our ongoing open source project and its potential use case. We believe that building and utilizing simulation-based evaluation platforms for offline RL and OPE will be of great interest and relevance for the RecSys and RTB community.",0
"The optimization of sequential decision-making in recommender systems (RecSys) and real-time bidding (RTB) for online advertisements often involves the use of bandit and reinforcement learning (RL) techniques. Offline reinforcement learning (offline RL) and off-policy evaluation (OPE) are particularly advantageous in these applications as they allow for safe policy optimization using only logged data, without the need for risky online interaction. This position paper explores the potential of simulation to accelerate practical research of offline RL and OPE, specifically in RecSys and RTB. Our argument is that simulations should be effectively used in empirical research of offline RL and OPE, despite counterclaims that experiments using only real-world data are preferable. We highlight the underlying risks and reproducibility issues in real-world experiments, and describe how simulations can address these issues. Additionally, we propose incorporating the benefits of both real-world and simulation-based experiments. We present an open challenge to facilitate practical research of offline RL and OPE in RecSys and RTB, and propose our ongoing open-source project as a possible solution. We believe that the development and utilization of simulation-based evaluation platforms for offline RL and OPE will be of great interest and relevance for the RecSys and RTB community.",1
"Temporal sentence grounding in videos(TSGV), which aims to localize one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities(i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which i) summarizes the taxonomy of existing methods, ii) provides a detailed description of the evaluation protocols(i.e., datasets and metrics) to be used in TSGV, and iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, end-to-end methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting edge research in TSGV. Besides, we also share our insights on several promising directions, including three typical tasks with new and practical settings based on TSGV.",0
"Over the past few years, there has been a growing interest in temporal sentence grounding in videos (TSGV), which involves identifying a specific segment in an untrimmed video based on a given sentence query. Unlike temporal action localization, TSGV is more adaptable as it can recognize complex activities through natural language without predefined categories. However, TSGV is also more challenging as it requires both textual and visual comprehension to align semantics between the two modalities. This survey provides a comprehensive overview of TSGV by summarizing existing methods, detailing evaluation protocols (including datasets and metrics), and discussing limitations and potential research directions. The existing TSGV approaches are categorized into four groups: two-stage methods, end-to-end methods, reinforcement learning-based methods, and weakly supervised methods. The benchmark datasets and evaluation metrics are presented to assess current research progress, and potential problems with the current evaluation protocols are discussed, highlighting the need for more advanced research in TSGV. Additionally, several promising directions are discussed, including three typical tasks with new and practical settings based on TSGV. This is the first systematic survey on temporal sentence grounding to our knowledge.",1
"The growing number of applications of Reinforcement Learning (RL) in real-world domains has led to the development of privacy-preserving techniques due to the inherently sensitive nature of data. Most existing works focus on differential privacy, in which information is revealed in the clear to an agent whose learned model should be robust against information leakage to malicious third parties. Motivated by use cases in which only encrypted data might be shared, such as information from sensitive sites, in this work we consider scenarios in which the inputs themselves are sensitive and cannot be revealed. We develop a simple extension to the MDP framework which provides for the encryption of states. We present a preliminary, experimental study of how a DQN agent trained on encrypted states performs in environments with discrete and continuous state spaces. Our results highlight that the agent is still capable of learning in small state spaces even in presence of non-deterministic encryption, but performance collapses in more complex environments.",0
"Due to the sensitive nature of data, privacy-preserving techniques have been developed for Reinforcement Learning (RL) applications in real-world domains. Existing techniques mainly focus on differential privacy, where information is exposed to an agent and its learned model should be resistant to information leaks to malicious third parties. However, scenarios exist where only encrypted data can be shared, such as sensitive sites where the inputs themselves are sensitive and cannot be revealed. To address this, we present a simple extension to the MDP framework that allows for the encryption of states. Our preliminary experimental study examines the performance of a DQN agent trained on encrypted states in environments with discrete and continuous state spaces. Results show that the agent can still learn in small state spaces despite non-deterministic encryption, but its performance deteriorates in more complex environments.",1
"Present-day Deep Reinforcement Learning (RL) systems show great promise towards building intelligent agents surpassing human-level performance. However, the computational complexity associated with the underlying deep neural networks (DNNs) leads to power-hungry implementations. This makes deep RL systems unsuitable for deployment on resource-constrained edge devices. To address this challenge, we propose a reconfigurable architecture with preemptive exits for efficient deep RL (RAPID-RL). RAPID-RL enables conditional activation of DNN layers based on the difficulty level of inputs. This allows to dynamically adjust the compute effort during inference while maintaining competitive performance. We achieve this by augmenting a deep Q-network (DQN) with side-branches capable of generating intermediate predictions along with an associated confidence score. We also propose a novel training methodology for learning the actions and branch confidence scores in a dynamic RL setting. Our experiments evaluate the proposed framework for Atari 2600 gaming tasks and a realistic Drone navigation task on an open-source drone simulator (PEDRA). We show that RAPID-RL incurs 0.34x (0.25x) number of operations (OPS) while maintaining performance above 0.88x (0.91x) on Atari (Drone navigation) tasks, compared to a baseline-DQN without any side-branches. The reduction in OPS leads to fast and efficient inference, proving to be highly beneficial for the resource-constrained edge where making quick decisions with minimal compute is essential.",0
"Although present-day Deep Reinforcement Learning (RL) systems have the potential to create intelligent agents that are better than humans, the deep neural networks (DNNs) underlying them are computationally complex and require a lot of power. This makes them unsuitable for deployment on edge devices that lack resources. To overcome this challenge, we propose a solution called RAPID-RL, which is a reconfigurable architecture with preemptive exits that allows the conditional activation of DNN layers based on input difficulty. This enables the dynamic adjustment of compute effort during inference while maintaining competitive performance. We achieve this by augmenting a deep Q-network (DQN) with side-branches that generate intermediate predictions and an associated confidence score. We also propose a new training methodology for learning actions and branch confidence scores in a dynamic RL setting. Our experiments evaluate RAPID-RL on Atari 2600 gaming tasks and a realistic drone navigation task using an open-source drone simulator (PEDRA). Results show that RAPID-RL incurs 0.34x (0.25x) fewer operations (OPS) than a baseline-DQN without side-branches while maintaining performance above 0.88x (0.91x) on Atari (Drone navigation) tasks. The reduction in OPS leads to fast and efficient inference, making it highly beneficial for resource-constrained edge devices where quick decisions with minimal compute are crucial.",1
"In batch reinforcement learning, there can be poorly explored state-action pairs resulting in poorly learned, inaccurate models and poorly performing associated policies. Various regularization methods can mitigate the problem of learning overly-complex models in Markov decision processes (MDPs), however they operate in technically and intuitively distinct ways and lack a common form in which to compare them. This paper unifies three regularization methods in a common framework -- a weighted average transition matrix. Considering regularization methods in this common form illuminates how the MDP structure and the state-action pair distribution of the batch data set influence the relative performance of regularization methods. We confirm intuitions generated from the common framework by empirical evaluation across a range of MDPs and data collection policies.",0
"When using batch reinforcement learning, some state-action pairs may not be fully explored, leading to inaccurate models and poor policies. While several regularization techniques can address the issue of overly complex models in Markov decision processes, they differ in their technical and intuitive approaches and lack a common basis for comparison. This study introduces a weighted average transition matrix as a shared framework for three regularization methods, enabling a better understanding of how the structure of the MDP and the distribution of state-action pairs in the batch data set affect the effectiveness of each method. Empirical assessment across various MDPs and data collection policies validates the insights gained from this unified approach.",1
"Offline reinforcement learning (RL) algorithms have shown promising results in domains where abundant pre-collected data is available. However, prior methods focus on solving individual problems from scratch with an offline dataset without considering how an offline RL agent can acquire multiple skills. We argue that a natural use case of offline RL is in settings where we can pool large amounts of data collected in various scenarios for solving different tasks, and utilize all of this data to learn behaviors for all the tasks more effectively rather than training each one in isolation. However, sharing data across all tasks in multi-task offline RL performs surprisingly poorly in practice. Thorough empirical analysis, we find that sharing data can actually exacerbate the distributional shift between the learned policy and the dataset, which in turn can lead to divergence of the learned policy and poor performance. To address this challenge, we develop a simple technique for data-sharing in multi-task offline RL that routes data based on the improvement over the task-specific data. We call this approach conservative data sharing (CDS), and it can be applied with multiple single-task offline RL methods. On a range of challenging multi-task locomotion, navigation, and vision-based robotic manipulation problems, CDS achieves the best or comparable performance compared to prior offline multi-task RL methods and previous data sharing approaches.",0
"Offline reinforcement learning (RL) algorithms have displayed promising outcomes in fields where ample pre-existing data is accessible. Nevertheless, prior approaches concentrate on solving individual problems from the beginning with an offline dataset, without considering how an offline RL agent can acquire various skills. We propose that a natural way to use offline RL is in environments where a large quantity of data is pooled from diverse scenarios to more efficiently learn behaviors for multiple tasks. Yet, data sharing across all tasks in multi-task offline RL performs poorly in practice. After conducting extensive empirical analysis, we discovered that sharing data can worsen the distributional shift between the learned policy and the dataset, resulting in a divergence of the learned policy and inferior performance. To tackle this challenge, we devised a simple technique for data-sharing in multi-task offline RL, where data is routed based on the improvement over the task-specific data, called conservative data sharing (CDS). CDS can be utilized with various single-task offline RL methods and achieves the best or comparable performance compared to prior offline multi-task RL techniques and previous data sharing approaches, in a variety of challenging multi-task robotic manipulation problems involving locomotion, navigation, and vision.",1
"Machine learning (ML) has made incredible impacts and transformations in a wide range of vehicular applications. As the use of ML in Internet of Vehicles (IoV) continues to advance, adversarial threats and their impact have become an important subject of research worth exploring. In this paper, we focus on Sybil-based adversarial threats against a deep reinforcement learning (DRL)-assisted IoV framework and more specifically, DRL-based dynamic service placement in IoV. We carry out an experimental study with real vehicle trajectories to analyze the impact on service delay and resource congestion under different attack scenarios for the DRL-based dynamic service placement application. We further investigate the impact of the proportion of Sybil-attacked vehicles in the network. The results demonstrate that the performance is significantly affected by Sybil-based data poisoning attacks when compared to adversary-free healthy network scenario.",0
"The use of machine learning (ML) has had a tremendous impact on various vehicular applications. However, as ML is increasingly utilized in the Internet of Vehicles (IoV), it is important to explore the potential effects of adversarial threats. This paper focuses on the Sybil-based adversarial threats against a deep reinforcement learning (DRL)-assisted IoV framework, specifically in the context of DRL-based dynamic service placement. Through an experimental study using real vehicle trajectories, we analyze the impact of different attack scenarios on service delay and resource congestion. We also examine the effect of the proportion of Sybil-attacked vehicles in the network. Our findings demonstrate that Sybil-based data poisoning attacks significantly affect the performance of the DRL-based dynamic service placement application compared to a healthy network scenario without adversaries.",1
"Recent studies demonstrated the vulnerability of control policies learned through deep reinforcement learning against adversarial attacks, raising concerns about the application of such models to risk-sensitive tasks such as autonomous driving. Threat models for these demonstrations are limited to (1) targeted attacks through real-time manipulation of the agent's observation, and (2) untargeted attacks through manipulation of the physical environment. The former assumes full access to the agent's states/observations at all times, while the latter has no control over attack outcomes. This paper investigates the feasibility of targeted attacks through visually learned patterns placed on physical object in the environment, a threat model that combines the practicality and effectiveness of the existing ones. Through analysis, we demonstrate that a pre-trained policy can be hijacked within a time window, e.g., performing an unintended self-parking, when an adversarial object is present. To enable the attack, we adopt an assumption that the dynamics of both the environment and the agent can be learned by the attacker. Lastly, we empirically show the effectiveness of the proposed attack on different driving scenarios, perform a location robustness test, and study the tradeoff between the attack strength and its effectiveness.",0
"Recent studies have raised concerns about the use of deep reinforcement learning for risk-sensitive tasks like autonomous driving, as these policies are vulnerable to adversarial attacks. The current threat models are limited to targeted attacks that manipulate the agent's observation in real-time, and untargeted attacks that manipulate the physical environment. This paper investigates the feasibility of a new threat model that combines practicality and effectiveness by placing visually learned patterns on physical objects in the environment. Through analysis, the authors demonstrate that a pre-trained policy can be hijacked within a specific time window, resulting in unintended actions like self-parking. To enable this attack, the attacker must learn the dynamics of both the environment and the agent. The authors empirically show the effectiveness of this attack on different driving scenarios, perform a location robustness test, and study the tradeoff between attack strength and effectiveness.",1
"We present a method for efficient differentiable simulation of articulated bodies. This enables integration of articulated body dynamics into deep learning frameworks, and gradient-based optimization of neural networks that operate on articulated bodies. We derive the gradients of the forward dynamics using spatial algebra and the adjoint method. Our approach is an order of magnitude faster than autodiff tools. By only saving the initial states throughout the simulation process, our method reduces memory requirements by two orders of magnitude. We demonstrate the utility of efficient differentiable dynamics for articulated bodies in a variety of applications. We show that reinforcement learning with articulated systems can be accelerated using gradients provided by our method. In applications to control and inverse problems, gradient-based optimization enabled by our work accelerates convergence by more than an order of magnitude.",0
"Our technique allows for the effective differentiation of articulated body simulations, making it possible to integrate articulated body dynamics into deep learning frameworks and optimize neural networks through gradient-based methods. Using spatial algebra and the adjoint method, we have derived the forward dynamics gradients, which are significantly faster than autodiff tools. Our method can reduce memory requirements by two orders of magnitude by only saving the initial states throughout the simulation. Our approach has proven useful for various applications, such as acceleration of reinforcement learning with articulated systems and gradient-based optimization for control and inverse problems, which has resulted in convergence acceleration of over an order of magnitude.",1
"In this paper, it has attempted to use Reinforcement learning to model the proper dosage of Warfarin for patients.The paper first examines two baselines: a fixed model of 35 mg/week dosages and a linear model that relies on patient data. We implemented a LinUCB bandit that improved performance measured on regret and percent incorrect. On top of the LinUCB bandit, we experimented with online supervised learning and reward reshaping to boost performance. Our results clearly beat the baselines and show the promise of using multi-armed bandits and artificial intelligence to aid physicians in deciding proper dosages.",0
"The main focus of this paper was to utilize Reinforcement learning for modeling the appropriate amount of Warfarin for patients. The initial examination involved the comparison of two baselines: a fixed model of 35 mg/week dosages and a linear model that relied on patient data. To improve performance, a LinUCB bandit was implemented, which showed a better performance based on regret and percent incorrect. Furthermore, online supervised learning and reward reshaping were experimented with to further enhance the performance of the LinUCB bandit. Ultimately, the results surpassed the baselines and highlighted the potential of using artificial intelligence and multi-armed bandits to assist physicians in determining the appropriate dosages.",1
"It is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery from observational data. However, searching the space of directed graphs and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the existing RL-based method to small scale problems. In this work, we propose a novel RL-based approach for causal discovery, by incorporating RL into the ordering-based paradigm. Specifically, we formulate the ordering search problem as a multi-step Markov decision process, implement the ordering generating process with an encoder-decoder architecture, and finally use RL to optimize the proposed model based on the reward mechanisms designed for~each ordering. A generated ordering would then be processed using variable selection to obtain the final causal graph. We analyze the consistency and computational complexity of the proposed method, and empirically show that a pretrained model can be exploited to accelerate training. Experimental results on both synthetic and real data sets shows that the proposed method achieves a much improved performance over existing RL-based method.",0
"In various empirical sciences, discovering causal relations among a set of variables has been a long-standing question. Reinforcement Learning (RL) has recently shown promising outcomes in causal discovery from observational data. However, the search for directed graphs and the enforcement of acyclicity through implicit penalties can be inefficient and restrict the existing RL-based method to small-scale problems. This paper presents a new RL-based approach for causal discovery by incorporating RL into the ordering-based paradigm. The ordering search problem is formulated as a multi-step Markov decision process, and the ordering generating process is implemented with an encoder-decoder architecture. RL is then used to optimize the proposed model based on the reward mechanisms designed for each ordering. A generated ordering is processed using variable selection to obtain the final causal graph. The consistency and computational complexity of the proposed method are analyzed, and it is empirically demonstrated that a pretrained model can be used to accelerate training. Experimental results on both synthetic and real datasets show that the proposed method outperforms the existing RL-based method.",1
"We consider the problem of offline reinforcement learning with model-based control, whose goal is to learn a dynamics model from the experience replay and obtain a pessimism-oriented agent under the learned model. Current model-based constraint includes explicit uncertainty penalty and implicit conservative regularization that pushes Q-values of out-of-distribution state-action pairs down and the in-distribution up. While the uncertainty estimation, on which the former relies on, can be loosely calibrated for complex dynamics, the latter performs slightly better. To extend the basic idea of regularization without uncertainty quantification, we propose distributionally robust offline model-based policy optimization (DROMO), which leverages the ideas in distributionally robust optimization to penalize a broader range of out-of-distribution state-action pairs beyond the standard empirical out-of-distribution Q-value minimization. We theoretically show that our method optimizes a lower bound on the ground-truth policy evaluation, and it can be incorporated into any existing policy gradient algorithms. We also analyze the theoretical properties of DROMO's linear and non-linear instantiations.",0
"Our focus is on offline reinforcement learning with model-based control. Our objective is to develop a pessimism-oriented agent by learning a dynamics model from experience replay. The current model-based constraint involves explicit uncertainty penalty and implicit conservative regularization, which aims to decrease Q-values of out-of-distribution state-action pairs and increase those of the in-distribution. However, uncertainty estimation is imprecise for complex dynamics, and the conservative regularization performs slightly better. To expand on the regularization idea without uncertainty quantification, we introduce distributionally robust offline model-based policy optimization (DROMO). This method employs distributionally robust optimization to penalize a broader range of out-of-distribution state-action pairs, extending beyond the empirical out-of-distribution Q-value minimization. We prove that our approach optimizes a lower bound on the ground-truth policy evaluation and can be integrated into any current policy gradient algorithms. Additionally, we examine the theoretical properties of DROMO's linear and nonlinear implementations.",1
"In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. In classical Reinforcement Learning (RL) setups, this capacity is learned from reward alone. We introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input.",0
"To efficiently reach known goals in a new environment, an agent must possess the ability to map it and utilize its observation history. This involves spatial reasoning, where the agent perceives spatial relationships, identifies regularities, and discovers object characteristics. In Reinforcement Learning, this capacity is typically learned through reward-based methods. However, we introduce auxiliary tasks that provide supplementary supervision to encourage the emergence of spatial perception capabilities in agents trained for downstream goal-reaching objectives. By learning to estimate metrics that quantify spatial relationships between the agent's location and goal, our approach significantly improves the performance of baseline agents in Multi-Object Navigation settings. Our method even matches the performance of oracle agents that use ground-truth maps as input, surpassing both explicit and implicit environment representation approaches.",1
"This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.",0
"In this survey, various Monte Carlo techniques that employ surrogate models are examined as a means of dealing with densities that are either intractable, costly, or noisy. Such problems are commonly encountered in stochastic optimization and reinforcement learning, where computing the density function may be expensive or involve physical costs that can yield different results each time. While surrogate models eliminate such costs, choosing and designing these methods requires careful consideration of important trade-offs. The study categorizes these methods into three classes and presents specific algorithmic instances under a unified notation, along with a modular approach that encompasses all the considered methods. The survey also discusses different application scenarios, with a particular focus on the likelihood-free setting and reinforcement learning, and offers several numerical comparisons.",1
"In this paper we revisit some of the fundamental premises for a reinforcement learning (RL) approach to self-learning traffic lights. We propose RLight, a combination of choices that offers robust performance and good generalization to unseen traffic flows. In particular, our main contributions are threefold: our lightweight and cluster-aware state representation leads to improved performance; we reformulate the MDP such that it skips redundant timesteps of yellow light, speeding up learning by 30%; and we investigate the action space and provide insight into the difference in performance between acyclic and cyclic phase transitions. Additionally, we provide insights into the generalisation of the methods to unseen traffic. Evaluations using the real-world Hangzhou traffic dataset show that RLight outperforms state-of-the-art rule-based and deep reinforcement learning algorithms, demonstrating the potential of RL-based methods to improve urban traffic flows.",0
"This paper aims to reconsider the basic principles of using reinforcement learning (RL) for self-learning traffic lights. Our proposed approach, named RLight, combines several options to achieve strong performance and effective generalization to unfamiliar traffic patterns. Our contributions are threefold: firstly, we utilize a lightweight and cluster-aware state representation to enhance performance; secondly, we modify the MDP to skip unnecessary yellow light timesteps for a 30% increase in learning speed; thirdly, we investigate the action space and offer insight into the performance differences between acyclic and cyclic phase transitions. Furthermore, we provide insights into the applicability of these methods to new traffic scenarios. Our evaluations using the Hangzhou traffic dataset demonstrate that RLight surpasses both rule-based and deep reinforcement learning methods, showcasing the potential for RL techniques to optimize urban traffic.",1
"We consider the problem of optimal charging/discharging of a bank of heterogenous battery units, driven by stochastic electricity generation and demand processes. The batteries in the battery bank may differ with respect to their capacities, ramp constraints, losses, as well as cycling costs. The goal is to minimize the degradation costs associated with battery cycling in the long run; this is posed formally as a Markov decision process. We propose a linear function approximation based Q-learning algorithm for learning the optimal solution, using a specially designed class of kernel functions that approximate the structure of the value functions associated with the MDP. The proposed algorithm is validated via an extensive case study.",0
"The problem we are addressing involves finding the best way to charge and discharge a group of batteries that have varying features, such as capacity, ramp constraints, losses, and cycling costs. This is influenced by unpredictable electricity generation and demand patterns. Our objective is to minimize the long-term costs of battery degradation caused by cycling, which we model as a Markov decision process. To find the optimal solution, we introduce a Q-learning algorithm that utilizes linear function approximation and a specific type of kernel functions that can approximate the value function structure of the MDP. We verify the effectiveness of our approach through a comprehensive case study.",1
"The dialogue management component of a task-oriented dialogue system is typically optimised via reinforcement learning (RL). Optimisation via RL is highly susceptible to sample inefficiency and instability. The hierarchical approach called Feudal Dialogue Management takes a step towards more efficient learning by decomposing the action space. However, it still suffers from instability due to the reward only being provided at the end of the dialogue. We propose the usage of an intrinsic reward based on information gain to address this issue. Our proposed reward favours actions that resolve uncertainty or query the user whenever necessary. It enables the policy to learn how to retrieve the users' needs efficiently, which is an integral aspect in every task-oriented conversation. Our algorithm, which we call FeudalGain, achieves state-of-the-art results in most environments of the PyDial framework, outperforming much more complex approaches. We confirm the sample efficiency and stability of our algorithm through experiments in simulation and a human trial.",0
"Reinforcement learning (RL) is commonly used to optimize the dialogue management component of task-oriented dialogue systems. However, RL is prone to sample inefficiency and instability. Feudal Dialogue Management is a hierarchical approach that decomposes the action space to enhance learning efficiency but still suffers from instability due to the reward being provided only at the end of the dialogue. To overcome this problem, we propose using an intrinsic reward based on information gain, which favors actions that resolve uncertainty or query the user when needed. Our proposed reward enables the policy to efficiently retrieve the user's needs, a crucial aspect of task-oriented conversations. Our algorithm, called FeudalGain, outperforms more complex approaches and achieves state-of-the-art results in most environments of the PyDial framework. We demonstrate the sample efficiency and stability of our algorithm through simulation experiments and a human trial.",1
"Fluid human-agent communication is essential for the future of human-in-the-loop reinforcement learning. An agent must respond appropriately to feedback from its human trainer even before they have significant experience working together. Therefore, it is important that learning agents respond well to various feedback schemes human trainers are likely to provide. This work analyzes the COnvergent Actor-Critic by Humans (COACH) algorithm under three different types of feedback-policy feedback, reward feedback, and advantage feedback. For these three feedback types, we find that COACH can behave sub-optimally. We propose a variant of COACH, episodic COACH (E-COACH), which we prove converges for all three types. We compare our COACH variant with two other reinforcement-learning algorithms: Q-learning and TAMER.",0
"For the advancement of human-in-the-loop reinforcement learning, it is crucial to establish smooth communication between humans and agents. The agent should be capable of interpreting the trainer's feedback, even in the early stages of their collaboration. Hence, the learning agents must be able to respond effectively to different feedback patterns that human trainers may provide. In this study, we examine the COACH algorithm's performance under three types of feedback - policy feedback, reward feedback, and advantage feedback. Our findings reveal that COACH may not always achieve optimal outcomes. To address this, we introduce a new variant of COACH, E-COACH, which we demonstrate to be capable of convergence for all three feedback types. We also compare our COACH variant with two other reinforcement-learning algorithms, Q-learning and TAMER.",1
"We study session-based recommendation scenarios where we want to recommend items to users during sequential interactions to improve their long-term utility. Optimizing a long-term metric is challenging because the learning signal (whether the recommendations achieved their desired goals) is delayed and confounded by other user interactions with the system. Targeting immediately measurable proxies such as clicks can lead to suboptimal recommendations due to misalignment with the long-term metric. We develop a new reinforcement learning algorithm called Short Horizon Policy Improvement (SHPI) that approximates policy-induced drift in user behavior across sessions. SHPI is a straightforward modification of episodic RL algorithms for session-based recommendation, that additionally gives an appropriate termination bonus in each session. Empirical results on four recommendation tasks show that SHPI can outperform state-of-the-art recommendation techniques like matrix factorization with offline proxy signals, bandits with myopic online proxies, and RL baselines with limited amounts of user interaction.",0
"Our focus is on improving long-term utility for users through session-based recommendations. However, optimizing for this can be difficult since the learning signal is delayed and influenced by other user interactions. Instead of relying on immediately measurable proxies like clicks, which can lead to suboptimal recommendations, we introduce a new reinforcement learning algorithm called Short Horizon Policy Improvement (SHPI). This algorithm approximates policy-induced drift in user behavior across sessions and includes an appropriate termination bonus in each session. Through empirical results on four recommendation tasks, we demonstrate that SHPI outperforms other state-of-the-art recommendation techniques, including matrix factorization and bandits with myopic online proxies, as well as RL baselines with limited user interaction.",1
"In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimisation. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimisation in parameter space can be leveraged to build a prior population, which when used to initialise QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to implement and scale, and furthermore, it is agnostic to the underlying models that are being trained. Experiments carried in both sparse and dense reward settings using robotic manipulation and navigation benchmarks show that it considerably reduces the number of generations that are required for QD optimisation in these environments.",0
"Recent research has focused on exploiting past learning experiences and designing Few-shot and Meta Learning approaches for various problem domains such as Computer Vision and Reinforcement Learning based control. However, Quality-Diversity (QD) optimization has received little to no attention in this regard, despite its effectiveness in dealing with deceptive minima and sparse rewards in Reinforcement Learning. The main issue with QD methods is their reliance on costly evolutionary processes. To overcome this, we propose a method that leverages information about optimization paths in parameter space to build a prior population from examples of a task distribution. This allows for few-shot adaptation when initializing QD methods in new environments without requiring backpropagation. Our method is simple to implement and scale, and it is model-agnostic. Our experiments in robotic manipulation and navigation benchmarks with both sparse and dense reward settings show that our proposed method significantly reduces the number of generations required for QD optimization in these environments.",1
"In a multirobot system, a number of cyber-physical attacks (e.g., communication hijack, observation perturbations) can challenge the robustness of agents. This robustness issue worsens in multiagent reinforcement learning because there exists the non-stationarity of the environment caused by simultaneously learning agents whose changing policies affect the transition and reward functions. In this paper, we propose a minimax MARL approach to infer the worst-case policy update of other agents. As the minimax formulation is computationally intractable to solve, we apply the convex relaxation of neural networks to solve the inner minimization problem. Such convex relaxation enables robustness in interacting with peer agents that may have significantly different behaviors and also achieves a certified bound of the original optimization problem. We evaluate our approach on multiple mixed cooperative-competitive tasks and show that our method outperforms the previous state of the art approaches on this topic.",0
"The resilience of agents in a multirobot system can be compromised by various cyber-physical attacks, such as communication hijack and observation perturbations. This issue is exacerbated in multiagent reinforcement learning due to the non-stationarity of the environment caused by agents learning simultaneously, leading to changes in transition and reward functions. This paper proposes a minimax MARL approach to address this problem by inferring the worst-case policy update of other agents. However, the minimax formulation is challenging to solve, so the paper applies the convex relaxation of neural networks to solve the inner minimization problem. This approach enables robustness in interacting with agents with different behaviors and achieves a certified bound of the original optimization problem. The paper evaluates this approach on multiple mixed cooperative-competitive tasks and demonstrates that it outperforms previous state-of-the-art approaches in this area.",1
"Soccer is a sparse rewarding game: any smart or careless action in critical situations can change the result of the match. Therefore players, coaches, and scouts are all curious about the best action to be performed in critical situations, such as the times with a high probability of losing ball possession or scoring a goal. This work proposes a new state representation for the soccer game and a batch reinforcement learning to train a smart policy network. This network gets the contextual information of the situation and proposes the optimal action to maximize the expected goal for the team. We performed extensive numerical experiments on the soccer logs made by InStat for 104 European soccer matches. The results show that in all 104 games, the optimized policy obtains higher rewards than its counterpart in the behavior policy. Besides, our framework learns policies that are close to the expected behavior in the real world. For instance, in the optimized policy, we observe that some actions such as foul, or ball out can be sometimes more rewarding than a shot in specific situations.",0
"Soccer is a game that can yield few rewards, where a single wise or imprudent move in crucial moments can alter the outcome of the match. This has caused great curiosity among players, coaches, and scouts as to the optimal actions to take in such critical situations, including instances with a high chance of losing possession of the ball or scoring a goal. In this study, we introduce a novel state representation for soccer and employ batch reinforcement learning to train an intelligent policy network. This network receives contextual information of the situation and suggests the most effective action to maximize the team's expected goal. We conducted extensive numerical experiments on soccer logs from 104 European matches provided by InStat. Our results indicate that the optimized policy consistently yields higher rewards than its behavior policy counterpart in all 104 games. Additionally, our approach produces policies that closely resemble expected behavior in the real world. Notably, we observe that in specific situations, actions such as fouls or ball outs may be more rewarding than shooting in the optimized policy.",1
"Multi-Agent reinforcement learning has received lot of attention in recent years and have applications in many different areas. Existing methods involving Centralized Training and Decentralized execution, attempts to train the agents towards learning a pattern of coordinated actions to arrive at optimal joint policy. However if some agents are stochastic to varying degrees of stochasticity, the above methods often fail to converge and provides poor coordination among agents. In this paper we show how this stochasticity of agents, which could be a result of malfunction or aging of robots, can add to the uncertainty in coordination and there contribute to unsatisfactory global coordination. In this case, the deterministic agents have to understand the behavior and limitations of the stochastic agents while arriving at optimal joint policy. Our solution, DSDF which tunes the discounted factor for the agents according to uncertainty and use the values to update the utility networks of individual agents. DSDF also helps in imparting an extent of reliability in coordination thereby granting stochastic agents tasks which are immediate and of shorter trajectory with deterministic ones taking the tasks which involve longer planning. Such an method enables joint co-ordinations of agents some of which may be partially performing and thereby can reduce or delay the investment of agent/robot replacement in many circumstances. Results on benchmark environment for different scenarios shows the efficacy of the proposed approach when compared with existing approaches.",0
"In recent years, Multi-Agent reinforcement learning has gained significant attention and has been applied in various domains. The current methods, such as Centralized Training and Decentralized execution, aim to train the agents to learn a coherent set of actions for optimal joint policy. However, if some of the agents exhibit stochastic behavior, which may be due to malfunction or aging of robots, the existing methods fail to converge, resulting in poor coordination among the agents. In this study, we explore how the stochastic behavior of agents can contribute to uncertainty in coordination, leading to unsatisfactory global coordination. To address this issue, we propose a solution called DSDF, which adjusts the discounted factor for the agents based on their level of uncertainty and uses these values to update the utility networks of individual agents. DSDF not only improves coordination but also imparts reliability by assigning immediate and short-trajectory tasks to stochastic agents and longer planning tasks to deterministic ones. This approach facilitates joint coordination among partially performing agents, thereby avoiding the need for agent/robot replacement in many cases. Our experimental results on benchmark environments demonstrate the effectiveness of our proposed approach compared to existing methods.",1
"Driving in a complex urban environment is a difficult task that requires a complex decision policy. In order to make informed decisions, one needs to gain an understanding of the long-range context and the importance of other vehicles. In this work, we propose to use Vision Transformer (ViT) to learn a driving policy in urban settings with birds-eye-view (BEV) input images. The ViT network learns the global context of the scene more effectively than with earlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's attention mechanism helps to learn an attention map for the scene which allows the ego car to determine which surrounding cars are important to its next decision. We demonstrate that a DQN agent with a ViT backbone outperforms baseline algorithms with ConvNet backbones pre-trained in various ways. In particular, the proposed method helps reinforcement learning algorithms to learn faster, with increased performance and less data than baselines.",0
"To drive in a complex urban environment, a sophisticated decision policy is necessary, which requires an understanding of the long-range context and the significance of other vehicles. Our proposed approach utilizes Vision Transformer (ViT) to learn a driving policy in urban settings with birds-eye-view (BEV) input images. The ViT network effectively learns the global context of the scene, surpassing earlier proposed Convolutional Neural Networks (ConvNets). Additionally, ViT's attention mechanism allows the ego car to determine the importance of surrounding cars to its next decision by learning an attention map for the scene. Our research demonstrates that a DQN agent with a ViT backbone outperforms baseline algorithms with ConvNet backbones pre-trained in various ways, enabling reinforcement learning algorithms to learn quicker, with improved performance and less data than the baselines.",1
"When designing algorithms for finite-time-horizon episodic reinforcement learning problems, a common approach is to introduce a fictitious discount factor and use stationary policies for approximations. Empirically, it has been shown that the fictitious discount factor helps reduce variance, and stationary policies serve to save the per-iteration computational cost. Theoretically, however, there is no existing work on convergence analysis for algorithms with this fictitious discount recipe. This paper takes the first step towards analyzing these algorithms. It focuses on two vanilla policy gradient (VPG) variants: the first being a widely used variant with discounted advantage estimations (DAE), the second with an additional fictitious discount factor in the score functions of the policy gradient estimators. Non-asymptotic convergence guarantees are established for both algorithms, and the additional discount factor is shown to reduce the bias introduced in DAE and thus improve the algorithm convergence asymptotically. A key ingredient of our analysis is to connect three settings of Markov decision processes (MDPs): the finite-time-horizon, the average reward and the discounted settings. To our best knowledge, this is the first theoretical guarantee on fictitious discount algorithms for the episodic reinforcement learning of finite-time-horizon MDPs, which also leads to the (first) global convergence of policy gradient methods for finite-time-horizon episodic reinforcement learning.",0
"One approach commonly used in designing algorithms for finite-time-horizon episodic reinforcement learning problems involves introducing a fictitious discount factor and utilizing stationary policies for approximations. According to empirical evidence, the incorporation of the fictitious discount factor helps to minimize variance and stationary policies are beneficial in terms of reducing per-iteration computational costs. However, there is currently no existing research on the convergence analysis of algorithms utilizing this discount recipe. This paper aims to fill this gap by analyzing two vanilla policy gradient (VPG) variants, one with discounted advantage estimations (DAE) and the other with an additional fictitious discount factor in the score functions of policy gradient estimators. The paper establishes non-asymptotic convergence guarantees for both algorithms and demonstrates how the additional discount factor reduces the bias introduced in DAE, thus improving the asymptotic algorithm convergence. The paper also connects three settings of Markov decision processes (MDPs): finite-time-horizon, average reward, and discounted settings. This is the first study to provide theoretical guarantees for fictitious discount algorithms in finite-time-horizon MDPs and global convergence of policy gradient methods for finite-time-horizon episodic reinforcement learning.",1
"Due to the advances in computing and sensing, deep learning (DL) has widely been applied in smart energy systems (SESs). These DL-based solutions have proved their potentials in improving the effectiveness and adaptiveness of the control systems. However, in recent years, increasing evidence shows that DL techniques can be manipulated by adversarial attacks with carefully-crafted perturbations. Adversarial attacks have been studied in computer vision and natural language processing. However, there is very limited work focusing on the adversarial attack deployment and mitigation in energy systems. In this regard, to better prepare the SESs against potential adversarial attacks, we propose an innovative adversarial attack model that can practically compromise dynamical controls of energy system. We also optimize the deployment of the proposed adversarial attack model by employing deep reinforcement learning (RL) techniques. In this paper, we present our first-stage work in this direction. In simulation section, we evaluate the performance of our proposed adversarial attack model using standard IEEE 9-bus system.",0
"Thanks to advancements in computing and sensing, deep learning (DL) is now widely utilized in smart energy systems (SESs) to enhance the effectiveness and adaptiveness of control systems. However, recent studies have shown that carefully-crafted perturbations can manipulate DL techniques, leading to adversarial attacks. Although adversarial attacks have been researched in computer vision and natural language processing, there is limited work on their deployment and mitigation in energy systems. Therefore, to better prepare SESs against potential adversarial attacks, we propose an innovative adversarial attack model that compromises dynamical controls of energy systems. Additionally, we optimize the deployment of this proposed model using deep reinforcement learning (RL) techniques. This paper presents our first-stage work in this direction, where we evaluate the performance of our proposed adversarial attack model on the standard IEEE 9-bus system in the simulation section.",1
"Reinforcement learning is widely used in applications where one needs to perform sequential decisions while interacting with the environment. The problem becomes more challenging when the decision requirement includes satisfying some safety constraints. The problem is mathematically formulated as constrained Markov decision process (CMDP). In the literature, various algorithms are available to solve CMDP problems in a model-free manner to achieve $\epsilon$-optimal cumulative reward with $\epsilon$ feasible policies. An $\epsilon$-feasible policy implies that it suffers from constraint violation. An important question here is whether we can achieve $\epsilon$-optimal cumulative reward with zero constraint violations or not. To achieve that, we advocate the use of a randomized primal-dual approach to solving the CMDP problems and propose a conservative stochastic primal-dual algorithm (CSPDA) which is shown to exhibit $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity to achieve $\epsilon$-optimal cumulative reward with zero constraint violations. In the prior works, the best available sample complexity for the $\epsilon$-optimal policy with zero constraint violation is $\tilde{\mathcal{O}}(1/\epsilon^5)$. Hence, the proposed algorithm provides a significant improvement as compared to the state of the art.",0
"Reinforcement learning is a common approach for making sequential decisions while interacting with the environment. However, incorporating safety constraints into the decision-making process poses a greater challenge. This problem is known as constrained Markov decision process (CMDP), and there are several model-free algorithms available to solve it and achieve $\epsilon$-optimal cumulative reward with $\epsilon$ feasible policies. However, feasible policies may still violate constraints. The question is whether we can achieve $\epsilon$-optimal cumulative reward without violating any constraints. To address this, we propose a randomized primal-dual approach using a conservative stochastic primal-dual algorithm (CSPDA). The CSPDA algorithm has a sample complexity of $\tilde{\mathcal{O}}(1/\epsilon^2)$, which represents a significant improvement compared to the prior state of the art, which had a sample complexity of $\tilde{\mathcal{O}}(1/\epsilon^5)$.",1
"We propose Automatic Curricula via Expert Demonstrations (ACED), a reinforcement learning (RL) approach that combines the ideas of imitation learning and curriculum learning in order to solve challenging robotic manipulation tasks with sparse reward functions. Curriculum learning solves complicated RL tasks by introducing a sequence of auxiliary tasks with increasing difficulty, yet how to automatically design effective and generalizable curricula remains a challenging research problem. ACED extracts curricula from a small amount of expert demonstration trajectories by dividing demonstrations into sections and initializing training episodes to states sampled from different sections of demonstrations. Through moving the reset states from the end to the beginning of demonstrations as the learning agent improves its performance, ACED not only learns challenging manipulation tasks with unseen initializations and goals, but also discovers novel solutions that are distinct from the demonstrations. In addition, ACED can be naturally combined with other imitation learning methods to utilize expert demonstrations in a more efficient manner, and we show that a combination of ACED with behavior cloning allows pick-and-place tasks to be learned with as few as 1 demonstration and block stacking tasks to be learned with 20 demonstrations.",0
"Our proposed method, Automatic Curricula via Expert Demonstrations (ACED), utilizes a reinforcement learning (RL) approach that incorporates both imitation learning and curriculum learning to solve complex robotic manipulation tasks with limited reward functions. While curriculum learning has been successful in addressing difficult RL tasks by introducing a sequence of progressively challenging auxiliary tasks, designing effective and generalizable curricula automatically remains a significant research challenge. ACED solves this problem by extracting curricula from a small number of expert demonstration trajectories. It does so by dividing demonstrations into sections and initializing training episodes to states sampled from different sections of demonstrations. By moving reset states from the end to the beginning of demonstrations as the agent's performance improves, ACED can learn challenging manipulation tasks with new initializations and goals, as well as discover novel solutions distinct from the demonstrations. Moreover, ACED can be combined with other imitation learning methods to use expert demonstrations more efficiently. By combining ACED with behavior cloning, pick-and-place tasks can be learned with just one demonstration, and block stacking tasks can be learned with as few as 20 demonstrations.",1
"An interesting observation in artificial neural networks is their favorable generalization error despite typically being extremely overparameterized. It is well known that classical statistical learning methods often result in vacuous generalization errors in the case of overparameterized neural networks. Adopting the recently developed Neural Tangent (NT) kernel theory, we prove uniform generalization bounds for overparameterized neural networks in kernel regimes, when the true data generating model belongs to the reproducing kernel Hilbert space (RKHS) corresponding to the NT kernel. Importantly, our bounds capture the exact error rates depending on the differentiability of the activation functions. In order to establish these bounds, we propose the information gain of the NT kernel as a measure of complexity of the learning problem. Our analysis uses a Mercer decomposition of the NT kernel in the basis of spherical harmonics and the decay rate of the corresponding eigenvalues. As a byproduct of our results, we show the equivalence between the RKHS corresponding to the NT kernel and its counterpart corresponding to the Mat\'ern family of kernels, that induces a very general class of models. We further discuss the implications of our analysis for some recent results on the regret bounds for reinforcement learning algorithms, which use overparameterized neural networks.",0
"Despite their overparameterization, artificial neural networks exhibit a favorable generalization error, which is not the case for classical statistical learning methods. By adopting the Neural Tangent (NT) kernel theory, we have been able to prove uniform generalization bounds for overparameterized neural networks in kernel regimes. This is achieved when the true data generating model belongs to the reproducing kernel Hilbert space (RKHS) corresponding to the NT kernel. Our bounds are unique as they capture the exact error rates based on the differentiability of the activation functions. To establish these bounds, we use the information gain of the NT kernel as a measure of complexity in the learning problem. Additionally, our analysis employs a Mercer decomposition of the NT kernel in the basis of spherical harmonics and the decay rate of the corresponding eigenvalues. Our findings also reveal the equivalence between the RKHS corresponding to the NT kernel and its counterpart, which belongs to the Mat\'ern family of kernels and induces a broad class of models. Lastly, we discuss the implications of our analysis on recent results regarding the regret bounds for reinforcement learning algorithms that utilize overparameterized neural networks.",1
"Credit assignment is one of the central problems in reinforcement learning. The predominant approach is to assign credit based on the expected return. However, we show that the expected return may depend on the policy in an undesirable way which could slow down learning. Instead, we borrow ideas from the causality literature and show that the advantage function can be interpreted as causal effects, which share similar properties with causal representations. Based on this insight, we propose the Direct Advantage Estimation (DAE), a novel method that can model the advantage function and estimate it directly from data without requiring the (action-)value function. If desired, value functions can also be seamlessly integrated into DAE and be updated in a similar way to Temporal Difference Learning. The proposed method is easy to implement and can be readily adopted by modern actor-critic methods. We test DAE empirically on the Atari domain and show that it can achieve competitive results with the state-of-the-art method for advantage estimation.",0
"Reinforcement learning encounters a significant issue in the determination of credit assignment. The common method is to allocate credit based on the expected return. However, this approach can impede learning due to the undesirable dependency of expected return on policy. To tackle this problem, we draw inspiration from the causality literature and demonstrate that the advantage function can be interpreted as causal effects that share similar traits with causal representations. Consequently, we introduce a new method, Direct Advantage Estimation (DAE), that can model and estimate the advantage function directly from data, without requiring the (action-)value function. The DAE method can seamlessly integrate value functions and update them in a similar fashion to Temporal Difference Learning. It is simple to implement and can be easily incorporated into modern actor-critic methods. To demonstrate its efficiency, we test DAE empirically on Atari and establish that it can achieve competitive outcomes with the current state-of-the-art method for advantage estimation.",1
"A promising characteristic of Deep Reinforcement Learning (DRL) is its capability to learn optimal policy in an end-to-end manner without relying on feature engineering. However, most approaches assume a fully observable state space, i.e. fully observable Markov Decision Processes (MDPs). In real-world robotics, this assumption is unpractical, because of issues such as sensor sensitivity limitations and sensor noise, and the lack of knowledge about whether the observation design is complete or not. These scenarios lead to Partially Observable MDPs (POMDPs). In this paper, we propose Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient (LSTM-TD3) by introducing a memory component to TD3, and compare its performance with other DRL algorithms in both MDPs and POMDPs. Our results demonstrate the significant advantages of the memory component in addressing POMDPs, including the ability to handle missing and noisy observation data.",0
"Deep Reinforcement Learning (DRL) has the potential to learn optimal policy without the need for feature engineering. However, most approaches rely on fully observable Markov Decision Processes (MDPs), which is not practical in real-world robotics due to issues such as sensor limitations and incomplete observation design. This results in Partially Observable MDPs (POMDPs). We propose LSTM-TD3, a Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient (TD3) that introduces a memory component to address POMDPs. Our study compares the performance of LSTM-TD3 with other DRL algorithms in both MDPs and POMDPs, demonstrating the advantages of the memory component in handling missing and noisy observation data.",1
"This work introduces a neuro-symbolic agent that combines deep reinforcement learning (DRL) with temporal logic (TL) to achieve systematic zero-shot, i.e., never-seen-before, generalisation of formally specified instructions. In particular, we present a neuro-symbolic framework where a symbolic module transforms TL specifications into a form that helps the training of a DRL agent targeting generalisation, while a neural module learns systematically to solve the given tasks. We study the emergence of systematic learning in different settings and find that the architecture of the convolutional layers is key when generalising to new instructions. We also provide evidence that systematic learning can emerge with abstract operators such as negation when learning from a few training examples, which previous research have struggled with.",0
"In this work, a neuro-symbolic agent is introduced that utilizes the combination of deep reinforcement learning (DRL) and temporal logic (TL) to accomplish systematic zero-shot generalization of formally specified instructions that have never been seen before. A neuro-symbolic framework is presented where a symbolic module converts TL specifications into a format that facilitates the training of a DRL agent that aims for generalization, while a neural module systematically learns to solve the assigned tasks. The emergence of systematic learning is explored in different settings, with the architecture of convolutional layers being identified as critical when generalizing to new instructions. Additionally, it is shown that systematic learning can arise with abstract operators like negation when learning from a few training examples, a challenge that prior research has encountered.",1
"Differentiable neural architecture search (DNAS) is known for its capacity in the automatic generation of superior neural networks. However, DNAS based methods suffer from memory usage explosion when the search space expands, which may prevent them from running successfully on even advanced GPU platforms. On the other hand, reinforcement learning (RL) based methods, while being memory efficient, are extremely time-consuming. Combining the advantages of both types of methods, this paper presents RADARS, a scalable RL-aided DNAS framework that can explore large search spaces in a fast and memory-efficient manner. RADARS iteratively applies RL to prune undesired architecture candidates and identifies a promising subspace to carry out DNAS. Experiments using a workstation with 12 GB GPU memory show that on CIFAR-10 and ImageNet datasets, RADARS can achieve up to 3.41% higher accuracy with 2.5X search time reduction compared with a state-of-the-art RL-based method, while the two DNAS baselines cannot complete due to excessive memory usage or search time. To the best of the authors' knowledge, this is the first DNAS framework that can handle large search spaces with bounded memory usage.",0
"Although Differentiable neural architecture search (DNAS) is recognized for its ability to automatically generate high-quality neural networks, it encounters a problem of memory usage explosion as the search space expands. This can hinder its success even on advanced GPU platforms. Conversely, reinforcement learning (RL) based methods are memory efficient but are excessively time-consuming. To address this, this paper introduces RADARS, a scalable RL-aided DNAS framework that can efficiently explore large search spaces. RADARS uses RL to eliminate unwanted architecture candidates and identify a promising subspace for DNAS. Experiments on CIFAR-10 and ImageNet datasets show that RADARS can achieve up to 3.41% higher accuracy with 2.5X search time reduction compared with a state-of-the-art RL-based method, while the two DNAS baselines fail due to excessive memory usage or search time. Notably, this is the first DNAS framework that can handle large search spaces with limited memory usage.",1
"Federated learning (FL) is a privacy-preserving machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous users without gathering their data. Extending FL beyond the conventional supervised learning paradigm, federated Reinforcement Learning (RL) was proposed to handle sequential decision-making problems for various privacy-sensitive applications such as autonomous driving. However, the existing federated RL algorithms directly combine model-free RL with FL, and thus generally have high sample complexity and lack theoretical guarantees. To address the above challenges, we propose a new federated RL algorithm that incorporates model-based RL and ensemble knowledge distillation into FL. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models from clients, and then train the policy by solely using the ensemble model without interacting with the real environment. Furthermore, we theoretically prove that the monotonic improvement of the proposed algorithm is guaranteed. Extensive experimental results demonstrate that our algorithm obtains significantly higher sample efficiency compared to federated model-free RL algorithms in the challenging continuous control benchmark environments. The results also show the impact of non-IID client data and local update steps on the performance of federated RL, validating the insights obtained from our theoretical analysis.",0
"Federated learning (FL) is a machine learning approach that maintains privacy by enabling cooperative training among diverse users without collecting their data. Federated Reinforcement Learning (RL) is an extension of FL that addresses privacy-sensitive applications such as autonomous driving by dealing with sequential decision-making problems. However, existing federated RL methods combine model-free RL with FL, resulting in high sample complexity and a lack of theoretical guarantees. We present a novel federated RL algorithm that integrates model-based RL and ensemble knowledge distillation into FL to overcome these challenges. We use FL and knowledge distillation to create an ensemble of dynamics models from clients, then train the policy solely with the ensemble model, without interacting with the actual environment. Furthermore, we prove theoretically that our algorithm guarantees a monotonic improvement. Our algorithm achieves significantly higher sample efficiency than federated model-free RL methods in complex continuous control benchmark environments. Experimental results also demonstrate the impact of non-IID client data and local update steps on federated RL performance, confirming our theoretical analysis insights.",1
"Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.",0
"Hybrid action spaces that combine both discrete and continuous actions are often found in practical problems such as robot control and game AI. However, previous Reinforcement Learning (RL) studies have typically focused on either discrete or continuous action spaces, neglecting hybrid action spaces. One approach to address hybrid action RL is to convert the hybrid action space into a unified action space through discretization or continualization. However, this approach overlooks the structure of the hybrid action space, leading to scalability issues and suboptimal results. To tackle this issue, we propose Hybrid Action Representation (HyAR), which learns a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the relationship between discrete action and continuous parameters through an embedding table and a conditional Variantional Auto-Encoder (VAE). The action representation is further trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, conventional DRL algorithms are used to learn the policy in the learned representation space, while interactions with the environment occur through the decoding of hybrid action embeddings to the original action space. HyAR is evaluated in various environments with discrete-continuous action spaces, demonstrating its superior performance compared to previous baselines, especially in high-dimensional action spaces.",1
"We consider the problem of tabular infinite horizon concave utility reinforcement learning (CURL) with convex constraints. Various learning applications with constraints, such as robotics, do not allow for policies that can violate constraints. To this end, we propose a model-based learning algorithm that achieves zero constraint violations. To obtain this result, we assume that the concave objective and the convex constraints have a solution interior to the set of feasible occupation measures. We then solve a tighter optimization problem to ensure that the constraints are never violated despite the imprecise model knowledge and model stochasticity. We also propose a novel Bellman error based analysis for tabular infinite-horizon setups which allows to analyse stochastic policies. Combining the Bellman error based analysis and tighter optimization equation, for $T$ interactions with the environment, we obtain a regret guarantee for objective which grows as $\Tilde{O}(1/\sqrt{T})$, excluding other factors.",0
"The focus of our study is on concave utility reinforcement learning (CURL) with convex constraints in a tabular infinite horizon setting. In certain learning applications, such as robotics, it is essential to ensure that policies adhere to constraints. Thus, we have developed a model-based learning algorithm that guarantees zero constraint violations. Our approach assumes that there is an interior solution for both the concave objective and the convex constraints within the feasible occupation measures. We address the issue of imprecise model knowledge and model stochasticity by solving a more rigorous optimization problem. Additionally, we introduce a novel Bellman error based analysis for tabular infinite-horizon scenarios, which allows us to evaluate stochastic policies. By combining this analysis with the tighter optimization equation, we can derive a regret guarantee for the objective that grows as $\Tilde{O}(1/\sqrt{T})$ over $T$ interactions with the environment, not accounting for other factors.",1
"This digital book contains a practical and comprehensive introduction of everything related to deep learning in the context of physical simulations. As much as possible, all topics come with hands-on code examples in the form of Jupyter notebooks to quickly get started. Beyond standard supervised learning from data, we'll look at physical loss constraints, more tightly coupled learning algorithms with differentiable simulations, as well as reinforcement learning and uncertainty modeling. We live in exciting times: these methods have a huge potential to fundamentally change what computer simulations can achieve.",0
"The digital book offers a thorough and handy guide to deep learning in the realm of physical simulations, complete with practical examples in Jupyter notebooks. In addition to conventional supervised learning, the book delves into physical loss constraints, integrated learning algorithms with differentiable simulations, reinforcement learning, and uncertainty modeling. These cutting-edge techniques have immense potential to revolutionize the capabilities of computer simulations, making this an exciting era.",1
"Our life is getting filled by Internet of Things (IoT) devices. These devices often rely on closed or poorly documented protocols, with unknown formats and semantics. Learning how to interact with such devices in an autonomous manner is the key for interoperability and automatic verification of their capabilities. In this paper, we propose RL-IoT, a system that explores how to automatically interact with possibly unknown IoT devices. We leverage reinforcement learning (RL) to recover the semantics of protocol messages and to take control of the device to reach a given goal, while minimizing the number of interactions. We assume to know only a database of possible IoT protocol messages, whose semantics are however unknown. RL-IoT exchanges messages with the target IoT device, learning those commands that are useful to reach the given goal. Our results show that RL-IoT is able to solve both simple and complex tasks. With properly tuned parameters, RL-IoT learns how to perform actions with the target device, a Yeelight smart bulb in our case study, completing non-trivial patterns with as few as 400 interactions. RL-IoT paves the road for automatic interactions with poorly documented IoT protocols, thus enabling interoperable systems.",0
"The proliferation of Internet of Things (IoT) devices has become ubiquitous in our daily lives. However, these devices often operate using closed or inadequately documented protocols, which may have obscure formats and meanings. It is therefore imperative to learn how to interact with such devices autonomously in order to ensure interoperability and automatic verification of their capabilities. To address this issue, we introduce RL-IoT, a system that utilizes reinforcement learning (RL) to automatically interact with potentially unknown IoT devices. By leveraging RL, we are able to recover the semantics of protocol messages and take control of the device to achieve a desired objective, while minimizing interactions. Although we only have access to a database of potential IoT protocol messages with unknown meanings, RL-IoT is able to learn useful commands by exchanging messages with the target IoT device. Our results demonstrate that RL-IoT can successfully accomplish simple and complex tasks, such as operating a Yeelight smart bulb with as few as 400 interactions. RL-IoT has the potential to facilitate automatic interactions with poorly documented IoT protocols, thereby enabling interoperable systems.",1
"A lowering in the cost of batteries and solar PV systems has led to a high uptake of solar battery home systems. In this work, we use the deep deterministic policy gradient algorithm to optimise the charging and discharging behaviour of a battery within such a system. Our approach outputs a continuous action space when it charges and discharges the battery, and can function well in a stochastic environment. We show good performance of this algorithm by lowering the expenditure of a single household on electricity to almost \$1AUD for large batteries across selected weeks within a year.",0
"The decreased costs of batteries and solar PV systems have resulted in a substantial increase in the adoption of solar battery home systems. Our study employs the deep deterministic policy gradient algorithm to enhance the charging and discharging patterns of batteries in such systems. Our technique generates a continuous action space for the battery's charging and discharging activities, and can effectively operate in a random setting. We demonstrate the effectiveness of this algorithm by reducing a household's electricity expenses to nearly \$1AUD for significant battery sizes over specific weeks in a year.",1
"Multi-task learning can leverage information learned by one task to benefit the training of other tasks. Despite this capacity, naive formulations often degrade performance and in particular, identifying the tasks that would benefit from co-training remains a challenging design question. In this paper, we analyze the dynamics of information transfer, or transference, across tasks throughout training. Specifically, we develop a similarity measure that can quantify transference among tasks and use this quantity to both better understand the optimization dynamics of multi-task learning as well as improve overall learning performance. In the latter case, we propose two methods to leverage our transference metric. The first operates at a macro-level by selecting which tasks should train together while the second functions at a micro-level by determining how to combine task gradients at each training step. We find these methods can lead to significant improvement over prior work on three supervised multi-task learning benchmarks and one multi-task reinforcement learning paradigm.",0
"The ability of multi-task learning to utilize knowledge gained from one task to enhance the training of other tasks is well-known. However, this potential is often undermined by unsophisticated approaches, and identifying which tasks will benefit from joint training remains a difficult problem. This paper explores the transference of information between tasks during training and proposes a similarity metric to quantify this process. The metric is then used to improve the optimization dynamics of multi-task learning and enhance overall learning performance through two methods: macro-level task selection and micro-level gradient combination. The effectiveness of these methods is demonstrated on three supervised multi-task learning benchmarks and one multi-task reinforcement learning paradigm.",1
"Learning to solve sparse-reward reinforcement learning problems is difficult, due to the lack of guidance towards the goal. But in some problems, prior knowledge can be used to augment the learning process. Reward shaping is a way to incorporate prior knowledge into the original reward function in order to speed up the learning. While previous work has investigated the use of expert knowledge to generate potential functions, in this work, we study whether we can use a search algorithm(A*) to automatically generate a potential function for reward shaping in Sokoban, a well-known planning task. The results showed that learning with shaped reward function is faster than learning from scratch. Our results indicate that distance functions could be a suitable function for Sokoban. This work demonstrates the possibility of solving multiple instances with the help of reward shaping. The result can be compressed into a single policy, which can be seen as the first phrase towards training a general policy that is able to solve unseen instances.",0
"Sparse-reward reinforcement learning problems pose a challenge due to the lack of clear direction towards the goal. However, incorporating prior knowledge can facilitate the learning process in some cases. One such method is reward shaping, which involves modifying the original reward function using prior knowledge to accelerate learning. Prior research has explored the use of expert knowledge to create potential functions. In contrast, our study investigates the use of a search algorithm (A*) to automatically generate a potential function for reward shaping in Sokoban, a well-known planning task. Our findings reveal that learning with a shaped reward function is more efficient than starting from scratch. We identify distance functions as a suitable function for Sokoban. This study demonstrates the potential of reward shaping in solving multiple instances and compressing the result into a single policy. This policy can be considered as an initial step towards training a general policy capable of tackling unseen instances.",1
"Offline policy evaluation (OPE) is considered a fundamental and challenging problem in reinforcement learning (RL). This paper focuses on the value estimation of a target policy based on pre-collected data generated from a possibly different policy, under the framework of infinite-horizon Markov decision processes. Motivated by the recently developed marginal importance sampling method in RL and the covariate balancing idea in causal inference, we propose a novel estimator with approximately projected state-action balancing weights for the policy value estimation. We obtain the convergence rate of these weights, and show that the proposed value estimator is semi-parametric efficient under technical conditions. In terms of asymptotics, our results scale with both the number of trajectories and the number of decision points at each trajectory. As such, consistency can still be achieved with a limited number of subjects when the number of decision points diverges. In addition, we make a first attempt towards characterizing the difficulty of OPE problems, which may be of independent interest. Numerical experiments demonstrate the promising performance of our proposed estimator.",0
"Reinforcement learning (RL) faces a challenging issue with offline policy evaluation (OPE), which involves estimating the value of a target policy based on data collected from a different policy within the framework of infinite-horizon Markov decision processes. This paper proposes a novel estimator that uses approximately projected state-action balancing weights for policy value estimation, drawing inspiration from the marginal importance sampling method in RL and covariate balancing in causal inference. The convergence rate of these weights is established, and under certain technical conditions, the proposed estimator is semi-parametrically efficient. The results scale with the number of trajectories and decision points, ensuring consistency even with a limited number of subjects when the number of decision points diverges. The paper also attempts to characterize the complexity of OPE problems, which may be of independent interest. Numerical experiments demonstrate the promising performance of the proposed estimator.",1
"We present a differentiable soft-body physics simulator that can be composed with neural networks as a differentiable layer. In contrast to other differentiable physics approaches that use explicit forward models to define state transitions, we focus on implicit state transitions defined via function minimization. Implicit state transitions appear in implicit numerical integration methods, which offer the benefits of large time steps and excellent numerical stability, but require a special treatment to achieve differentiability due to the absence of an explicit differentiable forward pass. In contrast to other implicit differentiation approaches that require explicit formulas for the force function and the force Jacobian matrix, we present an energy-based approach that allows us to compute these derivatives automatically and in a matrix-free fashion via reverse-mode automatic differentiation. This allows for more flexibility and productivity when defining physical models and is particularly important in the context of neural network training, which often relies on reverse-mode automatic differentiation (backpropagation). We demonstrate the effectiveness of our differentiable simulator in policy optimization for locomotion tasks and show that it achieves better sample efficiency than model-free reinforcement learning.",0
"Our soft-body physics simulator is differentiable and can be utilized as a differentiable layer with neural networks. Unlike other differentiable physics techniques that utilize explicit forward models to establish state transitions, we concentrate on implicit state transitions, which are defined through function minimization. Implicit state transitions are present in implicit numerical integration methods, which offer the advantages of excellent numerical stability and large time steps, but necessitate special measures to achieve differentiability due to the lack of an explicit differentiable forward pass. Our energy-based technique enables us to compute derivatives for the force function and the force Jacobian matrix automatically and in a matrix-free manner via reverse-mode automatic differentiation, in contrast to other implicit differentiation methods that require explicit formulas. This approach allows for greater flexibility and productivity in defining physical models, particularly in the context of neural network training, which frequently relies on reverse-mode automatic differentiation (backpropagation). We demonstrate the effectiveness of our differentiable simulator in policy optimization for locomotion tasks and show that it provides better sample efficiency than model-free reinforcement learning.",1
"Inverse Reinforcement Learning (IRL) is attractive in scenarios where reward engineering can be tedious. However, prior IRL algorithms use on-policy transitions, which require intensive sampling from the current policy for stable and optimal performance. This limits IRL applications in the real world, where environment interactions can become highly expensive. To tackle this problem, we present Off-Policy Inverse Reinforcement Learning (OPIRL), which (1) adopts off-policy data distribution instead of on-policy and enables significant reduction of the number of interactions with the environment, (2) learns a stationary reward function that is transferable with high generalization capabilities on changing dynamics, and (3) leverages mode-covering behavior for faster convergence. We demonstrate that our method is considerably more sample efficient and generalizes to novel environments through the experiments. Our method achieves better or comparable results on policy performance baselines with significantly fewer interactions. Furthermore, we empirically show that the recovered reward function generalizes to different tasks where prior arts are prone to fail.",0
"When it comes to situations where reward engineering is a laborious task, Inverse Reinforcement Learning (IRL) can be an appealing option. However, current IRL algorithms rely on on-policy transitions, a method that demands extensive sampling from the current policy for consistent and optimal performance. This severely limits the applicability of IRL in the real world, where interacting with the environment can be prohibitively expensive. To address this issue, we propose Off-Policy Inverse Reinforcement Learning (OPIRL), which (1) adopts off-policy data distribution instead of on-policy, resulting in a significant reduction in the amount of environment interactions required, (2) learns a stationary reward function with high generalization capabilities on changing dynamics, and (3) takes advantage of mode-covering behavior for faster convergence. Our experiments show that our approach is much more sample efficient and generalizes well to novel environments. In fact, we achieve better or comparable results on policy performance baselines with significantly fewer interactions. Moreover, we provide empirical evidence that our recovered reward function can generalize to different tasks where prior methods tend to fail.",1
"This paper addresses the problem of policy selection in domains with abundant logged data, but with a very restricted interaction budget. Solving this problem would enable safe evaluation and deployment of offline reinforcement learning policies in industry, robotics, and recommendation domains among others. Several off-policy evaluation (OPE) techniques have been proposed to assess the value of policies using only logged data. However, there is still a big gap between the evaluation by OPE and the full online evaluation in the real environment. At the same time, large amount of online interactions is often not feasible in practice. To overcome this problem, we introduce \emph{active offline policy selection} -- a novel sequential decision approach that combines logged data with online interaction to identify the best policy. This approach uses OPE estimates to warm start the online evaluation. Then, in order to utilize the limited environment interactions wisely, it relies on a Bayesian optimization method, with a kernel function that represents policy similarity, to decide which policy to evaluate next. We use multiple benchmarks with a large number of candidate policies to show that the proposed approach improves upon state-of-the-art OPE estimates and pure online policy evaluation.",0
"The main focus of this article is the issue of choosing appropriate policies for domains with a wealth of logged data, but limited interaction opportunities. Effective resolution of this problem would allow for the secure evaluation and implementation of offline reinforcement learning policies in various fields, including industry, robotics, and recommendation systems. Although several off-policy evaluation (OPE) techniques exist to evaluate policy value using only logged data, there is still a significant disparity between OPE evaluation and full online evaluation in actual environments. In addition, performing a large number of online interactions is often impractical. To address this challenge, the authors propose an innovative sequential decision-making strategy called ""active offline policy selection."" This approach combines logged data with online interaction to determine the best policy. It uses OPE evaluations to initiate online evaluation and relies on Bayesian optimization with a kernel function representing policy similarity to make informed decisions about which policy to evaluate next, utilizing limited environment interactions intelligently. The authors demonstrate the effectiveness of their approach using various benchmarks with numerous candidate policies, showing improvement over existing OPE evaluations and pure online policy evaluations.",1
"Motivated by the common strategic activities in crowdsourcing labeling, we study the problem of sequential eliciting information without verification (EIWV) for workers with a heterogeneous and unknown crowd. We propose a reinforcement learning-based approach that is effective against a wide range of settings including potential irrationality and collusion among workers. With the aid of a costly oracle and the inference method, our approach dynamically decides the oracle calls and gains robustness even under the presence of frequent collusion activities. Extensive experiments show the advantage of our approach. Our results also present the first comprehensive experiments of EIWV on large-scale real datasets and the first thorough study of the effects of environmental variables.",0
"Our research is focused on the issue of sequential information elicitation without verification (EIWV) for workers with varying abilities in crowdsourcing labeling. Our aim is to develop a reinforcement learning-based approach that can effectively tackle a wide range of scenarios, including those involving irrational or colluding workers. By utilizing an expensive oracle and an inference method, our approach can dynamically decide when to call upon the oracle and is robust even in the face of frequent collusion. Through extensive experimentation, we have demonstrated the advantages of our approach, which is the first comprehensive study of EIWV on large-scale real datasets, as well as the first in-depth exploration of the impact of environmental variables.",1
"To successfully tackle challenging manipulation tasks, autonomous agents must learn a diverse set of skills and how to combine them. Recently, self-supervised agents that set their own abstract goals by exploiting the discovered structure in the environment were shown to perform well on many different tasks. In particular, some of them were applied to learn basic manipulation skills in compositional multi-object environments. However, these methods learn skills without taking the dependencies between objects into account. Thus, the learned skills are difficult to combine in realistic environments. We propose a novel self-supervised agent that estimates relations between environment components and uses them to independently control different parts of the environment state. In addition, the estimated relations between objects can be used to decompose a complex goal into a compatible sequence of subgoals. We show that, by using this framework, an agent can efficiently and automatically learn manipulation tasks in multi-object environments with different relations between objects.",0
"In order for autonomous agents to effectively handle complex manipulation tasks, they must acquire a range of skills and learn how to integrate them. Recently, self-supervised agents have demonstrated success in various tasks by setting their own objectives based on the environment's structure. Some of these agents have been applied to learn basic manipulation skills in multi-object environments, but they do not consider the interdependencies between objects. Consequently, the acquired skills are difficult to combine in practical settings. Our proposal is a new self-supervised agent that estimates the relationships between environment components and utilizes them to independently control various parts of the environment state. Additionally, the relationships between objects can be used to break down a complex goal into a sequence of suitable subgoals. We demonstrate that using this framework, an agent can efficiently and automatically learn manipulation tasks in multi-object environments with different object relationships.",1
"The goal of object navigation is to reach the expected objects according to visual information in the unseen environments. Previous works usually implement deep models to train an agent to predict actions in real-time. However, in the unseen environment, when the target object is not in egocentric view, the agent may not be able to make wise decisions due to the lack of guidance. In this paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent in a coarse-to-fine manner, and an online-learning mechanism is also proposed to update HOZ according to the real-time observation in new environments. In particular, the HOZ graph is composed of scene nodes, zone nodes and object nodes. With the pre-learned HOZ graph, the real-time observation and the target goal, the agent can constantly plan an optimal path from zone to zone. In the estimated path, the next potential zone is regarded as sub-goal, which is also fed into the deep reinforcement learning model for action prediction. Our methods are evaluated on the AI2-Thor simulator. In addition to widely used evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE that focuses on the effective action rate. Experimental results demonstrate the effectiveness and efficiency of our proposed method.",0
"The aim of object navigation is to locate specific objects in unfamiliar surroundings using visual cues. Prior studies have employed deep models to train agents to make real-time predictions about actions. However, without guidance, agents may struggle to make sound decisions when the target object is not visible from the agent's perspective. This paper introduces a hierarchical object-to-zone (HOZ) graph that guides the agent in a coarse-to-fine manner, along with an online-learning mechanism that updates the HOZ based on real-time observations in new environments. The HOZ graph consists of scene, zone, and object nodes, allowing the agent to plan an optimal path from zone to zone based on the pre-learned graph, real-time observations, and the target goal. The next potential zone in the estimated path serves as a sub-goal that the deep reinforcement learning model uses to predict actions. We evaluate our approach on the AI2-Thor simulator using traditional metrics like SR and SPL, as well as a new metric called SAE, which measures the effective action rate. The experimental results show that our proposed method is effective and efficient.",1
"Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult that faces two main challenges: (1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing state-of-the-art methods.",0
"In recent years, there has been increasing research interest in Temporal Knowledge Graph (TKG) reasoning, which is a crucial task. However, most existing methods focus on reasoning at past timestamps to complete missing facts, with only a few works addressing forecasting future facts on known TKGs. The forecasting task is more challenging than the completion task, as it faces two primary challenges: (1) how to effectively model time information to handle future timestamps, and (2) how to make inductive inference to handle previously unseen entities that emerge over time. To tackle these challenges, we propose a novel reinforcement learning method for forecasting. Our method uses an agent that travels on historical knowledge graph snapshots to search for answers. We define a relative time encoding function to capture timespan information and design a unique time-shaped reward based on Dirichlet distribution to guide model learning. Additionally, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for link prediction task at future timestamps and demonstrate substantial performance improvement, along with higher explainability, less calculation, and fewer parameters compared to existing state-of-the-art methods in extensive experiments on four benchmark datasets.",1
"Mean field control (MFC) is an effective way to mitigate the curse of dimensionality of cooperative multi-agent reinforcement learning (MARL) problems. This work considers a collection of $N_{\mathrm{pop}}$ heterogeneous agents that can be segregated into $K$ classes such that the $k$-th class contains $N_k$ homogeneous agents. We aim to prove approximation guarantees of the MARL problem for this heterogeneous system by its corresponding MFC problem. We consider three scenarios where the reward and transition dynamics of all agents are respectively taken to be functions of $(1)$ joint state and action distributions across all classes, $(2)$ individual distributions of each class, and $(3)$ marginal distributions of the entire population. We show that, in these cases, the $K$-class MARL problem can be approximated by MFC with errors given as $e_1=\mathcal{O}(\frac{\sqrt{|\mathcal{X}||\mathcal{U}|}}{N_{\mathrm{pop}}}\sum_{k}\sqrt{N_k})$, $e_2=\mathcal{O}(\sqrt{|\mathcal{X}||\mathcal{U}|}\sum_{k}\frac{1}{\sqrt{N_k}})$ and $e_3=\mathcal{O}\left(\sqrt{|\mathcal{X}||\mathcal{U}|}\left[\frac{A}{N_{\mathrm{pop}}}\sum_{k\in[K]}\sqrt{N_k}+\frac{B}{\sqrt{N_{\mathrm{pop}}}}\right]\right)$, respectively, where $A, B$ are some constants and $|\mathcal{X}|,|\mathcal{U}|$ are the sizes of state and action spaces of each agent. Finally, we design a Natural Policy Gradient (NPG) based algorithm that, in the three cases stated above, can converge to an optimal MARL policy within $\mathcal{O}(e_j)$ error with a sample complexity of $\mathcal{O}(e_j^{-3})$, $j\in\{1,2,3\}$, respectively.",0
"To combat the curse of dimensionality in cooperative multi-agent reinforcement learning problems, Mean field control (MFC) is an effective approach. This study focuses on a collection of N_pop heterogeneous agents, which can be classified into K groups, with each group containing N_k homogeneous agents. The goal is to demonstrate the approximation guarantees of the multi-agent reinforcement learning problem for this diverse system using the corresponding MFC problem. Three scenarios are considered, where the reward and transition dynamics of all agents are functions of joint state and action distributions, individual distributions of each class, and marginal distributions of the entire population. It is shown that the K-class MARL problem can be approximated by MFC with errors given by e_1, e_2, and e_3, respectively. Finally, a Natural Policy Gradient (NPG) based algorithm is devised that can converge to an optimal MARL policy with a sample complexity of O(e_j^-3) and an error of O(e_j) for all three cases.",1
"While significant research advances have been made in the field of deep reinforcement learning, a major challenge to widespread industrial adoption of deep reinforcement learning that has recently surfaced but little explored is the potential vulnerability to privacy breaches. In particular, there have been no concrete adversarial attack strategies in literature tailored for studying the vulnerability of deep reinforcement learning algorithms to membership inference attacks. To address this gap, we propose an adversarial attack framework tailored for testing the vulnerability of deep reinforcement learning algorithms to membership inference attacks. More specifically, we design a series of experiments to investigate the impact of temporal correlation, which naturally exists in reinforcement learning training data, on the probability of information leakage. Furthermore, we study the differences in the performance of \emph{collective} and \emph{individual} membership attacks against deep reinforcement learning algorithms. Experimental results show that the proposed adversarial attack framework is surprisingly effective at inferring the data used during deep reinforcement training with an accuracy exceeding $84\%$ in individual and $97\%$ in collective mode on two different control tasks in OpenAI Gym, which raises serious privacy concerns in the deployment of models resulting from deep reinforcement learning. Moreover, we show that the learning state of a reinforcement learning algorithm significantly influences the level of the privacy breach.",0
"Despite significant research advancements in deep reinforcement learning, there is a major challenge hindering its widespread industrial adoption that has only recently come to light and is not well explored - the potential vulnerability to privacy breaches. In existing literature, there are no clear strategies for studying the susceptibility of deep reinforcement learning algorithms to membership inference attacks. To address this gap, we propose an adversarial attack framework specifically designed to test the vulnerability of deep reinforcement learning algorithms to membership inference attacks. Our experiments investigate the impact of temporal correlation, which is naturally present in reinforcement learning training data, on the probability of information leakage. We also examine the differences between collective and individual membership attacks against deep reinforcement learning algorithms. The results of our experiments demonstrate that our proposed adversarial attack framework is surprisingly effective at inferring the data used during deep reinforcement training, with an accuracy exceeding 84% in individual mode and 97% in collective mode on two different control tasks in OpenAI Gym. This raises serious concerns about privacy in the deployment of models resulting from deep reinforcement learning. Additionally, we show that the learning state of a reinforcement learning algorithm significantly affects the level of privacy breach.",1
"Learning to communicate in order to share state information is an active problem in the area of multi-agent reinforcement learning (MARL). The credit assignment problem, the non-stationarity of the communication environment and the creation of influenceable agents are major challenges within this research field which need to be overcome in order to learn a valid communication protocol. This paper introduces the novel multi-agent counterfactual communication learning (MACC) method which adapts counterfactual reasoning in order to overcome the credit assignment problem for communicating agents. Secondly, the non-stationarity of the communication environment while learning the communication Q-function is overcome by creating the communication Q-function using the action policy of the other agents and the Q-function of the action environment. Additionally, a social loss function is introduced in order to create influenceable agents which is required to learn a valid communication protocol. Our experiments show that MACC is able to outperform the state-of-the-art baselines in four different scenarios in the Particle environment.",0
"The problem of learning to communicate in order to share state information is a challenging issue in multi-agent reinforcement learning (MARL). Within this research field, major challenges include the credit assignment problem, non-stationarity of the communication environment, and creation of influenceable agents. To address these challenges and learn a valid communication protocol, this paper proposes the multi-agent counterfactual communication learning (MACC) method. MACC employs counterfactual reasoning to overcome the credit assignment problem and creates the communication Q-function using the action policy of other agents and the Q-function of the action environment to address non-stationarity. Additionally, a social loss function is introduced to create influenceable agents. Experimental results demonstrate that MACC outperforms state-of-the-art baselines in four different scenarios in the Particle environment.",1
"Deep Reinforcement Learning (DRL) has numerous applications in the real world thanks to its outstanding ability in quickly adapting to the surrounding environments. Despite its great advantages, DRL is susceptible to adversarial attacks, which precludes its use in real-life critical systems and applications (e.g., smart grids, traffic controls, and autonomous vehicles) unless its vulnerabilities are addressed and mitigated. Thus, this paper provides a comprehensive survey that discusses emerging attacks in DRL-based systems and the potential countermeasures to defend against these attacks. We first cover some fundamental backgrounds about DRL and present emerging adversarial attacks on machine learning techniques. We then investigate more details of the vulnerabilities that the adversary can exploit to attack DRL along with the state-of-the-art countermeasures to prevent such attacks. Finally, we highlight open issues and research challenges for developing solutions to deal with attacks for DRL-based intelligent systems.",0
"Due to its exceptional ability to adapt to various environments, Deep Reinforcement Learning (DRL) has numerous real-world applications. However, its susceptibility to adversarial attacks poses a significant obstacle to its use in critical systems such as traffic controls, smart grids, and autonomous vehicles. This paper aims to provide a comprehensive survey of emerging attacks on DRL-based systems and potential countermeasures to defend against them. We begin by discussing fundamental DRL background knowledge and emerging adversarial attacks on machine learning techniques. Next, we delve into the vulnerabilities that adversaries can exploit to attack DRL and explore cutting-edge countermeasures to prevent such attacks. Finally, we identify open issues and research challenges that need to be addressed to develop solutions for defending DRL-based intelligent systems against attacks.",1
"As online shopping prevails and e-commerce platforms emerge, there is a tremendous number of parcels being transported every day. Thus, it is crucial for the logistics industry on how to assign a candidate logistics route for each shipping parcel properly as it leaves a significant impact on the total logistics cost optimization and business constraints satisfaction such as transit hub capacity and delivery proportion of delivery providers. This online route-assignment problem can be viewed as a constrained online decision-making problem. Notably, the large amount (beyond ${10^5}$) of daily parcels, the variability and non-Markovian characteristics of parcel information impose difficulties on attaining (near-) optimal solution without violating constraints excessively. In this paper, we develop a model-free DRL approach named PPO-RA, in which Proximal Policy Optimization (PPO) is improved with dedicated techniques to address the challenges for route assignment (RA). The actor and critic networks use attention mechanism and parameter sharing to accommodate each incoming parcel with varying numbers and identities of candidate routes, without modeling non-Markovian parcel arriving dynamics since we make assumption of i.i.d. parcel arrival. We use recorded delivery parcel data to evaluate the performance of PPO-RA by comparing it with widely-used baselines via simulation. The results show the capability of the proposed approach to achieve considerable cost savings while satisfying most constraints.",0
"As the popularity of online shopping continues to grow and e-commerce platforms emerge, a significant number of packages are being transported daily. The logistics industry must ensure that each shipping parcel is assigned to an appropriate logistics route, as this decision has a significant impact on both the optimization of total logistics costs and the satisfaction of business constraints, such as transit hub capacity and delivery provider delivery rates. However, this online route-assignment problem is complex due to the large number of daily parcels (over 10^5), the variability and non-Markovian nature of parcel information, and the need to find optimal solutions while adhering to constraints. To address these challenges, we present a model-free DRL approach called PPO-RA, which uses Proximal Policy Optimization (PPO) with specialized techniques to improve route assignment (RA). The actor and critic networks use attention mechanisms and parameter sharing to handle incoming parcels with varying routes, without modeling parcel arrival dynamics. We evaluate the performance of PPO-RA using recorded delivery parcel data, comparing it with widely-used baselines via simulation. The results demonstrate that PPO-RA can achieve significant cost savings while meeting most constraints.",1
"The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a solution to an equation of the form $\mathbf{f}(\boldsymbol{\theta}) = \mathbf{0}$ where $\mathbf{f} : \mathbb{R}^d \rightarrow \mathbb{R}^d$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. In the literature to date, one can make a distinction between ""synchronous"" updating, whereby the entire vector of the current guess $\boldsymbol{\theta}_t$ is updated at each time, and ""asynchronous"" updating, whereby ony one component of $\boldsymbol{\theta}_t$ is updated. In convex and nonconvex optimization, there is also the notion of ""batch"" updating, whereby some but not all components of $\boldsymbol{\theta}_t$ are updated at each time $t$. In addition, there is also a distinction between using a ""local"" clock versus a ""global"" clock. In the literature to date, convergence proofs when a local clock is used make the assumption that the measurement noise is an i.i.d\ sequence, an assumption that does not hold in Reinforcement Learning (RL).   In this note, we provide a general theory of convergence for batch asymchronous stochastic approximation (BASA), that works whether the updates use a local clock or a global clock, for the case where the measurement noises form a martingale difference sequence. This is the most general result to date and encompasses all others.",0
"The stochastic approximation (SA) algorithm is extensively utilized to find a solution to an equation of the form $\mathbf{f}(\boldsymbol{\theta}) = \mathbf{0}$ where $\mathbf{f} : \mathbb{R}^d \rightarrow \mathbb{R}^d$, in the presence of imprecise measurements of $\mathbf{f}(\cdot)$. The literature distinguishes between ""synchronous"" and ""asynchronous"" updating of the current guess $\boldsymbol{\theta}_t$, where the former updates the entire vector, and the latter updates only one component. Moreover, in convex and nonconvex optimization, ""batch"" updating updates some, but not all, components of $\boldsymbol{\theta}_t$ at each time $t$. The literature also distinguishes between using a ""local"" clock and a ""global"" clock. Convergence proofs when a local clock is used assume that the measurement noise is an i.i.d sequence, which is not valid in Reinforcement Learning (RL). This note presents a general theory of convergence for batch asynchronous stochastic approximation (BASA), applicable to updates using both local and global clocks, in the presence of martingale difference sequence measurement noise. This is the most comprehensive result to date that includes all others.",1
"Combining off-policy reinforcement learning methods with function approximators such as neural networks has been found to lead to overestimation of the value function and sub-optimal solutions. Improvement such as TD3 has been proposed to address this issue. However, we surprisingly find that its performance lags behind the vanilla actor-critic methods (such as DDPG) in some primitive environments. In this paper, we show that the failure of some cases can be attributed to insufficient exploration. We reveal the culprit of insufficient exploration in TD3, and propose a novel algorithm toward this problem that ADapts between Exploration and Robustness, namely ADER. To enhance the exploration ability while eliminating the overestimation bias, we introduce a dynamic penalty term in value estimation calculated from estimated uncertainty, which takes into account different compositions of the uncertainty in different learning stages. Experiments in several challenging environments demonstrate the supremacy of the proposed method in continuous control tasks.",0
"When off-policy reinforcement learning methods are combined with function approximators like neural networks, they often result in overestimation of the value function and sub-optimal solutions. However, TD3 has been proposed to address this problem, but we have found that it can perform worse than vanilla actor-critic methods like DDPG in certain environments. In this paper, we reveal that insufficient exploration is the cause of this issue. To tackle this problem, we introduce a novel algorithm called ADER that adapts between exploration and robustness. We enhance the exploration ability and eliminate the overestimation bias by introducing a dynamic penalty term in value estimation calculated from estimated uncertainty. This term takes into account different compositions of uncertainty in different learning stages. Our experiments in several challenging environments demonstrate that ADER outperforms other methods in continuous control tasks.",1
"In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that achieves Bayesian regret bound of $O(HS\sqrt{AT})$ in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here $H$ is an upper bound on the span of the bias function, $S$ is the number of states, $A$ is the number of joint actions and $T$ is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary time-adaptive history-dependent strategy. This improves the best existing regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et. al., 2017 under the same assumption and matches the theoretical lower bound in $A$ and $T$.",0
"In this article, we present PSRL-ZSG, an online learning algorithm for zero-sum stochastic games with average-reward criteria. It achieves a Bayesian regret bound of $O(HS\sqrt{AT})$, which is the first of its kind. The variables $H$, $S$, $A$, and $T$ represent an upper bound on the bias function span, the number of states, the number of joint actions, and the horizon, respectively. In this online setting, the opponent's strategy is history-dependent and time-adaptive, and cannot be controlled. This algorithm improves upon the existing regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) and matches the theoretical lower bound for $A$ and $T$.",1
"Training sample re-weighting is an effective approach for tackling data biases such as imbalanced and corrupted labels. Recent methods develop learning-based algorithms to learn sample re-weighting strategies jointly with model training based on the frameworks of reinforcement learning and meta learning. However, depending on additional unbiased reward data is limiting their general applicability. Furthermore, existing learning-based sample re-weighting methods require nested optimizations of models and weighting parameters, which requires expensive second-order computation. This paper addresses these two problems and presents a novel learning-based fast sample re-weighting (FSR) method that does not require additional reward data. The method is based on two key ideas: learning from history to build proxy reward data and feature sharing to reduce the optimization cost. Our experiments show the proposed method achieves competitive results compared to state of the arts on label noise robustness and long-tailed recognition, and does so while achieving significantly improved training efficiency. The source code is publicly available at https://github.com/google-research/google-research/tree/master/ieg.",0
"One effective method for addressing data biases, such as imbalanced and corrupted labels, is training sample re-weighting. Recent approaches have utilized learning-based algorithms to jointly learn re-weighting strategies and model training through reinforcement learning and meta learning frameworks. However, their limited general applicability stems from the dependence on unbiased reward data. Additionally, current learning-based sample re-weighting methods require costly nested optimizations of models and weighting parameters. To address these issues, this paper introduces a novel learning-based fast sample re-weighting (FSR) method that eliminates the need for additional reward data. FSR utilizes two key concepts: historical data to generate proxy reward data and feature sharing to reduce optimization costs. Our experiments demonstrate that FSR achieves competitive results compared to state-of-the-art methods for label noise robustness and long-tailed recognition while significantly improving training efficiency. The source code for FSR is publicly available at https://github.com/google-research/google-research/tree/master/ieg.",1
"Many of the challenges facing today's reinforcement learning (RL) algorithms, such as robustness, generalization, transfer, and computational efficiency are closely related to compression. Prior work has convincingly argued why minimizing information is useful in the supervised learning setting, but standard RL algorithms lack an explicit mechanism for compression. The RL setting is unique because (1) its sequential nature allows an agent to use past information to avoid looking at future observations and (2) the agent can optimize its behavior to prefer states where decision making requires few bits. We take advantage of these properties to propose a method (RPC) for learning simple policies. This method brings together ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. Our method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. We demonstrate that our method achieves much tighter compression than prior methods, achieving up to 5x higher reward than a standard information bottleneck. We also demonstrate that our method learns policies that are more robust and generalize better to new tasks.",0
"Compression is closely related to many of the challenges faced by reinforcement learning (RL) algorithms today, including robustness, generalization, transfer, and computational efficiency. While previous research has shown the benefits of minimizing information in the supervised learning setting, RL algorithms lack an explicit compression mechanism. However, the sequential nature of the RL setting allows agents to use past information and optimize their behavior to prefer states that require fewer bits for decision making. We propose a method called RPC, which combines information bottlenecks, model-based RL, and bits-back coding to learn simple policies. Our method jointly optimizes a latent-space model and policy to be self-consistent, avoiding inaccurate states. Compared to prior methods, RPC achieves much tighter compression and higher rewards, up to 5x higher than a standard information bottleneck. Additionally, our method learns policies that are more robust and generalize better to new tasks.",1
"Quantum Machine Learning (QML) is considered to be one of the most promising applications of near term quantum devices. However, the optimization of quantum machine learning models presents numerous challenges arising from the imperfections of hardware and the fundamental obstacles in navigating an exponentially scaling Hilbert space. In this work, we evaluate the potential of contemporary methods in deep reinforcement learning to augment gradient based optimization routines in quantum variational circuits. We find that reinforcement learning augmented optimizers consistently outperform gradient descent in noisy environments. All code and pretrained weights are available to replicate the results or deploy the models at https://github.com/lockwo/rl_qvc_opt.",0
"Near term quantum devices show great promise in Quantum Machine Learning (QML). However, optimizing QML models is challenging due to hardware imperfections and navigating an exponentially scaling Hilbert space. This study examines the potential of modern deep reinforcement learning methods to enhance gradient-based optimization routines in quantum variational circuits. Results show that reinforcement learning augmented optimizers outperform gradient descent in noisy environments. The code and pre-trained weights can be accessed at https://github.com/lockwo/rl_qvc_opt for replication or model deployment.",1
"Inferring programs which generate 2D and 3D shapes is important for reverse engineering, editing, and more. Training such inference models is challenging due to the lack of paired (shape, program) data in most domains. A popular approach is to pre-train a model on synthetic data and then fine-tune on real shapes using slow, unstable reinforcement learning. In this paper, we argue that self-training is a viable alternative for fine-tuning such models. Self-training is a semi-supervised learning paradigm where a model assigns pseudo-labels to unlabeled data, and then retrains with (data, pseudo-label) pairs as the new ground truth. We show that for constructive solid geometry and assembly-based modeling, self-training outperforms state-of-the-art reinforcement learning approaches. Additionally, shape program inference has a unique property that circumvents a potential downside of self-training (incorrect pseudo-label assignment): inferred programs are executable. For a given shape from our distribution of interest $\mathbf{x}^*$ and its predicted program $\mathbf{z}$, one can execute $\mathbf{z}$ to obtain a shape $\mathbf{x}$ and train on $(\mathbf{z}, \mathbf{x})$ pairs, rather than $(\mathbf{z}, \mathbf{x}^*)$ pairs. We term this procedure latent execution self training (LEST). We demonstrate that self training infers shape programs with higher shape reconstruction accuracy and converges significantly faster than reinforcement learning approaches, and in some domains, LEST can further improve this performance.",0
"The inference of programs that create 2D and 3D shapes is crucial for tasks such as reverse engineering and editing. However, training such models is challenging due to the scarcity of paired (shape, program) data in most domains. One common method is to pre-train a model on synthetic data and then use reinforcement learning to fine-tune it on real shapes, which can be slow and unstable. In this article, we propose self-training as a viable alternative for fine-tuning these models. Self-training is a semi-supervised learning technique in which a model assigns pseudo-labels to unlabeled data and then retrains using these pairs as the new ground truth. We demonstrate that self-training outperforms reinforcement learning approaches for constructive solid geometry and assembly-based modeling. Moreover, the inferred programs are executable, which mitigates the risk of incorrect pseudo-label assignment. We introduce a novel approach called latent execution self-training (LEST), where the model executes the predicted program to obtain a new shape and trains on the (program, shape) pairs. LEST significantly improves performance in some domains and converges faster than reinforcement learning.",1
"Recent advances in GPU accelerated global and detail placement have reduced the time to solution by an order of magnitude. This advancement allows us to leverage data driven optimization (such as Reinforcement Learning) in an effort to improve the final quality of placement results. In this work we augment state-of-the-art, force-based global placement solvers with a reinforcement learning agent trained to improve the final detail placed Half Perimeter Wire Length (HPWL).   We propose novel control schemes with either global or localized control of the placement process. We then train reinforcement learning agents to use these controls to guide placement to improved solutions. In both cases, the augmented optimizer finds improved placement solutions.   Our trained agents achieve an average 1% improvement in final detail place HPWL across a range of academic benchmarks and more than 1% in global place HPWL on real industry designs.",0
"Thanks to recent developments in GPU-accelerated global and detail placement, the time required to achieve a solution has been significantly reduced. This improvement has opened the way for data-driven optimization techniques, such as Reinforcement Learning, to enhance the quality of placement results. In this study, we have combined state-of-the-art, force-based global placement solvers with a reinforcement learning agent that has been trained to optimize the final Half Perimeter Wire Length (HPWL) of detail placement. We have introduced novel control methods to guide the placement process, either globally or locally, and trained reinforcement learning agents to use these controls to achieve better solutions. The results show that our augmented optimizer has successfully improved placement solutions, with trained agents achieving an average of 1% improvement in final detail place HPWL across various academic benchmarks, and over 1% in global place HPWL on real industry designs.",1
"In physical design, human designers typically place macros via trial and error, which is a Markov decision process. Reinforcement learning (RL) methods have demonstrated superhuman performance on the macro placement. In this paper, we propose an extension to this prior work (Mirhoseini et al., 2020). We first describe the details of the policy and value network architecture. We replace the force-directed method with DREAMPlace for placing standard cells in the RL environment. We also compare our improved method with other academic placers on public benchmarks.",0
"Physical design involves human designers placing macros through a Markov decision process of trial and error. However, reinforcement learning (RL) methods have shown to perform better than humans in macro placement. This paper presents an extension to the work of Mirhoseini et al. (2020) by detailing the architecture of the policy and value networks. We also introduce DREAMPlace as a replacement for the force-directed method for placing standard cells in the RL environment. Additionally, we compare our enhanced method with other academic placers using public benchmarks.",1
"Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed self-driving system. To encourage an early and accurate decision, existing approaches typically focus on capturing the cues of spatial and temporal context before a future accident occurs. However, their decision-making lacks visual explanation and ignores the dynamic interaction with the environment. In this paper, we propose Deep ReInforced accident anticipation with Visual Explanation, named DRIVE. The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam observation environment so that the decision from the proposed stochastic multi-task agent can be visually explained by attentive regions. Moreover, the proposed dense anticipation reward and sparse fixation reward are effective in training the DRIVE model with our improved reinforcement learning algorithm. Experimental results show that the DRIVE model achieves state-of-the-art performance on multiple real-world traffic accident datasets. Code and pre-trained model are available at \url{https://www.rit.edu/actionlab/drive}.",0
"The objective of traffic accident anticipation is to predict future accidents from dashcam videos accurately and quickly, which is crucial for a self-driving system that prioritizes safety. Existing methods primarily concentrate on capturing spatial and temporal context cues before an accident occurs to promote timely and precise decision-making. However, these approaches lack visual explanation and overlook the dynamic interplay with the environment. In this study, we introduce DRIVE, a Deep ReInforced accident anticipation model that incorporates both bottom-up and top-down visual attention mechanisms in a dashcam observation setting to produce visually explicable decisions. Additionally, our improved reinforcement learning algorithm uses dense anticipation rewards and sparse fixation rewards to train the DRIVE model effectively. Our experiments on multiple real-world traffic accident datasets demonstrate that the DRIVE model outperforms existing methods. The code and pre-trained model are accessible at \url{https://www.rit.edu/actionlab/drive}.",1
"Designing optimal reward functions has been desired but extremely difficult in reinforcement learning (RL). When it comes to modern complex tasks, sophisticated reward functions are widely used to simplify policy learning yet even a tiny adjustment on them is expensive to evaluate due to the drastically increasing cost of training. To this end, we propose a hindsight reward tweaking approach by designing a novel paradigm for deep reinforcement learning to model the influences of reward functions within a near-optimal space. We simply extend the input observation with a condition vector linearly correlated with the effective environment reward parameters and train the model in a conventional manner except for randomizing reward configurations, obtaining a hyper-policy whose characteristics are sensitively regulated over the condition space. We demonstrate the feasibility of this approach and study one of its potential application in policy performance boosting with multiple MuJoCo tasks.",0
"Creating optimal reward functions in reinforcement learning (RL) has been a desired but challenging task. Even slight adjustments to sophisticated reward functions used for complex modern tasks can be costly to evaluate, due to the high cost of training. To address this, we introduce a novel approach called hindsight reward tweaking, which models reward function influences within a near-optimal space. We achieve this by adding a condition vector linearly correlated with environment reward parameters to the input observation. We train the model conventionally, but with randomized reward configurations, resulting in a hyper-policy that is sensitively regulated over the condition space. We demonstrate the feasibility of this approach and its potential to enhance policy performance in multiple MuJoCo tasks.",1
"We propose a black-box reduction that turns a certain reinforcement learning algorithm with optimal regret in a (near-)stationary environment into another algorithm with optimal dynamic regret in a non-stationary environment, importantly without any prior knowledge on the degree of non-stationarity. By plugging different algorithms into our black-box, we provide a list of examples showing that our approach not only recovers recent results for (contextual) multi-armed bandits achieved by very specialized algorithms, but also significantly improves the state of the art for (generalized) linear bandits, episodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most cases our algorithm achieves the optimal dynamic regret $\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$ where $T$ is the number of rounds and $L$ and $\Delta$ are the number and amount of changes of the world respectively, while previous works only obtain suboptimal bounds and/or require the knowledge of $L$ and $\Delta$.",0
"Our proposal offers a black-box reduction that can transform a reinforcement learning algorithm, which exhibits optimal regret in a stationary or nearly stationary setting, into an algorithm that achieves optimal dynamic regret in a non-stationary environment. It is noteworthy that this can be accomplished without prior knowledge of the degree of non-stationarity. By integrating various algorithms into our black-box, we have demonstrated that our approach not only replicates the recent findings achieved by specialized algorithms in the realm of (contextual) multi-armed bandits, but also greatly enhances the state of the art for (generalized) linear bandits, episodic MDPs, and infinite-horizon MDPs in diverse ways. Our algorithm typically attains the optimal dynamic regret $\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$, where $T$ denotes the number of rounds, and $L$ and $\Delta$ refer to the respective number and magnitude of changes in the environment. It is worth noting that prior works have either achieved suboptimal bounds or required knowledge of $L$ and $\Delta$.",1
"It is notoriously difficult to control the behavior of reinforcement learning agents. Agents often learn to exploit the environment or reward signal and need to be retrained multiple times. The multi-objective reinforcement learning (MORL) framework separates a reward function into several objectives. An ideal MORL agent learns to generalize to novel combinations of objectives allowing for better control of an agent's behavior without requiring retraining. Many MORL approaches use a weight vector to parameterize the importance of each objective. However, this approach suffers from lack of expressiveness and interpretability. We propose using propositional logic to specify the importance of multiple objectives. By using a logic where predicates correspond directly to objectives, specifications are inherently more interpretable. Additionally the set of specifications that can be expressed with formal languages is a superset of what can be expressed by weight vectors. In this paper, we define a formal language based on propositional logic with quantitative semantics. We encode logical specifications using a recurrent neural network and show that MORL agents parameterized by these encodings are able to generalize to novel specifications over objectives and achieve performance comparable to single objective baselines.",0
"Reinforcement learning agents are notoriously difficult to control as they often learn to exploit the environment or reward signal, requiring multiple retraining sessions. To address this issue, the multi-objective reinforcement learning (MORL) framework separates a reward function into several objectives. An ideal MORL agent should be able to generalize to novel combinations of objectives, enabling better control of an agent's behavior without additional retraining. However, most MORL approaches use a weight vector to parameterize the importance of each objective, which is limited in expressiveness and interpretability. In this paper, we propose using propositional logic to specify the importance of multiple objectives. This logical approach offers more interpretability and allows for a broader range of specifications than weight vectors. We define a formal language based on propositional logic with quantitative semantics, which we encode using a recurrent neural network to show that MORL agents can generalize to novel specifications over objectives and achieve performance comparable to single objective baselines.",1
The function approximators employed by traditional image based Deep Reinforcement Learning (DRL) algorithms usually lack a temporal learning component and instead focus on learning the spatial component. We propose a technique wherein both temporal as well as spatial components are jointly learned. Our tested was tested with a generic DQN and it outperformed it in terms of maximum rewards as well as sample complexity. This algorithm has implications in the robotics as well as sequential decision making domains.,0
"In typical image-based Deep Reinforcement Learning (DRL) algorithms, the function approximators used tend to overlook temporal learning and concentrate solely on spatial learning. Our proposed method involves simultaneous learning of both temporal and spatial aspects. We evaluated our technique using a standard DQN and discovered that it outperformed it in terms of maximum rewards and sample complexity. This algorithm has potential implications in the fields of robotics and sequential decision making.",1
"Risk management is critical in decision making, and mean-variance (MV) trade-off is one of the most common criteria. However, in reinforcement learning (RL) for sequential decision making under uncertainty, most of the existing methods for MV control suffer from computational difficulties caused by the double sampling problem. In this paper, in contrast to strict MV control, we consider learning MV efficient policies that achieve Pareto efficiency regarding MV trade-off. To achieve this purpose, we train an agent to maximize the expected quadratic utility function, a common objective of risk management in finance and economics. We call our approach direct expected quadratic utility maximization (EQUM). The EQUM does not suffer from the double sampling issue because it does not include gradient estimation of variance. We confirm that the maximizer of the objective in the EQUM directly corresponds to an MV efficient policy under a certain condition. We conduct experiments with benchmark settings to demonstrate the effectiveness of the EQUM.",0
"In decision making, risk management is crucial, and the mean-variance (MV) trade-off is a commonly used criterion. However, most existing MV control methods for reinforcement learning (RL) in sequential decision making under uncertainty encounter computational difficulties due to the double sampling problem. In contrast to strict MV control, this study proposes learning MV efficient policies that achieve Pareto efficiency when it comes to MV trade-off. To accomplish this, an agent is trained to maximize the expected quadratic utility function, a widely used objective in risk management in finance and economics. This approach, known as direct expected quadratic utility maximization (EQUM), circumvents the double sampling issue by excluding gradient estimation of variance. The EQUM's maximizer corresponds directly to an MV efficient policy under specific conditions, as verified in experiments using benchmark settings to demonstrate EQUM's effectiveness.",1
"The recent progress in multi-agent deep reinforcement learning(MADRL) makes it more practical in real-world tasks, but its relatively poor scalability and the partially observable constraints raise challenges to its performance and deployment. Based on our intuitive observation that the human society could be regarded as a large-scale partially observable environment, where each individual has the function of communicating with neighbors and remembering its own experience, we propose a novel network structure called hierarchical graph recurrent network(HGRN) for multi-agent cooperation under partial observability. Specifically, we construct the multi-agent system as a graph, use the hierarchical graph attention network(HGAT) to achieve communication between neighboring agents, and exploit GRU to enable agents to record historical information. To encourage exploration and improve robustness, we design a maximum-entropy learning method to learn stochastic policies of a configurable target action entropy. Based on the above technologies, we proposed a value-based MADRL algorithm called Soft-HGRN and its actor-critic variant named SAC-HRGN. Experimental results based on three homogeneous tasks and one heterogeneous environment not only show that our approach achieves clear improvements compared with four baselines, but also demonstrates the interpretability, scalability, and transferability of the proposed model. Ablation studies prove the function and necessity of each component.",0
"Although multi-agent deep reinforcement learning (MADRL) has made significant strides in real-world tasks, its scalability and partial observability constraints present performance and deployment challenges. To address these challenges, we introduce the hierarchical graph recurrent network (HGRN), which takes inspiration from the human society's large-scale partially observable environment. Our approach constructs the multi-agent system as a graph, leverages hierarchical graph attention network (HGAT) for communication between neighboring agents, and uses GRU to enable agents to record historical information. Our maximum-entropy learning method encourages exploration and improves robustness by learning stochastic policies of a configurable target action entropy. Using these technologies, we propose the Soft-HGRN value-based MADRL algorithm and its actor-critic variant named SAC-HRGN. We evaluate our approach on three homogeneous tasks and one heterogeneous environment, showing significant improvements compared to four baselines, while also demonstrating interpretability, scalability, and transferability. Ablation studies confirm the function and necessity of each component.",1
"With AlphaGo defeats top human players, reinforcement learning(RL) algorithms have gradually become the code-base of building stronger artificial intelligence(AI). The RL algorithm design firstly needs to adapt to the specific environment, so the designed environment guides the rapid and profound development of RL algorithms. However, the existing environments, which can be divided into real world games and customized toy environments, have obvious shortcomings. For real world games, it is designed for human entertainment, and too much difficult for most of RL researchers. For customized toy environments, there is no widely accepted unified evaluation standard for all RL algorithms. Therefore, we introduce the first virtual user-friendly environment framework for RL. In this framework, the environment can be easily configured to realize all kinds of RL tasks in the mainstream research. Then all the mainstream state-of-the-art(SOTA) RL algorithms can be conveniently evaluated and compared. Therefore, our contributions mainly includes the following aspects: 1.single configured environment for all classification of SOTA RL algorithms; 2.combined environment of more than one classification RL algorithms; 3.the evaluation standard for all kinds of RL algorithms. With all these efforts, a possibility for breeding an AI with capability of general competency in a variety of tasks is provided, and maybe it will open up a new chapter for AI.",0
"Reinforcement learning (RL) algorithms have become the foundation for building stronger artificial intelligence (AI) since AlphaGo's victory over top human players. The design of RL algorithms needs to adapt to the specific environment, and the environment guides the rapid and profound development of RL algorithms. However, existing environments, including real-world games and customized toy environments, have noticeable drawbacks. Real-world games are too challenging for most RL researchers, while there is no widely accepted unified evaluation standard for all RL algorithms in customized toy environments. Therefore, we have introduced the first virtual user-friendly environment framework for RL, which enables easy configuration of the environment to realize all kinds of RL tasks in mainstream research. This framework allows for convenient evaluation and comparison of all mainstream state-of-the-art (SOTA) RL algorithms. Our contributions include a single configured environment for all classification of SOTA RL algorithms, a combined environment of more than one classification of RL algorithms, and an evaluation standard for all kinds of RL algorithms. With these efforts, we provide a possibility for breeding an AI with general competency in various tasks and perhaps open a new chapter for AI.",1
"We develop Upside-Down Reinforcement Learning (UDRL), a method for learning to act using only supervised learning techniques. Unlike traditional algorithms, UDRL does not use reward prediction or search for an optimal policy. Instead, it trains agents to follow commands such as ""obtain so much total reward in so much time."" Many of its general principles are outlined in a companion report; the goal of this paper is to develop a practical learning algorithm and show that this conceptually simple perspective on agent training can produce a range of rewarding behaviors for multiple episodic environments. Experiments show that on some tasks UDRL's performance can be surprisingly competitive with, and even exceed that of some traditional baseline algorithms developed over decades of research. Based on these results, we suggest that alternative approaches to expected reward maximization have an important role to play in training useful autonomous agents.",0
"We introduce Upside-Down Reinforcement Learning (UDRL), a novel method for teaching agents to act using only supervised learning techniques. UDRL diverges from conventional algorithms by eschewing reward prediction or policy optimization in favor of training agents to execute specific instructions like ""achieve a set amount of reward within a specified timeframe."" While the fundamental tenets of UDRL are discussed in a related report, this paper aims to establish a practical learning algorithm and demonstrate that this straightforward approach to agent training can yield diverse and desirable outcomes across multiple episodic environments. Experimental results indicate that UDRL can perform remarkably well on certain tasks, even outpacing some traditional benchmarks that have been honed over many years of research. Consequently, we contend that unconventional strategies for maximizing expected reward could serve as a valuable tool for training autonomous agents with real-world applications.",1
"We propose a theoretical framework for approximate planning and learning in partially observed systems. Our framework is based on the fundamental notion of information state. We provide two equivalent definitions of information state -- i) a function of history which is sufficient to compute the expected reward and predict its next value; ii) equivalently, a function of the history which can be recursively updated and is sufficient to compute the expected reward and predict the next observation. An information state always leads to a dynamic programming decomposition. Our key result is to show that if a function of the history (called approximate information state (AIS)) approximately satisfies the properties of the information state, then there is a corresponding approximate dynamic program. We show that the policy computed using this is approximately optimal with bounded loss of optimality. We show that several approximations in state, observation and action spaces in literature can be viewed as instances of AIS. In some of these cases, we obtain tighter bounds. A salient feature of AIS is that it can be learnt from data. We present AIS based multi-time scale policy gradient algorithms. and detailed numerical experiments with low, moderate and high dimensional environments.",0
"Our proposal presents a theoretical framework that focuses on learning and planning in partially observed systems. The foundation of our framework is the concept of an information state, which we define in two ways: firstly, as a function of the history that can calculate the expected reward and predict its future value; secondly, as a function of the history that can be recursively updated and can predict the next observation. An information state always allows for dynamic programming decomposition. Our main finding is that if a function of the history (called approximate information state (AIS)) roughly meets the requirements of an information state, then there is an associated approximate dynamic program. Using this, we demonstrate that the policy generated is almost optimal with a limited loss of optimality. Furthermore, we show that various approximations in state, observation, and action spaces in literature can be seen as examples of AIS, and in some cases, we achieve more precise bounds. The significant aspect of AIS is that it can be learned from data. We illustrate this with AIS-based multi-time scale policy gradient algorithms and various numerical experiments in low, moderate, and high dimensional environments.",1
"Both single-agent and multi-agent actor-critic algorithms are an important class of Reinforcement Learning algorithms. In this work, we propose three fully decentralized multi-agent natural actor-critic (MAN) algorithms. The agents' objective is to collectively learn a joint policy that maximizes the sum of averaged long-term returns of these agents. In the absence of a central controller, agents communicate the information to their neighbors via a time-varying communication network while preserving privacy. We prove the convergence of all the 3 MAN algorithms to a globally asymptotically stable point of the ODE corresponding to the actor update; these use linear function approximations. We use the Fisher information matrix to obtain the natural gradients. The Fisher information matrix captures the curvature of the Kullback-Leibler (KL) divergence between polices at successive iterates. We also show that the gradient of this KL divergence between policies of successive iterates is proportional to the objective function's gradient. Our MAN algorithms indeed use this \emph{representation} of the objective function's gradient. Under certain conditions on the Fisher information matrix, we prove that at each iterate, the optimal value via MAN algorithms can be better than that of the multi-agent actor-critic (MAAC) algorithm using the standard gradients. To validate the usefulness of our proposed algorithms, we implement all the 3 MAN algorithms on a bi-lane traffic network to reduce the average network congestion. We observe an almost 25% reduction in the average congestion in 2 MAN algorithms; the average congestion in another MAN algorithm is on par with the MAAC algorithm. We also consider a generic 15 agent MARL; the performance of the MAN algorithms is again as good as the MAAC algorithm. We attribute the better performance of the MAN algorithms to their use of the above representation.",0
"Reinforcement Learning algorithms are categorized as single-agent and multi-agent actor-critic algorithms, both of which are significant. Our research introduces three fully decentralized multi-agent natural actor-critic (MAN) algorithms that aim to collectively teach agents a joint policy that maximizes the sum of averaged long-term returns. As there is no central controller, agents' communication happens through a time-varying network while maintaining privacy. We demonstrate the convergence of all three MAN algorithms to an asymptotically stable point of the actor update's ODE using linear function approximations. For obtaining natural gradients, we use the Fisher information matrix, which captures the curvature of the Kullback-Leibler divergence between policies at successive iterates. The gradient of this KL divergence is proportional to the objective function's gradient, which is used in our MAN algorithms. Our algorithms perform better than the multi-agent actor-critic (MAAC) algorithm using standard gradients, subject to certain conditions on the Fisher information matrix. We apply our proposed algorithms to a bi-lane traffic network, which resulted in a 25% reduction in average congestion for two MAN algorithms, and the third algorithm showed results similar to the MAAC algorithm. We further tested a generic 15 agent MARL and found the performance of the MAN algorithms comparable to the MAAC algorithm. The MAN algorithms' superior performance is credited to the representation of the objective function's gradient used in our algorithms.",1
"While conventional reinforcement learning focuses on designing agents that can perform one task, meta-learning aims, instead, to solve the problem of designing agents that can generalize to different tasks (e.g., environments, obstacles, and goals) that were not considered during the design or the training of these agents. In this spirit, in this paper, we consider the problem of training a provably safe Neural Network (NN) controller for uncertain nonlinear dynamical systems that can generalize to new tasks that were not present in the training data while preserving strong safety guarantees. Our approach is to learn a set of NN controllers during the training phase. When the task becomes available at runtime, our framework will carefully select a subset of these NN controllers and compose them to form the final NN controller. Critical to our approach is the ability to compute a finite-state abstraction of the nonlinear dynamical system. This abstract model captures the behavior of the closed-loop system under all possible NN weights, and is used to train the NNs and compose them when the task becomes available. We provide theoretical guarantees that govern the correctness of the resulting NN. We evaluated our approach on the problem of controlling a wheeled robot in cluttered environments that were not present in the training data.",0
"Meta-learning differs from conventional reinforcement learning in that it aims to create agents that can perform a variety of tasks, rather than just one. This paper focuses on the challenge of training a Neural Network (NN) controller for uncertain nonlinear dynamical systems that can generalize to new tasks while maintaining safety. The approach involves training a set of NN controllers and selecting a subset to compose the final NN controller at runtime. To accomplish this, the framework uses a finite-state abstraction of the nonlinear dynamical system to capture the behavior of the closed-loop system under all possible NN weights. The resulting NN is theoretically guaranteed to be correct. The approach was tested on controlling a wheeled robot in cluttered environments not present in the training data.",1
"Despite the recent success of deep reinforcement learning (RL), domain adaptation remains an open problem. Although the generalization ability of RL agents is critical for the real-world applicability of Deep RL, zero-shot policy transfer is still a challenging problem since even minor visual changes could make the trained agent completely fail in the new task. To address this issue, we propose a two-stage RL agent that first learns a latent unified state representation (LUSR) which is consistent across multiple domains in the first stage, and then do RL training in one source domain based on LUSR in the second stage. The cross-domain consistency of LUSR allows the policy acquired from the source domain to generalize to other target domains without extra training. We first demonstrate our approach in variants of CarRacing games with customized manipulations, and then verify it in CARLA, an autonomous driving simulator with more complex and realistic visual observations. Our results show that this approach can achieve state-of-the-art domain adaptation performance in related RL tasks and outperforms prior approaches based on latent-representation based RL and image-to-image translation.",0
"While deep reinforcement learning (RL) has seen success, domain adaptation remains a challenge. The generalization ability of RL agents is crucial for practical applications, but zero-shot policy transfer is difficult due to even small visual changes causing the trained agent to fail. To tackle this issue, we propose a two-stage RL agent. In the first stage, it learns a latent unified state representation (LUSR) that is consistent across multiple domains. The second stage involves RL training in one source domain based on LUSR. The cross-domain consistency of LUSR allows the policy learned in the source domain to generalize to other target domains without additional training. We demonstrate our approach in CarRacing games with customized manipulations and in the CARLA autonomous driving simulator with more complex visual observations. Our approach achieves state-of-the-art domain adaptation performance in related RL tasks, outperforming prior methods based on latent-representation-based RL and image-to-image translation.",1
"Reinforcement learning has been found useful in solving optimal power flow (OPF) problems in electric power distribution systems. However, the use of largely model-free reinforcement learning algorithms that completely ignore the physics-based modeling of the power grid compromises the optimizer performance and poses scalability challenges. This paper proposes a novel approach to synergistically combine the physics-based models with learning-based algorithms using imitation learning to solve distribution-level OPF problems. Specifically, we propose imitation learning based improvements in deep reinforcement learning (DRL) methods to solve the OPF problem for a specific case of battery storage dispatch in the power distribution systems. The proposed imitation learning algorithm uses the approximate optimal solutions obtained from a linearized model-based OPF solver to provide a good initial policy for the DRL algorithms while improving the training efficiency. The effectiveness of the proposed approach is demonstrated using IEEE 34-bus and 123-bus distribution feeders with numerous distribution-level battery storage systems.",0
"The application of reinforcement learning has proven to be advantageous in resolving optimal power flow (OPF) issues in electric power distribution systems. However, relying solely on model-free reinforcement learning algorithms that disregard the physics-based modeling of the power grid can negatively impact the performance of the optimizer and lead to scalability challenges. This study introduces a new approach that combines physics-based models with learning-based algorithms using imitation learning to address distribution-level OPF problems. Specifically, the authors propose enhancements to deep reinforcement learning (DRL) methods using imitation learning to tackle the OPF problem of battery storage dispatch in power distribution systems. The imitation learning algorithm leverages the approximate optimal solutions from a linearized model-based OPF solver to provide a solid initial policy for the DRL algorithms, thus improving training efficiency. The authors demonstrate the effectiveness of their approach using IEEE 34-bus and 123-bus distribution feeders with numerous distribution-level battery storage systems.",1
"Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.",0
"The use of curricula in machine learning has proven to be effective in improving learning outcomes by avoiding local optima of training objectives. Curricula are particularly useful in reinforcement learning, where the exploration-exploitation trade-off often leads to getting stuck in local optima. Recent automatic curriculum generation approaches have shown promise in improving performance with less expert knowledge than manually designed curricula. However, these approaches lack theoretical investigation, hindering a deeper understanding of their mechanics. This paper presents an approach for automated curriculum generation in RL with a clear theoretical underpinning. We formalize the self-paced learning paradigm as inducing a distribution over training tasks, balancing task complexity and the objective of matching a desired task distribution. Our experiments demonstrate that training on this induced distribution helps to avoid poor local optima in RL algorithms across different tasks with uninformative rewards and challenging exploration requirements.",1
"Exploration is an essential component of reinforcement learning algorithms, where agents need to learn how to predict and control unknown and often stochastic environments. Reinforcement learning agents depend crucially on exploration to obtain informative data for the learning process as the lack of enough information could hinder effective learning. In this article, we provide a survey of modern exploration methods in (Sequential) reinforcement learning, as well as a taxonomy of exploration methods.",0
"Reinforcement learning algorithms require exploration to teach agents how to handle unpredictable and frequently random environments. Without adequate data, learning can be impeded, making exploration vital for reinforcement learning agents to obtain valuable information. This article outlines contemporary exploration techniques in (Sequential) reinforcement learning and categorizes them.",1
"Building embodied autonomous agents capable of participating in social interactions with humans is one of the main challenges in AI. Within the Deep Reinforcement Learning (DRL) field, this objective motivated multiple works on embodied language use. However, current approaches focus on language as a communication tool in very simplified and non-diverse social situations: the ""naturalness"" of language is reduced to the concept of high vocabulary size and variability. In this paper, we argue that aiming towards human-level AI requires a broader set of key social skills: 1) language use in complex and variable social contexts; 2) beyond language, complex embodied communication in multimodal settings within constantly evolving social worlds. We explain how concepts from cognitive sciences could help AI to draw a roadmap towards human-like intelligence, with a focus on its social dimensions. As a first step, we propose to expand current research to a broader set of core social skills. To do this, we present SocialAI, a benchmark to assess the acquisition of social skills of DRL agents using multiple grid-world environments featuring other (scripted) social agents. We then study the limits of a recent SOTA DRL approach when tested on SocialAI and discuss important next steps towards proficient social agents. Videos and code are available at https://sites.google.com/view/socialai.",0
"Developing autonomous agents that can interact socially with humans is a significant challenge in the field of AI. Researchers in Deep Reinforcement Learning (DRL) have explored embodied language use as a means of achieving this objective. However, current approaches have limited language to high vocabulary and variability in simplified social situations. To achieve human-level AI, a broader range of social skills is needed, including language use in complex and varied social contexts, as well as complex embodied communication in multimodal settings within constantly changing social environments. This paper proposes that cognitive science concepts can aid AI in developing a roadmap to human-like intelligence, with a focus on social dimensions. To start, the authors suggest expanding research to incorporate a broader set of core social skills, as demonstrated in their benchmark tool, SocialAI. This tool assesses DRL agent social skill acquisition through multiple grid-world environments featuring scripted social agents. The authors then explore the limitations of a recent SOTA DRL approach when evaluated on SocialAI and highlight crucial next steps towards proficient social agents. Videos and code for the SocialAI tool are available at https://sites.google.com/view/socialai.",1
"The powerful learning ability of deep neural networks enables reinforcement learning (RL) agents to learn competent control policies directly from high-dimensional and continuous environments. In theory, to achieve stable performance, neural networks assume i.i.d. inputs, which unfortunately does no hold in the general RL paradigm where the training data is temporally correlated and non-stationary. This issue may lead to the phenomenon of ""catastrophic interference"" and the collapse in performance as later training is likely to overwrite and interfer with previously learned policies. In this paper, we introduce the concept of ""context"" into single-task RL and develop a novel scheme, termed as Context Division and Knowledge Distillation (CDaKD) driven RL, to divide all states experienced during training into a series of contexts. Its motivation is to mitigate the challenge of aforementioned catastrophic interference in deep RL, thereby improving the stability and plasticity of RL models. At the heart of CDaKD is a value function, parameterized by a neural network feature extractor shared across all contexts, and a set of output heads, each specializing on an individual context. In CDaKD, we exploit online clustering to achieve context division, and interference is further alleviated by a knowledge distillation regularization term on the output layers for learned contexts. In addition, to effectively obtain the context division in high-dimensional state spaces (e.g., image inputs), we perform clustering in the lower-dimensional representation space of a randomly initialized convolutional encoder, which is fixed throughout training. Our results show that, with various replay memory capacities, CDaKD can consistently improve the performance of existing RL algorithms on classic OpenAI Gym tasks and the more complex high-dimensional Atari tasks, incurring only moderate computational overhead.",0
"Deep neural networks possess a remarkable ability to learn, which allows reinforcement learning (RL) agents to develop control policies directly from complex, continuous environments. However, neural networks assume independent and identically distributed (i.i.d.) inputs to achieve stable performance. This assumption does not hold in the general RL paradigm, where the training data is temporally correlated and non-stationary. As a result, the phenomenon of ""catastrophic interference"" may occur, leading to a collapse in performance as later training overwrites previously learned policies. In this study, we propose a new approach called Context Division and Knowledge Distillation (CDaKD) driven RL to address this issue. CDaKD introduces the concept of ""contexts"" into single-task RL and divides all states experienced during training into a series of contexts. A value function, parameterized by a neural network feature extractor shared across all contexts, and a set of output heads, each specializing on an individual context, form the heart of CDaKD. We employ online clustering to achieve context division and mitigate interference by incorporating a knowledge distillation regularization term on the output layers for learned contexts. We perform clustering in the lower-dimensional representation space of a fixed, randomly initialized convolutional encoder to effectively obtain the context division in high-dimensional state spaces. Our experimental results show that CDaKD consistently improves the performance of existing RL algorithms on classic OpenAI Gym tasks and complex high-dimensional Atari tasks, with only moderate computational overhead.",1
"Accurately predicting the dynamics of robotic systems is crucial for model-based control and reinforcement learning. The most common way to estimate dynamics is by fitting a one-step ahead prediction model and using it to recursively propagate the predicted state distribution over long horizons. Unfortunately, this approach is known to compound even small prediction errors, making long-term predictions inaccurate. In this paper, we propose a new parametrization to supervised learning on state-action data to stably predict at longer horizons -- that we call a trajectory-based model. This trajectory-based model takes an initial state, a future time index, and control parameters as inputs, and directly predicts the state at the future time index. Experimental results in simulated and real-world robotic tasks show that trajectory-based models yield significantly more accurate long term predictions, improved sample efficiency, and the ability to predict task reward. With these improved prediction properties, we conclude with a demonstration of methods for using the trajectory-based model for control.",0
"Precise forecasting of the movements of robotic systems is vital for model-based control and reinforcement learning. The conventional approach to estimating dynamics involves building a one-step ahead prediction model and using it to progressively propagate the predicted state distribution over extended periods. Unfortunately, this approach is prone to magnifying even minor prediction errors, which can lead to imprecise long-term predictions. In this paper, we propose a new supervised learning parametrization on state-action data, known as a trajectory-based model, that can accurately predict movements over longer horizons. The trajectory-based model takes an initial state, a future time index, and control parameters as inputs and directly predicts the state at the future time index. Results from simulated and real-world robotic tasks demonstrate that trajectory-based models provide significantly more accurate long-term predictions, greater sample efficiency, and the ability to predict task reward. To showcase the improved prediction properties, we conclude with an illustration of how the trajectory-based model can be utilized for control.",1
"We introduce a new unsupervised pretraining objective for reinforcement learning. During the unsupervised reward-free pretraining phase, the agent maximizes mutual information between tasks and states induced by the policy. Our key contribution is a novel lower bound of this intractable quantity. We show that by reinterpreting and combining variational successor features~\citep{Hansen2020Fast} with nonparametric entropy maximization~\citep{liu2021behavior}, the intractable mutual information can be efficiently optimized. The proposed method Active Pretraining with Successor Feature (APS) explores the environment via nonparametric entropy maximization, and the explored data can be efficiently leveraged to learn behavior by variational successor features. APS addresses the limitations of existing mutual information maximization based and entropy maximization based unsupervised RL, and combines the best of both worlds. When evaluated on the Atari 100k data-efficiency benchmark, our approach significantly outperforms previous methods combining unsupervised pretraining with task-specific finetuning.",0
"Our paper presents a new approach to unsupervised pretraining for reinforcement learning. Our method involves maximizing the mutual information between tasks and states induced by the policy during the reward-free pretraining phase. We propose a novel lower bound for this intractable quantity, which combines variational successor features and nonparametric entropy maximization. Our approach, called Active Pretraining with Successor Feature (APS), uses nonparametric entropy maximization to explore the environment and variational successor features to efficiently learn behavior. APS overcomes the limitations of existing unsupervised RL methods based on mutual information maximization or entropy maximization alone. We evaluate our approach on the Atari 100k data-efficiency benchmark, and it outperforms previous methods that combine unsupervised pretraining with task-specific finetuning.",1
"Device-edge co-inference, which partitions a deep neural network between a resource-constrained mobile device and an edge server, recently emerges as a promising paradigm to support intelligent mobile applications. To accelerate the inference process, on-device model sparsification and intermediate feature compression are regarded as two prominent techniques. However, as the on-device model sparsity level and intermediate feature compression ratio have direct impacts on computation workload and communication overhead respectively, and both of them affect the inference accuracy, finding the optimal values of these hyper-parameters brings a major challenge due to the large search space. In this paper, we endeavor to develop an efficient algorithm to determine these hyper-parameters. By selecting a suitable model split point and a pair of encoder/decoder for the intermediate feature vector, this problem is casted as a sequential decision problem, for which, a novel automated machine learning (AutoML) framework is proposed based on deep reinforcement learning (DRL). Experiment results on an image classification task demonstrate the effectiveness of the proposed framework in achieving a better communication-computation trade-off and significant inference speedup against various baseline schemes.",0
"A promising approach called device-edge co-inference has emerged to support intelligent mobile applications. This approach partitions a deep neural network between a resource-constrained mobile device and an edge server. To speed up the inference process, two prominent techniques are used: on-device model sparsification and intermediate feature compression. However, finding the optimal values of these hyper-parameters is a major challenge due to the large search space. The reason for this is that the on-device model sparsity level and intermediate feature compression ratio directly impact the computation workload and communication overhead respectively, and both of them affect the inference accuracy. In this paper, we propose an efficient algorithm to determine these hyper-parameters by selecting a suitable model split point and a pair of encoder/decoder for the intermediate feature vector. This problem is casted as a sequential decision problem using a novel automated machine learning (AutoML) framework based on deep reinforcement learning (DRL). Our experiment results on an image classification task show that our proposed framework achieves a better communication-computation trade-off and significant inference speedup compared to various baseline schemes.",1
"Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding.",0
"In recent times, deep reinforcement learning (DRL) techniques have demonstrated exceptional performance in various domains. However, the neural network policies generated by DRL methods are not easy for humans to interpret and often struggle to apply to new situations. Previous studies have tackled this problem by exploring programmatic policies that are more structured and interpretable for generalization. However, these studies either use limited policy representations or require stronger supervision. To address this issue, we present a framework that learns to create a program that explains how to complete a task in a flexible and expressive manner, based solely on reward signals. We propose to learn a program embedding space first, which parameterizes various behaviors continuously in an unsupervised way, and then search over it to produce a program that maximizes task returns. Our experimental results demonstrate that our proposed framework performs better than DRL and program synthesis baselines, generating interpretable and more generalizable policies. We also justify the need for the two-stage learning scheme and analyze different methods for learning the program embedding.",1
"Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/.",0
"In recent years, self-supervised representation learning has achieved remarkable success. This approach is capable of utilizing the vast amount of unlabeled images that are available on the Internet and in photographic datasets, without relying on supervised labels. However, in order to develop truly intelligent agents, we need to create representation learning algorithms that can learn from both datasets and environments. Agents in natural environments cannot depend on curated data and must explore their surroundings to acquire the necessary data. To achieve this, we propose a framework called curious representation learning (CRL), which simultaneously learns a visual representation model and a reinforcement learning policy. The policy is taught to maximize the representation learner's error, which encourages exploration of the environment. As the policy feeds the representation learner progressively harder data, the learned representation becomes stronger. Our learned representations enable better transfer to downstream navigation tasks than ImageNet pretraining, without using any supervision. Moreover, our learned representations can produce interpretable results on real images, despite being trained in simulation. The code for our framework is available at https://yilundu.github.io/crl/.",1
"In this paper we present an end-to-end framework for addressing the problem of dynamic pricing (DP) on E-commerce platform using methods based on deep reinforcement learning (DRL). By using four groups of different business data to represent the states of each time period, we model the dynamic pricing problem as a Markov Decision Process (MDP). Compared with the state-of-the-art DRL-based dynamic pricing algorithms, our approaches make the following three contributions. First, we extend the discrete set problem to the continuous price set. Second, instead of using revenue as the reward function directly, we define a new function named difference of revenue conversion rates (DRCR). Third, the cold-start problem of MDP is tackled by pre-training and evaluation using some carefully chosen historical sales data. Our approaches are evaluated by both offline evaluation method using real dataset of Alibaba Inc., and online field experiments starting from July 2018 with thousands of items, lasting for months on Tmall.com. To our knowledge, there is no other DP field experiment using DRL before. Field experiment results suggest that DRCR is a more appropriate reward function than revenue, which is widely used by current literature. Also, continuous price sets have better performance than discrete sets and our approaches significantly outperformed the manual pricing by operation experts.",0
"This paper introduces a comprehensive framework for solving the problem of dynamic pricing (DP) on E-commerce platforms using deep reinforcement learning (DRL) methods. The dynamic pricing problem is modeled as a Markov Decision Process (MDP) using four different groups of business data to represent each time period's state. Our approach makes three significant contributions compared to the current DRL-based DP algorithms. Firstly, we extend the discrete set problem to a continuous price set. Secondly, we introduce a new reward function called the difference of revenue conversion rates (DRCR) instead of using revenue directly. Thirdly, we address the cold-start problem of MDP by pre-training and evaluating historical sales data. The efficacy of our approach is validated using both offline evaluation methods on Alibaba Inc.'s real dataset and online field experiments on Tmall.com. Our field experiments demonstrate that DRCR is a more appropriate reward function than revenue, and continuous price sets perform better than discrete sets. Furthermore, our approach significantly outperforms manual pricing done by operation experts. Notably, our field experiments are the first to utilize DRL for DP.",1
"Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.",0
"The performance of deep reinforcement learning (RL) algorithms is usually assessed by comparing their performance on a variety of tasks. However, most evaluations only consider point estimates of aggregate performance, such as mean and median scores, without taking into account the statistical uncertainty due to the finite number of training runs. As benchmarks become more computationally demanding, evaluations are often limited to a small number of runs per task, which exacerbates the uncertainty in point estimates. To avoid hindering progress in the field, it is essential to account for the uncertainty in results, even when using few runs. This paper proposes reporting interval estimates of aggregate performance and using performance profiles to address the variability in results. Additionally, more robust and efficient aggregate metrics, such as interquartile mean scores, can be utilized to achieve small uncertainty in results. The study highlights the discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis on the Atari 100k benchmark and other widely used RL benchmarks. To improve the evaluation methodology, the paper presents an open-source library, rliable, to prevent unreliable results from impeding progress in the field.",1
"Adversarial training has become the primary method to defend against adversarial samples. However, it is hard to practically apply due to many shortcomings. One of the shortcomings of adversarial training is that it will reduce the recognition accuracy of normal samples. Adaptive perturbation adversarial training is proposed to alleviate this problem. It uses marginal adversarial samples that are close to the decision boundary but does not cross the decision boundary for adversarial training, which improves the accuracy of model recognition while maintaining the robustness of the model. However, searching for marginal adversarial samples brings additional computational costs. This paper proposes a method for finding marginal adversarial samples based on reinforcement learning, and combines it with the latest fast adversarial training technology, which effectively speeds up training process and reduces training costs.",0
"Adversarial training is currently the primary defense against adversarial samples, but it is impractical due to several limitations. One major drawback is that it decreases the recognition accuracy of normal samples. To address this issue, a new approach called Adaptive Perturbation Adversarial Training (APAT) has been introduced. APAT uses marginal adversarial samples near the decision boundary for training, effectively enhancing model recognition accuracy while maintaining its robustness. However, finding these marginal samples is computationally expensive. To solve this problem, this paper proposes a reinforcement learning-based method to discover marginal adversarial samples and combines it with fast adversarial training technology to accelerate the training process and lower training costs.",1
"Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods to improve the robustness of deep reinforcement learning agents to adversarial perturbations based on training in the presence of these imperceptible perturbations (i.e. adversarial training). In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we follow two distinct parallel approaches to investigate the outcomes of adversarial training on deep neural policies based on worst-case distributional shift and feature sensitivity. For the first approach, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. For the second approach, we propose a novel method to measure the feature sensitivities of deep neural policies and we compare these feature sensitivity differences in state-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.",0
"Deep neural network-based reinforcement learning policies, like neural network image classifiers, can be easily compromised by imperceptible adversarial perturbations in their inputs. To improve the robustness of these agents, researchers have proposed adversarial training methods that involve training the agent in the presence of such perturbations. In this study, we investigate the effects of adversarial training on the neural policy learned by the agent. We adopt two parallel approaches to examine these effects based on worst-case distributional shift and feature sensitivity. Through experiments in OpenAI Atari environments, we demonstrate that adversarially trained policies exhibit a higher sensitivity to low frequency perturbations, as indicated by the Fourier spectrum of minimal perturbations. Moreover, we introduce a novel method to measure feature sensitivities and compare these differences in adversarially trained and vanilla trained deep neural policies. Our findings are an essential first step towards understanding the relationship between adversarial training and various forms of neural policy robustness.",1
"In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number (i.e., population size). Every single MG induced by varying population sizes may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent algorithms. In this work, we focus on creating agents that generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set that is formed by effective strategies across a variety of games. We propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the common strategic knowledge and diverse strategic modes are discovered with an iterative optimization procedure. We prove that as an approximation to a constrained mutual information maximization objective, the learned policies can reach Nash Equilibrium in every evaluation MG under the assumption of Lipschitz game on a sufficiently large latent space. When deploying it at practical latent models with limited size, fast adaptation can be achieved by leveraging the first-order gradient information. Extensive experiments show the effectiveness of MRA on both training performance and generalization ability in hard and unseen games.",0
"The usual practice in multi-agent reinforcement learning is for agents to learn behaviors in a single Markov Game (MG) that are limited to the specific agent number or population size. Each MG with varying population sizes may have its own optimal strategies and game-specific knowledge that are modeled independently in modern multi-agent algorithms. This study aims to create agents that can generalize across population-varying MGs by teaching each agent to learn a policy set consisting of effective strategies from various games instead of just a unimodal policy. The proposed Meta Representations for Agents (MRA) model includes game-common and game-specific strategic knowledge, and multi-modal latent policies are used to represent the policy sets. An iterative optimization procedure is employed to discover the common strategic knowledge and diverse strategic modes. The learned policies are proven to reach Nash Equilibrium in every evaluation MG, assuming a Lipschitz game on a sufficiently large latent space. When deployed in practical latent models with limited size, fast adaptation can be achieved by leveraging first-order gradient information. Extensive experiments demonstrate the effectiveness of MRA in both training performance and generalization ability, particularly in hard and unfamiliar games.",1
"Although well-established in general reinforcement learning (RL), value-based methods are rarely explored in constrained RL (CRL) for their incapability of finding policies that can randomize among multiple actions. To apply value-based methods to CRL, a recent groundbreaking line of game-theoretic approaches uses the mixed policy that randomizes among a set of carefully generated policies to converge to the desired constraint-satisfying policy. However, these approaches require storing a large set of policies, which is not policy efficient, and may incur prohibitive memory costs in constrained deep RL. To address this problem, we propose an alternative approach. Our approach first reformulates the CRL to an equivalent distance optimization problem. With a specially designed linear optimization oracle, we derive a meta-algorithm that solves it using any off-the-shelf RL algorithm and any conditional gradient (CG) type algorithm as subroutines. We then propose a new variant of the CG-type algorithm, which generalizes the minimum norm point (MNP) method. The proposed method matches the convergence rate of the existing game-theoretic approaches and achieves the worst-case optimal policy efficiency. The experiments on a navigation task show that our method reduces the memory costs by an order of magnitude, and meanwhile achieves better performance, demonstrating both its effectiveness and efficiency.",0
"Value-based methods are commonly used in reinforcement learning (RL) but have not been widely explored in constrained RL (CRL) due to their inability to randomize among multiple actions. Recently, a game-theoretic approach was introduced to apply value-based methods to CRL by using a mixed policy that randomizes among a set of policies to converge to the desired constraint-satisfying policy. However, this approach requires storing a large set of policies, which is not policy efficient and may incur prohibitive memory costs in constrained deep RL. To address this issue, we propose an alternative method that reformulates CRL as a distance optimization problem and uses a linear optimization oracle with any off-the-shelf RL algorithm and any conditional gradient (CG) type algorithm as subroutines. We also introduce a new variant of the CG-type algorithm that generalizes the minimum norm point (MNP) method, which achieves worst-case optimal policy efficiency and matches the convergence rate of existing game-theoretic approaches. Our experiments on a navigation task demonstrate that our method significantly reduces memory costs while achieving better performance, highlighting its effectiveness and efficiency.",1
"Learning requires both study and curiosity. A good learner is not only good at extracting information from the data given to it, but also skilled at finding the right new information to learn from. This is especially true when a human operator is required to provide the ground truth - such a source should only be queried sparingly. In this work, we address the problem of curiosity as it relates to online, real-time, human-in-the-loop training of an object detection algorithm onboard a robotic platform, one where motion produces new views of the subject. We propose a deep reinforcement learning approach that decides when to ask the human user for ground truth, and when to move. Through a series of experiments, we demonstrate that our agent learns a movement and request policy that is at least 3x more effective at using human user interactions to train an object detector than untrained approaches, and is generalizable to a variety of subjects and environments.",0
"To effectively learn, one must possess both the ability to study and a natural curiosity. A successful learner is capable of extracting information from given data, as well as seeking out new knowledge. It is important to limit the use of human operators as a source of truth in order to be efficient. The focus of this study is on the issue of curiosity in real-time, online training of an object detection algorithm on a robotic platform that produces new perspectives. Our proposed solution involves a deep reinforcement learning method that determines when to request human interaction and when to move. Our experiments show that our approach is more effective than untrained methods and can be applied to various subjects and environments. The agent's movement and request policy is at least 3 times more efficient in using human interaction to train an object detector.",1
"State-of-the-art object detection models are frequently trained offline using available datasets, such as ImageNet: large and overly diverse data that are unbalanced and hard to cluster semantically. This kind of training drops the object detection performance should the change in illumination, in the environmental conditions (e.g., rain), or in the lens positioning (out-of-focus blur) occur. We propose a decentralized hierarchical multi-agent deep reinforcement learning approach for intelligently controlling the camera and the lens focusing settings, leading to significant improvement to the capacity of the popular detection models (YOLO, Fast R-CNN, and Retina are considered). The algorithm relies on the latent representation of the camera's stream and, thus, it is the first method to allow a completely no-reference tuning of the camera, where the system trains itself to auto-focus itself.",0
"Object detection models that are considered state-of-the-art are often trained offline using available datasets, such as ImageNet. However, these datasets are large and overly diverse, making them unbalanced and difficult to cluster semantically. This type of training can result in a drop in object detection performance if there are changes in illumination, environmental conditions (e.g., rain), or lens positioning (out-of-focus blur). Our proposed solution is a decentralized hierarchical multi-agent deep reinforcement learning approach that intelligently controls the camera and lens focusing settings. This approach leads to a significant improvement in the capacity of popular detection models (such as YOLO, Fast R-CNN, and Retina). The algorithm utilizes the latent representation of the camera's stream and is the first method to allow for a completely no-reference tuning of the camera. This means that the system trains itself to auto-focus, resulting in improved performance.",1
"The reinforcement learning (RL) research area is very active, with several important applications. However, certain challenges still need to be addressed, amongst which one can mention the ability to find policies that achieve sufficient exploration and coordination while solving a given task. In this work, we present an algorithmic framework of two RL agents each with a different objective. We introduce a novel function approximation approach to assess the influence $F$ of a certain policy on others. While optimizing $F$ as a regularizer of $\pi$'s objective, agents learn to coordinate team behavior while exploiting high-reward regions of the solution space. Additionally, both agents use prediction error as intrinsic motivation to learn policies that behave as differently as possible, thus achieving the exploration criterion. Our method was evaluated on the suite of OpenAI gym tasks as well as cooperative and mixed scenarios, where agent populations are able to discover various physical and informational coordination strategies, showing state-of-the-art performance when compared to famous baselines.",0
"The field of reinforcement learning (RL) is currently very active and has many important applications. Nonetheless, there are still challenges that need to be addressed, including the difficulty of finding policies that are effective at exploration and coordination while solving a given task. In this study, we propose an algorithmic framework consisting of two RL agents, each with a distinct objective. We introduce a new function approximation method to evaluate the influence of a policy on others, known as $F$. By optimizing $F$ as a regularizer of $\pi$'s objective, the agents learn to coordinate their behavior effectively while also exploiting high-reward regions of the solution space. In addition, both agents use prediction error as intrinsic motivation to learn policies that behave as differently as possible, thereby satisfying the exploration criterion. Our approach was tested on a variety of OpenAI gym tasks as well as cooperative and mixed scenarios, and the agent populations were able to discover a range of physical and informational coordination strategies, achieving state-of-the-art performance in comparison to well-known baselines.",1
"Bilevel optimization has arisen as a powerful tool for many machine learning problems such as meta-learning, hyperparameter optimization, and reinforcement learning. In this paper, we investigate the nonconvex-strongly-convex bilevel optimization problem. For deterministic bilevel optimization, we provide a comprehensive convergence rate analysis for two popular algorithms respectively based on approximate implicit differentiation (AID) and iterative differentiation (ITD). For the AID-based method, we orderwisely improve the previous convergence rate analysis due to a more practical parameter selection as well as a warm start strategy, and for the ITD-based method we establish the first theoretical convergence rate. Our analysis also provides a quantitative comparison between ITD and AID based approaches. For stochastic bilevel optimization, we propose a novel algorithm named stocBiO, which features a sample-efficient hypergradient estimator using efficient Jacobian- and Hessian-vector product computations. We provide the convergence rate guarantee for stocBiO, and show that stocBiO outperforms the best known computational complexities orderwisely with respect to the condition number $\kappa$ and the target accuracy $\epsilon$. We further validate our theoretical results and demonstrate the efficiency of bilevel optimization algorithms by the experiments on meta-learning and hyperparameter optimization.",0
"The utilization of bilevel optimization has become an effective strategy for addressing various machine learning problems such as reinforcement learning, hyperparameter optimization and meta-learning. This study focuses on examining the nonconvex-strongly-convex bilevel optimization problem. For deterministic bilevel optimization, we conduct an extensive analysis of the convergence rates of two widely used algorithms, namely, approximate implicit differentiation (AID) and iterative differentiation (ITD). We enhance the previous convergence rate analysis of the AID-based method by utilizing more practical parameter selection and a warm start strategy. Moreover, we establish the first theoretical convergence rate for the ITD-based approach and provide a comparison between ITD and AID-based methods. Furthermore, we propose a new algorithm called stocBiO for stochastic bilevel optimization. This algorithm employs a sample-efficient hypergradient estimator that utilizes efficient Jacobian- and Hessian-vector product computations. We guarantee the convergence rate of stocBiO and demonstrate that it outperforms the best-known computational complexities concerning the condition number $\kappa$ and the target accuracy $\epsilon$. Our experimental results on meta-learning and hyperparameter optimization validate our theoretical results and demonstrate the efficiency of bilevel optimization algorithms.",1
"Maintenance scheduling is a complex decision-making problem in the production domain, where a number of maintenance tasks and resources has to be assigned and scheduled to production entities in order to prevent unplanned production downtime. Intelligent maintenance strategies are required that are able to adapt to the dynamics and different conditions of production systems. The paper introduces a deep reinforcement learning approach for condition-oriented maintenance scheduling in flow line systems. Different policies are learned, analyzed and evaluated against a benchmark scheduling heuristic based on reward modelling. The evaluation of the learned policies shows that reinforcement learning based maintenance strategies meet the requirements of the presented use case and are suitable for maintenance scheduling in the shop floor.",0
"In the production industry, scheduling maintenance is a complicated decision-making process that involves assigning tasks and resources to prevent unexpected downtime. To accommodate the ever-changing conditions of production systems, intelligent maintenance strategies must be implemented. This study proposes a deep reinforcement learning method for condition-oriented maintenance scheduling in flow line systems. Multiple policies were learned, analyzed, and compared to a benchmark scheduling heuristic using reward modeling. The results indicate that reinforcement learning-based maintenance strategies fulfill the requirements of the presented use case and are an effective solution for maintenance scheduling in the shop floor.",1
"Active inference has emerged as an alternative approach to control problems given its intuitive (probabilistic) formalism. However, despite its theoretical utility, computational implementations have largely been restricted to low-dimensional, deterministic settings. This paper highlights that this is a consequence of the inability to adequately model stochastic transition dynamics, particularly when an extensive policy (i.e., action trajectory) space must be evaluated during planning. Fortunately, recent advancements propose a modified planning algorithm for finite temporal horizons. We build upon this work to assess the utility of active inference for a stochastic control setting. For this, we simulate the classic windy grid-world task with additional complexities, namely: 1) environment stochasticity; 2) learning of transition dynamics; and 3) partial observability. Our results demonstrate the advantage of using active inference, compared to reinforcement learning, in both deterministic and stochastic settings.",0
"Active inference has gained popularity as an alternative method for controlling problems due to its intuitive probabilistic structure. However, despite its theoretical usefulness, its application has been limited to low-dimensional, deterministic scenarios. This is due to the challenge of modeling stochastic transition dynamics, especially when evaluating a large policy space during planning. Fortunately, recent advances propose a modified planning algorithm for finite temporal horizons that can address this limitation. In this study, we further investigate the effectiveness of active inference in a stochastic control setting. Specifically, we simulate a complex version of the windy grid-world task that includes environmental stochasticity, learning of transition dynamics, and partial observability. Our findings reveal that active inference outperforms reinforcement learning in both deterministic and stochastic settings.",1
"While we have made significant progress on understanding hand-object interactions in computer vision, it is still very challenging for robots to perform complex dexterous manipulation. In this paper, we propose a new platform and pipeline, DexMV (Dexterous Manipulation from Videos), for imitation learning to bridge the gap between computer vision and robot learning. We design a platform with: (i) a simulation system for complex dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer vision system to record large-scale demonstrations of a human hand conducting the same tasks. In our new pipeline, we extract 3D hand and object poses from the videos, and convert them to robot demonstrations via motion retargeting. We then apply and compare multiple imitation learning algorithms with the demonstrations. We show that the demonstrations can indeed improve robot learning by a large margin and solve the complex tasks which reinforcement learning alone cannot solve. Project page with video: https://yzqin.github.io/dexmv",0
"Despite significant progress in the computer vision field regarding hand-object interactions, robots still struggle to perform complex dexterous manipulation. To address this issue, we introduce DexMV (Dexterous Manipulation from Videos), a new platform and pipeline for imitation learning. Our platform includes a simulation system for complex dexterous manipulation tasks using a multi-finger robot hand, as well as a computer vision system to capture large-scale demonstrations of a human hand performing the same tasks. Our pipeline involves extracting 3D hand and object poses from the videos and converting them to robot demonstrations through motion retargeting. We then apply and compare multiple imitation learning algorithms with the demonstrations. Our results show that the demonstrations significantly improve robot learning, enabling them to solve complex tasks that reinforcement learning alone cannot. For more information and a video of our project, please visit our project page at https://yzqin.github.io/dexmv.",1
"We introduce a new semantic communication mechanism, whose key idea is to preserve the semantic information instead of strictly securing the bit-level precision. Starting by analyzing the defects of existing joint source channel coding (JSCC) methods, we show that the commonly used bit-level metrics are vulnerable of catching important semantic meaning and structures. To address this problem, we take advantage of learning from semantic similarity, instead of relying on conventional paired bit-level supervisions like cross entropy and bit error rate. However, to develop such a semantic communication system is indeed a nontrivial task, considering the nondifferentiability of most semantic metrics as well as the instability from noisy channels. To further resolve these issues, we put forward a reinforcement learning (RL)-based solution which allows us to simultaneously optimize any user-defined semantic measurement by using the policy gradient technique, and to interact with the surrounding noisy environment in a natural way. We have testified the proposed method in the challenging European-parliament dataset. Experiments on both AWGN and phase-invariant fading channel have confirmed the superiority of our method in revealing the semantic meanings, and better handling the channel noise especially in low-SNR situations. Apart from the experimental results, we further provide an indepth look at how the semantics model behaves, along with its superb generalization ability in real-life examples. As a brand new method in learning-based JSCC tasks, we also exemplify an RL-based image transmission paradigm, both to prove the generalization ability, and to leave this new topic for future discussion.",0
"A novel semantic communication mechanism is presented, which prioritizes preserving semantic information over securing bit-level precision. The inadequacies of current joint source channel coding (JSCC) techniques are analyzed, revealing that bit-level metrics are prone to losing significant semantic meaning and structures. To overcome this issue, the approach utilizes semantic similarity learning, rather than conventional bit-level supervisions such as cross entropy and bit error rate. However, developing such a semantic communication system is challenging due to the non-differentiability of most semantic metrics and instability from noisy channels. To address these challenges, a reinforcement learning (RL)-based solution is proposed. This approach allows for simultaneous optimization of user-defined semantic measurements using the policy gradient technique and interaction with the noisy environment in a natural way. The proposed method is tested on the European-parliament dataset, with experiments on AWGN and phase-invariant fading channel confirming its superiority in revealing semantic meanings and handling channel noise, particularly in low-SNR situations. In addition to experimental results, the generalization ability of the semantics model is explored through real-life examples. An RL-based image transmission paradigm is also exemplified, demonstrating the new method's generalization ability and leaving room for further discussion.",1
"This paper presents a comprehensive survey of Federated Reinforcement Learning (FRL), an emerging and promising field in Reinforcement Learning (RL). Starting with a tutorial of Federated Learning (FL) and RL, we then focus on the introduction of FRL as a new method with great potential by leveraging the basic idea of FL to improve the performance of RL while preserving data-privacy. According to the distribution characteristics of the agents in the framework, FRL algorithms can be divided into two categories, i.e. Horizontal Federated Reinforcement Learning (HFRL) and Vertical Federated Reinforcement Learning (VFRL). We provide the detailed definitions of each category by formulas, investigate the evolution of FRL from a technical perspective, and highlight its advantages over previous RL algorithms. In addition, the existing works on FRL are summarized by application fields, including edge computing, communication, control optimization, and attack detection. Finally, we describe and discuss several key research directions that are crucial to solving the open problems within FRL.",0
"This article offers a complete examination of Federated Reinforcement Learning (FRL), a new and promising area in Reinforcement Learning (RL). It begins with an overview of Federated Learning (FL) and RL, and then introduces FRL as a novel method with significant potential. This technique employs the fundamental concept of FL to enhance the performance of RL while protecting data privacy. FRL algorithms can be classified into two categories based on agent distribution within the framework: Horizontal Federated Reinforcement Learning (HFRL) and Vertical Federated Reinforcement Learning (VFRL). The definitions of each category are provided through formulas, and the evolution of FRL is explored through a technical lens, emphasizing its advantages over previous RL algorithms. Moreover, the article summarizes the existing works on FRL by application fields such as edge computing, communication, control optimization, and attack detection. Finally, the article identifies several critical research directions to address open problems within FRL and discusses them in detail.",1
"Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a monolithic behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of switching between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promising and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time-scales.",0
"Reinforcement learning (RL) continues to face the major challenge of exploration. Most current methods rely on a single, gradually changing behaviour policy, while the exploration behaviours of animals and humans are diverse and involve mode-switching. This study aims to address non-monolithic exploration for RL by examining different modes to switch between, the optimal timescales for switching, and effective switching triggers. Our proposed algorithmic components ensure adaptability and robustness, eliminating the need for excessive hyper-parameter-tuning. We present a promising analysis of Atari using a two-mode exploration approach with sub-episodic time-scale switching.",1
"Active visual exploration aims to assist an agent with a limited field of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to address this problem either by using reinforcement learning, which is difficult to train, or by uncertainty maps, which are task-specific and can only be implemented for dense prediction tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-specific uncertainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to further improve the representations learned. Unlike previous works, we show the application of our model on multiple tasks like reconstruction, segmentation and classification. Our model provides encouraging results while being less dependent on dataset bias in driving the exploration. We further perform an ablation study to investigate the features and attention learned by our model. Finally, we show that our self-attention module learns to attend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/soroushseifi/glimpse-attend-explore.",0
"The goal of active visual exploration is to help an agent with a limited field of view understand its surroundings by selecting the best viewing directions in the scene based on partial observations. Existing methods have attempted to solve this issue either through reinforcement learning, which is challenging to train, or uncertainty maps, which are task-specific and only suitable for dense prediction tasks. This study introduces the Glimpse-Attend-and-Explore model, which (a) employs self-attention to direct visual exploration instead of task-specific uncertainty maps, (b) can be used for both dense and sparse prediction tasks, and (c) uses a contrastive stream to enhance the representations learned. Unlike previous methods, our model is tested on multiple tasks such as reconstruction, segmentation, and classification, and it yields promising results while being less reliant on dataset bias to guide exploration. We also conduct an ablation study to examine the features and attention learned by our model. Finally, we demonstrate that our self-attention module learns to attend to different parts of the scene by minimizing the loss on the downstream task. The code can be found at https://github.com/soroushseifi/glimpse-attend-explore.",1
"End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To address these issues, we train a reinforcement learning expert that maps bird's-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides informative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense benchmark and state-of-the-art performance on the more challenging CARLA LeaderBoard.",0
"Autonomous driving systems that use end-to-end approaches often require demonstrations from experts. However, humans are not suitable coaches for algorithms that require dense on-policy supervision. On the other hand, automated experts that use privileged information can efficiently provide both on-policy and off-policy demonstrations on a large scale. Unfortunately, existing automated experts for urban driving rely heavily on hand-crafted rules and do not perform well even on driving simulators that have ground-truth information. To solve this problem, we developed a reinforcement learning expert that can map bird's-eye view images to continuous low-level actions. Our expert achieved a new performance upper-bound on CARLA and also provides informative supervision signals for imitation learning agents to learn from. By being supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieved expert-level performance. Our end-to-end agent reached a 78% success rate while also generalizing to a new town and new weather on the NoCrash-dense benchmark and has state-of-the-art performance on the more challenging CARLA LeaderBoard.",1
"As a notable machine learning paradigm, the research efforts in the context of reinforcement learning have certainly progressed leaps and bounds. When compared with reinforcement learning methods with the given system model, the methodology of the reinforcement learning architecture based on the unknown model generally exhibits significantly broader universality and applicability. In this work, a new reinforcement learning architecture based on iterative linear quadratic regulator (iLQR) is developed and presented without the requirement of any prior knowledge of the system model, which is termed as an approach of a ""neural network iterative linear quadratic regulator (NNiLQR)"". Depending solely on measurement data, this method yields a completely new non-parametric routine for the establishment of the optimal policy (without the necessity of system modeling) through iterative refinements of the neural network system. Rather importantly, this approach significantly outperforms the classical iLQR method in terms of the given objective function because of the innovative utilization of further exploration in the methodology. As clearly indicated from the results attained in two illustrative examples, these significant merits of the NNiLQR method are demonstrated rather evidently.",0
"The progress in reinforcement learning research has been remarkable, making it a notable machine learning paradigm. Compared to reinforcement learning methods that utilize a given system model, the reinforcement learning architecture based on an unknown model has greater universality and applicability. This study introduces a new approach, named ""neural network iterative linear quadratic regulator (NNiLQR),"" which utilizes an iterative linear quadratic regulator (iLQR) architecture without prior knowledge of the system model. By relying solely on measurement data, the NNiLQR method establishes an optimal policy through iterative refinements of the neural network system, resulting in a non-parametric routine. Importantly, this approach outperforms the classical iLQR method by incorporating further exploration in the methodology. The results of two illustrative examples clearly demonstrate the significant advantages of the NNiLQR method.",1
"Safety is essential for reinforcement learning (RL) applied in the real world. Adding chance constraints (or probabilistic constraints) is a suitable way to enhance RL safety under uncertainty. Existing chance-constrained RL methods like the penalty methods and the Lagrangian methods either exhibit periodic oscillations or learn an over-conservative or unsafe policy. In this paper, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. We first review the constrained policy optimization process from a feedback control perspective, which regards the penalty weight as the control input and the safe probability as the control output. Based on this, the penalty method is formulated as a proportional controller, and the Lagrangian method is formulated as an integral controller. We then unify them and present a proportional-integral Lagrangian method to get both their merits, with an integral separation technique to limit the integral value in a reasonable range. To accelerate training, the gradient of safe probability is computed in a model-based manner. We demonstrate our method can reduce the oscillations and conservatism of RL policy in a car-following simulation. To prove its practicality, we also apply our method to a real-world mobile robot navigation task, where our robot successfully avoids a moving obstacle with highly uncertain or even aggressive behaviors.",0
"In order to apply reinforcement learning (RL) in the real world, safety measures are crucial. One effective approach to increase safety in the face of uncertainty is to use chance constraints. However, current chance-constrained RL methods such as penalty and Lagrangian methods have limitations, such as oscillations or overly cautious policies. In this paper, we propose the separated proportional-integral Lagrangian (SPIL) algorithm to address these issues. We examine the constrained policy optimization process from a feedback control perspective and use this to formulate the penalty and Lagrangian methods as a proportional controller and integral controller, respectively. We then combine them into a proportional-integral Lagrangian method with an integral separation technique to keep the integral value within a reasonable range. To speed up training, we use a model-based approach to compute the safe probability gradient. Our method is demonstrated to reduce oscillations and conservatism in a car-following simulation and is also applied successfully to a real-world mobile robot navigation task with uncertain or aggressive obstacles.",1
"In this paper, we study the problem of regret minimization in reinforcement learning (RL) under differential privacy constraints. This work is motivated by the wide range of RL applications for providing personalized service, where privacy concerns are becoming paramount. In contrast to previous works, we take the first step towards non-tabular RL settings, while providing a rigorous privacy guarantee. In particular, we consider the adaptive control of differentially private linear quadratic (LQ) systems. We develop the first private RL algorithm, PRL, which is able to attain a sub-linear regret while guaranteeing privacy protection. More importantly, the additional cost due to privacy is only on the order of $\frac{\ln(1/\delta)^{1/4}}{\epsilon^{1/2}}$ given privacy parameters $\epsilon, \delta > 0$. Through this process, we also provide a general procedure for adaptive control of LQ systems under changing regularizers, which not only generalizes previous non-private controls, but also serves as the basis for general private controls.",0
"The focus of this research is on minimizing regret in reinforcement learning (RL) while adhering to differential privacy constraints. With privacy concerns becoming increasingly important in RL applications, we aim to explore non-tabular RL settings and offer a robust privacy guarantee. Our study revolves around the control of differentially private linear quadratic (LQ) systems, and we introduce the first private RL algorithm, PRL. Through PRL, we demonstrate that sub-linear regret can be achieved while ensuring privacy protection. Moreover, the incurred privacy cost is only on the order of $\frac{\ln(1/\delta)^{1/4}}{\epsilon^{1/2}}$ with privacy parameters $\epsilon, \delta > 0$. Our research also presents a general process for adaptive control of LQ systems under varying regularizers, which not only expands on prior non-private controls, but also lays the foundation for general private controls.",1
"We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of interest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and it must use its eyes and ears to automatically separate out the sounds originating from a target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent's camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented reality (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simulations in 3D environments, we demonstrate our model's ability to find minimal movement sequences with maximal payoff for audio source separation. Project: http://vision.cs.utexas.edu/projects/move2hear.",0
"The problem of active audio-visual source separation is introduced in this study. The aim is for an agent to move intelligently to isolate sounds coming from a specific object in its surroundings. The agent is exposed to multiple audio sources at once and must use both its eyes and ears to automatically separate the sounds originating from the target object within a limited time frame. To accomplish this, a reinforcement learning approach is used to train movement policies that control the agent's camera and microphone placement over time, with the goal of improving predicted audio separation quality. The study demonstrates the approach in both augmented reality and mobile robotics scenarios, using state-of-the-art realistic audio-visual simulations in 3D environments. The model's ability to find minimal movement sequences with maximal payoff for audio source separation is showcased. The project's website is http://vision.cs.utexas.edu/projects/move2hear.",1
"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",0
"The utilization of deep neural networks enhances the reinforcement learning framework, resulting in what is known as deep reinforcement learning. This approach has shown impressive achievements across various domains, including finance, medicine, healthcare, video games, robotics, and computer vision. This paper provides an in-depth review of recent and state-of-the-art research advances in deep reinforcement learning applied to computer vision. The authors first explain the theories behind deep learning, reinforcement learning, and deep reinforcement learning. They then categorize deep reinforcement learning methodologies based on their applications in computer vision, dividing them into seven main categories: landmark localization, object detection, object tracking, registration on both 2D image and 3D image volumetric data, image segmentation, videos analysis, and other applications. For each category, reinforcement learning techniques, network design, and performance are further analyzed. Additionally, the authors examine the availability of publicly available datasets and source code. Finally, the paper concludes by discussing open issues and future research directions related to deep reinforcement learning in computer vision.",1
"In this work, we present a memory-augmented approach for image-goal navigation. Earlier attempts, including RL-based and SLAM-based approaches have either shown poor generalization performance, or are heavily-reliant on pose/depth sensors. Our method uses an attention-based end-to-end model that leverages an episodic memory to learn to navigate. First, we train a state-embedding network in a self-supervised fashion, and then use it to embed previously-visited states into the agent's memory. Our navigation policy takes advantage of this information through an attention mechanism. We validate our approach with extensive evaluations, and show that our model establishes a new state of the art on the challenging Gibson dataset. Furthermore, we achieve this impressive performance from RGB input alone, without access to additional information such as position or depth, in stark contrast to related work.",0
"In this research, we introduce a technique for image-goal navigation that utilizes memory augmentation. Previous methods, consisting of RL-based and SLAM-based approaches, have displayed weak generalization performance or are excessively dependent on pose/depth sensors. Our approach uses an end-to-end model, based on attention, which encompasses an episodic memory to learn navigation. Initially, we train a state-embedding network in a self-supervised manner, and then use it to embed previously-visited states into the agent's memory. Our navigation policy employs an attention mechanism to utilize this information. We extensively evaluate our approach and demonstrate that our model has set a new standard on the challenging Gibson dataset. Moreover, we achieve this remarkable performance only from RGB input, without access to supplementary information such as position or depth, which is in contrast to related work.",1
"Appropriate credit assignment for delay rewards is a fundamental challenge for reinforcement learning. To tackle this problem, we introduce a delay reward calibration paradigm inspired from a classification perspective. We hypothesize that well-represented state vectors share similarities with each other since they contain the same or equivalent essential information. To this end, we define an empirical sufficient distribution, where the state vectors within the distribution will lead agents to environmental reward signals in the consequent steps. Therefore, a purify-trained classifier is designed to obtain the distribution and generate the calibrated rewards. We examine the correctness of sufficient state extraction by tracking the real-time extraction and building different reward functions in environments. The results demonstrate that the classifier could generate timely and accurate calibrated rewards. Moreover, the rewards are able to make the model training process more efficient. Finally, we identify and discuss that the sufficient states extracted by our model resonate with the observations of humans.",0
"Reinforcement learning faces a significant challenge when it comes to assigning appropriate credit for delay rewards. Our solution to this problem is a delay reward calibration paradigm that is based on a classification perspective. We believe that state vectors that are well-represented share similarities with each other as they contain the same or equivalent essential information. With this in mind, we define an empirical sufficient distribution that contains state vectors that will lead agents to environmental reward signals in the following steps. To obtain this distribution and generate calibrated rewards, we use a purify-trained classifier. We assess the accuracy of sufficient state extraction by tracking real-time extraction and creating various reward functions in environments. The classifier performs well in generating timely and precise calibrated rewards, making the model training process more efficient. Our model's sufficient states also resonate with human observations, which we identify and discuss.",1
"A reinforcement learning environment with adversary agents is proposed in this work for pursuit-evasion game in the presence of fog of war, which is of both scientific significance and practical importance in aerospace applications. One of the most popular learning environments, StarCraft, is adopted here and the associated mini-games are analyzed to identify the current limitation for training adversary agents. The key contribution includes the analysis of the potential performance of an agent by incorporating control and differential game theory into the specific reinforcement learning environment, and the development of an adversary agents challenge (SAAC) environment by extending the current StarCraft mini-games. The subsequent study showcases the use of this learning environment and the effectiveness of an adversary agent for evasion units. Overall, the proposed SAAC environment should benefit pursuit-evasion studies with rapidly-emerging reinforcement learning technologies. Last but not least, the corresponding tutorial code can be found at GitHub.",0
"In this work, a reinforcement learning environment is suggested for a pursuit-evasion game that includes adversary agents and a fog of war. This is significant for both scientific research and practical applications in aerospace. The popular StarCraft learning environment and its mini-games are analyzed to identify limitations in training adversary agents. The main contribution of this work involves incorporating control and differential game theory to evaluate an agent's potential performance in the specific reinforcement learning environment. An adversary agent challenge (SAAC) environment is also developed by extending the current StarCraft mini-games. The study demonstrates the effectiveness of an adversary agent for evasion units in this learning environment. Overall, the proposed SAAC environment is expected to enhance pursuit-evasion research with the use of reinforcement learning technologies. Tutorial code for this work can be found on GitHub.",1
"Encouraging exploration is a critical issue in deep reinforcement learning. We investigate the effect of initial entropy that significantly influences the exploration, especially at the earlier stage. Our main observations are as follows: 1) low initial entropy increases the probability of learning failure, and 2) this initial entropy is biased towards a low value that inhibits exploration. Inspired by the investigations, we devise entropy-aware model initialization, a simple yet powerful learning strategy for effective exploration. We show that the devised learning strategy significantly reduces learning failures and enhances performance, stability, and learning speed through experiments.",0
"Deep reinforcement learning necessitates the promotion of exploration, which is a crucial concern. Our study concentrates on the impact of initial entropy, which has a significant effect on exploration, particularly in the early stages. Our primary observations indicate that a low initial entropy raises the likelihood of learning failure and that this initial entropy is skewed toward a low value that hinders exploration. Drawing inspiration from our findings, we have created an entropy-aware model initialization, a straightforward but potent learning technique that fosters effective exploration. Our experiments demonstrate that this learning approach drastically reduces learning failures and improves performance, consistency, and learning speed.",1
"We propose a general formulation for stochastic treatment recommendation problems in settings with clinical survival data, which we call the Deep Survival Dose Response Function (DeepSDRF). That is, we consider the problem of learning the conditional average dose response (CADR) function solely from historical data in which unobserved factors (confounders) affect both observed treatment and time-to-event outcomes. The estimated treatment effect from DeepSDRF enables us to develop recommender algorithms with explanatory insights. We compared two recommender approaches based on random search and reinforcement learning and found similar performance in terms of patient outcome. We tested the DeepSDRF and the corresponding recommender on extensive simulation studies and two empirical databases: 1) the Clinical Practice Research Datalink (CPRD) and 2) the eICU Research Institute (eRI) database. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing the stochastic treatment effect with observational data in a medical context.",0
"Our proposed solution for stochastic treatment recommendation problems in clinical survival settings is the Deep Survival Dose Response Function (DeepSDRF). This formulation involves learning the conditional average dose response (CADR) function using historical data that is affected by unobserved confounders that influence both the treatment and time-to-event outcomes. By using DeepSDRF, we are able to develop recommender algorithms that provide explanatory insights. We compared two recommender methods, random search and reinforcement learning, and found that they perform similarly in terms of patient outcomes. In order to test the effectiveness of DeepSDRF and the corresponding recommender, we conducted simulations and analyzed two real-world datasets: the Clinical Practice Research Datalink (CPRD) and the eICU Research Institute (eRI) database. This is the first time that confounders have been considered in addressing the stochastic treatment effect in a medical context using observational data.",1
"This paper proposes a cascading failure mitigation strategy based on Reinforcement Learning (RL). The motivation of the Multi-Stage Cascading Failure (MSCF) problem and its connection with the challenge of climate change are introduced. The bottom-level corrective control of the MCSF problem is formulated based on DCOPF (Direct Current Optimal Power Flow). Then, to mitigate the MSCF issue by a high-level RL-based strategy, physics-informed reward, action, and state are devised. Besides, both shallow and deep neural network architectures are tested. Experiments on the IEEE 118-bus system by the proposed mitigation strategy demonstrate a promising performance in reducing system collapses.",0
"A strategy for mitigating cascading failures is proposed in this paper, which is based on Reinforcement Learning (RL). The paper introduces the Multi-Stage Cascading Failure (MSCF) problem and its link to the challenge of climate change. The MCSF problem's bottom-level corrective control is formulated using DCOPF (Direct Current Optimal Power Flow). To address the MSCF issue with a high-level RL-based strategy, the paper devises physics-informed reward, action, and state. Additionally, both shallow and deep neural network architectures are tested. The paper demonstrates promising results in reducing system collapses by implementing the proposed mitigation strategy through experiments conducted on the IEEE 118-bus system.",1
"We present a reinforcement learning (RL) approach for robust optimisation of risk-aware performance criteria. To allow agents to express a wide variety of risk-reward profiles, we assess the value of a policy using rank dependent expected utility (RDEU). RDEU allows the agent to seek gains, while simultaneously protecting themselves against downside events. To robustify optimal policies against model uncertainty, we assess a policy not by its distribution, but rather, by the worst possible distribution that lies within a Wasserstein ball around it. Thus, our problem formulation may be viewed as an actor choosing a policy (the outer problem), and the adversary then acting to worsen the performance of that strategy (the inner problem). We develop explicit policy gradient formulae for the inner and outer problems, and show its efficacy on three prototypical financial problems: robust portfolio allocation, optimising a benchmark, and statistical arbitrage",0
"Our paper proposes a robust optimisation method for risk-aware performance criteria using reinforcement learning (RL). The approach enables agents to express diverse risk-reward profiles by evaluating the value of a policy using rank dependent expected utility (RDEU), which allows agents to pursue gains while safeguarding against potential negative outcomes. To ensure optimal policies remain robust against model uncertainty, we evaluate policies based on the worst possible distribution within a Wasserstein ball around it, instead of its distribution. This means that the actor selects a policy (outer problem), and the adversary acts to worsen the strategy's performance (inner problem). We derive explicit policy gradient formulas for both the inner and outer problems and demonstrate the effectiveness of our approach on three typical financial problems: robust portfolio allocation, benchmark optimisation, and statistical arbitrage.",1
"This position paper proposes a fresh look at Reinforcement Learning (RL) from the perspective of data-efficiency. Data-efficient RL has gone through three major stages: pure on-line RL where every data-point is considered only once, RL with a replay buffer where additional learning is done on a portion of the experience, and finally transition memory based RL, where, conceptually, all transitions are stored and re-used in every update step. While inferring knowledge from all explicitly stored experience has lead to a tremendous gain in data-efficiency, the question of how this data is collected has been vastly understudied. We argue that data-efficiency can only be achieved through careful consideration of both aspects. We propose to make this insight explicit via a paradigm that we call 'Collect and Infer', which explicitly models RL as two separate but interconnected processes, concerned with data collection and knowledge inference respectively. We discuss implications of the paradigm, how its ideas are reflected in the literature, and how it can guide future research into data efficient RL.",0
"In this paper, a fresh approach to Reinforcement Learning (RL) is proposed with a focus on data-efficiency. The evolution of data-efficient RL is examined, including the stages of pure on-line RL, RL with a replay buffer, and transition memory based RL. The use of all explicitly stored experience has led to significant improvements in data-efficiency, but the process of collecting this data has been overlooked. The authors argue that achieving data-efficiency requires careful consideration of both data collection and knowledge inference. They introduce the 'Collect and Infer' paradigm, which views RL as two interconnected processes. The implications of this paradigm are discussed, along with its reflection in existing literature and its potential to guide future research in data-efficient RL.",1
"Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We develop concepts and establish a regret bound that together offer principled guidance. The bound sheds light on questions of what information to seek, how to seek that information, and it what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that demonstrate improvements in data efficiency.",0
"Simulated environments have shown that reinforcement learning agents can achieve impressive feats. However, the transition to real environments is hindered by the challenge of data efficiency. To design agents that can efficiently acquire and represent information, a deeper understanding of these processes is necessary. Our research establishes a regret bound and related concepts that can provide guidance on what information to seek, how to seek it, and what information to retain. We showcase the effectiveness of these concepts by developing simple agents and presenting computational results that demonstrate enhanced data efficiency.",1
"We study efficient algorithms for reinforcement learning in Markov decision processes whose complexity is independent of the number of states. This formulation succinctly captures large scale problems, but is also known to be computationally hard in its general form. Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value function, or by relaxing the solution guarantee to a local optimality condition.   We consider the methodology of boosting, borrowed from supervised learning, for converting weak learners into an accurate policy. The notion of weak learning we study is that of sampled-based approximate optimization of linear functions over policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods, till global optimality is reached. We prove sample complexity and running time bounds on our method, that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular, our bound does not depend on the number of states.   A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of the Frank-Wolfe method, coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee, to overcome the non-convexity and attain global convergence.",0
"Our focus is on developing effective algorithms for reinforcement learning in Markov decision processes, specifically those with a complexity that is not tied to the number of states. Although this formulation is ideal for addressing large-scale problems, it can be challenging to compute in its general form. Previous approaches have attempted to address this by assuming structure in either the transition or value function, or by relaxing the solution guarantee. Our approach involves utilizing the boosting methodology from supervised learning, which converts weak learners into a more accurate policy. We focus on the idea of sampled-based approximate optimization of linear functions over policies as a form of weak learnability. Our algorithm is efficient and capable of improving the accuracy of such weak learning methods until global optimality is achieved. We have established sample complexity and running time bounds that are polynomial in the problem's natural parameters, including the approximation guarantee, discount factor, distribution mismatch, and number of actions. Notably, our bound is independent of the number of states. One technical challenge we faced was the non-convexity of the value function over policy space. However, we overcame this by using a non-convex variant of the Frank-Wolfe method and recent advancements in gradient boosting that enabled us to incorporate a weak learner with multiplicative approximation guarantee, resulting in global convergence.",1
"To overcome the curses of dimensionality and modeling of Dynamic Programming (DP) methods to solve Markov Decision Process (MDP) problems, Reinforcement Learning (RL) methods are adopted in practice. Contrary to traditional RL algorithms which do not consider the structural properties of the optimal policy, we propose a structure-aware learning algorithm to exploit the ordered multi-threshold structure of the optimal policy, if any. We prove the asymptotic convergence of the proposed algorithm to the optimal policy. Due to the reduction in the policy space, the proposed algorithm provides remarkable improvements in storage and computational complexities over classical RL algorithms. Simulation results establish that the proposed algorithm converges faster than other RL algorithms.",0
"Reinforcement Learning (RL) has been implemented as a solution to the challenges posed by Dynamic Programming (DP) techniques in solving Markov Decision Process (MDP) problems caused by the curse of dimensionality. However, traditional RL algorithms tend to overlook the optimal policy's structural features. To address this, we present a structure-conscious learning algorithm that can exploit the optimal policy's ordered multi-threshold structure, if one exists. We establish that the proposed algorithm converges asymptotically to the optimal policy. Moreover, the reduction in policy space results in a significant reduction in storage and computational complexities compared to classical RL algorithms. Simulation results confirm that the proposed algorithm achieves faster convergence than other RL algorithms.",1
"In this paper we propose BlockCopy, a scheme that accelerates pretrained frame-based CNNs to process video more efficiently, compared to standard frame-by-frame processing. To this end, a lightweight policy network determines important regions in an image, and operations are applied on selected regions only, using custom block-sparse convolutions. Features of non-selected regions are simply copied from the preceding frame, reducing the number of computations and latency. The execution policy is trained using reinforcement learning in an online fashion without requiring ground truth annotations. Our universal framework is demonstrated on dense prediction tasks such as pedestrian detection, instance segmentation and semantic segmentation, using both state of the art (Center and Scale Predictor, MGAN, SwiftNet) and standard baseline networks (Mask-RCNN, DeepLabV3+). BlockCopy achieves significant FLOPS savings and inference speedup with minimal impact on accuracy.",0
"BlockCopy is a proposed scheme that enhances pretrained frame-based CNNs to improve video processing efficiency compared to conventional frame-by-frame processing. The approach employs a lightweight policy network to identify significant regions in an image, and custom block-sparse convolutions are applied to selected regions only. Features of non-selected regions are copied from the previous frame, resulting in fewer computations and lower latency. The reinforcement learning-based execution policy is trained online, without the need for ground truth annotations. The proposed framework is applicable to dense prediction tasks such as instance segmentation, semantic segmentation, and pedestrian detection, using both standard baseline networks and state of the art models. BlockCopy provides substantial FLOPS savings and faster inference without compromising accuracy.",1
"Learning socially-aware motion representations is at the core of recent advances in multi-agent problems, such as human motion forecasting and robot navigation in crowds. Despite promising progress, existing representations learned with neural networks still struggle to generalize in closed-loop predictions (e.g., output colliding trajectories). This issue largely arises from the non-i.i.d. nature of sequential prediction in conjunction with ill-distributed training data. Intuitively, if the training data only comes from human behaviors in safe spaces, i.e., from ""positive"" examples, it is difficult for learning algorithms to capture the notion of ""negative"" examples like collisions. In this work, we aim to address this issue by explicitly modeling negative examples through self-supervision: (i) we introduce a social contrastive loss that regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; (ii) we construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Our method substantially reduces the collision rates of recent trajectory forecasting, behavioral cloning and reinforcement learning algorithms, outperforming state-of-the-art methods on several benchmarks. Our code is available at https://github.com/vita-epfl/social-nce.",0
"Recent advances in multi-agent problems, including human motion forecasting and robot navigation in crowds, rely heavily on socially-aware motion representations. However, despite progress, neural networks struggle to generalize in closed-loop predictions due to the non-i.i.d. nature of sequential prediction and ill-distributed training data. This is because if training data only comes from ""positive"" examples, such as human behaviors in safe spaces, the learning algorithms find it challenging to capture the notion of ""negative"" examples, like collisions. In this study, we address this issue by explicitly modeling negative examples through self-supervision. We introduce a social contrastive loss that distinguishes ground-truth positive events from synthetic negative ones and construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Our approach significantly reduces collision rates and outperforms state-of-the-art methods on various benchmarks, including trajectory forecasting, behavioral cloning, and reinforcement learning algorithms. Our code is available at https://github.com/vita-epfl/social-nce.",1
"While reinforcement learning has achieved considerable successes in recent years, state-of-the-art models are often still limited by the size of state and action spaces. Model-free reinforcement learning approaches use some form of state representations and the latest work has explored embedding techniques for actions, both with the aim of achieving better generalization and applicability. However, these approaches consider only states or actions, ignoring the interaction between them when generating embedded representations. In this work, we establish the theoretical foundations for the validity of training a reinforcement learning agent using embedded states and actions. We then propose a new approach for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, we use a model of the environment to obtain embeddings for states and actions and present a generic architecture that leverages these to learn a policy. In this way, the embedded representations obtained via our approach enable better generalization over both states and actions by capturing similarities in the embedding spaces. Evaluations of our approach on several gaming, robotic control, and recommender systems show it significantly outperforms state-of-the-art models in both discrete/continuous domains with large state/action spaces, thus confirming its efficacy.",0
"Although reinforcement learning has made significant progress in recent years, the current leading models are often restricted by the size of state and action spaces. To enhance generalization and applicability, model-free reinforcement learning methods have explored embedding techniques for actions and state representations. However, these methods do not consider the interaction between states and actions when generating embedded representations. This study establishes the theoretical basis for the effectiveness of training a reinforcement learning agent using embedded states and actions. We present a novel approach that combines model-based and model-free reinforcement learning to jointly learn embeddings for states and actions for both continuous and discrete domains. We use a model of the environment to generate embeddings and introduce a generic architecture that leverages these embeddings to learn a policy. Our approach achieves better generalization over both states and actions by capturing similarities in the embedding spaces. We evaluate our approach on various gaming, robotic control, and recommender systems and demonstrate that it significantly outperforms state-of-the-art models in domains with large state/action spaces, confirming its effectiveness.",1
"Policy gradient (PG) methods are popular reinforcement learning (RL) methods where a baseline is often applied to reduce the variance of gradient estimates. In multi-agent RL (MARL), although the PG theorem can be naturally extended, the effectiveness of multi-agent PG (MAPG) methods degrades as the variance of gradient estimates increases rapidly with the number of agents. In this paper, we offer a rigorous analysis of MAPG methods by, firstly, quantifying the contributions of the number of agents and agents' explorations to the variance of MAPG estimators. Based on this analysis, we derive the optimal baseline (OB) that achieves the minimal variance. In comparison to the OB, we measure the excess variance of existing MARL algorithms such as vanilla MAPG and COMA. Considering using deep neural networks, we also propose a surrogate version of OB, which can be seamlessly plugged into any existing PG methods in MARL. On benchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique effectively stabilises training and improves the performance of multi-agent PPO and COMA algorithms by a significant margin.",0
"Reinforcement learning (RL) methods, specifically Policy Gradient (PG) methods, are commonly used and often employ a baseline to decrease gradient estimate variance. In the case of multi-agent RL (MARL), the PG theorem can be extended, but the efficacy of multi-agent PG (MAPG) methods declines as the variance of gradient estimates rises with the number of agents. This study conducts a rigorous analysis of MAPG methods and quantifies the impact of the number of agents and their explorations on the variance of MAPG estimators. The study derives an optimal baseline (OB) that minimizes variance and compares the OB to existing MARL algorithms such as vanilla MAPG and COMA. Additionally, the study proposes a surrogate version of OB, compatible with any existing PG methods in MARL when using deep neural networks. The OB technique stabilizes training and significantly improves the performance of multi-agent PPO and COMA algorithms on benchmarks of Multi-Agent MuJoCo and StarCraft challenges.",1
"This monograph develops a comprehensive statistical learning framework that is robust to (distributional) perturbations in the data using Distributionally Robust Optimization (DRO) under the Wasserstein metric. Beginning with fundamental properties of the Wasserstein metric and the DRO formulation, we explore duality to arrive at tractable formulations and develop finite-sample, as well as asymptotic, performance guarantees. We consider a series of learning problems, including (i) distributionally robust linear regression; (ii) distributionally robust regression with group structure in the predictors; (iii) distributionally robust multi-output regression and multiclass classification, (iv) optimal decision making that combines distributionally robust regression with nearest-neighbor estimation; (v) distributionally robust semi-supervised learning, and (vi) distributionally robust reinforcement learning. A tractable DRO relaxation for each problem is being derived, establishing a connection between robustness and regularization, and obtaining bounds on the prediction and estimation errors of the solution. Beyond theory, we include numerical experiments and case studies using synthetic and real data. The real data experiments are all associated with various health informatics problems, an application area which provided the initial impetus for this work.",0
"This paper presents a statistical learning framework that can handle perturbations in the data distribution. It uses Distributionally Robust Optimization (DRO) under the Wasserstein metric and covers various learning problems such as linear regression, regression with group structure, multi-output regression, multiclass classification, optimal decision making, semi-supervised learning, and reinforcement learning. The paper derives a tractable DRO relaxation for each problem, connects robustness with regularization, and provides bounds on prediction and estimation errors. The framework is tested on synthetic and real data, specifically health informatics problems which inspired the research.",1
"Sample-efficient generalisation of reinforcement learning approaches have always been a challenge, especially, for complex scenes with many components. In this work, we introduce Plug and Play Markov Decision Processes, an object-based representation that allows zero-shot integration of new objects from known object classes. This is achieved by representing the global transition dynamics as a union of local transition functions, each with respect to one active object in the scene. Transition dynamics from an object class can be pre-learnt and thus would be ready to use in a new environment. Each active object is also endowed with its reward function. Since there is no central reward function, addition or removal of objects can be handled efficiently by only updating the reward functions of objects involved. A new transfer learning mechanism is also proposed to adapt reward function in such cases. Experiments show that our representation can achieve sample-efficiency in a variety of set-ups.",0
"Challenges have always existed with regard to achieving sample-efficient generalisation of reinforcement learning approaches, particularly in complex scenes featuring multiple components. The Plug and Play Markov Decision Processes introduced in this study enable zero-shot integration of new objects from known object classes. This is accomplished by representing the global transition dynamics as a combination of local transition functions, each pertaining to an active object in the scene. The transition dynamics from an object class can be pre-learned, making them immediately usable in a new environment. Additionally, each active object has its own reward function, eliminating the need for a central reward function. When objects are added or removed, only the reward functions of the relevant objects need to be updated. A new transfer learning mechanism is also proposed to adapt the reward function in such situations. Experimental results demonstrate that our representation can achieve sample efficiency in a variety of scenarios.",1
"Recent learning-based multi-view stereo (MVS) methods show excellent performance with dense cameras and small depth ranges. However, non-learning based approaches still outperform for scenes with large depth ranges and sparser wide-baseline views, in part due to their PatchMatch optimization over pixelwise estimates of depth, normals, and visibility. In this paper, we propose an end-to-end trainable PatchMatch-based MVS approach that combines advantages of trainable costs and regularizations with pixelwise estimates. To overcome the challenge of the non-differentiable PatchMatch optimization that involves iterative sampling and hard decisions, we use reinforcement learning to minimize expected photometric cost and maximize likelihood of ground truth depth and normals. We incorporate normal estimation by using dilated patch kernels, and propose a recurrent cost regularization that applies beyond frontal plane-sweep algorithms to our pixelwise depth/normal estimates. We evaluate our method on widely used MVS benchmarks, ETH3D and Tanks and Temples (TnT), and compare to other state of the art learning based MVS models. On ETH3D, our method outperforms other recent learning-based approaches and performs comparably on advanced TnT.",0
"Learning-based multi-view stereo (MVS) techniques have been successful in achieving high performance with dense cameras and small depth ranges. However, when dealing with wider depth ranges and sparser wide-baseline views, non-learning based approaches still surpass them. This is because of the PatchMatch optimization that non-learning based approaches use, which is better than pixelwise estimates of depth, normals, and visibility. In this study, we present a PatchMatch-based MVS approach that is end-to-end trainable and combines trainable costs and regularizations with pixelwise estimates. We use reinforcement learning to tackle the challenge of non-differentiable PatchMatch optimization, which involves iterative sampling and hard decisions. We also propose a recurrent cost regularization that goes beyond frontal plane-sweep algorithms to our pixelwise depth/normal estimates. We use dilated patch kernels to include normal estimation. We evaluate our method on widely used MVS benchmarks, ETH3D and Tanks and Temples (TnT), and compare it to other state-of-the-art learning-based MVS models. On ETH3D, our approach outperforms recent learning-based methods and performs comparably on advanced TnT.",1
"Actor-critic methods are widely used in offline reinforcement learning practice, but are not so well-understood theoretically. We propose a new offline actor-critic algorithm that naturally incorporates the pessimism principle, leading to several key advantages compared to the state of the art. The algorithm can operate when the Bellman evaluation operator is closed with respect to the action value function of the actor's policies; this is a more general setting than the low-rank MDP model. Despite the added generality, the procedure is computationally tractable as it involves the solution of a sequence of second-order programs. We prove an upper bound on the suboptimality gap of the policy returned by the procedure that depends on the data coverage of any arbitrary, possibly data dependent comparator policy. The achievable guarantee is complemented with a minimax lower bound that is matching up to logarithmic factors.",0
"While actor-critic methods are commonly used in offline reinforcement learning, their theoretical understanding is limited. Our proposed offline actor-critic algorithm integrates the pessimism principle, providing several advantages over current practices. This algorithm can function when the Bellman evaluation operator is closed with respect to the actor's policies' action value function, which is a more comprehensive setting than the low-rank MDP model. Despite its broader applicability, our approach remains computationally feasible by solving a sequence of second-order programs. We establish an upper bound on the suboptimality gap of the returned policy, dependent on the data coverage of a potentially data-dependent comparator policy. Our guarantee is further supported by a matching minimax lower bound, with logarithmic factors.",1
"Point cloud registration is a fundamental problem in 3D computer vision. In this paper, we cast point cloud registration into a planning problem in reinforcement learning, which can seek the transformation between the source and target point clouds through trial and error. By modeling the point cloud registration process as a Markov decision process (MDP), we develop a latent dynamic model of point clouds, consisting of a transformation network and evaluation network. The transformation network aims to predict the new transformed feature of the point cloud after performing a rigid transformation (i.e., action) on it while the evaluation network aims to predict the alignment precision between the transformed source point cloud and target point cloud as the reward signal. Once the dynamic model of the point cloud is trained, we employ the cross-entropy method (CEM) to iteratively update the planning policy by maximizing the rewards in the point cloud registration process. Thus, the optimal policy, i.e., the transformation between the source and target point clouds, can be obtained via gradually narrowing the search space of the transformation. Experimental results on ModelNet40 and 7Scene benchmark datasets demonstrate that our method can yield good registration performance in an unsupervised manner.",0
"The process of registering point clouds is a crucial aspect of 3D computer vision. This study proposes a novel approach to point cloud registration, framing it as a planning problem in reinforcement learning. Through trial and error, the method seeks to determine the transformation between the source and target point clouds. The point cloud registration process is modeled as a Markov decision process, with a latent dynamic model comprising a transformation network and an evaluation network. The transformation network predicts the new transformed feature of the point cloud after applying a rigid transformation, while the evaluation network predicts the alignment precision between the transformed source and target point clouds as the reward signal. The planning policy is iteratively updated using the cross-entropy method, gradually narrowing the search space of the transformation and leading to the optimal policy. Experimental results demonstrate that this unsupervised approach yields good registration performance on ModelNet40 and 7Scene benchmark datasets.",1
"Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyze due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equations (ODEs) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-couples the critic ODE from the actor ODE. The convergence of the critic to the solution of the Bellman equation and the actor to the optimal policy are proven. In addition, a convergence rate to this global minimum is also established. Our convergence analysis holds under specific choices for the learning rates and exploration rates in the actor-critic algorithm, which could provide guidance for the implementation of actor-critic algorithms in practice.",0
"Reinforcement learning commonly employs actor-critic algorithms, but their analysis is difficult due to the arrival of non-i.i.d. data samples online. These samples' distribution constantly changes as the model updates, creating a complex feedback loop between the data and the reinforcement learning algorithm. However, we have proven that the online actor-critic algorithm, with tabular parametrization, converges to ordinary differential equations (ODEs) with time rescaling as the number of updates increases. Our proof establishes the data samples' geometric ergodicity under a fixed actor policy and uses a Poisson equation to show that the data samples' fluctuations around a dynamic probability measure vanish as the number of updates grows. The convergence properties of the ODE limit are then studied using a two time-scale analysis, which asymptotically separates the critic ODE from the actor ODE. We have proven that the critic converges to the solution of the Bellman equation, and the actor converges to the optimal policy, with a rate of convergence to this global minimum established. Our convergence analysis applies to specific learning rates and exploration rates chosen in the actor-critic algorithm, which could aid in implementing these algorithms in practice.",1
"In modern deep learning research, finding optimal (or near optimal) neural network models is one of major research directions and it is widely studied in many applications. In this paper, the main research trends of neural architecture search (NAS) are classified as neuro-evolutionary algorithms, reinforcement learning based algorithms, and one-shot architecture search approaches. Furthermore, each research trend is introduced and finally all the major three trends are compared. Lastly, the future research directions of NAS research trends are discussed.",0
"One of the primary focuses in contemporary deep learning research is discovering the most effective neural network models, which has been examined extensively in various applications. This particular paper categorizes the primary research directions of neural architecture search (NAS) as neuro-evolutionary algorithms, reinforcement learning-based algorithms, and one-shot architecture search approaches. Additionally, each research direction is presented and ultimately compared to the other two trends. Finally, potential future research directions for NAS are deliberated.",1
"The field of Meta Reinforcement Learning (Meta-RL) has seen substantial advancements recently. In particular, off-policy methods were developed to improve the data efficiency of Meta-RL techniques. \textit{Probabilistic embeddings for actor-critic RL} (PEARL) is currently one of the leading approaches for multi-MDP adaptation problems. A major drawback of many existing Meta-RL methods, including PEARL, is that they do not explicitly consider the safety of the prior policy when it is exposed to a new task for the very first time. This is very important for some real-world applications, including field robots and Autonomous Vehicles (AVs). In this paper, we develop the PEARL PLUS (PEARL$^+$) algorithm, which optimizes the policy for both prior safety and posterior adaptation. Building on top of PEARL, our proposed PEARL$^+$ algorithm introduces a prior regularization term in the reward function and a new Q-network for recovering the state-action value with prior context assumption, to improve the robustness and safety of the trained network exposing to a new task for the first time. The performance of the PEARL$^+$ method is demonstrated by solving three safety-critical decision-making problems related to robots and AVs, including two MuJoCo benchmark problems. From the simulation experiments, we show that the safety of the prior policy is significantly improved compared to that of the original PEARL method.",0
"Recent advances in Meta Reinforcement Learning (Meta-RL) have resulted in the development of off-policy techniques to enhance the data efficiency of Meta-RL methods. A leading approach for multi-MDP adaptation problems is the Probabilistic embeddings for actor-critic RL (PEARL) method. However, existing Meta-RL methods, including PEARL, have a major drawback as they do not consider the safety of the prior policy when exposed to a new task for the first time. This is crucial for real-world applications such as Autonomous Vehicles (AVs) and field robots. To address this issue, we propose the PEARL PLUS (PEARL$^+$) algorithm, which optimizes the policy for both prior safety and posterior adaptation. PEARL$^+$ builds on top of PEARL by introducing a prior regularization term in the reward function and a new Q-network to recover the state-action value with prior context assumption. These modifications improve the robustness and safety of the trained network when exposed to a new task for the first time. We demonstrate the efficacy of the PEARL$^+$ method by solving three safety-critical decision-making problems related to robots and AVs, including two MuJoCo benchmark problems. Simulation experiments show that the safety of the prior policy is significantly enhanced compared to the original PEARL method.",1
"We initiate the study of multi-stage episodic reinforcement learning under adversarial corruptions in both the rewards and the transition probabilities of the underlying system extending recent results for the special case of stochastic bandits. We provide a framework which modifies the aggressive exploration enjoyed by existing reinforcement learning approaches based on ""optimism in the face of uncertainty"", by complementing them with principles from ""action elimination"". Importantly, our framework circumvents the major challenges posed by naively applying action elimination in the RL setting, as formalized by a lower bound we demonstrate. Our framework yields efficient algorithms which (a) attain near-optimal regret in the absence of corruptions and (b) adapt to unknown levels corruption, enjoying regret guarantees which degrade gracefully in the total corruption encountered. To showcase the generality of our approach, we derive results for both tabular settings (where states and actions are finite) as well as linear-function-approximation settings (where the dynamics and rewards admit a linear underlying representation). Notably, our work provides the first sublinear regret guarantee which accommodates any deviation from purely i.i.d. transitions in the bandit-feedback model for episodic reinforcement learning.",0
"We are exploring multi-stage episodic reinforcement learning in the presence of adversarial corruptions in both rewards and transition probabilities. This extends recent results for stochastic bandits. Our framework modifies existing reinforcement learning approaches, which rely on ""optimism in the face of uncertainty,"" with principles from ""action elimination."" Our framework overcomes the challenges posed by naively applying action elimination in the RL setting, as we demonstrate with a lower bound. We present efficient algorithms that achieve near-optimal regret without corruptions and adapt to unknown levels of corruption, with regret guarantees that degrade gracefully as the total corruption increases. We demonstrate the generality of our approach by providing results for both tabular and linear-function-approximation settings. Notably, our work provides the first sublinear regret guarantee that can accommodate any deviation from purely i.i.d. transitions in the bandit-feedback model for episodic reinforcement learning.",1
"In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. It is observed that the most informative region in each frame of a video is usually a small image patch, which shifts smoothly across frames. Therefore, we model the patch localization problem as a sequential decision task, and propose a reinforcement learning based approach for efficient spatially adaptive video recognition (AdaFocus). In specific, a light-weighted ConvNet is first adopted to quickly process the full video sequence, whose features are used by a recurrent policy network to localize the most task-relevant regions. Then the selected patches are inferred by a high-capacity network for the final prediction. During offline inference, once the informative patch sequence has been generated, the bulk of computation can be done in parallel, and is efficient on modern GPU devices. In addition, we demonstrate that the proposed method can be easily extended by further considering the temporal redundancy, e.g., dynamically skipping less valuable frames. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is significantly more efficient than the competitive baselines. Code is available at https://github.com/blackfeather-wang/AdaFocus.",0
"The aim of this study is to enhance computational efficiency in video recognition by examining spatial redundancy. It has been noted that a small image patch, which smoothly moves across frames, is typically the most informative region in each frame of a video. As a result, we propose a reinforcement learning-based approach called AdaFocus to address the patch localization problem as a sequential decision task. Initially, a lightweight ConvNet is utilized to process the complete video sequence quickly. The features of the ConvNet are then used by a recurrent policy network to identify the most relevant regions for the task. Finally, a high-capacity network is used to make the final prediction based on the selected patches. During offline inference, most of the computations can be done in parallel, making it efficient on modern GPU devices. Additionally, we show that our method can be extended by considering temporal redundancy, such as dynamically skipping less valuable frames. Our approach is compared to competitive baselines on five benchmark datasets: ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, and is found to be significantly more efficient. Code is available at https://github.com/blackfeather-wang/AdaFocus.",1
"Explainable reinforcement learning allows artificial agents to explain their behavior in a human-like manner aiming at non-expert end-users. An efficient alternative of creating explanations is to use an introspection-based method that transforms Q-values into probabilities of success used as the base to explain the agent's decision-making process. This approach has been effectively used in episodic and discrete scenarios, however, to compute the probability of success in non-episodic and more complex environments has not been addressed yet. In this work, we adapt the introspection method to be used in a non-episodic task and try it in a continuous Atari game scenario solved with the Rainbow algorithm. Our initial results show that the probability of success can be computed directly from the Q-values for all possible actions.",0
"Artificial agents can now explain their actions to non-expert users using explainable reinforcement learning. One way to generate explanations is through an introspection-based method that converts Q-values into success probabilities, which can be used to clarify the decision-making process of the agent. While this method has been successful in episodic and discrete scenarios, it has not been applied to more complex non-episodic environments. In this study, we modified the introspection approach to work in a continuous Atari game scenario solved using the Rainbow algorithm. Our findings indicate that success probabilities can be easily computed from Q-values for any available action.",1
"A reinforcement learning (RL) policy trained in a nominal environment could fail in a new/perturbed environment due to the existence of dynamic variations. Existing robust methods try to obtain a fixed policy for all envisioned dynamic variation scenarios through robust or adversarial training. These methods could lead to conservative performance due to emphasis on the worst case, and often involve tedious modifications to the training environment. We propose an approach to robustifying a pre-trained non-robust RL policy with $\mathcal{L}_1$ adaptive control. Leveraging the capability of an $\mathcal{L}_1$ control law in the fast estimation of and active compensation for dynamic variations, our approach can significantly improve the robustness of an RL policy trained in a standard (i.e., non-robust) way, either in a simulator or in the real world. Numerical experiments are provided to validate the efficacy of the proposed approach.",0
"If a reinforcement learning (RL) policy is trained in a nominal environment, it may not perform well in a new or perturbed environment where dynamic variations exist. To address this issue, there are existing robust methods that attempt to obtain a fixed policy for all possible dynamic variation scenarios through robust or adversarial training. However, these methods can result in conservative performance as they focus on the worst-case scenario and require tedious modifications to the training environment. Our proposed approach involves using $\mathcal{L}_1$ adaptive control to robustify a pre-trained non-robust RL policy. By leveraging the capabilities of an $\mathcal{L}_1$ control law to quickly estimate and actively compensate for dynamic variations, our approach can significantly enhance the robustness of an RL policy trained in a standard way, whether in a simulator or in real-life situations. Numerical experiments have been conducted to verify the effectiveness of our proposed approach.",1
"Hindsight experience replay (HER) is a goal relabelling technique typically used with off-policy deep reinforcement learning algorithms to solve goal-oriented tasks; it is well suited to robotic manipulation tasks that deliver only sparse rewards. In HER, both trajectories and transitions are sampled uniformly for training. However, not all of the agent's experiences contribute equally to training, and so naive uniform sampling may lead to inefficient learning. In this paper, we propose diversity-based trajectory and goal selection with HER (DTGSH). Firstly, trajectories are sampled according to the diversity of the goal states as modelled by determinantal point processes (DPPs). Secondly, transitions with diverse goal states are selected from the trajectories by using k-DPPs. We evaluate DTGSH on five challenging robotic manipulation tasks in simulated robot environments, where we show that our method can learn more quickly and reach higher performance than other state-of-the-art approaches on all tasks.",0
"The use of hindsight experience replay (HER) is common in off-policy deep reinforcement learning algorithms for goal-oriented tasks, particularly in situations where sparse rewards are the norm, such as robotic manipulation. However, uniform sampling of both trajectories and transitions during training can lead to inefficient learning, as not all experiences contribute equally. To address this, we introduce diversity-based trajectory and goal selection with HER (DTGSH), which involves sampling trajectories based on the diversity of goal states using determinantal point processes (DPPs), and selecting transitions with diverse goal states using k-DPPs. In our study, we apply DTGSH to five challenging robotic manipulation tasks in simulated robot environments and demonstrate that our method achieves faster learning and higher performance compared to other state-of-the-art approaches on all tasks.",1
"The past decade has seen the rapid development of Reinforcement Learning, which acquires impressive performance with numerous training resources. However, one of the greatest challenges in RL is generalization efficiency (i.e., generalization performance in a unit time). This paper proposes a framework of Active Reinforcement Learning (ARL) over MDPs to improve generalization efficiency in a limited resource by instance selection. Given a number of instances, the algorithm chooses out valuable instances as training sets while training the policy, thereby costing fewer resources. Unlike existing approaches, we attempt to actively select and use training data rather than train on all the given data, thereby costing fewer resources. Furthermore, we introduce a general instance evaluation metrics and selection mechanism into the framework. Experiments results reveal that the proposed framework with Proximal Policy Optimization as policy optimizer can effectively improve generalization efficiency than unselect-ed and unbiased selected methods.",0
"Over the past decade, Reinforcement Learning has undergone rapid development and has demonstrated impressive performance with extensive training resources. However, one of the primary obstacles in RL is the challenge of achieving generalization efficiency within a limited time frame. This paper introduces a framework for Active Reinforcement Learning (ARL) in MDPs, which aims to enhance generalization efficiency through instance selection. The algorithm selects valuable instances from a given set during policy training, thus reducing resource costs. Unlike existing methods, our approach actively selects and utilizes training data, resulting in fewer resources being consumed. Additionally, we incorporate a general instance evaluation metric and selection mechanism into the framework. Our experimental results demonstrate that the proposed framework, with Proximal Policy Optimization as the policy optimizer, can significantly enhance generalization efficiency compared to unselected and unbiased selected techniques.",1
"Several real-world scenarios, such as remote control and sensing, are comprised of action and observation delays. The presence of delays degrades the performance of reinforcement learning (RL) algorithms, often to such an extent that algorithms fail to learn anything substantial. This paper formally describes the notion of Markov Decision Processes (MDPs) with stochastic delays and shows that delayed MDPs can be transformed into equivalent standard MDPs (without delays) with significantly simplified cost structure. We employ this equivalence to derive a model-free Delay-Resolved RL framework and show that even a simple RL algorithm built upon this framework achieves near-optimal rewards in environments with stochastic delays in actions and observations. The delay-resolved deep Q-network (DRDQN) algorithm is bench-marked on a variety of environments comprising of multi-step and stochastic delays and results in better performance, both in terms of achieving near-optimal rewards and minimizing the computational overhead thereof, with respect to the currently established algorithms.",0
"The performance of reinforcement learning (RL) algorithms is often hindered by delays in action and observation, which are present in real-world scenarios like remote control and sensing. These delays can cause the failure of RL algorithms to learn effectively. This paper introduces the concept of Markov Decision Processes (MDPs) with stochastic delays and demonstrates that delayed MDPs can be transformed into standard MDPs without delays, resulting in a simplified cost structure. Using this equivalence, a model-free Delay-Resolved RL framework is developed that achieves near-optimal rewards in environments with stochastic delays in actions and observations. The delay-resolved deep Q-network (DRDQN) algorithm is benchmarked on various environments, including those with multi-step and stochastic delays, and performs better than current algorithms in terms of achieving near-optimal rewards and minimizing computational overhead.",1
"Graph matching (GM) has been a building block in many areas including computer vision and pattern recognition. Despite the recent impressive progress, existing deep GM methods often have difficulty in handling outliers in both graphs, which are ubiquitous in practice. We propose a deep reinforcement learning (RL) based approach RGM for weighted graph matching, whose sequential node matching scheme naturally fits with the strategy for selective inlier matching against outliers, and supports seed graph matching. A revocable action scheme is devised to improve the agent's flexibility against the complex constrained matching task. Moreover, we propose a quadratic approximation technique to regularize the affinity matrix, in the presence of outliers. As such, the RL agent can finish inlier matching timely when the objective score stop growing, for which otherwise an additional hyperparameter i.e. the number of common inliers is needed to avoid matching outliers. In this paper, we are focused on learning the back-end solver for the most general form of GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can also boost other solvers using the affinity input. Experimental results on both synthetic and real-world datasets showcase its superior performance regarding both matching accuracy and robustness.",0
"Many areas, such as computer vision and pattern recognition, have benefited from graph matching (GM). However, current deep GM methods struggle to handle outliers in graphs, which are common in practice. To address this challenge, we introduce a deep reinforcement learning (RL) based approach, RGM, for weighted graph matching. Our sequential node matching scheme effectively handles outliers, while also supporting seed graph matching. We improve the agent's flexibility with a revocable action scheme for the complex constrained matching task. Additionally, our quadratic approximation technique regularizes the affinity matrix in the presence of outliers, enabling the RL agent to finish inlier matching in a timely manner. Our focus is on learning the back-end solver for the most general form of GM, the Lawler's QAP, but our approach can also enhance other solvers using the affinity input. Our experimental results on synthetic and real-world datasets demonstrate RGM's superior performance in terms of both matching accuracy and robustness.",1
"Multi-agent reinforcement learning (MARL), despite its popularity and empirical success, suffers from the curse of dimensionality. This paper builds the mathematical framework to approximate cooperative MARL by a mean-field control (MFC) approach, and shows that the approximation error is of $\mathcal{O}(\frac{1}{\sqrt{N}})$. By establishing an appropriate form of the dynamic programming principle for both the value function and the Q function, it proposes a model-free kernel-based Q-learning algorithm (MFC-K-Q), which is shown to have a linear convergence rate for the MFC problem, the first of its kind in the MARL literature. It further establishes that the convergence rate and the sample complexity of MFC-K-Q are independent of the number of agents $N$, which provides an $\mathcal{O}(\frac{1}{\sqrt{N}})$ approximation to the MARL problem with $N$ agents in the learning environment. Empirical studies for the network traffic congestion problem demonstrate that MFC-K-Q outperforms existing MARL algorithms when $N$ is large, for instance when $N>50$.",0
"MARL, despite being popular and successful, is limited by the curse of dimensionality. This paper presents a mean-field control (MFC) approach to approximating cooperative MARL mathematically and proves that the approximation error is of $\mathcal{O}(\frac{1}{\sqrt{N}})$. By proposing a model-free kernel-based Q-learning algorithm (MFC-K-Q) that establishes an appropriate form of the dynamic programming principle for both the value function and the Q function, the paper achieves a linear convergence rate for the MFC problem, which is a first in MARL literature. The convergence rate and sample complexity of MFC-K-Q are independent of the number of agents $N$, providing an $\mathcal{O}(\frac{1}{\sqrt{N}})$ approximation to the MARL problem with $N$ agents in the learning environment. Empirical studies on network traffic congestion show that MFC-K-Q outperforms existing MARL algorithms when $N$ is large, for instance when $N>50$.",1
"Despite the empirical success of the deep Q network (DQN) reinforcement learning algorithm and its variants, DQN is still not well understood and it does not guarantee convergence. In this work, we show that DQN can diverge and cease to operate in realistic settings. Although there exist gradient-based convergent methods, we show that they actually have inherent problems in learning behaviour and elucidate why they often fail in practice. To overcome these problems, we propose a convergent DQN algorithm (C-DQN) by carefully modifying DQN, and we show that the algorithm is convergent and can work with large discount factors (0.9998). It learns robustly in difficult settings and can learn several difficult games in the Atari 2600 benchmark where DQN fail, within a moderate computational budget. Our codes have been publicly released and can be used to reproduce our results.",0
"Despite the success of the deep Q network (DQN) reinforcement learning algorithm and its variations, DQN's operation and convergence are not fully understood. Our study reveals that DQN can fail and become inoperative in practical scenarios. Additionally, we discovered that gradient-based convergent methods have inherent issues with learning behavior and frequently fail in real-world applications. To address these concerns, we propose a convergent DQN algorithm (C-DQN) that modifies DQN and guarantees convergence. Our algorithm can handle large discount factors (0.9998) and performs well in challenging conditions, learning various games in the Atari 2600 benchmark where DQN falls short. Our code is public, allowing for replication of our results.",1
"Reinforcement learning (RL) has been applied to attack graphs for penetration testing, however, trained agents do not reflect reality because the attack graphs lack operational nuances typically captured within the intelligence preparation of the battlefield (IPB) that include notions of (cyber) terrain. In particular, current practice constructs attack graphs exclusively using the Common Vulnerability Scoring System (CVSS) and its components. We present methods for constructing attack graphs using notions from IPB on cyber terrain analysis of obstacles, avenues of approach, key terrain, observation and fields of fire, and cover and concealment. We demonstrate our methods on an example where firewalls are treated as obstacles and represented in (1) the reward space and (2) the state dynamics. We show that terrain analysis can be used to bring realism to attack graphs for RL.",0
"Although reinforcement learning (RL) has been utilized for penetration testing attack graphs, the trained agents do not accurately reflect reality as the attack graphs lack the operational nuances found in intelligence preparation of the battlefield (IPB), including concepts related to cyber terrain. Currently, attack graphs are constructed solely using the Common Vulnerability Scoring System (CVSS) and its components, which falls short of capturing these nuances. To address this issue, we propose constructing attack graphs using IPB's cyber terrain analysis of obstacles, avenues of approach, key terrain, observation and fields of fire, and cover and concealment. We illustrate our approach by treating firewalls as obstacles and incorporating them into both the reward space and state dynamics. Our results demonstrate the potential of terrain analysis in bringing realism to RL-based attack graphs.",1
"Deep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several issues that make them unreliable for use in the real world, including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency to memorise noise (Zhang et al. [292]), being over-confident on incorrect predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of these issues in detail, investigate their causes, and propose computationally cheap algorithms for mitigating them in practice. To do this, we identify structures in deep neural networks that can be exploited to mitigate the above causes of unreliability of deep learning algorithms.",0
"In recent years, the field of deep learning has made significant progress in various domains, such as computer vision, natural language processing, and reinforcement learning. The exceptional performance of these systems has led many to believe they have the potential to revolutionize our lives positively. However, recent studies have highlighted several concerns that make these systems unreliable in real-world situations. These issues include susceptibility to adversarial attacks, tendency to memorize noise, overconfidence in incorrect predictions, and unsuitability for handling private data. This thesis takes a closer look at each of these issues, investigating their causes and proposing low-cost algorithms to mitigate them. Our approach involves identifying structures within deep neural networks that can be exploited to address the aforementioned problems and improve the reliability of deep learning algorithms.",1
"The $Q$-function is a central quantity in many Reinforcement Learning (RL) algorithms for which RL agents behave following a (soft)-greedy policy w.r.t. to $Q$. It is a powerful tool that allows action selection without a model of the environment and even without explicitly modeling the policy. Yet, this scheme can only be used in discrete action tasks, with small numbers of actions, as the softmax cannot be computed exactly otherwise. Especially the usage of function approximation, to deal with continuous action spaces in modern actor-critic architectures, intrinsically prevents the exact computation of a softmax. We propose to alleviate this issue by parametrizing the $Q$-function implicitly, as the sum of a log-policy and of a value function. We use the resulting parametrization to derive a practical off-policy deep RL algorithm, suitable for large action spaces, and that enforces the softmax relation between the policy and the $Q$-value. We provide a theoretical analysis of our algorithm: from an Approximate Dynamic Programming perspective, we show its equivalence to a regularized version of value iteration, accounting for both entropy and Kullback-Leibler regularization, and that enjoys beneficial error propagation results. We then evaluate our algorithm on classic control tasks, where its results compete with state-of-the-art methods.",0
"In Reinforcement Learning (RL), the $Q$-function plays a central role in many algorithms where agents follow a (soft)-greedy policy with respect to $Q$. It allows for action selection without requiring a model of the environment or explicit policy modeling. However, this approach is limited to discrete action tasks with a small number of actions since the softmax cannot be computed accurately otherwise. In modern actor-critic architectures, using function approximation to deal with continuous action spaces prevents the exact computation of a softmax. To address this issue, we propose an implicit parameterization of the $Q$-function as the sum of a log-policy and a value function. This parameterization enables us to derive a practical off-policy deep RL algorithm for large action spaces that enforces the softmax relationship between the policy and the $Q$-value. We provide a theoretical analysis of our algorithm, showing its equivalence to a regularized version of value iteration that accounts for entropy and Kullback-Leibler regularization and enjoys beneficial error propagation results. Finally, we evaluate our algorithm on classic control tasks and demonstrate competitive results compared to state-of-the-art methods.",1
"Actor-critic (AC) algorithms are known for their efficacy and high performance in solving reinforcement learning problems, but they also suffer from low sampling efficiency. An AC based policy optimization process is iterative and needs to frequently access the agent-environment system to evaluate and update the policy by rolling out the policy, collecting rewards and states (i.e. samples), and learning from them. It ultimately requires a huge number of samples to learn an optimal policy. To improve sampling efficiency, we propose a strategy to optimize the training dataset that contains significantly less samples collected from the AC process. The dataset optimization is made of a best episode only operation, a policy parameter-fitness model, and a genetic algorithm module. The optimal policy network trained by the optimized training dataset exhibits superior performance compared to many contemporary AC algorithms in controlling autonomous dynamical systems. Evaluation on standard benchmarks show that the method improves sampling efficiency, ensures faster convergence to optima, and is more data-efficient than its counterparts.",0
"AC algorithms are renowned for their effectiveness and proficiency in addressing reinforcement learning difficulties, but they are also hampered by inadequate sampling efficiency. AC-based policy optimization is a repetitive process that requires frequent access to the agent-environment system to assess and revise the policy by executing the policy, gathering rewards and states (i.e., samples), and learning from them. It ultimately necessitates a large number of samples to learn an optimal policy. To increase sampling efficiency, we suggest a technique for optimizing the training dataset, which includes far fewer samples collected from the AC process. The optimization of the dataset consists of a ""best episode only"" operation, a policy parameter-fitness model, and a genetic algorithm module. The optimal policy network trained using the optimized training dataset outperforms many current AC algorithms in managing self-governing dynamic systems. Standard benchmarks demonstrate that the method enhances sampling efficiency, ensures faster convergence to optima, and is more data-efficient than its counterparts.",1
"While deep reinforcement learning has achieved promising results in challenging decision-making tasks, the main bones of its success --- deep neural networks are mostly black-boxes. A feasible way to gain insight into a black-box model is to distill it into an interpretable model such as a decision tree, which consists of if-then rules and is easy to grasp and be verified. However, the traditional model distillation is usually a supervised learning task under a stationary data distribution assumption, which is violated in reinforcement learning. Therefore, a typical policy distillation that clones model behaviors with even a small error could bring a data distribution shift, resulting in an unsatisfied distilled policy model with low fidelity or low performance. In this paper, we propose to address this issue by changing the distillation objective from behavior cloning to maximizing an advantage evaluation. The novel distillation objective maximizes an approximated cumulative reward and focuses more on disastrous behaviors in critical states, which controls the data shift effect. We evaluate our method on several Gym tasks, a commercial fight game, and a self-driving car simulator. The empirical results show that the proposed method can preserve a higher cumulative reward than behavior cloning and learn a more consistent policy to the original one. Moreover, by examining the extracted rules from the distilled decision trees, we demonstrate that the proposed method delivers reasonable and robust decisions.",0
"Although deep reinforcement learning has shown promise in complex decision-making tasks, the success of this approach relies on deep neural networks that are largely opaque. To gain insight into these black-box models, it is often helpful to distill them into interpretable models such as decision trees, which are easy to understand and verify. However, traditional model distillation is typically a supervised learning task that assumes a stationary data distribution, which is not applicable in reinforcement learning. As a result, cloning model behaviors through policy distillation can lead to a data distribution shift and a poorly performing distilled policy model. To address this issue, we propose a novel approach that changes the distillation objective from behavior cloning to maximizing an advantage evaluation. By focusing on disastrous behaviors in critical states, we can control the data shift effect and preserve a higher cumulative reward than behavior cloning. We tested our method on several challenging tasks, including a commercial fight game and a self-driving car simulator, and found that it consistently delivers reasonable and robust decisions.",1
"The emergence of quantum computing enables for researchers to apply quantum circuit on many existing studies. Utilizing quantum circuit and quantum differential programming, many research are conducted such as \textit{Quantum Machine Learning} (QML). In particular, quantum reinforcement learning is a good field to test the possibility of quantum machine learning, and a lot of research is being done. This work will introduce the concept of quantum reinforcement learning using a variational quantum circuit, and confirm its possibility through implementation and experimentation. We will first present the background knowledge and working principle of quantum reinforcement learning, and then guide the implementation method using the PennyLane library. We will also discuss the power and possibility of quantum reinforcement learning from the experimental results obtained through this work.",0
"The emergence of quantum computing has allowed researchers to apply quantum circuits to numerous existing studies. One area of research that has benefited from the use of quantum circuits and quantum differential programming is Quantum Machine Learning (QML). The field of quantum reinforcement learning in particular has been a promising area for testing the potential of quantum machine learning, resulting in a great deal of research being conducted. This study aims to introduce the concept of quantum reinforcement learning using a variational quantum circuit and confirm its feasibility through implementation and experimentation. The first part of this study will cover the background knowledge and principles of quantum reinforcement learning, followed by a demonstration of the implementation method using the PennyLane library. Finally, the power and potential of quantum reinforcement learning will be discussed based on the experimental results obtained in this work.",1
"We propose a unified framework to study policy evaluation (PE) and the associated temporal difference (TD) methods for reinforcement learning in continuous time and space. We show that PE is equivalent to maintaining the martingale condition of a process. From this perspective, we find that the mean--square TD error approximates the quadratic variation of the martingale and thus is not a suitable objective for PE. We present two methods to use the martingale characterization for designing PE algorithms. The first one minimizes a ""martingale loss function"", whose solution is proved to be the best approximation of the true value function in the mean--square sense. This method interprets the classical gradient Monte-Carlo algorithm. The second method is based on a system of equations called the ""martingale orthogonality conditions"" with ""test functions"". Solving these equations in different ways recovers various classical TD algorithms, such as TD($\lambda$), LSTD, and GTD. Different choices of test functions determine in what sense the resulting solutions approximate the true value function. Moreover, we prove that any convergent time-discretized algorithm converges to its continuous-time counterpart as the mesh size goes to zero. We demonstrate the theoretical results and corresponding algorithms with numerical experiments and applications.",0
"Our proposal is a unified framework that examines policy evaluation (PE) and associated temporal difference (TD) methods for reinforcement learning in continuous time and space. We establish that maintaining the martingale condition of a process is equivalent to PE. Consequently, the mean-square TD error fails to be a suitable objective for PE since it approximates the quadratic variation of the martingale. We provide two approaches for designing PE algorithms using the martingale characterization. The first method employs a ""martingale loss function,"" which minimizes the best approximation of the true value function in the mean-square sense. This method aligns with the classical gradient Monte-Carlo algorithm. The second method involves solving a system of equations called ""martingale orthogonality conditions"" with ""test functions"" to recover various classical TD algorithms, including TD($\lambda$), LSTD, and GTD. The choice of test functions determines how the resulting solutions approximate the true value function. We also prove that any convergent time-discretized algorithm converges to its continuous-time counterpart as the mesh size approaches zero. Finally, we conduct numerical experiments and applications to demonstrate the theoretical results and corresponding algorithms.",1
"Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor which will provide value to the community, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we examine how offline training can be leveraged to minimize data costs (accelerate convergence) and program implementation costs. We present two approaches to doing so: pretraining our model to warm start the experiment with simulated tasks, and using a planning model trained to simulate the real world's rewards to the agent. We present results that demonstrate the utility of offline reinforcement learning to efficient price-setting in the energy demand response problem.",0
"Our team intends to conduct a comprehensive energy demand response experiment in an office building, which is an exciting initiative that will benefit the community. However, training data collection for the reinforcement learning agent is expensive and limited. In this study, we investigate how offline training can be utilized to reduce data costs (speed up convergence) and program implementation costs. We propose two methods: pretraining the model to begin the experiment with simulated tasks, and employing a planning model trained to simulate real-world rewards to the agent. Our findings exhibit the effectiveness of offline reinforcement learning in achieving efficient price-setting in the energy demand response issue.",1
"Microscopic epidemic models are powerful tools for government policy makers to predict and simulate epidemic outbreaks, which can capture the impact of individual behaviors on the macroscopic phenomenon. However, existing models only consider simple rule-based individual behaviors, limiting their applicability. This paper proposes a deep-reinforcement-learning-powered microscopic model named Microscopic Pandemic Simulator (MPS). By replacing rule-based agents with rational agents whose behaviors are driven to maximize rewards, the MPS provides a better approximation of real world dynamics. To efficiently simulate with massive amounts of agents in MPS, we propose Scalable Million-Agent DQN (SMADQN). The MPS allows us to efficiently evaluate the impact of different government strategies. This paper first calibrates the MPS against real-world data in Allegheny, US, then demonstratively evaluates two government strategies: information disclosure and quarantine. The results validate the effectiveness of the proposed method. As a broad impact, this paper provides novel insights for the application of DRL in large scale agent-based networks such as economic and social networks.",0
"Government policy makers can use microscopic epidemic models to forecast and simulate epidemic outbreaks, taking into account individual behaviors' impact on the macroscopic phenomenon. However, the current models only consider simple rule-based behaviors, which limits their usefulness. To address this limitation, this paper introduces a deep-reinforcement-learning-powered microscopic model called Microscopic Pandemic Simulator (MPS). By replacing rule-based agents with rational agents motivated to maximize rewards, the MPS provides a more accurate representation of real-world dynamics. To enable efficient simulation with a large number of agents, this paper proposes Scalable Million-Agent DQN (SMADQN). The MPS enables the assessment of different government strategies' impact, such as information disclosure and quarantine. This paper validates the effectiveness of the proposed method by calibrating it against real-world data from Allegheny, US, and evaluating two government strategies. The results provide novel insights for applying DRL in large-scale agent-based networks, such as economic and social networks.",1
"Reinforcement learning (RL) is well known for requiring large amounts of data in order for RL agents to learn to perform complex tasks. Recent progress in model-based RL allows agents to be much more data-efficient, as it enables them to learn behaviors of visual environments in imagination by leveraging an internal World Model of the environment. Improved sample efficiency can also be achieved by reusing knowledge from previously learned tasks, but transfer learning is still a challenging topic in RL. Parameter-based transfer learning is generally done using an all-or-nothing approach, where the network's parameters are either fully transferred or randomly initialized. In this work we present a simple alternative approach: fractional transfer learning. The idea is to transfer fractions of knowledge, opposed to discarding potentially useful knowledge as is commonly done with random initialization. Using the World Model-based Dreamer algorithm, we identify which type of components this approach is applicable to, and perform experiments in a new multi-source transfer learning setting. The results show that fractional transfer learning often leads to substantially improved performance and faster learning compared to learning from scratch and random initialization.",0
"Reinforcement learning (RL) is notorious for its extensive data requirements for RL agents to tackle intricate tasks. However, recent advancements in model-based RL have made agents more data-efficient by enabling them to train their visual environments' behaviors in their imagination, utilizing an internal World Model. Although reusing knowledge from previously learned tasks can also enhance sample efficiency, transfer learning remains a challenging topic in RL. Typically, parameter-based transfer learning follows an all-or-nothing approach, where the network's parameters are either fully transferred or randomly initialized. In contrast, this study proposes a straightforward alternative approach called fractional transfer learning, which involves transferring fractions of knowledge rather than discarding potentially useful information as is commonly done with random initialization. The World Model-based Dreamer algorithm determines which components are suitable for this approach and conducts experiments in a new multi-source transfer learning setting. The findings suggest that fractional transfer learning often yields significantly improved performance and faster learning compared to learning from scratch and random initialization.",1
"Sarcopenia is a medical condition characterized by a reduction in muscle mass and function. A quantitative diagnosis technique consists of localizing the CT slice passing through the middle of the third lumbar area (L3) and segmenting muscles at this level. In this paper, we propose a deep reinforcement learning method for accurate localization of the L3 CT slice. Our method trains a reinforcement learning agent by incentivizing it to discover the right position. Specifically, a Deep Q-Network is trained to find the best policy to follow for this problem. Visualizing the training process shows that the agent mimics the scrolling of an experienced radiologist. Extensive experiments against other state-of-the-art deep learning based methods for L3 localization prove the superiority of our technique which performs well even with a limited amount of data and annotations.",0
"Sarcopenia is a medical condition where there is a decrease in muscle mass and function. To diagnose this condition, one method is to locate the CT slice passing through the middle of the third lumbar area (L3) and segment the muscles at this level. This paper proposes a deep reinforcement learning technique to accurately locate the L3 CT slice. The method involves training a reinforcement learning agent to find the correct position by using a Deep Q-Network to determine the best policy. The training process shows that the agent imitates the scrolling behavior of an experienced radiologist. The technique outperforms other state-of-the-art deep learning methods for L3 localization, even with limited data and annotations.",1
"The past decade has seen a rapid penetration of electric vehicles (EV) in the market, more and more logistics and transportation companies start to deploy EVs for service provision. In order to model the operations of a commercial EV fleet, we utilize the EV routing problem with time windows (EVRPTW). In this research, we propose an end-to-end deep reinforcement learning framework to solve the EVRPTW. In particular, we develop an attention model incorporating the pointer network and a graph embedding technique to parameterize a stochastic policy for solving the EVRPTW. The model is then trained using policy gradient with rollout baseline. Our numerical studies show that the proposed model is able to efficiently solve EVRPTW instances of large sizes that are not solvable with any existing approaches.",0
"Over the past decade, electric vehicles (EVs) have rapidly gained popularity in the market, leading to an increase in the number of logistics and transportation companies utilizing EVs for their services. To effectively model the operations of a commercial EV fleet, we have implemented the EV routing problem with time windows (EVRPTW). Our research proposes an end-to-end deep reinforcement learning framework to address this problem. Specifically, we have created an attention model that incorporates a pointer network and a graph embedding technique to parameterize a stochastic policy for solving the EVRPTW. The model is trained using policy gradient with rollout baseline. Our numerical studies demonstrate that the proposed model can efficiently handle large-scale EVRPTW cases that were previously unsolvable with existing methods.",1
"The Backprop algorithm for learning in neural networks utilizes two mechanisms: first, stochastic gradient descent and second, initialization with small random weights, where the latter is essential to the effectiveness of the former. We show that in continual learning setups, Backprop performs well initially, but over time its performance degrades. Stochastic gradient descent alone is insufficient to learn continually; the initial randomness enables only initial learning but not continual learning. To the best of our knowledge, ours is the first result showing this degradation in Backprop's ability to learn. To address this issue, we propose an algorithm that continually injects random features alongside gradient descent using a new generate-and-test process. We call this the Continual Backprop algorithm. We show that, unlike Backprop, Continual Backprop is able to continually adapt in both supervised and reinforcement learning problems. We expect that as continual learning becomes more common in future applications, a method like Continual Backprop will be essential where the advantages of random initialization are present throughout learning.",0
"The Backprop algorithm, which employs stochastic gradient descent and initialization with small random weights, is effective in learning in neural networks. However, in continual learning scenarios, Backprop's performance deteriorates over time because stochastic gradient descent alone cannot facilitate continual learning and the initial randomness only supports initial learning and not continuous learning. Our study is the first to show this decline in Backprop's learning capabilities. To tackle this issue, we propose Continual Backprop, an algorithm that employs a novel generate-and-test process to continually inject random features alongside gradient descent. Unlike Backprop, Continual Backprop can continually adapt in both supervised and reinforcement learning problems. Given the increasing prevalence of continual learning, we anticipate that methods like Continual Backprop will become necessary, leveraging the benefits of random initialization throughout the learning process.",1
"Deep neural networks (DNNs) have gained significant popularity in recent years, becoming the state of the art in a variety of domains. In particular, deep reinforcement learning (DRL) has recently been employed to train DNNs that realize control policies for various types of real-world systems. In this work, we present the whiRL 2.0 tool, which implements a new approach for verifying complex properties of interest for DRL systems. To demonstrate the benefits of whiRL 2.0, we apply it to case studies from the communication networks domain that have recently been used to motivate formal verification of DRL systems, and which exhibit characteristics that are conducive for scalable verification. We propose techniques for performing k-induction and semi-automated invariant inference on such systems, and leverage these techniques for proving safety and liveness properties that were previously impossible to verify due to the scalability barriers of prior approaches. Furthermore, we show how our proposed techniques provide insights into the inner workings and the generalizability of DRL systems. whiRL 2.0 is publicly available online.",0
"Recently, deep neural networks (DNNs) have gained immense popularity and are now considered the state of the art in various domains. Among these, deep reinforcement learning (DRL) has been utilized to train DNNs that can control various real-world systems. Our work introduces whiRL 2.0, a tool that presents a new method for verifying complex properties of DRL systems. We demonstrate the effectiveness of whiRL 2.0 by applying it to case studies from communication networks, where formal verification of DRL systems is necessary. These case studies exhibit features that facilitate scalable verification. We propose techniques such as k-induction and semi-automated invariant inference to prove safety and liveness properties that were previously unverifiable due to scalability barriers. Moreover, our proposed techniques provide insights into the inner workings and generalizability of DRL systems. whiRL 2.0 is available online for public use.",1
"Traditional off-policy actor-critic Reinforcement Learning (RL) algorithms learn value functions of a single target policy. However, when value functions are updated to track the learned policy, they forget potentially useful information about old policies. We introduce a class of value functions called Parameter-Based Value Functions (PBVFs) whose inputs include the policy parameters. They can generalize across different policies. PBVFs can evaluate the performance of any policy given a state, a state-action pair, or a distribution over the RL agent's initial states. First we show how PBVFs yield novel off-policy policy gradient theorems. Then we derive off-policy actor-critic algorithms based on PBVFs trained by Monte Carlo or Temporal Difference methods. We show how learned PBVFs can zero-shot learn new policies that outperform any policy seen during training. Finally our algorithms are evaluated on a selection of discrete and continuous control tasks using shallow policies and deep neural networks. Their performance is comparable to state-of-the-art methods.",0
"Conventional off-policy actor-critic Reinforcement Learning (RL) algorithms acquire the value functions of a solitary target policy. However, updating value functions to keep up with the learned policy might cause them to forget beneficial information about previous policies. To address this, we introduce Parameter-Based Value Functions (PBVFs), which incorporate the policy parameters as inputs and can generalize across various policies. PBVFs can evaluate any policy's performance by providing a state, a state-action pair, or an RL agent's initial state distribution. In this paper, we demonstrate how PBVFs generate new off-policy policy gradient theorems and derive off-policy actor-critic algorithms based on Monte Carlo or Temporal Difference methods. Furthermore, we show how PBVFs can learn new policies and outperform any policy seen during training. Finally, we evaluate our algorithms' performance on discrete and continuous control tasks using shallow policies and deep neural networks, and their performance is comparable to state-of-the-art methods.",1
"In this paper, we consider the problem of multi-agent navigation in partially observable grid environments. This problem is challenging for centralized planning approaches as they, typically, rely on the full knowledge of the environment. We suggest utilizing the reinforcement learning approach when the agents, first, learn the policies that map observations to actions and then follow these policies to reach their goals. To tackle the challenge associated with learning cooperative behavior, i.e. in many cases agents need to yield to each other to accomplish a mission, we use a mixing Q-network that complements learning individual policies. In the experimental evaluation, we show that such approach leads to plausible results and scales well to large number of agents.",0
"The focus of this article is on the issue of navigating in partially observable grid environments with multiple agents. This poses a formidable problem for centralized planning methods, which typically require complete knowledge of the environment. We propose using a reinforcement learning approach where the agents first learn policies that map observations to actions, which they then follow to achieve their objectives. To address the difficulty of learning cooperative behavior, such as when agents need to yield to each other to accomplish their mission, we employ a mixing Q-network that supplements the learning of individual policies. Through our experimental evaluation, we demonstrate that this approach produces reasonable outcomes and can be scaled effectively to a large number of agents.",1
"We use functional mirror ascent to propose a general framework (referred to as FMA-PG) for designing policy gradient methods. The functional perspective distinguishes between a policy's functional representation (what are its sufficient statistics) and its parameterization (how are these statistics represented) and naturally results in computationally efficient off-policy updates. For simple policy parameterizations, the FMA-PG framework ensures that the optimal policy is a fixed point of the updates. It also allows us to handle complex policy parameterizations (e.g., neural networks) while guaranteeing policy improvement. Our framework unifies several PG methods and opens the way for designing sample-efficient variants of existing methods. Moreover, it recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) in a principled way. With a softmax functional representation, FMA-PG results in a variant of TRPO with additional desirable properties. It also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on MuJoCo. Via experiments on simple reinforcement learning problems, we evaluate algorithms instantiated by FMA-PG.",0
"Functional mirror ascent is utilized to propose a general framework, referred to as FMA-PG, for creating policy gradient methods. This functional perspective distinguishes the policy's functional representation, which refers to its sufficient statistics, from its parameterization, which pertains to how these statistics are represented. This approach naturally yields computationally efficient off-policy updates. FMA-PG guarantees that the optimal policy is a fixed point of updates for simple policy parameterizations and can handle complex policy parameterizations, such as neural networks, while ensuring policy improvement. Our framework unifies various PG methods and facilitates the creation of sample-efficient variants of existing methods. Additionally, it provides a principled way to recover important implementation heuristics, such as using forward versus reverse KL divergence. When a softmax functional representation is applied, FMA-PG produces a variant of TRPO with additional favorable properties. Furthermore, it suggests an improved version of PPO, which we empirically demonstrate to be both robust and efficient on MuJoCo. By conducting experiments on simple reinforcement learning problems, we assess the algorithms instantiated by FMA-PG.",1
"Machine Learning requires large amounts of labeled data to fit a model. Many datasets are already publicly available, nevertheless forcing application possibilities of machine learning to the domains of those public datasets. The ever-growing penetration of machine learning algorithms in new application areas requires solutions for the need for data in those new domains. This thesis works on active learning as one possible solution to reduce the amount of data that needs to be processed by hand, by processing only those datapoints that specifically benefit the training of a strong model for the task. A newly proposed framework for framing the active learning workflow as a reinforcement learning problem is adapted for image classification and a series of three experiments is conducted. Each experiment is evaluated and potential issues with the approach are outlined. Each following experiment then proposes improvements to the framework and evaluates their impact. After the last experiment, a final conclusion is drawn, unfortunately rejecting this work's hypothesis and outlining that the proposed framework at the moment is not capable of improving active learning for image classification with a trained reinforcement learning agent.",0
"To effectively fit a model in Machine Learning, a significant amount of labeled data is required. While there are numerous publicly available datasets, these limit the scope of machine learning applications to the domains of those datasets. As a result, it is necessary to find solutions to address the need for data in new domains where machine learning algorithms are being increasingly utilized. This thesis explores the use of active learning as a possible solution to reduce the amount of manual data processing by targeting only the datapoints that contribute to training a robust model for a given task. The study adapts a recently proposed framework that frames the active learning workflow as a reinforcement learning problem for image classification. Three experiments are conducted to evaluate the efficacy of the approach, highlight potential issues, and propose improvements to the framework. Unfortunately, the results of the study reject the hypothesis that the proposed framework can enhance active learning for image classification using a trained reinforcement learning agent.",1
"Off-the-shelf convolutional neural network features achieve outstanding results in many image retrieval tasks. However, their invariance to target data is pre-defined by the network architecture and training data. Existing image retrieval approaches require fine-tuning or modification of pre-trained networks to adapt to variations unique to the target data. In contrast, our method enhances the invariance of off-the-shelf features by aggregating features extracted from images augmented at test-time, with augmentations guided by a policy learned through reinforcement learning. The learned policy assigns different magnitudes and weights to the selected transformations, which are selected from a list of image transformations. Policies are evaluated using a metric learning protocol to learn the optimal policy. The model converges quickly and the cost of each policy iteration is minimal as we propose an off-line caching technique to greatly reduce the computational cost of extracting features from augmented images. Experimental results on large trademark retrieval (METU trademark dataset) and landmark retrieval (ROxford5k and RParis6k scene datasets) tasks show that the learned ensemble of transformations is highly effective for improving performance, and is practical, and transferable.",0
"Many image retrieval tasks see remarkable success when utilizing pre-existing convolutional neural network features. However, the network's architecture and training data predefine its invariance to the target data. Current image retrieval approaches require modifying or fine-tuning pre-trained networks to accommodate the distinct variations of target data. Conversely, our approach bolsters the invariance of off-the-shelf features by gathering characteristics from images that have undergone augmentations during test-time. The augmentations are guided by a policy learned through reinforcement learning, which assigns varying magnitudes and weights to the chosen transformations from a list of image transformations. To learn the optimal policy, policies are evaluated through a metric learning protocol. The cost of each policy iteration is minimal as we propose an offline caching technique to significantly reduce the computational cost of extracting features from augmented images. Our experimental results on large trademark retrieval tasks and landmark retrieval tasks demonstrate that the learned ensemble of transformations is highly effective, practical, and transferable for enhancing performance.",1
"We study query and computationally efficient planning algorithms with linear function approximation and a simulator. We assume that the agent only has local access to the simulator, meaning that the agent can only query the simulator at states that have been visited before. This setting is more practical than many prior works on reinforcement learning with a generative model. We propose an algorithm named confident Monte Carlo least square policy iteration (Confident MC-LSPI) for this setting. Under the assumption that the Q-functions of all deterministic policies are linear in known features of the state-action pairs, we show that our algorithm has polynomial query and computational complexities in the dimension of the features, the effective planning horizon and the targeted sub-optimality, while these complexities are independent of the size of the state space. One technical contribution of our work is the introduction of a novel proof technique that makes use of a virtual policy iteration algorithm. We use this method to leverage existing results on $\ell_\infty$-bounded approximate policy iteration to show that our algorithm can learn the optimal policy for the given initial state even only with local access to the simulator. We believe that this technique can be extended to broader settings beyond this work.",0
"Our research focuses on developing efficient planning algorithms that utilize linear function approximation and a simulator. The agent we consider only has access to the simulator locally, which restricts its ability to query states that have not been visited before. This practical setting is more feasible than previous reinforcement learning approaches that utilize a generative model. We introduce a new algorithm, called confident Monte Carlo least square policy iteration (Confident MC-LSPI), specifically designed for this setting. Assuming that the Q-functions of all deterministic policies are linear in known state-action features, we demonstrate that our algorithm has polynomial computational and query complexities in the feature dimension, planning horizon, and targeted sub-optimality. Notably, these complexities are independent of the size of the state space. Our work also presents a novel proof technique that employs a virtual policy iteration algorithm. By leveraging existing results on $\ell_\infty$-bounded approximate policy iteration, we show that our algorithm can learn the optimal policy for the initial state, even with only local access to the simulator. We believe that this proof technique can be extended to other settings beyond the scope of our work.",1
"Despite decades of research and recent progress in adaptive control and reinforcement learning, there remains a fundamental lack of understanding in designing controllers that provide robustness to inherent non-asymptotic uncertainties arising from models estimated with finite, noisy data. We propose a robust adaptive control algorithm that explicitly incorporates such non-asymptotic uncertainties into the control design. The algorithm has three components: (1) a least-squares nominal model estimator; (2) a bootstrap resampling method that quantifies non-asymptotic variance of the nominal model estimate; and (3) a non-conventional robust control design method using an optimal linear quadratic regulator (LQR) with multiplicative noise. A key advantage of the proposed approach is that the system identification and robust control design procedures both use stochastic uncertainty representations, so that the actual inherent statistical estimation uncertainty directly aligns with the uncertainty the robust controller is being designed against. We show through numerical experiments that the proposed robust adaptive controller can significantly outperform the certainty equivalent controller on both expected regret and measures of regret risk.",0
"Despite progress in adaptive control and reinforcement learning, there is still a lack of understanding in designing controllers that can handle the uncertainties arising from finite and noisy data. To solve this issue, we propose a robust adaptive control algorithm that takes non-asymptotic uncertainties into account. The algorithm consists of three components: a nominal model estimator, a bootstrap resampling method to quantify non-asymptotic variance, and a robust control design method using an optimal linear quadratic regulator (LQR) with multiplicative noise. Our approach uses stochastic uncertainty representations in both system identification and robust control design, aligning the inherent statistical estimation uncertainty with the uncertainty the robust controller is designed against. Through numerical experiments, we demonstrate that our robust adaptive controller outperforms the certainty equivalent controller on expected regret and measures of regret risk.",1
"Reasoning about the future -- understanding how decisions in the present time affect outcomes in the future -- is one of the central challenges for reinforcement learning (RL), especially in highly-stochastic or partially observable environments. While predicting the future directly is hard, in this work we introduce a method that allows an agent to ""look into the future"" without explicitly predicting it. Namely, we propose to allow an agent, during its training on past experience, to observe what \emph{actually} happened in the future at that time, while enforcing an information bottleneck to avoid the agent overly relying on this privileged information. This gives our agent the opportunity to utilize rich and useful information about the future trajectory dynamics in addition to the present. Our method, Policy Gradients Incorporating the Future (PGIF), is easy to implement and versatile, being applicable to virtually any policy gradient algorithm. We apply our proposed method to a number of off-the-shelf RL algorithms and show that PGIF is able to achieve higher reward faster in a variety of online and offline RL domains, as well as sparse-reward and partially observable environments.",0
"One of the main challenges in reinforcement learning (RL) is to understand how present decisions affect future outcomes. This is particularly difficult in uncertain or partially observable environments. Although predicting the future directly can be challenging, we present a new method that allows the agent to ""look into the future"" without explicitly predicting it. During training, our proposed method, Policy Gradients Incorporating the Future (PGIF), allows the agent to observe what actually happened in the future at that time, while still enforcing an information bottleneck to avoid over-reliance on this information. This approach provides the agent with valuable information about future trajectory dynamics. Our method is simple to implement and can be applied to any policy gradient algorithm. We tested PGIF on various off-the-shelf RL algorithms and found that it can achieve higher rewards faster in different types of online and offline RL domains, including sparse-reward and partially observable environments.",1
"For the problem of task-agnostic reinforcement learning (RL), an agent first collects samples from an unknown environment without the supervision of reward signals, then is revealed with a reward and is asked to compute a corresponding near-optimal policy. Existing approaches mainly concern the worst-case scenarios, in which no structural information of the reward/transition-dynamics is utilized. Therefore the best sample upper bound is $\propto\widetilde{\mathcal{O}}(1/\epsilon^2)$, where $\epsilon>0$ is the target accuracy of the obtained policy, and can be overly pessimistic. To tackle this issue, we provide an efficient algorithm that utilizes a gap parameter, $\rho>0$, to reduce the amount of exploration. In particular, for an unknown finite-horizon Markov decision process, the algorithm takes only $\widetilde{\mathcal{O}} (1/\epsilon \cdot (H^3SA / \rho + H^4 S^2 A) )$ episodes of exploration, and is able to obtain an $\epsilon$-optimal policy for a post-revealed reward with sub-optimality gap at least $\rho$, where $S$ is the number of states, $A$ is the number of actions, and $H$ is the length of the horizon, obtaining a nearly \emph{quadratic saving} in terms of $\epsilon$. We show that, information-theoretically, this bound is nearly tight for $\rho < \Theta(1/(HS))$ and $H>1$. We further show that $\propto\widetilde{\mathcal{O}}(1)$ sample bound is possible for $H=1$ (i.e., multi-armed bandit) or with a sampling simulator, establishing a stark separation between those settings and the RL setting.",0
"In task-agnostic reinforcement learning, an agent must gather information from an unknown environment without any reward signals, and then use that information to create an optimal policy once a reward is revealed. Current approaches focus on worst-case scenarios where no knowledge of reward or transition-dynamics is utilized, resulting in a sample upper bound that is overly pessimistic. To address this problem, we propose an algorithm that uses a gap parameter to reduce the amount of exploration required. For a finite-horizon Markov decision process, our algorithm requires fewer episodes of exploration compared to existing approaches, and can obtain an optimal policy with a sub-optimality gap of at least $\rho$. This approach can achieve nearly quadratic savings in $\epsilon$, where $\epsilon$ is the target accuracy of the policy, and $S$, $A$, and $H$ are the number of states, number of actions, and length of the horizon, respectively. Our proposed bound is nearly tight for $\rho < \Theta(1/(HS))$ and $H>1$, and we demonstrate that a sample bound of $\propto\widetilde{\mathcal{O}}(1)$ is possible for $H=1$ or with a sampling simulator, which highlights a significant difference between these settings and the RL setting.",1
"Natural policy gradient (NPG) methods with function approximation achieve impressive empirical success in reinforcement learning problems with large state-action spaces. However, theoretical understanding of their convergence behaviors remains limited in the function approximation setting. In this paper, we perform a finite-time analysis of NPG with linear function approximation and softmax parameterization, and prove for the first time that widely used entropy regularization method, which encourages exploration, leads to linear convergence rate. Under considerably weaker regularity conditions, we prove that entropy-regularized Q-NPG variant with linear function approximation achieves $\tilde{O}(1/T)$ convergence rate. We adopt a Lyapunov drift analysis to prove the convergence results and explain the effectiveness of entropy regularization in improving the convergence rates.",0
"Reinforcement learning problems with large state-action spaces have been addressed by Natural Policy Gradient (NPG) methods with function approximation, which have shown impressive empirical success. However, limited theoretical understanding of their convergence behaviors in the function approximation setting remains. To address this gap, we conducted a finite-time analysis of NPG with linear function approximation and softmax parameterization. Our analysis proves that the widely used entropy regularization method, which encourages exploration, leads to a linear convergence rate. We also proved that the entropy-regularized Q-NPG variant with linear function approximation achieves a convergence rate of $\tilde{O}(1/T)$ under considerably weaker regularity conditions. Our convergence results were established using a Lyapunov drift analysis, which also explains the effectiveness of entropy regularization in improving the convergence rates.",1
"Reinforcement learning (RL) is successful at learning to play games where the entire environment is visible. However, RL approaches are challenged in complex games like Starcraft II and in real-world environments where the entire environment is not visible. In these more complex games with more limited visual information, agents must choose where to look and how to optimally use their limited visual information in order to succeed at the game. We verify that with a relatively simple model the agent can learn where to look in scenarios with a limited visual bandwidth. We develop a method for masking part of the environment in Atari games to force the RL agent to learn both where to look and how to play the game in order to study where the RL agent learns to look. In addition, we develop a neural network architecture and method for allowing the agent to choose where to look and what action to take in the Pong game. Further, we analyze the strategies the agent learns to better understand how the RL agent learns to play the game.",0
"While reinforcement learning (RL) has proven successful in games where the entire environment is visible, it struggles in complex games like Starcraft II and real-world scenarios where the environment is not entirely visible. In these situations, agents must make optimal use of their limited visual information and choose where to focus their attention. We conducted experiments using a simple model to demonstrate that agents can learn where to look in scenarios with limited visual bandwidth. To study this further, we developed a technique for masking parts of the environment in Atari games, forcing RL agents to learn where to look and how to play the game simultaneously. Additionally, we created a neural network architecture that enables agents to choose where to focus their attention and what actions to take in the game of Pong. Finally, we analyzed the strategies the agents adopt to gain a better understanding of how RL agents learn to play games.",1
"Emphatic Temporal Difference (TD) methods are a class of off-policy Reinforcement Learning (RL) methods involving the use of followon traces. Despite the theoretical success of emphatic TD methods in addressing the notorious deadly triad (Sutton and Barto, 2018) of off-policy RL, there are still three open problems. First, the motivation for emphatic TD methods proposed by Sutton et al. (2016) does not align with the convergence analysis of Yu (2015). Namely, a quantity used by Sutton et al. (2016) that is expected to be essential for the convergence of emphatic TD methods is not used in the actual convergence analysis of Yu (2015). Second, followon traces typically suffer from large variance, making them hard to use in practice. Third, despite the seminal work of Yu (2015) confirming the asymptotic convergence of some emphatic TD methods for prediction problems, there is still no finite sample analysis for any emphatic TD method for prediction, much less control. In this paper, we address those three open problems simultaneously via using truncated followon traces in emphatic TD methods. Unlike the original followon traces, which depend on all previous history, truncated followon traces depend on only finite history, reducing variance and enabling the finite sample analysis of our proposed emphatic TD methods for both prediction and control.",0
"Emphatic TD methods, which utilize followon traces, are a form of off-policy RL that has shown success in addressing the deadly triad. However, there are three unresolved issues. Firstly, the motivation for the methods proposed by Sutton et al. does not align with the convergence analysis of Yu. Secondly, followon traces often have high variance, making them impractical. Finally, although Yu confirmed the asymptotic convergence of some methods for prediction problems, there is no finite sample analysis for any method for prediction or control. In this paper, we propose using truncated followon traces to address these problems. Unlike original followon traces, truncated followon traces depend on only finite history, reducing variance and enabling finite sample analysis for both prediction and control.",1
"Group fairness definitions such as Demographic Parity and Equal Opportunity make assumptions about the underlying decision-problem that restrict them to classification problems. Prior work has translated these definitions to other machine learning environments, such as unsupervised learning and reinforcement learning, by implementing their closest mathematical equivalent. As a result, there are numerous bespoke interpretations of these definitions. Instead, we provide a generalized set of group fairness definitions that unambiguously extend to all machine learning environments while still retaining their original fairness notions. We derive two fairness principles that enable such a generalized framework. First, our framework measures outcomes in terms of utilities, rather than predictions, and does so for both the decision-algorithm and the individual. Second, our framework considers counterfactual outcomes, rather than just observed outcomes, thus preventing loopholes where fairness criteria are satisfied through self-fulfilling prophecies. We provide concrete examples of how our counterfactual utility fairness framework resolves known fairness issues in classification, clustering, and reinforcement learning problems. We also show that many of the bespoke interpretations of Demographic Parity and Equal Opportunity fit nicely as special cases of our framework.",0
"The definitions of group fairness, such as Demographic Parity and Equal Opportunity, are limited to classification problems due to underlying assumptions about the decision-making process. Previous attempts to apply these definitions to other machine learning settings, such as unsupervised learning and reinforcement learning, have resulted in various interpretations of the concepts. In contrast, we propose a comprehensive set of group fairness definitions that can be applied to all machine learning environments without compromising their original fairness principles. Our framework is based on two key principles: measuring outcomes in terms of utilities and considering counterfactual outcomes to prevent unfair practices. We demonstrate how our counterfactual utility fairness approach resolves fairness concerns in various machine learning scenarios, including classification, clustering, and reinforcement learning. Furthermore, we illustrate how our framework encompasses previous interpretations of Demographic Parity and Equal Opportunity as special cases.",1
"In the past decade, contextual bandit and reinforcement learning algorithms have been successfully used in various interactive learning systems such as online advertising, recommender systems, and dynamic pricing. However, they have yet to be widely adopted in high-stakes application domains, such as healthcare. One reason may be that existing approaches assume that the underlying mechanisms are static in the sense that they do not change over different environments. In many real world systems, however, the mechanisms are subject to shifts across environments which may invalidate the static environment assumption. In this paper, we tackle the problem of environmental shifts under the framework of offline contextual bandits. We view the environmental shift problem through the lens of causality and propose multi-environment contextual bandits that allow for changes in the underlying mechanisms. We adopt the concept of invariance from the causality literature and introduce the notion of policy invariance. We argue that policy invariance is only relevant if unobserved confounders are present and show that, in that case, an optimal invariant policy is guaranteed to generalize across environments under suitable assumptions. Our results may be a first step towards solving the environmental shift problem. They also establish concrete connections among causality, invariance and contextual bandits.",0
"Over the last ten years, contextual bandit and reinforcement learning algorithms have proven effective in interactive learning systems, such as online advertising, recommender systems, and dynamic pricing. However, these algorithms have yet to be widely applied in high-stakes domains, such as healthcare. One possible reason for this is the assumption that the underlying mechanisms are static and do not change across different environments. However, many real-world systems experience shifting mechanisms across different environments, rendering this assumption invalid. This paper addresses the problem of environmental shifts in offline contextual bandits, using causality as a framework. We propose multi-environment contextual bandits that allow for changes in underlying mechanisms, using the concept of invariance and introducing policy invariance. We argue that this is only relevant if unobserved confounders are present and show that an optimal invariant policy is guaranteed to generalize across environments under suitable assumptions. Our findings have implications for solving the environmental shift problem and establish connections among causality, invariance, and contextual bandits.",1
"Neural painting refers to the procedure of producing a series of strokes for a given image and non-photo-realistically recreating it using neural networks. While reinforcement learning (RL) based agents can generate a stroke sequence step by step for this task, it is not easy to train a stable RL agent. On the other hand, stroke optimization methods search for a set of stroke parameters iteratively in a large search space; such low efficiency significantly limits their prevalence and practicality. Different from previous methods, in this paper, we formulate the task as a set prediction problem and propose a novel Transformer-based framework, dubbed Paint Transformer, to predict the parameters of a stroke set with a feed forward network. This way, our model can generate a set of strokes in parallel and obtain the final painting of size 512 * 512 in near real time. More importantly, since there is no dataset available for training the Paint Transformer, we devise a self-training pipeline such that it can be trained without any off-the-shelf dataset while still achieving excellent generalization capability. Experiments demonstrate that our method achieves better painting performance than previous ones with cheaper training and inference costs. Codes and models are available.",0
"The term ""neural painting"" refers to the process of using neural networks to non-photorealistically recreate an image by producing a series of strokes. While reinforcement learning (RL) based agents can generate a stroke sequence for this task, training a stable RL agent is difficult. Stroke optimization methods are inefficient as they search for a set of stroke parameters iteratively in a large search space. In contrast to these methods, this paper presents a novel Transformer-based framework called Paint Transformer, which formulates the task as a set prediction problem and predicts stroke set parameters with a feed forward network. This allows the model to generate a set of strokes in parallel and produce a final painting of size 512 * 512 in near real time. To train the Paint Transformer without an off-the-shelf dataset, a self-training pipeline is devised that achieves excellent generalization capability. Experimental results demonstrate that our method outperforms previous methods and has lower training and inference costs. The codes and models are available.",1
"Data processing and analytics are fundamental and pervasive. Algorithms play a vital role in data processing and analytics where many algorithm designs have incorporated heuristics and general rules from human knowledge and experience to improve their effectiveness. Recently, reinforcement learning, deep reinforcement learning (DRL) in particular, is increasingly explored and exploited in many areas because it can learn better strategies in complicated environments it is interacting with than statically designed algorithms. Motivated by this trend, we provide a comprehensive review of recent works focusing on utilizing deep reinforcement learning to improve data processing and analytics. First, we present an introduction to key concepts, theories, and methods in deep reinforcement learning. Next, we discuss deep reinforcement learning deployment on database systems, facilitating data processing and analytics in various aspects, including data organization, scheduling, tuning, and indexing. Then, we survey the application of deep reinforcement learning in data processing and analytics, ranging from data preparation, natural language interface to healthcare, fintech, etc. Finally, we discuss important open challenges and future research directions of using deep reinforcement learning in data processing and analytics.",0
"The utilization of data processing and analytics is essential and widespread. Algorithms play a crucial role in data processing and analytics by incorporating heuristics and general rules from human knowledge and experience to enhance their effectiveness. Deep reinforcement learning (DRL) has gained popularity in recent times as it can learn better strategies in complicated environments than statically designed algorithms. With this trend in mind, we present a comprehensive review of recent works that use DRL to improve data processing and analytics. Initially, we discuss the key concepts, theories, and methods in DRL, followed by its deployment on database systems to facilitate data processing and analytics in various aspects. We then survey the application of DRL in various domains like healthcare, fintech, natural language interface, etc., for data preparation and analysis. Lastly, we explore the open challenges and future research directions for using DRL in data processing and analytics.",1
"Multi-agent control problems constitute an interesting area of application for deep reinforcement learning models with continuous action spaces. Such real-world applications, however, typically come with critical safety constraints that must not be violated. In order to ensure safety, we enhance the well-known multi-agent deep deterministic policy gradient (MADDPG) framework by adding a safety layer to the deep policy network. In particular, we extend the idea of linearizing the single-step transition dynamics, as was done for single-agent systems in Safe DDPG (Dalal et al., 2018), to multi-agent settings. We additionally propose to circumvent infeasibility problems in the action correction step using soft constraints (Kerrigan & Maciejowski, 2000). Results from the theory of exact penalty functions can be used to guarantee constraint satisfaction of the soft constraints under mild assumptions. We empirically find that the soft formulation achieves a dramatic decrease in constraint violations, making safety available even during the learning procedure.",0
"Deep reinforcement learning models with continuous action spaces are capable of solving multi-agent control problems, which are of great interest in real-world applications. However, these applications often have critical safety constraints that must never be violated. To ensure safety, we have enhanced the well-known MADDPG framework by incorporating a safety layer into the deep policy network. This is achieved by extending the concept of linearizing single-step transition dynamics, which was previously done for single-agent systems in Safe DDPG (Dalal et al., 2018), to multi-agent settings. To avoid infeasibility issues during the action correction step, we propose using soft constraints (Kerrigan & Maciejowski, 2000). Using the theory of exact penalty functions, we can ensure that the soft constraints are satisfied under mild assumptions. Empirical results demonstrate that this soft formulation significantly reduces constraint violations, thereby ensuring safety even during the learning process.",1
"Text-based image retrieval has seen considerable progress in recent years. However, the performance of existing methods suffers in real life since the user is likely to provide an incomplete description of an image, which often leads to results filled with false positives that fit the incomplete description. In this work, we introduce the partial-query problem and extensively analyze its influence on text-based image retrieval. Previous interactive methods tackle the problem by passively receiving users' feedback to supplement the incomplete query iteratively, which is time-consuming and requires heavy user effort. Instead, we propose a novel retrieval framework that conducts the interactive process in an Ask-and-Confirm fashion, where AI actively searches for discriminative details missing in the current query, and users only need to confirm AI's proposal. Specifically, we propose an object-based interaction to make the interactive retrieval more user-friendly and present a reinforcement-learning-based policy to search for discriminative objects. Furthermore, since fully-supervised training is often infeasible due to the difficulty of obtaining human-machine dialog data, we present a weakly-supervised training strategy that needs no human-annotated dialogs other than a text-image dataset. Experiments show that our framework significantly improves the performance of text-based image retrieval. Code is avaiable at https://github.com/CuthbertCai/Ask-Confirm.",0
"Over the past few years, there has been notable progress in text-based image retrieval. However, current methods face issues when used in real-life scenarios because users tend to provide incomplete descriptions of images, resulting in inaccurate search results with false positives. This paper introduces the partial-query problem and analyzes its impact on text-based image retrieval. Traditional interactive methods rely on user feedback to supplement incomplete queries, which is time-consuming and requires significant user effort. Instead, this paper proposes a novel retrieval framework that uses an Ask-and-Confirm approach, where AI actively searches for missing details and users confirm its proposals. Object-based interaction is introduced to make the process more user-friendly, and a reinforcement-learning-based policy is used to search for discriminative objects. Due to the difficulty of obtaining human-machine dialog data, a weakly-supervised training strategy that requires no human-annotated dialogs is presented. The experiments demonstrate that the proposed framework significantly enhances the performance of text-based image retrieval. The code for this framework is available at https://github.com/CuthbertCai/Ask-Confirm.",1
"Imitation Learning algorithms learn a policy from demonstrations of expert behavior. Somewhat counterintuitively, we show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning, which is commonly considered more difficult. We conduct experiments which confirm that our reduction works well in practice for a continuous control task.",0
"Algorithms for Imitation Learning acquire a policy by observing expert actions. Surprisingly, we demonstrate that for experts who act deterministically, the supposedly more challenging Reinforcement Learning method can be simplified to achieve Imitation Learning. Practical tests verify the effectiveness of our simplified approach in tackling a continuous control task.",1
"In this work we present a novel approach to hierarchical reinforcement learning for linearly-solvable Markov decision processes. Our approach assumes that the state space is partitioned, and the subtasks consist in moving between the partitions. We represent value functions on several levels of abstraction, and use the compositionality of subtasks to estimate the optimal values of the states in each partition. The policy is implicitly defined on these optimal value estimates, rather than being decomposed among the subtasks. As a consequence, our approach can learn the globally optimal policy, and does not suffer from the non-stationarity of high-level decisions. If several partitions have equivalent dynamics, the subtasks of those partitions can be shared. If the set of boundary states is smaller than the entire state space, our approach can have significantly smaller sample complexity than that of a flat learner, and we validate this empirically in several experiments.",0
"This paper introduces a fresh perspective on hierarchical reinforcement learning for Markov decision processes that can be solved linearly. Our method involves dividing the state space into partitions and creating subtasks that involve transitioning between them. We establish value functions at different levels of abstraction and utilize the compositionality of subtasks to calculate the optimal values for each partition's states. Rather than dividing the policy among the subtasks, our approach defines the policy implicitly based on these optimal value estimates. Consequently, it can learn the globally optimal policy and avoid the instability of high-level decisions. If some partitions have similar dynamics, their subtasks can be shared. Furthermore, if the set of boundary states is smaller than the entire state space, our method can have significantly lower sample complexity than a flat learner, which we verify through various experiments.",1
"As the field of machine learning for combinatorial optimization advances, traditional problems are resurfaced and readdressed through this new perspective. The overwhelming majority of the literature focuses on small graph problems, while several real-world problems are devoted to large graphs. Here, we focus on two such problems that are related: influence estimation, a \#P-hard counting problem, and influence maximization, an NP-hard problem. We develop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an upper bound of influence estimation and train it on small simulated graphs. Experiments show that GLIE can provide accurate predictions faster than the alternatives for graphs 10 times larger than the train set. More importantly, it can be used on arbitrary large graphs for influence maximization, as the predictions can rank effectively seed sets even when the accuracy deteriorates. To showcase this, we propose a version of a standard Influence Maximization (IM) algorithm where we substitute traditional influence estimation with the predictions of GLIE.We also transfer GLIE into a reinforcement learning model that learns how to choose seeds to maximize influence sequentially using GLIE's hidden representations and predictions. The final results show that the proposed methods surpasses a previous GNN-RL approach and perform on par with a state-of-the-art IM algorithm.",0
"The progression of machine learning for combinatorial optimization has brought about a new outlook on traditional problems. However, most existing literature concentrates on small graph problems, disregarding several real-world problems associated with large graphs. This article focuses on two related problems: influence estimation, a \#P-hard counting problem, and influence maximization, an NP-hard problem. To address this issue, the authors introduce GLIE, a Graph Neural Network (GNN) that provides an inherent parameterization of an upper bound of influence estimation. They train GLIE on small simulated graphs and show that it can offer accurate predictions faster than other options for graphs that are ten times larger than the train set. Even more importantly, GLIE can be applied to arbitrarily large graphs for influence maximization, as the predictions can effectively rank seed sets even when accuracy decreases. In this study, the authors propose a version of a standard Influence Maximization (IM) algorithm, which replaces traditional influence estimation with GLIE's predictions. They also transform GLIE into a reinforcement learning model that uses hidden representations and predictions to learn how to choose seeds to maximize influence sequentially. The final results display that the proposed approaches surpass a previous GNN-RL approach and perform similarly to a state-of-the-art IM algorithm.",1
"Recent technology development brings the booming of numerous new Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand delivery, express systems and warehousing. In DDS, a service loop is an elemental structure, including its service worker, the service providers and corresponding service targets. The service workers should transport either humans or parcels from the providers to the target locations. Various planning tasks within DDS can thus be classified into two individual stages: 1) Dispatching, which is to form service loops from demand/supply distributions, and 2)Routing, which is to decide specific serving orders within the constructed loops. Generating high-quality strategies in both stages is important to develop DDS but faces several challenging. Meanwhile, deep reinforcement learning (DRL) has been developed rapidly in recent years. It is a powerful tool to solve these problems since DRL can learn a parametric model without relying on too many problem-based assumptions and optimize long-term effect by learning sequential decisions. In this survey, we first define DDS, then highlight common applications and important decision/control problems within. For each problem, we comprehensively introduce the existing DRL solutions, and further summarize them in \textit{https://github.com/tsinghua-fib-lab/DDS\_Survey}. We also introduce open simulation environments for development and evaluation of DDS applications. Finally, we analyze remaining challenges and discuss further research opportunities in DRL solutions for DDS.",0
"The development of recent technology has resulted in the emergence of several new Demand-Driven Services (DDS) in urban areas, such as ridesharing, on-demand delivery, express systems, and warehousing. DDS comprises a service loop, consisting of a service worker, service providers, and corresponding service targets. The service worker is responsible for transporting either humans or parcels from the providers to the target locations. Planning tasks in DDS can be divided into two stages: Dispatching and Routing. Dispatching involves forming service loops from demand/supply distributions while Routing involves deciding on specific serving orders within the constructed loops. Developing high-quality strategies in both stages is crucial for DDS but poses several challenges. However, deep reinforcement learning (DRL) has emerged as a powerful tool in recent years to address these challenges. DRL can learn a parametric model without relying on problem-based assumptions and optimize long-term effects by learning sequential decisions. This survey defines DDS, highlights common applications and important decision/control problems, and comprehensively introduces existing DRL solutions for each problem. The survey also introduces open simulation environments for the development and evaluation of DDS applications. Finally, it analyzes remaining challenges and discusses further research opportunities in DRL solutions for DDS.",1
"Safety is a critical concern for the next generation of autonomy that is likely to rely heavily on deep neural networks for perception and control. Formally verifying the safety and robustness of well-trained DNNs and learning-enabled systems under attacks, model uncertainties, and sensing errors is essential for safe autonomy. This research proposes a framework to repair unsafe DNNs in safety-critical systems with reachability analysis. The repair process is inspired by adversarial training which has demonstrated high effectiveness in improving the safety and robustness of DNNs. Different from traditional adversarial training approaches where adversarial examples are utilized from random attacks and may not be representative of all unsafe behaviors, our repair process uses reachability analysis to compute the exact unsafe regions and identify sufficiently representative examples to enhance the efficacy and efficiency of the adversarial training.   The performance of our framework is evaluated on two types of benchmarks without safe models as references. One is a DNN controller for aircraft collision avoidance with access to training data. The other is a rocket lander where our framework can be seamlessly integrated with the well-known deep deterministic policy gradient (DDPG) reinforcement learning algorithm. The experimental results show that our framework can successfully repair all instances on multiple safety specifications with negligible performance degradation. In addition, to increase the computational and memory efficiency of the reachability analysis algorithm, we propose a depth-first-search algorithm that combines an existing exact analysis method with an over-approximation approach based on a new set representation. Experimental results show that our method achieves a five-fold improvement in runtime and a two-fold improvement in memory usage compared to exact analysis.",0
"Ensuring safety is of utmost importance for the upcoming generation of autonomy which will heavily depend on deep neural networks for perception and control. It is crucial to formally verify the safety and robustness of well-trained DNNs and learning-enabled systems against attacks, uncertainties in models, and sensing errors to ensure safe autonomy. This study proposes a framework that utilizes reachability analysis to repair unsafe DNNs in safety-critical systems. The repair process is inspired by adversarial training and uses reachability analysis to identify representative examples to enhance the efficacy and efficiency of adversarial training. Unlike traditional adversarial training approaches, our process computes exact unsafe regions. Our framework's performance is evaluated on two types of benchmarks without safe models as references. The experimental results show that our framework can successfully repair all instances on multiple safety specifications with negligible performance degradation. Additionally, we propose a depth-first-search algorithm to increase the computational and memory efficiency of the reachability analysis algorithm, achieving a five-fold improvement in runtime and a two-fold improvement in memory usage compared to exact analysis.",1
"We present a study of the manners by which Domain information has been incorporated when building models with Neural Networks. Integrating space data is uniquely important to the development of Knowledge understanding model, as well as other fields that aid in understanding information by utilizing the human-machine interface and Reinforcement Learning. On numerous such occasions, machine-based model development may profit essentially from the human information on the world encoded in an adequately exact structure. This paper inspects expansive ways to affect encode such information as sensible and mathematical limitations and portrays methods and results that came to a couple of subcategories under all of those methodologies.",0
"In this paper, we investigate the incorporation of Domain information into Neural Network models. The integration of spatial data is crucial for the development of Knowledge understanding models and other fields that utilize the human-machine interface and Reinforcement Learning to comprehend information. In many cases, the development of machine-based models can greatly benefit from human knowledge encoded in an accurate structure. The study explores different methods of encoding such information as mathematical and logical constraints and categorizes the approaches based on their results.",1
"With the continuous improvement of the performance of object detectors via advanced model architectures, imbalance problems in the training process have received more attention. It is a common paradigm in object detection frameworks to perform multi-scale detection. However, each scale is treated equally during training. In this paper, we carefully study the objective imbalance of multi-scale detector training. We argue that the loss in each scale level is neither equally important nor independent. Different from the existing solutions of setting multi-task weights, we dynamically optimize the loss weight of each scale level in the training process. Specifically, we propose an Adaptive Variance Weighting (AVW) to balance multi-scale loss according to the statistical variance. Then we develop a novel Reinforcement Learning Optimization (RLO) to decide the weighting scheme probabilistically during training. The proposed dynamic methods make better utilization of multi-scale training loss without extra computational complexity and learnable parameters for backpropagation. Experiments show that our approaches can consistently boost the performance over various baseline detectors on Pascal VOC and MS COCO benchmark.",0
"Attention has been drawn to imbalance problems during the training process as advanced model architectures have led to improved object detector performance. While multi-scale detection is a common practice in object detection frameworks, each scale is treated equally during training, which we argue is incorrect. In this study, we examine the objective imbalance in multi-scale detector training and suggest that the loss in each scale level is neither equally important nor independent. Instead of using existing solutions of setting multi-task weights, we propose the Adaptive Variance Weighting (AVW) technique to balance multi-scale loss based on statistical variance. In addition, we introduce Reinforcement Learning Optimization (RLO) to probabilistically determine the weighting scheme during training. Our dynamic methods enable better utilization of multi-scale training loss without increasing computational complexity or adding learnable parameters for backpropagation. Experiments conducted on Pascal VOC and MS COCO benchmark show consistent performance boosts compared to various baseline detectors.",1
"Tuning the hyperparameters in the differentially private stochastic gradient descent (DPSGD) is a fundamental challenge. Unlike the typical SGD, private datasets cannot be used many times for hyperparameter search in DPSGD; e.g., via a grid search. Therefore, there is an essential need for algorithms that, within a given search space, can find near-optimal hyperparameters for the best achievable privacy-utility tradeoffs efficiently. We formulate this problem into a general optimization framework for establishing a desirable privacy-utility tradeoff, and systematically study three cost-effective algorithms for being used in the proposed framework: evolutionary, Bayesian, and reinforcement learning. Our experiments, for hyperparameter tuning in DPSGD conducted on MNIST and CIFAR-10 datasets, show that these three algorithms significantly outperform the widely used grid search baseline. As this paper offers a first-of-a-kind framework for hyperparameter tuning in DPSGD, we discuss existing challenges and open directions for future studies. As we believe our work has implications to be utilized in the pipeline of private deep learning, we open-source our code at https://github.com/AmanPriyanshu/DP-HyperparamTuning.",0
"The task of tuning hyperparameters in differentially private stochastic gradient descent (DPSGD) poses a significant challenge. In contrast to the standard SGD, DPSGD cannot use private datasets multiple times for hyperparameter exploration, such as with grid searches. Therefore, efficient algorithms are necessary to identify close-to-optimal hyperparameters within a defined search space that balance privacy and utility. To achieve this, we have created a general optimization framework that systematically examines three cost-effective algorithms: evolutionary, Bayesian, and reinforcement learning. Our tests on MNIST and CIFAR-10 datasets show that these three algorithms surpass the traditional grid search approach. As the first framework of its kind for hyperparameter tuning in DPSGD, we also outline potential obstacles and future research directions. We are confident that our work will be useful in the private deep learning pipeline and have made our code available for open-source use at https://github.com/AmanPriyanshu/DP-HyperparamTuning.",1
"The advances in deep neural networks (DNN) have significantly enhanced real-time detection of anomalous data in IoT applications. However, the complexity-accuracy-delay dilemma persists: complex DNN models offer higher accuracy, but typical IoT devices can barely afford the computation load, and the remedy of offloading the load to the cloud incurs long delay. In this paper, we address this challenge by proposing an adaptive anomaly detection scheme with hierarchical edge computing (HEC). Specifically, we first construct multiple anomaly detection DNN models with increasing complexity, and associate each of them to a corresponding HEC layer. Then, we design an adaptive model selection scheme that is formulated as a contextual-bandit problem and solved by using a reinforcement learning policy network. We also incorporate a parallelism policy training method to accelerate the training process by taking advantage of distributed models. We build an HEC testbed using real IoT devices, implement and evaluate our contextual-bandit approach with both univariate and multivariate IoT datasets. In comparison with both baseline and state-of-the-art schemes, our adaptive approach strikes the best accuracy-delay tradeoff on the univariate dataset, and achieves the best accuracy and F1-score on the multivariate dataset with only negligibly longer delay than the best (but inflexible) scheme.",0
"The development of deep neural networks (DNN) has considerably improved the real-time detection of anomalous data in IoT applications. However, the challenge of balancing complexity, accuracy, and delay persists, as complex DNN models offer high accuracy but put a strain on IoT devices, and the solution of transferring the load to the cloud results in long delays. This paper proposes an adaptive anomaly detection system with hierarchical edge computing (HEC) to address this issue. The system involves constructing multiple anomaly detection DNN models with different complexities and linking each model to a corresponding HEC layer. An adaptive model selection scheme is then developed, which is formulated as a contextual-bandit problem and solved using a reinforcement learning policy network. Additionally, a parallelism policy training method is incorporated to speed up the training process by utilizing distributed models. A real IoT device HEC testbed is used to evaluate the approach with both univariate and multivariate IoT datasets. The results show that the adaptive approach strikes the best accuracy-delay balance on the univariate dataset and achieves the best accuracy and F1-score on the multivariate dataset with only slightly longer delay than the best (but inflexible) scheme.",1
"We consider a class of restless bandit problems that finds a broad application area in stochastic optimization, reinforcement learning and operations research. In our model, there are $N$ independent $2$-state Markov processes that may be observed and accessed for accruing rewards. The observation is error-prone, i.e., both false alarm and miss detection may happen. Furthermore, the user can only choose a subset of $M~(M<N)$ processes to observe at each discrete time. If a process in state~$1$ is correctly observed, then it will offer some reward. Due to the partial and imperfect observation model, the system is formulated as a restless multi-armed bandit problem with an information state space of uncountable cardinality. Restless bandit problems with finite state spaces are PSPACE-HARD in general. In this paper, we establish a low-complexity algorithm that achieves a strong performance for this class of restless bandits. Under certain conditions, we theoretically prove the existence (indexability) of Whittle index and its equivalence to our algorithm. When those conditions do not hold, we show by numerical experiments the near-optimal performance of our algorithm in general.",0
"We examine a type of restless bandit problems that have numerous applications in stochastic optimization, operations research, and reinforcement learning. Our model comprises $N$ independent Markov processes with two states that can be observed to earn rewards. However, the observation is prone to error, which means that both false alarms and miss detections may occur. Moreover, at each discrete time, the user can only choose a subset of $M$ processes to observe out of the $N$ available ones, where $M<N$. If the user observes a process in state 1 correctly, it earns a reward. Due to the limited and imperfect observation model, the system is formulated as a restless multi-armed bandit problem with an information state space of uncountable cardinality. Restless bandit problems with finite state spaces are generally PSPACE-HARD. In this paper, we present a low-complexity algorithm that performs well for this type of restless bandit problems. We mathematically prove the existence (indexability) of Whittle index and its equivalence to our algorithm under certain conditions. When these conditions are not met, we demonstrate through numerical experiments that our algorithm delivers nearly optimal performance in general.",1
"Recent studies in multi-agent communicative reinforcement learning (MACRL) demonstrate that multi-agent coordination can be significantly improved when communication between agents is allowed. Meanwhile, advances in adversarial machine learning (ML) have shown that ML and reinforcement learning (RL) models are vulnerable to a variety of attacks that significantly degrade the performance of learned behaviours. However, despite the obvious and growing importance, the combination of adversarial ML and MACRL remains largely uninvestigated. In this paper, we make the first step towards conducting message attacks on MACRL methods. In our formulation, one agent in the cooperating group is taken over by an adversary and can send malicious messages to disrupt a deployed MACRL-based coordinated strategy during the deployment phase. We further our study by developing a defence method via message reconstruction. Finally, we address the resulting arms race, i.e., we consider the ability of the malicious agent to adapt to the changing and improving defensive communicative policies of the benign agents. Specifically, we model the adversarial MACRL problem as a two-player zero-sum game and then utilize Policy-Space Response Oracle to achieve communication robustness. Empirically, we demonstrate that MACRL methods are vulnerable to message attacks while our defence method the game-theoretic framework can effectively improve the robustness of MACRL.",0
"New research has shown that multi-agent coordination can be improved through communication in multi-agent communicative reinforcement learning (MACRL). However, advancements in adversarial machine learning (ML) have revealed that ML and reinforcement learning (RL) models are susceptible to attacks that can significantly impede their performance. Despite the growing importance of MACRL and adversarial ML, their combination remains largely unexplored. In this study, we take the first step towards message attacks on MACRL methods by formulating a scenario where an adversary takes over one agent in a cooperating group and sends malicious messages to disrupt a deployed MACRL-based coordinated strategy. To address this issue, we develop a defence method through message reconstruction and consider the arms race between malicious and benign agents. We model the adversarial MACRL problem as a two-player zero-sum game and use Policy-Space Response Oracle to achieve communication robustness. Our empirical results demonstrate that MACRL methods can be vulnerable to message attacks, but our game-theoretic framework can effectively improve their robustness.",1
"Recent state-of-the-art artificial agents lack the ability to adapt rapidly to new tasks, as they are trained exclusively for specific objectives and require massive amounts of interaction to learn new skills. Meta-reinforcement learning (meta-RL) addresses this challenge by leveraging knowledge learned from training tasks to perform well in previously unseen tasks. However, current meta-RL approaches limit themselves to narrow parametric task distributions, ignoring qualitative differences between tasks that occur in the real world. In this paper, we introduce TIGR, a Task-Inference-based meta-RL algorithm using Gaussian mixture models (GMM) and gated Recurrent units, designed for tasks in non-parametric environments. We employ a generative model involving a GMM to capture the multi-modality of the tasks. We decouple the policy training from the task-inference learning and efficiently train the inference mechanism on the basis of an unsupervised reconstruction objective. We provide a benchmark with qualitatively distinct tasks based on the half-cheetah environment and demonstrate the superior performance of TIGR compared to state-of-the-art meta-RL approaches in terms of sample efficiency (3-10 times faster), asymptotic performance, and applicability in non-parametric environments with zero-shot adaptation.",0
"Cutting-edge artificial agents currently lack the ability to quickly adapt to new tasks, as they are trained for specific objectives and require extensive interaction to learn new skills. Meta-reinforcement learning (meta-RL) aims to solve this issue by utilizing knowledge from training tasks to perform well in new ones. However, existing meta-RL techniques are limited to narrow task distributions and fail to account for qualitative differences between real-world tasks. To address this, we propose TIGR, a Task-Inference-based meta-RL algorithm that uses Gaussian mixture models and gated Recurrent units to handle tasks in non-parametric environments. We use a generative model with a GMM to capture task multi-modality and decouple policy training from task-inference learning, training the inference mechanism using an unsupervised reconstruction objective. We provide a benchmark with distinct tasks based on the half-cheetah environment and demonstrate TIGR's superior performance in terms of sample efficiency (3-10 times faster), asymptotic performance, and applicability in non-parametric environments with zero-shot adaptation.",1
"Combinatorial optimization problems (COPs) on the graph with real-life applications are canonical challenges in Computer Science. The difficulty of finding quality labels for problem instances holds back leveraging supervised learning across combinatorial problems. Reinforcement learning (RL) algorithms have recently been adopted to solve this challenge automatically. The underlying principle of this approach is to deploy a graph neural network (GNN) for encoding both the local information of the nodes and the graph-structured data in order to capture the current state of the environment. Then, it is followed by the actor to learn the problem-specific heuristics on its own and make an informed decision at each state for finally reaching a good solution. Recent studies on this subject mainly focus on a family of combinatorial problems on the graph, such as the travel salesman problem, where the proposed model aims to find an ordering of vertices that optimizes a given objective function. We use the security-aware phone clone allocation in the cloud as a classical quadratic assignment problem (QAP) to investigate whether or not deep RL-based model is generally applicable to solve other classes of such hard problems. Extensive empirical evaluation shows that existing RL-based model may not generalize to QAP.",0
"Canonical challenges in Computer Science involve solving Combinatorial optimization problems (COPs) on graphs with real-life applications. However, the task of finding quality labels for problem instances hinders the application of supervised learning across combinatorial problems. To address this, Reinforcement learning (RL) algorithms have been recently adopted to automatically solve this challenge. This involves deploying a graph neural network (GNN) to encode local node information and graph-structured data to capture the current state of the environment. Then, the actor learns problem-specific heuristics to make informed decisions for reaching a good solution. Studies have focused on a family of combinatorial problems, such as the travel salesman problem. We investigate if deep RL-based models can solve other classes of hard problems, using the security-aware phone clone allocation in the cloud as a classical quadratic assignment problem (QAP). However, empirical evaluation shows that existing RL-based models may not generalize to QAP.",1
"The recent emergence of reinforcement learning has created a demand for robust statistical inference methods for the parameter estimates computed using these algorithms. Existing methods for statistical inference in online learning are restricted to settings involving independently sampled observations, while existing statistical inference methods in reinforcement learning (RL) are limited to the batch setting. The online bootstrap is a flexible and efficient approach for statistical inference in linear stochastic approximation algorithms, but its efficacy in settings involving Markov noise, such as RL, has yet to be explored. In this paper, we study the use of the online bootstrap method for statistical inference in RL. In particular, we focus on the temporal difference (TD) learning and Gradient TD (GTD) learning algorithms, which are themselves special instances of linear stochastic approximation under Markov noise. The method is shown to be distributionally consistent for statistical inference in policy evaluation, and numerical experiments are included to demonstrate the effectiveness of this algorithm at statistical inference tasks across a range of real RL environments.",0
"The emergence of reinforcement learning has created a need for reliable statistical inference methods for parameter estimates obtained through these algorithms. Current statistical inference methods for online learning are limited to independently sampled observations, while those for reinforcement learning are only applicable in a batch setting. Although the online bootstrap approach is efficient in statistical inference for linear stochastic approximation algorithms, its effectiveness in Markov noise settings like reinforcement learning has not been explored. This study investigates the use of the online bootstrap method in reinforcement learning, with a focus on TD and GTD learning algorithms. The method proves to be distributionally consistent in policy evaluation, and numerical experiments illustrate its effectiveness across various real RL environments.",1
"The ability to plan into the future while utilizing only raw high-dimensional observations, such as images, can provide autonomous agents with broad capabilities. Visual model-based reinforcement learning (RL) methods that plan future actions directly have shown impressive results on tasks that require only short-horizon reasoning, however, these methods struggle on temporally extended tasks. We argue that it is easier to solve long-horizon tasks by planning sequences of states rather than just actions, as the effects of actions greatly compound over time and are harder to optimize. To achieve this, we draw on the idea of collocation, which has shown good results on long-horizon tasks in optimal control literature, and adapt it to the image-based setting by utilizing learned latent state space models. The resulting latent collocation method (LatCo) optimizes trajectories of latent states, which improves over previously proposed shooting methods for visual model-based RL on tasks with sparse rewards and long-term goals. Videos and code at https://orybkin.github.io/latco/.",0
"Autonomous agents can have extensive abilities by planning for the future with raw high-dimensional observations. Although visual model-based reinforcement learning (RL) methods have shown impressive results for tasks that require short-horizon reasoning, they struggle with temporally extended tasks. We suggest that planning sequences of states instead of actions is a more feasible solution for long-horizon tasks, as the effects of actions are harder to optimize and compound over time. We adapt collocation, a concept from optimal control literature that has worked well on long-horizon tasks, to the image-based setting by using learned latent state space models. The resulting Latent Collocation Method (LatCo) optimizes trajectories of latent states and surpasses previously proposed shooting methods for visual model-based RL on tasks with sparse rewards and long-term goals. Visit https://orybkin.github.io/latco/ for videos and code.",1
"Effectively operating electrical vehicle charging station (EVCS) is crucial for enabling the rapid transition of electrified transportation. To solve this problem using reinforcement learning (RL), the dimension of state/action spaces scales with the number of EVs and is thus very large and time-varying. This dimensionality issue affects the efficiency and convergence properties of generic RL algorithms. We develop aggregation schemes that are based on the emergency of EV charging, namely the laxity value. A least-laxity first (LLF) rule is adopted to consider only the total charging power of the EVCS which ensures the feasibility of individual EV schedules. In addition, we propose an equivalent state aggregation that can guarantee to attain the same optimal policy. Based on the proposed representation, policy gradient method is used to find the best parameters for the linear Gaussian policy . Numerical results have validated the performance improvement of the proposed representation approaches in attaining higher rewards and more effective policies as compared to existing approximation based approach.",0
"The effective operation of electric vehicle charging stations (EVCS) is essential in facilitating the rapid transition to electric transportation. However, applying reinforcement learning (RL) to solve this issue presents challenges due to the large and constantly changing state/action spaces that scale with the number of EVs. This challenge negatively impacts the efficiency and convergence properties of generic RL algorithms. To address this issue, we propose aggregation schemes based on the urgency of EV charging (i.e., laxity value), utilizing a least-laxity first (LLF) rule to consider only the total charging power of the EVCS, ensuring the feasibility of individual EV schedules. In addition, we introduce an equivalent state aggregation that guarantees optimal policy attainment. The proposed representation is utilized in a policy gradient method that finds the best parameters for a linear Gaussian policy. Our numerical results demonstrate the superiority of our representation approaches in terms of higher rewards and more effective policies compared to existing approximation-based approaches.",1
"Animals exhibit an innate ability to learn regularities of the world through interaction. By performing experiments in their environment, they are able to discern the causal factors of variation and infer how they affect the world's dynamics. Inspired by this, we attempt to equip reinforcement learning agents with the ability to perform experiments that facilitate a categorization of the rolled-out trajectories, and to subsequently infer the causal factors of the environment in a hierarchical manner. We introduce {\em causal curiosity}, a novel intrinsic reward, and show that it allows our agents to learn optimal sequences of actions and discover causal factors in the dynamics of the environment. The learned behavior allows the agents to infer a binary quantized representation for the ground-truth causal factors in every environment. Additionally, we find that these experimental behaviors are semantically meaningful (e.g., our agents learn to lift blocks to categorize them by weight), and are learnt in a self-supervised manner with approximately 2.5 times less data than conventional supervised planners. We show that these behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or other downstream tasks). Finally, we show that the knowledge of causal factor representations aids zero-shot learning for more complex tasks. Visit https://sites.google.com/usc.edu/causal-curiosity/home for website.",0
"Animals possess an innate ability to learn about the world by interacting with it, enabling them to identify the causal factors that influence its dynamics. Drawing inspiration from this, we aim to equip reinforcement learning agents with the capability to conduct experiments that facilitate the categorization of trajectories and subsequently deduce the environment's causal factors in a hierarchical manner. Our approach, called ""causal curiosity,"" involves introducing a novel intrinsic reward that enables our agents to discover the optimal sequences of actions and identify causal factors in the environment's dynamics. Our agents learn to categorize objects based on their weight and other meaningful behaviors in a self-supervised manner, using approximately 2.5 times less data than conventional supervised planners. These behaviors can be repurposed and fine-tuned for other downstream tasks, and the knowledge of causal factor representations aids in zero-shot learning for more complex tasks. For more information, visit https://sites.google.com/usc.edu/causal-curiosity/home.",1
"Humans and animals have the ability to reason and make predictions about different courses of action at many time scales. In reinforcement learning, option models (Sutton, Precup \& Singh, 1999; Precup, 2000) provide the framework for this kind of temporally abstract prediction and reasoning. Natural intelligent agents are also able to focus their attention on courses of action that are relevant or feasible in a given situation, sometimes termed affordable actions. In this paper, we define a notion of affordances for options, and develop temporally abstract partial option models, that take into account the fact that an option might be affordable only in certain situations. We analyze the trade-offs between estimation and approximation error in planning and learning when using such models, and identify some interesting special cases. Additionally, we demonstrate empirically the potential impact of partial option models on the efficiency of planning.",0
"The capacity for humans and animals to use reasoning and forecasting to evaluate various actions at different time scales is a crucial aspect of reinforcement learning. To accomplish this, option models (Sutton, Precup \& Singh, 1999; Precup, 2000) are used to provide a framework for temporally abstract prediction and reasoning. Natural intelligent agents are also skilled at directing their attention towards actions that are relevant or feasible in a given situation, known as affordable actions. This study introduces the concept of affordances for options and develops partially abstract option models that consider the possibility that an option may only be affordable in certain circumstances. The trade-offs between estimation and approximation errors in planning and learning are analyzed when using these models, and some interesting special cases are identified. Additionally, the potential impact of partially abstract option models on planning efficiency is empirically demonstrated.",1
"The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level optimization.",0
"The instability of a reinforcement learning algorithm can be caused by the deadly triad, which occurs when the algorithm uses off-policy learning, function approximation, and bootstrapping simultaneously. This paper explores the use of a target network as a way to break the deadly triad and stabilize training. The authors propose a new target network update rule that includes two projections and analyze its effectiveness. They also apply this technique and ridge regularization to various off-policy, linear function approximation, and bootstrapping algorithms. The results show that these algorithms converge to regularized TD fixed points, even under nonrestrictive and changing behavior policies, without bi-level optimization. This paper presents the first convergent linear $Q$-learning algorithms in these settings.",1
"We propose a novel approach to optimize fleet management by combining multi-agent reinforcement learning with graph neural network. To provide ride-hailing service, one needs to optimize dynamic resources and demands over spatial domain. While the spatial structure was previously approximated with a regular grid, our approach represents the road network with a graph, which better reflects the underlying geometric structure. Dynamic resource allocation is formulated as multi-agent reinforcement learning, whose action-value function (Q function) is approximated with graph neural networks. We use stochastic policy update rule over the graph with deep Q-networks (DQN), and achieve superior results over the greedy policy update. We design a realistic simulator that emulates the empirical taxi call data, and confirm the effectiveness of the proposed model under various conditions.",0
"Our innovative solution to optimizing fleet management involves merging multi-agent reinforcement learning with graph neural network. The provision of ride-hailing services demands the optimization of dynamic resources and demands across a spatial domain. Previously, the spatial structure was approximated using a regular grid. However, our approach utilizes a graph to represent the road network, which more accurately reflects the underlying geometric structure. To achieve dynamic resource allocation, we formulated multi-agent reinforcement learning, with the action-value function being approximated using graph neural networks. We utilized a stochastic policy update rule over the graph with deep Q-networks (DQN), which provided superior results compared to a greedy policy update. Additionally, we created a realistic simulator that emulates empirical taxi call data, and our proposed model proved effective under various conditions.",1
"Optimizing economic and public policy is critical to address socioeconomic issues and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex mechanism design problem. A policy designer needs to consider multiple objectives, policy levers, and behavioral responses from strategic actors who optimize for their individual objectives. Moreover, real-world policies should be explainable and robust to simulation-to-reality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow set of policy levers or objectives that are hard to measure, do not yield explicit optimal policies, or do not consider strategic behavior, for example. Hence, it remains challenging to optimize policy in real-world scenarios. Here we show that the AI Economist framework enables effective, flexible, and interpretable policy design using two-level reinforcement learning (RL) and data-driven simulations. We validate our framework on optimizing the stringency of US state policies and Federal subsidies during a pandemic, e.g., COVID-19, using a simulation fitted to real data. We find that log-linear policies trained using RL significantly improve social welfare, based on both public health and economic outcomes, compared to past outcomes. Their behavior can be explained, e.g., well-performing policies respond strongly to changes in recovery and vaccination rates. They are also robust to calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world policymaking has not seen adoption of machine learning methods at large, including RL and AI-driven simulations. Our results show the potential of AI to guide policy design and improve social welfare amidst the complexity of the real world.",0
"Effectively addressing socioeconomic issues and trade-offs, such as improving equality, productivity, or wellness, requires optimizing economic and public policy. However, this poses a complex problem for policy designers, who must consider multiple objectives and policy levers, as well as the strategic behavior of individuals who optimize for their own goals. Furthermore, policies must be explainable and robust to discrepancies between simulations and reality. Existing approaches often have limitations, such as a narrow focus on certain policy levers or objectives that are difficult to measure. This makes it challenging to optimize policy in real-world scenarios. To overcome these limitations, the AI Economist framework utilizes two-level reinforcement learning and data-driven simulations to enable effective, flexible, and interpretable policy design. By applying this framework to stringency of US state policies and Federal subsidies during the COVID-19 pandemic, we demonstrate that log-linear policies trained using RL significantly improve social welfare based on both public health and economic outcomes, compared to past outcomes. These policies also respond strongly to changes in recovery and vaccination rates and are robust to calibration errors. Our findings suggest that adopting machine learning methods, including RL and AI-driven simulations, can guide policy design and improve social welfare in the complex real-world setting.",1
"Watkins' and Dayan's Q-learning is a model-free reinforcement learning algorithm that iteratively refines an estimate for the optimal action-value function of an MDP by stochastically ""visiting"" many state-ation pairs [Watkins and Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent state-of-the-art achievements in reinforcement learning, including the superhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this paper is to reproduce a precise and (nearly) self-contained proof that Q-learning converges. Much of the available literature leverages powerful theory to obtain highly generalizable results in this vein. However, this approach requires the reader to be familiar with and make many deep connections to different research areas. A student seeking to deepen their understand of Q-learning risks becoming caught in a vicious cycle of ""RL-learning Hell"". For this reason, we give a complete proof from start to finish using only one external result from the field of stochastic approximation, despite the fact that this minimal dependence on other results comes at the expense of some ""shininess"".",0
"The Q-learning algorithm developed by Watkins and Dayan is a reinforcement learning model that improves the estimation of the optimal action-value function of an MDP by visiting various state-action pairs. This algorithm has been used in many recent state-of-the-art achievements in reinforcement learning, such as the deep Q-network used to play Atari games. The aim of this paper is to provide a clear and self-contained proof of Q-learning's convergence. While previous works have used complex theories to obtain generalizable results, these may be challenging for readers unfamiliar with the different research areas. Thus, we present a complete proof that relies on only one external result from stochastic approximation, even though this may not be as impressive.",1
"Drivers have unique and rich driving behaviors when operating vehicles in traffic. This paper presents a novel driver behavior learning approach that captures the uniqueness and richness of human driver behavior in realistic driving scenarios. A stochastic inverse reinforcement learning (SIRL) approach is proposed to learn a distribution of cost function, which represents the richness of the human driver behavior with a given set of driver-specific demonstrations. Evaluations are conducted on the realistic driving data collected from the 3D driver-in-the-loop driving simulation. The results show that the learned stochastic driver model is capable of expressing the richness of the human driving strategies under different realistic driving scenarios. Compared to the deterministic baseline driver behavior model, the results reveal that the proposed stochastic driver behavior model can better replicate the driver's unique and rich driving strategies in a variety of traffic conditions.",0
"This paper introduces a new method for learning about the diverse and distinct ways in which drivers operate their vehicles in traffic. The approach is designed to capture the unique and complex nature of human driver behavior in realistic driving scenarios. The method, known as stochastic inverse reinforcement learning (SIRL), involves learning a distribution of cost function that reflects the richness of human driver behavior based on a set of driver-specific demonstrations. The efficacy of the approach is evaluated using data collected from a 3D driver-in-the-loop driving simulation. The findings demonstrate that the learned stochastic driver model effectively captures the diversity of human driving strategies in various realistic driving scenarios. Furthermore, compared to a deterministic baseline driver behavior model, the proposed stochastic driver behavior model is better able to replicate the distinctive and intricate driving strategies of individual drivers in different traffic conditions.",1
"The rapid increase in the percentage of chronic disease patients along with the recent pandemic pose immediate threats on healthcare expenditure and elevate causes of death. This calls for transforming healthcare systems away from one-on-one patient treatment into intelligent health systems, to improve services, access and scalability, while reducing costs. Reinforcement Learning (RL) has witnessed an intrinsic breakthrough in solving a variety of complex problems for diverse applications and services. Thus, we conduct in this paper a comprehensive survey of the recent models and techniques of RL that have been developed/used for supporting Intelligent-healthcare (I-health) systems. This paper can guide the readers to deeply understand the state-of-the-art regarding the use of RL in the context of I-health. Specifically, we first present an overview for the I-health systems challenges, architecture, and how RL can benefit these systems. We then review the background and mathematical modeling of different RL, Deep RL (DRL), and multi-agent RL models. After that, we provide a deep literature review for the applications of RL in I-health systems. In particular, three main areas have been tackled, i.e., edge intelligence, smart core network, and dynamic treatment regimes. Finally, we highlight emerging challenges and outline future research directions in driving the future success of RL in I-health systems, which opens the door for exploring some interesting and unsolved problems.",0
"The rise in chronic disease patients and the recent pandemic present a pressing issue for healthcare expenditure and mortality rates. Consequently, healthcare systems must shift from individualized patient treatment to intelligent health systems to improve services, accessibility, scalability, and reduce costs. Reinforcement Learning has proven to be effective in addressing complex issues across various applications and services. Hence, our paper presents a comprehensive overview of the recent models and techniques of RL utilized in Intelligent-healthcare systems. This paper aims to inform readers about the current state-of-the-art regarding the use of RL in I-health by outlining the challenges faced by I-health systems, discussing the architecture of RL, Deep RL, and multi-agent RL models, and reviewing literature on the applications of RL in I-health, focusing on edge intelligence, smart core network, and dynamic treatment regimes. We also tackle emerging challenges and suggest future research directions to promote the success of RL in I-health systems and explore unresolved issues.",1
"AI and reinforcement learning (RL) have improved many areas, but are not yet widely adopted in economic policy design, mechanism design, or economics at large. At the same time, current economic methodology is limited by a lack of counterfactual data, simplistic behavioral models, and limited opportunities to experiment with policies and evaluate behavioral responses. Here we show that machine-learning-based economic simulation is a powerful policy and mechanism design framework to overcome these limitations. The AI Economist is a two-level, deep RL framework that trains both agents and a social planner who co-adapt, providing a tractable solution to the highly unstable and novel two-level RL challenge. From a simple specification of an economy, we learn rational agent behaviors that adapt to learned planner policies and vice versa. We demonstrate the efficacy of the AI Economist on the problem of optimal taxation. In simple one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In complex, dynamic economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies, while accounting for agent interactions and behavioral change more accurately than economic theory. These results demonstrate for the first time that two-level, deep RL can be used for understanding and as a complement to theory for economic design, unlocking a new computational learning-based approach to understanding economic policy.",0
"Although AI and reinforcement learning (RL) have made significant advancements, they have not been widely implemented in economic policy design, mechanism design, or economics as a whole. The current economic methodology is restricted by a lack of counterfactual data, basic behavioral models, and limited opportunities to experiment with policies and evaluate behavioral responses. In this study, we demonstrate that machine-learning-based economic simulation offers a potent policy and mechanism design framework to overcome these constraints. The AI Economist is a two-level, deep RL framework that trains both agents and a social planner in a co-adaptive manner, providing a viable solution to the highly unstable and novel two-level RL challenge. By starting with a simple economy specification, we learn rational agent behaviors that adapt to learned planner policies, and vice versa. The effectiveness of the AI Economist is demonstrated in the context of optimal taxation. In simple one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In complex, dynamic economies, the AI Economist substantially enhances both utilitarian social welfare and the trade-off between equality and productivity over baselines. It accomplishes this despite emergent tax-gaming strategies, while also more precisely accounting for agent interactions and behavioral change than economic theory. These outcomes demonstrate for the first time that two-level, deep RL can be used for understanding and as a complement to theory for economic design, unlocking a new computational learning-based approach to understanding economic policy.",1
"One of the challenges for multi-agent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial information of the entire system. In this system, it is desirable to learn policies of a decentralized type. A recent and promising paradigm to analyze such decentralized MARL is to take network structures into consideration. While exciting progress has been made to analyze decentralized MARL with the network of agents, often found in social networks and team video games, little is known theoretically for decentralized MARL with the network of states, frequently used for modeling self-driving vehicles, ride-sharing, and data and traffic routing.   This paper proposes a framework called localized training and decentralized execution to study MARL with network of states, with homogeneous (a.k.a. mean-field type) agents. Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that, after the training stage, agents can execute the learned decentralized policies, which only requires knowledge of the agents' current states. The key idea is to utilize the homogeneity of agents and regroup them according to their states, thus the formulation of a networked Markov decision process with teams of agents, enabling the update of the Q-function in a localized fashion. In order to design an efficient and scalable reinforcement learning algorithm under such a framework, we adopt the actor-critic approach with over-parameterized neural networks, and establish the convergence and sample complexity for our algorithm, shown to be scalable with respect to the size of both agents and states.",0
"Developing efficient learning algorithms for multi-agent reinforcement learning (MARL) is a challenge due to limited or partial information available to each agent in a large system. Decentralized policies are preferred in such systems, and analyzing decentralized MARL with network structures has shown promising results in social networks and team video games. However, theoretical knowledge for decentralized MARL with state networks, commonly used for modeling self-driving vehicles, ride-sharing, and data and traffic routing, is limited. To address this, we propose a framework called localized training and decentralized execution for homogeneous agents in a networked Markov decision process. Agents only collect local information in the training phase and execute learned decentralized policies based on their current states. We utilize homogeneity to group agents by their states and update the Q-function in a localized manner. We adopt the actor-critic approach with over-parameterized neural networks and demonstrate scalability with respect to the number of agents and states. Our algorithm is shown to be efficient and scalable for MARL with state networks.",1
"The theory of reinforcement learning has focused on two fundamental problems: achieving low regret, and identifying $\epsilon$-optimal policies. While a simple reduction allows one to apply a low-regret algorithm to obtain an $\epsilon$-optimal policy and achieve the worst-case optimal rate, it is unknown whether low-regret algorithms can obtain the instance-optimal rate for policy identification. We show that this is not possible -- there exists a fundamental tradeoff between achieving low regret and identifying an $\epsilon$-optimal policy at the instance-optimal rate.   Motivated by our negative finding, we propose a new measure of instance-dependent sample complexity for PAC tabular reinforcement learning which explicitly accounts for the attainable state visitation distributions in the underlying MDP. We then propose and analyze a novel, planning-based algorithm which attains this sample complexity -- yielding a complexity which scales with the suboptimality gaps and the ``reachability'' of a state. We show that our algorithm is nearly minimax optimal, and on several examples that our instance-dependent sample complexity offers significant improvements over worst-case bounds.",0
"The theory of reinforcement learning has addressed two key problems: minimizing regret and identifying policies that are $\epsilon$-optimal. Although a simple approach can use a low-regret algorithm to achieve an $\epsilon$-optimal policy and the worst-case optimal rate, it is uncertain whether low-regret algorithms can attain the instance-optimal rate for policy identification. We prove that this is not feasible and that there is a fundamental tradeoff between achieving low regret and identifying an $\epsilon$-optimal policy at the instance-optimal rate. Based on this negative result, we propose a new measure of instance-dependent sample complexity for PAC tabular reinforcement learning that considers the achievable state visitation distributions in the underlying MDP. We then introduce and analyze a novel planning-based algorithm that achieves this sample complexity, which scales with the suboptimality gaps and the ""reachability"" of a state. We demonstrate that our algorithm is nearly minimax optimal and that our instance-dependent sample complexity improves on worst-case bounds for several examples.",1
"Safety and robustness are two desired properties for any reinforcement learning algorithm. CMDPs can handle additional safety constraints and RMDPs can perform well under model uncertainties. In this paper, we propose to unite these two frameworks resulting in robust constrained MDPs (RCMDPs). The motivation is to develop a framework that can satisfy safety constraints while also simultaneously offer robustness to model uncertainties. We develop the RCMDP objective, derive gradient update formula to optimize this objective and then propose policy gradient based algorithms. We also independently propose Lyapunov based reward shaping for RCMDPs, yielding better stability and convergence properties.",0
"The desirable traits of reinforcement learning algorithms include safety and robustness. CMDPs can accommodate safety constraints, while RMDPs perform well in the face of uncertain models. Our paper proposes a union of these frameworks, creating robust constrained MDPs (RCMDPs), which simultaneously satisfy safety constraints and offer model uncertainty robustness. We introduce the RCMDP objective, provide a gradient update formula to optimize it, and propose policy gradient-based algorithms. Additionally, we suggest Lyapunov-based reward shaping for RCMDPs, resulting in improved stability and convergence properties.",1
"Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.",0
"Agents trained in reinforcement learning settings can effectively communicate with each other using discrete tokens, which enables them to accomplish tasks together that they cannot achieve alone. However, the current use of one-hot vectors as tokens restricts agents from acquiring more desirable aspects of communication, such as zero-shot understanding. To address this, we propose a new technique inspired by word embedding techniques in natural language processing. Our approach allows agents to communicate via tokens derived from a continuous space, optimizing communication in a wide range of scenarios. In self-play experiments, our method demonstrates that agents learn to cluster tokens in meaningful ways, enabling effective communication in noisy environments. Furthermore, our technique outperforms one-hot communication in both responding to human communication and allowing humans to understand emergent agent communication.",1
"Hybrid FSO/RF system requires an efficient FSO and RF link switching mechanism to improve the system capacity by realizing the complementary benefits of both the links. The dynamics of network conditions, such as fog, dust, and sand storms compound the link switching problem and control complexity. To address this problem, we initiate the study of deep reinforcement learning (DRL) for link switching of hybrid FSO/RF systems. Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under atmospheric turbulences. To formulate the problem, we define the state, action, and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates the deployed policy that interacts with the environment in a hybrid FSO/RF system, resulting in high switching costs. To overcome this, we lift this problem to ensemble consensus-based representation learning for deep reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF DRL approach uses consensus learned features representations based on an ensemble of asynchronous threads to update the deployed policy. Experimental results corroborate that the proposed DQNEnsemble-FSO/RF's consensus-learned features switching achieves better performance than Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching cost significantly low.",0
"To enhance the capacity of a hybrid FSO/RF system by leveraging the benefits of both links, an effective switching mechanism is essential. However, environmental factors such as fog, dust, and sand storms make link switching challenging and increase control complexity. To tackle this issue, we explore the use of deep reinforcement learning (DRL) for link switching in hybrid FSO/RF systems. Our study focuses on two DRL methods: Actor/Critic-FSO/RF and DQN-FSO/RF, which address atmospheric turbulence. We define the state, action, and reward function of a hybrid FSO/RF system to formulate the problem. However, DQN-FSO/RF incurs high switching costs due to frequent policy updates. To mitigate this, we propose a novel approach called DQNEnsemble-FSO/RF, which utilizes consensus learned feature representations based on an ensemble of asynchronous threads. Experimental results demonstrate that DQNEnsemble-FSO/RF achieves better performance than Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while minimizing switching costs.",1
"Policy gradient methods are appealing in deep reinforcement learning but suffer from high variance of gradient estimate. To reduce the variance, the state value function is applied commonly. However, the effect of the state value function becomes limited in stochastic dynamic environments, where the unexpected state dynamics and rewards will increase the variance. In this paper, we propose to replace the state value function with a novel hindsight value function, which leverages the information from the future to reduce the variance of the gradient estimate for stochastic dynamic environments.   Particularly, to obtain an ideally unbiased gradient estimate, we propose an information-theoretic approach, which optimizes the embeddings of the future to be independent of previous actions. In our experiments, we apply the proposed hindsight value function in stochastic dynamic environments, including discrete-action environments and continuous-action environments. Compared with the standard state value function, the proposed hindsight value function consistently reduces the variance, stabilizes the training, and improves the eventual policy.",0
"Deep reinforcement learning often utilizes policy gradient methods, but these methods have a downside of having a high variance of gradient estimate. To address this issue, the state value function is typically used, but its effectiveness is limited in stochastic dynamic environments where unexpected state dynamics and rewards lead to increased variance. This paper introduces a novel hindsight value function that replaces the state value function, leveraging future information to decrease gradient estimate variance in stochastic dynamic environments. An information-theoretic approach is proposed to achieve an unbiased gradient estimate, optimizing future embeddings to be independent of prior actions. The proposed hindsight value function is applied to both discrete- and continuous-action environments in experiments, consistently reducing variance, stabilizing training, and enhancing policies compared to the standard state value function.",1
"Electronic Health Records often suffer from missing data, which poses a major problem in clinical practice and clinical studies. A novel approach for dealing with missing data are Generative Adversarial Nets (GANs), which have been generating huge research interest in image generation and transformation. Recently, researchers have attempted to apply GANs to missing data generation and imputation for EHR data: a major challenge here is the categorical nature of the data. State-of-the-art solutions to the GAN-based generation of categorical data involve either reinforcement learning, or learning a bidirectional mapping between the categorical and the real latent feature space, so that the GANs only need to generate real-valued features. However, these methods are designed to generate complete feature vectors instead of imputing only the subsets of missing features. In this paper we propose a simple and yet effective approach that is based on previous work on GANs for data imputation. We first motivate our solution by discussing the reason why adversarial training often fails in case of categorical features. Then we derive a novel way to re-code the categorical features to stabilize the adversarial training. Based on experiments on two real-world EHR data with multiple settings, we show that our imputation approach largely improves the prediction accuracy, compared to more traditional data imputation approaches.",0
"Clinical practice and studies are greatly affected by the problem of missing data in Electronic Health Records (EHRs). Generative Adversarial Nets (GANs) have gained interest in image generation and transformation, and researchers have attempted to use them for missing data generation and imputation in EHRs. However, the categorical nature of the data poses a challenge for GAN-based generation. Current solutions involve generating complete feature vectors instead of imputing only missing features. This paper proposes a simple and effective approach for data imputation based on previous GAN research. We discuss why adversarial training fails for categorical features and propose a novel way to re-code them to stabilize training. Our experiments on two real-world EHR datasets show that our approach improves prediction accuracy compared to traditional methods.",1
"For reinforcement learning (RL), it is challenging for an agent to master a task that requires a specific series of actions due to sparse rewards. To solve this problem, reverse curriculum generation (RCG) provides a reverse expansion approach that automatically generates a curriculum for the agent to learn. More specifically, RCG adapts the initial state distribution from the neighborhood of a goal to a distance as training proceeds. However, the initial state distribution generated for each iteration might be biased, thus making the policy overfit or slowing down the reverse expansion rate. While training RCG for actor-critic (AC) based RL algorithms, this poor generalization and slow convergence might be induced by the tight coupling between an AC pair. Therefore, we propose a parallelized approach that simultaneously trains multiple AC pairs and periodically exchanges their critics. We empirically demonstrate that this proposed approach can improve RCG in performance and convergence, and it can also be applied to other AC based RL algorithms with adapted initial state distribution.",0
"Sparse rewards can present a challenge for agents in reinforcement learning (RL) when attempting to master tasks that require a specific sequence of actions. Reverse curriculum generation (RCG) attempts to solve this issue through a reverse expansion approach that automatically creates a curriculum for the agent to learn. RCG modifies the initial state distribution from the goal's neighborhood to a certain distance as training progresses. However, the generated initial state distribution for each iteration can exhibit bias, resulting in overfitting or slowing the reverse expansion rate. In RL algorithms based on actor-critic (AC), the tight coupling between AC pairs may cause poor generalization and slow convergence. We propose a parallelized approach to address this issue, where multiple AC pairs are trained simultaneously, and their critics are exchanged periodically. Empirical evidence shows that this approach improves RCG's performance and convergence and can also be applied to other AC-based RL algorithms with adapted initial state distribution.",1
"Document layout analysis (DLA) aims to divide a document image into different types of regions. DLA plays an important role in the document content understanding and information extraction systems. Exploring a method that can use less data for effective training contributes to the development of DLA. We consider a Human-in-the-loop (HITL) collaborative intelligence in the DLA. Our approach was inspired by the fact that the HITL push the model to learn from the unknown problems by adding a small amount of data based on knowledge. The HITL select key samples by using confidence. However, using confidence to find key samples is not suitable for DLA tasks. We propose the Key Samples Selection (KSS) method to find key samples in high-level tasks (semantic segmentation) more accurately through agent collaboration, effectively reducing costs. Once selected, these key samples are passed to human beings for active labeling, then the model will be updated with the labeled samples. Hence, we revisited the learning system from reinforcement learning and designed a sample-based agent update strategy, which effectively improves the agent's ability to accept new samples. It achieves significant improvement results in two benchmarks (DSSE-200 (from 77.1% to 86.3%) and CS-150 (from 88.0% to 95.6%)) by using 10% of labeled data.",0
"The goal of Document Layout Analysis (DLA) is to categorize various regions within a document image. DLA is crucial for comprehending document content and extracting information. To enhance DLA development, we investigated a method that requires less data for effective training. We incorporated a Human-in-the-loop (HITL) collaborative intelligence approach in DLA. Our approach was influenced by HITL's ability to enable models to learn from unknown problems by supplementing a small amount of data based on knowledge. The HITL selects important samples by using confidence, but this approach is not appropriate for DLA tasks. Therefore, we proposed the Key Samples Selection (KSS) method, which utilizes agent collaboration to accurately identify key samples in high-level tasks like semantic segmentation, effectively lowering costs. Once identified, humans label the key samples, and the model is updated with the labeled samples. We revisited the learning system from reinforcement learning and devised a sample-based agent update strategy that significantly enhances the agent's ability to accept new samples. By using only 10% of labeled data, our approach achieved remarkable improvement results in two benchmarks (DSSE-200 from 77.1% to 86.3% and CS-150 from 88.0% to 95.6%).",1
"Deep reinforcement learning (DRL) requires large samples and a long training time to operate optimally. Yet humans rarely require long periods training to perform well on novel tasks, such as computer games, once they are provided with an accurate program of instructions. We used perceptual control theory (PCT) to construct a simple closed-loop model which requires no training samples and training time within a video game study using the Arcade Learning Environment (ALE). The model was programmed to parse inputs from the environment into hierarchically organised perceptual signals, and it computed a dynamic error signal by subtracting the incoming signal for each perceptual variable from a reference signal to drive output signals to reduce this error. We tested the same model across two different Atari paddle games Breakout and Pong to achieve performance at least as high as DRL paradigms, and close to good human performance. Our study shows that perceptual control models, based on simple assumptions, can perform well without learning. We conclude by specifying a parsimonious role of learning that may be more similar to psychological functioning.",0
"Deep reinforcement learning (DRL) necessitates considerable amounts of data and a prolonged training period for optimal functioning. However, humans rarely require extended training periods to excel in new tasks, such as computer games, once they have precise instructions. Our study employed perceptual control theory (PCT) to create a basic closed-loop model for a video game study utilizing the Arcade Learning Environment (ALE), which does not require training data or time. The model was designed to organize environmental inputs into a hierarchical arrangement of perceptual signals, and it calculated a dynamic error signal by subtracting the incoming signal for each perceptual variable from a reference signal to enable output signals to decrease this error. We implemented the same model in two different Atari paddle games, Breakout and Pong, and achieved results comparable to those of DRL models and near to human performance. Our research demonstrates that perceptual control models based on simple principles can perform proficiently without training. We conclude by identifying a modest role of learning that may correspond more closely to psychological functions.",1
"Neural networks are powerful models that have a remarkable ability to extract patterns that are too complex to be noticed by humans or other machine learning models. Neural networks are the first class of models that can train end-to-end systems with large learning capacities. However, we still have the difficult challenge of designing the neural network, which requires human experience and a long process of trial and error. As a solution, we can use a neural architecture search to find the best network architecture for the task at hand. Existing NAS algorithms generally evaluate the fitness of a new architecture by fully training from scratch, resulting in the prohibitive computational cost, even if operated on high-performance computers. In this paper, an end-to-end offline performance predictor is proposed to accelerate the evaluation of sampled architectures.   Index Terms- Learning Curve Prediction, Neural Architecture Search, Reinforcement Learning.",0
"Neural networks are models that possess a remarkable ability to identify complex patterns that may go unnoticed by humans or other machine learning models. They are the first models that can train end-to-end systems with large learning capabilities. However, the challenge remains in designing the neural network, which requires human expertise and a lengthy trial and error process. To address this issue, a neural architecture search can be conducted to find the optimal network architecture for the given task. However, existing NAS algorithms often involve computationally expensive full training to evaluate the fitness of a new architecture, even with high-performance computers. This paper proposes an end-to-end offline performance predictor to speed up the evaluation process of sampled architectures. The paper also discusses Learning Curve Prediction, Neural Architecture Search, and Reinforcement Learning as index terms.",1
"Risk-bounded motion planning is an important yet difficult problem for safety-critical tasks. While existing mathematical programming methods offer theoretical guarantees in the context of constrained Markov decision processes, they either lack scalability in solving larger problems or produce conservative plans. Recent advances in deep reinforcement learning improve scalability by learning policy networks as function approximators. In this paper, we propose an extension of soft actor critic model to estimate the execution risk of a plan through a risk critic and produce risk-bounded policies efficiently by adding an extra risk term in the loss function of the policy network. We define the execution risk in an accurate form, as opposed to approximating it through a summation of immediate risks at each time step that leads to conservative plans. Our proposed model is conditioned on a continuous spectrum of risk bounds, allowing the user to adjust the risk-averse level of the agent on the fly. Through a set of experiments, we show the advantage of our model in terms of both computational time and plan quality, compared to a state-of-the-art mathematical programming baseline, and validate its performance in more complicated scenarios, including nonlinear dynamics and larger state space.",0
"Motion planning with bounded risk is a complex issue in tasks that require high levels of safety. Although current mathematical programming methods offer assurances in terms of constrained Markov decision processes, they suffer from a lack of scalability or produce plans that are overly cautious. Recent developments in deep reinforcement learning have enhanced scalability by utilizing policy networks as function approximators. This study proposes an expansion of the soft actor critic model that estimates plan execution risk through a risk critic and efficiently produces risk-bounded policies by adding an additional risk term to the policy network's loss function. The execution risk is precisely defined, as opposed to being approximated by the summation of immediate risks at each time step, which leads to overly cautious plans. Our model is dependent on a continuous range of risk bounds, enabling the user to adjust the agent's risk aversion level on the fly. Through a series of experiments, we demonstrate the superiority of our model in terms of both computational time and plan quality, in comparison to a state-of-the-art mathematical programming baseline, and validate its effectiveness in more complex scenarios, such as nonlinear dynamics and larger state spaces.",1
"Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics model and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS), which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates, learned via adversarial training, ensure the policy's safety assuming calibrated learned dynamics model. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary. Prior methods require hundreds of violations to achieve decent rewards on these tasks, whereas our proposed algorithms incur zero violations.",0
"The use of reinforcement learning algorithms in real-world scenarios has raised concerns about safety violations during training. This paper aims to address the challenge of developing safe RL algorithms that do not encounter any training-time safety violations. The proposed approach, Co-trained Barrier Certificate for Safe RL (CRABS), is designed to operate in a setting where only a safe but trivial-reward initial policy is available, without any prior knowledge of the dynamics model or offline data. The algorithm learns barrier certificates, dynamics models, and policies iteratively, with barrier certificates ensuring policy safety, learned through adversarial training, and a regularization term encouraging exploration. Empirical simulations show that our proposed algorithm, unlike prior methods, achieves zero safety violations in simple environments with 2-4 dimensional state space, even when high-reward policies need to approach safety boundaries.",1
"In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition probabilities calculated from the dataset can be much different from the transition probabilities induced by the learned policies of other agents, creating large errors in value estimates. Moreover, the experience distributions of agents' datasets may vary wildly due to diverse behavior policies, causing large difference in value estimates between agents. Consequently, agents will learn uncoordinated suboptimal policies. In this paper, we propose MABCQ, which exploits value deviation and transition normalization to modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the biased transition probabilities of next states. They together encourage agents to discover potential optimal and coordinated policies. Mathematically, we prove the convergence of Q-learning under the non-stationary transition probabilities after modification. Empirically, we show that MABCQ greatly outperforms baselines and reduces the difference in value estimates between agents.",0
"Due to high cost and risk, in many real-world multi-agent cooperative tasks, agents must rely on offline datasets for learning instead of interacting with the environment and collecting experiences. However, the transition probabilities calculated from these datasets may differ significantly from those induced by the learned policies of other agents, resulting in errors in value estimates. Furthermore, the experience distributions of agents' datasets can vary widely due to diverse behavior policies, leading to significant differences in value estimates and uncoordinated suboptimal policies. Thus, we propose MABCQ, which modifies transition probabilities through value deviation and transition normalization, encouraging the discovery of potential optimal and coordinated policies. We prove mathematically that Q-learning converges under non-stationary transition probabilities after modification and empirically demonstrate superior performance compared to baselines while reducing differences in value estimates between agents.",1
"Context: Machine Learning (ML) has been at the heart of many innovations over the past years. However, including it in so-called 'safety-critical' systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches.   Objective: This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question 'How to Certify Machine Learning Based Safety-critical Systems?'.   Method: We conduct a Systematic Literature Review (SLR) of research papers published between 2015 to 2020, covering topics related to the certification of ML systems. In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted.   Results: The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of models. It also emphasized the need to further develop connections between academia and industries to deepen the domain study. Finally, it also illustrated the necessity to build connections between the above mention main pillars that are for now mainly studied separately.   Conclusion: We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions.",0
"Machine Learning (ML) has been a catalyst for numerous innovations in recent years. However, integrating it into safety-critical systems, such as automotive or aeronautic, presents significant challenges due to the paradigm shift that ML brings and its impact on traditional certification approaches. This paper aims to explore the challenges related to certifying ML-based safety-critical systems and the proposed solutions in the literature to address them. To achieve this objective, we conducted a Systematic Literature Review (SLR) of research papers published between 2015 to 2020, covering topics related to the certification of ML systems. The SLR identified 217 papers that focused on the main pillars of ML certification, including Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted. Our results highlighted the enthusiasm of the community for this subject, as well as the need for more diversity in terms of datasets and models. We also emphasized the importance of developing connections between academia and industries and building bridges between the main pillars of ML certification. Finally, we discussed current efforts and future research directions for enabling the certification of ML-based software systems.",1
"We study how robots can autonomously learn skills that require a combination of navigation and grasping. While reinforcement learning in principle provides for automated robotic skill learning, in practice reinforcement learning in the real world is challenging and often requires extensive instrumentation and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Our proposed system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where manipulation policy uncertainty drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically, in around 40 hours of autonomous real-world training.",0
"Our focus is on exploring the autonomous acquisition of navigation and grasping skills by robots. Although reinforcement learning is capable of automating this process, its practical implementation in the real world can be challenging and require extensive supervision and instrumentation. Our ultimate goal is to develop a robotic reinforcement learning system that can learn both skills together, autonomously and without human intervention, allowing for continual learning under realistic circumstances. Our proposed system, ReLMM, has the ability to learn continuously on a real-world platform without any environmental instrumentation or access to privileged information. To achieve this, our method employs a modularized policy with separate components for manipulation and navigation. The manipulation module provides rewards for navigation, and the exploration of the navigation controller is driven by the uncertainty of the manipulation policy. We evaluated our method on a room cleanup task, where the robot successfully navigated to and picked up items scattered on the floor. After a grasp curriculum training phase, ReLMM acquired the navigation and grasping skills together automatically over a period of 40 hours of autonomous real-world training.",1
"We present a hybrid ML-heuristic approach that we name ""Heuristically Assisted Deep Reinforcement Learning (HA-DRL)"" to solve the problem of Network Slice Placement Optimization. The proposed approach leverages recent works on Deep Reinforcement Learning (DRL) for slice placement and Virtual Network Embedding (VNE) and uses a heuristic function to optimize the exploration of the action space by giving priority to reliable actions indicated by an efficient heuristic algorithm. The evaluation results show that the proposed HA-DRL algorithm can accelerate the learning of an efficient slice placement policy improving slice acceptance ratio when compared with state-of-the-art approaches that are based only on reinforcement learning.",0
"Our solution to the Network Slice Placement Optimization problem is a combination of machine learning and heuristic techniques, which we call ""Heuristically Assisted Deep Reinforcement Learning (HA-DRL)"". This approach incorporates recent advancements in both Deep Reinforcement Learning (DRL) for slice placement and Virtual Network Embedding (VNE), while also utilizing a heuristic function to enhance the exploration of the action space. This heuristic algorithm prioritizes reliable actions, which ultimately leads to faster and more efficient learning of an optimal slice placement policy. Compared to existing reinforcement learning-based methods, our HA-DRL algorithm significantly improves the slice acceptance ratio, as demonstrated by our evaluation results.",1
"The field of Continual Learning (CL) seeks to develop algorithms that accumulate knowledge and skills over time through interaction with non-stationary environments and data distributions. Measuring progress in CL can be difficult because a plethora of evaluation procedures (ettings) and algorithmic solutions (methods) have emerged, each with their own potentially disjoint set of assumptions about the CL problem. In this work, we view each setting as a set of assumptions. We then create a tree-shaped hierarchy of the research settings in CL, in which more general settings become the parents of those with more restrictive assumptions. This makes it possible to use inheritance to share and reuse research, as developing a method for a given setting also makes it directly applicable onto any of its children. We instantiate this idea as a publicly available software framework called Sequoia, which features a variety of settings from both the Continual Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains. Sequoia also includes a growing suite of methods which are easy to extend and customize, in addition to more specialized methods from third-party libraries. We hope that this new paradigm and its first implementation can serve as a foundation for the unification and acceleration of research in CL. You can help us grow the tree by visiting www.github.com/lebrice/Sequoia.",0
"The Continual Learning (CL) field aims to create algorithms that can gain knowledge and skills over time by interacting with non-stationary environments and data distributions. Measuring progress in CL can be challenging due to the numerous evaluation procedures and algorithmic solutions that exist, each with their own set of assumptions about the CL problem. In this study, we treat each setting as a set of assumptions and develop a hierarchical tree of research settings in CL, where more general settings serve as the parents of those with more restrictive assumptions. This approach allows for the sharing and reuse of research, as developing a method for a given setting makes it readily applicable to any of its descendants. We have named this concept Sequoia and have created a publicly available software framework that includes various settings from both the Continual Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains, as well as a range of customizable methods. We hope that Sequoia and its novel approach can establish a foundation for the unification and acceleration of research in CL. To support our project, please visit www.github.com/lebrice/Sequoia.",1
"Reinforcement learning from large-scale offline datasets provides us with the ability to learn policies without potentially unsafe or impractical exploration. Significant progress has been made in the past few years in dealing with the challenge of correcting for differing behavior between the data collection and learned policies. However, little attention has been paid to potentially changing dynamics when transferring a policy to the online setting, where performance can be up to 90% reduced for existing methods. In this paper we address this problem with Augmented World Models (AugWM). We augment a learned dynamics model with simple transformations that seek to capture potential changes in physical properties of the robot, leading to more robust policies. We not only train our policy in this new setting, but also provide it with the sampled augmentation as a context, allowing it to adapt to changes in the environment. At test time we learn the context in a self-supervised fashion by approximating the augmentation which corresponds to the new environment. We rigorously evaluate our approach on over 100 different changed dynamics settings, and show that this simple approach can significantly improve the zero-shot generalization of a recent state-of-the-art baseline, often achieving successful policies where the baseline fails.",0
"Learning policies through reinforcement from large-scale offline datasets can eliminate potentially unsafe or impractical exploration. Although progress has been made in rectifying varying behavior between data collection and learned policies, little attention has been given to the possibility of dynamic changes when transferring a policy to an online setting. This can result in a performance reduction of up to 90% for existing methods. In this paper, we present Augmented World Models (AugWM) to address this problem. Our approach involves augmenting a learned dynamics model with simple transformations that capture potential changes in the robot's physical properties. This leads to more robust policies, as we not only train our policy in the new setting but also provide it with the sampled augmentation as a context, allowing it to adapt to changes in the environment. At test time, we learn the context in a self-supervised fashion by approximating the augmentation that corresponds to the new environment. We evaluate our approach on over 100 different changed dynamics settings and show that this simple approach can significantly improve zero-shot generalization, often achieving successful policies where the baseline fails.",1
"Reinforcement learning is important part of artificial intelligence. In this paper, we review model-free reinforcement learning that utilizes the average reward optimality criterion in the infinite horizon setting. Motivated by the solo survey by Mahadevan (1996a), we provide an updated review of work in this area and extend it to cover policy-iteration and function approximation methods (in addition to the value-iteration and tabular counterparts). We present a comprehensive literature mapping. We also identify and discuss opportunities for future work.",0
"The significance of reinforcement learning in the realm of artificial intelligence cannot be overstated. This article delves into the analysis of model-free reinforcement learning, which employs the optimal average reward criteria in an infinite horizon setup. Our examination builds upon the previous survey conducted by Mahadevan (1996a) and encompasses policy-iteration and function approximation approaches, as well as the value-iteration and tabular methods. A detailed literature mapping is provided, along with an exploration of potential avenues for future research.",1
"Model-based reinforcement learning is a widely accepted solution for solving excessive sample demands. However, the predictions of the dynamics models are often not accurate enough, and the resulting bias may incur catastrophic decisions due to insufficient robustness. Therefore, it is highly desired to investigate how to improve the robustness of model-based RL algorithms while maintaining high sampling efficiency. In this paper, we propose Model-Based Double-dropout Planning (MBDP) to balance robustness and efficiency. MBDP consists of two kinds of dropout mechanisms, where the rollout-dropout aims to improve the robustness with a small cost of sample efficiency, while the model-dropout is designed to compensate for the lost efficiency at a slight expense of robustness. By combining them in a complementary way, MBDP provides a flexible control mechanism to meet different demands of robustness and efficiency by tuning two corresponding dropout ratios. The effectiveness of MBDP is demonstrated both theoretically and experimentally.",0
"To overcome the issue of excessive sample demands, model-based reinforcement learning is a commonly adopted approach. Unfortunately, the predictions of dynamics models can be inaccurate, leading to biased decisions and poor robustness. Therefore, there is a need to explore methods for improving the robustness of model-based RL algorithms while maintaining high sampling efficiency. This paper introduces Model-Based Double-dropout Planning (MBDP), which balances efficiency and robustness. MBDP employs two types of dropout mechanisms: rollout-dropout enhances robustness at the expense of sample efficiency, while model-dropout compensates for lost efficiency with a slight impact on robustness. By adjusting the dropout ratios, MBDP offers flexible control to meet different requirements. The effectiveness of MBDP is demonstrated through theoretical and experimental analysis.",1
"Efficient methods to evaluate new algorithms are critical for improving interactive bandit and reinforcement learning systems such as recommendation systems. A/B tests are reliable, but are time- and money-consuming, and entail a risk of failure. In this paper, we develop an alternative method, which predicts the performance of algorithms given historical data that may have been generated by a different algorithm. Our estimator has the property that its prediction converges in probability to the true performance of a counterfactual algorithm at a rate of $\sqrt{N}$, as the sample size $N$ increases. We also show a correct way to estimate the variance of our prediction, thus allowing the analyst to quantify the uncertainty in the prediction. These properties hold even when the analyst does not know which among a large number of potentially important state variables are actually important. We validate our method by a simulation experiment about reinforcement learning. We finally apply it to improve advertisement design by a major advertisement company. We find that our method produces smaller mean squared errors than state-of-the-art methods.",0
"In order to enhance recommendation systems and other interactive bandit and reinforcement learning systems, it is imperative to have efficient means of assessing new algorithms. While A/B testing is dependable, it is a costly and time-consuming process that comes with the risk of failure. To address this issue, our paper introduces an alternative method for predicting algorithm performance using historical data that may have been generated by a different algorithm. Our estimator has the advantageous property of converging in probability to the true performance of a counterfactual algorithm at a rate of $\sqrt{N}$ with increasing sample size $N$. Additionally, we demonstrate how to accurately estimate the variance of our prediction, enabling analysts to quantify the prediction's uncertainty. Even when analysts are unsure which state variables are important, our method's properties hold. We validate our method through a simulation experiment on reinforcement learning and apply it to a major advertisement company to improve advertisement design, resulting in smaller mean squared errors than state-of-the-art methods.",1
"Back-door attack poses a severe threat to deep learning systems. It injects hidden malicious behaviors to a model such that any input stamped with a special pattern can trigger such behaviors. Detecting back-door is hence of pressing need. Many existing defense techniques use optimization to generate the smallest input pattern that forces the model to misclassify a set of benign inputs injected with the pattern to a target label. However, the complexity is quadratic to the number of class labels such that they can hardly handle models with many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we propose a K-Arm optimization method for backdoor detection. By iteratively and stochastically selecting the most promising labels for optimization with the guidance of an objective function, we substantially reduce the complexity, allowing to handle models with many classes. Moreover, by iteratively refining the selection of labels to optimize, it substantially mitigates the uncertainty in choosing the right labels, improving detection accuracy. At the time of submission, the evaluation of our method on over 4000 models in the IARPA TrojAI competition from round 1 to the latest round 4 achieves top performance on the leaderboard. Our technique also supersedes three state-of-the-art techniques in terms of accuracy and the scanning time needed.",0
"Deep learning systems face a serious threat from back-door attacks, which add hidden malicious behaviors to a model. This means that any input with a particular pattern can trigger these behaviors. Detecting back-doors is crucial, but current defense techniques use optimization to create the smallest input pattern that forces the model to misclassify benign inputs. However, this becomes difficult with models that have many classes. To address this problem, we propose a K-Arm optimization method inspired by Multi-Arm Bandit in Reinforcement Learning. This method reduces complexity by iteratively selecting the most promising labels for optimization with an objective function, allowing us to handle models with many classes. Additionally, we refine the selection of labels to optimize, improving detection accuracy and mitigating uncertainty. Our method has achieved top performance on the leaderboard of the IARPA TrojAI competition by evaluating over 4000 models from round 1 to round 4. It also outperforms three state-of-the-art techniques in terms of accuracy and scanning time.",1
"Recent development of Deep Reinforcement Learning has demonstrated superior performance of neural networks in solving challenging problems with large or even continuous state spaces. One specific approach is to deploy neural networks to approximate value functions by minimising the Mean Squared Bellman Error function. Despite great successes of Deep Reinforcement Learning, development of reliable and efficient numerical algorithms to minimise the Bellman Error is still of great scientific interest and practical demand. Such a challenge is partially due to the underlying optimisation problem being highly non-convex or using incorrect gradient information as done in Semi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman Error from a smooth optimisation perspective combined with a Residual Gradient formulation. Our contribution is two-fold.   First, we analyse critical points of the error function and provide technical insights on the optimisation procure and design choices for neural networks. When the existence of global minima is assumed and the objective fulfils certain conditions we can eliminate suboptimal local minima when using over-parametrised neural networks. We can construct an efficient Approximate Newton's algorithm based on our analysis and confirm theoretical properties of this algorithm such as being locally quadratically convergent to a global minimum numerically.   Second, we demonstrate feasibility and generalisation capabilities of the proposed algorithm empirically using continuous control problems and provide a numerical verification of our critical point analysis. We outline the short coming of Semi-Gradients. To benefit from an approximate Newton's algorithm complete derivatives of the Mean Squared Bellman error must be considered during training.",0
"The recent progress in Deep Reinforcement Learning has illustrated the effectiveness of neural networks in tackling difficult problems that possess large or continuous state spaces. One particular method involves employing neural networks to estimate value functions by minimizing the Mean Squared Bellman Error function. Despite the success of Deep Reinforcement Learning, the development of dependable and efficient numerical algorithms for minimizing the Bellman Error remains a significant scientific concern and practical need. This difficulty is in part due to the optimization problem's highly non-convex nature or the use of incorrect gradient information, as seen in Semi-Gradient algorithms. This study analyzes the Mean Squared Bellman Error from a smooth optimization viewpoint combined with a Residual Gradient approach. Our contribution is twofold. Firstly, we examine critical points of the error function and provide technical insights into the optimization process and design choices for neural networks. We can eliminate suboptimal local minima when employing over-parametrized neural networks under certain conditions, assuming the existence of global minima and fulfilling specific objectives. Based on our analysis, we can construct an efficient Approximate Newton's algorithm and numerically verify its theoretical properties, such as locally quadratically converging to a global minimum. Secondly, we demonstrate the feasibility and generalization capabilities of the proposed algorithm empirically using continuous control problems and provide numerical verification of our critical point analysis. We outline the shortcomings of Semi-Gradients, highlighting the need to consider complete derivatives of the Mean Squared Bellman error during training to benefit from an approximate Newton's algorithm.",1
"Nodule segmentation from breast ultrasound images is challenging yet essential for the diagnosis. Weakly-supervised segmentation (WSS) can help reduce time-consuming and cumbersome manual annotation. Unlike existing weakly-supervised approaches, in this study, we propose a novel and general WSS framework called Flip Learning, which only needs the box annotation. Specifically, the target in the label box will be erased gradually to flip the classification tag, and the erased region will be considered as the segmentation result finally. Our contribution is three-fold. First, our proposed approach erases on superpixel level using a Multi-agent Reinforcement Learning framework to exploit the prior boundary knowledge and accelerate the learning process. Second, we design two rewards: classification score and intensity distribution reward, to avoid under- and over-segmentation, respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the residual errors and improve the segmentation performance. Extensively validated on a large dataset, our proposed approach achieves competitive performance and shows great potential to narrow the gap between fully-supervised and weakly-supervised learning.",0
"Breast ultrasound image nodule segmentation is vital yet challenging for accurate diagnosis and often involves tedious manual annotation. To streamline this process, weakly-supervised segmentation (WSS) techniques have been developed. In this study, we propose a new WSS framework named Flip Learning, which utilizes only box annotation. The classification tag is flipped by gradually erasing the target in the label box, and the erased region is considered as the segmentation result. Our approach has three key contributions. Firstly, we use a Multi-agent Reinforcement Learning framework to erase on a superpixel level, which exploits prior knowledge of boundaries and speeds up the learning process. Secondly, we designed two rewards to avoid under- and over-segmentation, the classification score and intensity distribution reward, respectively. Thirdly, we adopt a coarse-to-fine learning strategy to reduce residual errors and improve segmentation performance. Our proposed approach is extensively validated on a large dataset and achieves competitive performance, indicating great potential for reducing the gap between fully-supervised and weakly-supervised learning in breast ultrasound image nodule segmentation.",1
"This paper seeks to tackle the bin packing problem (BPP) through a learning perspective. Building on self-attention-based encoding and deep reinforcement learning algorithms, we propose a new end-to-end learning model for this task of interest. By decomposing the combinatorial action space, as well as utilizing a new training technique denoted as prioritized oversampling, which is a general scheme to speed up on-policy learning, we achieve state-of-the-art performance in a range of experimental settings. Moreover, although the proposed approach attend2pack targets offline-BPP, we strip our method down to the strict online-BPP setting where it is also able to achieve state-of-the-art performance. With a set of ablation studies as well as comparisons against a range of previous works, we hope to offer as a valid baseline approach to this field of study.",0
"This article aims to address the bin packing problem (BPP) from a learning perspective. We propose a novel end-to-end learning model for this task by utilizing self-attention-based encoding and deep reinforcement learning algorithms. To enhance the learning process, we decompose the combinatorial action space and employ a new training technique called prioritized oversampling, which accelerates on-policy learning and leads to state-of-the-art results in various experimental settings. Although our approach, attend2pack, focuses on offline-BPP, we demonstrate that it also outperforms previous works in the strict online-BPP setting. Through ablation studies and comparisons with prior research, we hope to present a reliable baseline approach to this area of research.",1
"Action-constrained reinforcement learning (RL) is a widely-used approach in various real-world applications, such as scheduling in networked systems with resource constraints and control of a robot with kinematic constraints. While the existing projection-based approaches ensure zero constraint violation, they could suffer from the zero-gradient problem due to the tight coupling of the policy gradient and the projection, which results in sample-inefficient training and slow convergence. To tackle this issue, we propose a learning algorithm that decouples the action constraints from the policy parameter update by leveraging state-wise Frank-Wolfe and a regression-based policy update scheme. Moreover, we show that the proposed algorithm enjoys convergence and policy improvement properties in the tabular case as well as generalizes the popular DDPG algorithm for action-constrained RL in the general case. Through experiments, we demonstrate that the proposed algorithm significantly outperforms the benchmark methods on a variety of control tasks.",0
"In real-world applications such as scheduling in networked systems with resource constraints and controlling robots with kinematic constraints, action-constrained reinforcement learning (RL) is commonly used. However, the current projection-based approaches, which guarantee zero constraint violation, may face the zero-gradient problem and thus result in inefficient training and slow convergence due to the close relationship between the policy gradient and the projection. To address this problem, we propose a learning algorithm that separates the action constraints from the policy parameter update by using state-wise Frank-Wolfe and a regression-based policy update scheme. Additionally, we demonstrate that our algorithm has convergence and policy improvement properties in both the tabular case and the general case, and it also enhances the popular DDPG algorithm for action-constrained RL. Our experiments show that our algorithm outperforms benchmark methods on a range of control tasks.",1
"Improving the sample efficiency in reinforcement learning has been a long-standing research problem. In this work, we aim to reduce the sample complexity of existing policy gradient methods. We propose a novel policy gradient algorithm called SRVR-PG, which only requires $O(1/\epsilon^{3/2})$ episodes to find an $\epsilon$-approximate stationary point of the nonconcave performance function $J(\boldsymbol{\theta})$ (i.e., $\boldsymbol{\theta}$ such that $\|\nabla J(\boldsymbol{\theta})\|_2^2\leq\epsilon$). This sample complexity improves the existing result $O(1/\epsilon^{5/3})$ for stochastic variance reduced policy gradient algorithms by a factor of $O(1/\epsilon^{1/6})$. In addition, we also propose a variant of SRVR-PG with parameter exploration, which explores the initial policy parameter from a prior probability distribution. We conduct numerical experiments on classic control problems in reinforcement learning to validate the performance of our proposed algorithms.",0
"For a long time, researchers have been trying to improve the sample efficiency in reinforcement learning. Our goal in this study is to decrease the sample complexity of current policy gradient methods. We have come up with a new policy gradient algorithm, SRVR-PG, which can locate an $\epsilon$-approximate stationary point of the nonconcave performance function $J(\boldsymbol{\theta})$ (i.e., $\boldsymbol{\theta}$ such that $\|\nabla J(\boldsymbol{\theta})\|_2^2\leq\epsilon$) in only $O(1/\epsilon^{3/2})$ episodes. This is an improvement compared to the previous result of $O(1/\epsilon^{5/3})$ for stochastic variance reduced policy gradient algorithms, by a factor of $O(1/\epsilon^{1/6})$. Additionally, we have introduced a variant of SRVR-PG with parameter exploration, which explores the initial policy parameter from a prior probability distribution. To validate our proposed algorithms, we have conducted numerical experiments on classic control problems in reinforcement learning.",1
"In adversarial environments, one side could gain an advantage by identifying the opponent's strategy. For example, in combat games, if an opponents strategy is identified as overly aggressive, one could lay a trap that exploits the opponent's aggressive nature. However, an opponent's strategy is not always apparent and may need to be estimated from observations of their actions. This paper proposes to use inverse reinforcement learning (IRL) to identify strategies in adversarial environments. Specifically, the contributions of this work are 1) the demonstration of this concept on gaming combat data generated from three pre-defined strategies and 2) the framework for using IRL to achieve strategy identification. The numerical experiments demonstrate that the recovered rewards can be identified using a variety of techniques. In this paper, the recovered reward are visually displayed, clustered using unsupervised learning, and classified using a supervised learner.",0
"In competitive situations, gaining insight into an opponent's strategy can provide a significant advantage. For example, in combat games, a player could exploit an overly aggressive opponent by setting a trap. However, identifying an opponent's strategy is not always straightforward and may require observation of their behavior. This paper proposes the use of inverse reinforcement learning (IRL) to identify strategies in adversarial environments. The contributions of this work include demonstrating the concept on gaming combat data which was generated from three pre-defined strategies, and providing a framework for using IRL to achieve strategy identification. Numerical experiments were conducted to show that recovered rewards can be identified using various techniques. This paper presents the recovered rewards visually, clusters them using unsupervised learning, and classifies them using a supervised learner.",1
"The performance of many medical image analysis tasks are strongly associated with image data quality. When developing modern deep learning algorithms, rather than relying on subjective (human-based) image quality assessment (IQA), task amenability potentially provides an objective measure of task-specific image quality. To predict task amenability, an IQA agent is trained using reinforcement learning (RL) with a simultaneously optimised task predictor, such as a classification or segmentation neural network. In this work, we develop transfer learning or adaptation strategies to increase the adaptability of both the IQA agent and the task predictor so that they are less dependent on high-quality, expert-labelled training data. The proposed transfer learning strategy re-formulates the original RL problem for task amenability in a meta-reinforcement learning (meta-RL) framework. The resulting algorithm facilitates efficient adaptation of the agent to different definitions of image quality, each with its own Markov decision process environment including different images, labels and an adaptable task predictor. Our work demonstrates that the IQA agents pre-trained on non-expert task labels can be adapted to predict task amenability as defined by expert task labels, using only a small set of expert labels. Using 6644 clinical ultrasound images from 249 prostate cancer patients, our results for image classification and segmentation tasks show that the proposed IQA method can be adapted using data with as few as respective 19.7% and 29.6% expert-reviewed consensus labels and still achieve comparable IQA and task performance, which would otherwise require a training dataset with 100% expert labels.",0
"The quality of medical image data greatly impacts the success of various analysis tasks. When developing deep learning algorithms, relying on subjective human-based image quality assessment (IQA) is not ideal. Instead, task amenability can serve as an objective measure of task-specific image quality. To predict task amenability, an IQA agent is trained using reinforcement learning (RL) alongside an optimised task predictor such as a classification or segmentation neural network. In this study, we have developed transfer learning strategies to increase the adaptability of both the IQA agent and the task predictor, reducing their dependence on high-quality, expert-labelled training data. Our proposed transfer learning strategy reformulates the RL problem for task amenability within a meta-reinforcement learning (meta-RL) framework. This results in an algorithm that efficiently adapts the agent to different definitions of image quality, each with its own Markov decision process environment, including different images, labels, and an adaptable task predictor. Our work demonstrates that pre-trained IQA agents on non-expert task labels can be adapted to predict task amenability as defined by expert task labels using only a small set of expert labels. With 6644 clinical ultrasound images from 249 prostate cancer patients, our results show that the proposed IQA method can achieve comparable IQA and task performance with as few as 19.7% and 29.6% expert-reviewed consensus labels, respectively, which would otherwise require a training dataset with 100% expert labels.",1
"Model free techniques have been successful at optimal control of complex systems at an expense of copious amounts of data and computation. However, it is often desired to obtain a control policy in a short period of time with minimal data use and computational burden. To this end, we make use of the NFQ algorithm for steering position control of a golf cart in both a real hardware and a simulated environment that was built from real-world interaction. The controller learns to apply a sequence of voltage signals in the presence of environmental uncertainties and inherent non-linearities that challenge the the control task. We were able to increase the rate of successful control under four minutes in simulation and under 11 minutes in real hardware.",0
"Although model free techniques have been effective in achieving optimal control of complex systems, they require significant amounts of data and computation. However, it is often desirable to obtain a control policy quickly and with minimal data usage and computational burden. To accomplish this, we utilized the NFQ algorithm to control the steering position of a golf cart in both a real hardware and a simulated environment that was created from real-world interactions. The controller learned to apply a sequence of voltage signals in the presence of environmental uncertainties and inherent non-linearities that posed challenges to the control task. We were able to increase the success rate of control to under four minutes in simulation and under 11 minutes in real hardware.",1
"Model-based reinforcement learning (MBRL) is believed to have much higher sample efficiency compared to model-free algorithms by learning a predictive model of the environment. However, the performance of MBRL highly relies on the quality of the learned model, which is usually built in a black-box manner and may have poor predictive accuracy outside of the data distribution. The deficiencies of the learned model may prevent the policy from being fully optimized. Although some uncertainty analysis-based remedies have been proposed to alleviate this issue, model bias still poses a great challenge for MBRL. In this work, we propose to leverage the prior knowledge of underlying physics of the environment, where the governing laws are (partially) known. In particular, we developed a physics-informed MBRL framework, where governing equations and physical constraints are utilized to inform the model learning and policy search. By incorporating the prior information of the environment, the quality of the learned model can be notably improved, while the required interactions with the environment are significantly reduced, leading to better sample efficiency and learning performance. The effectiveness and merit have been demonstrated over a handful of classic control problems, where the environments are governed by canonical ordinary/partial differential equations.",0
"It is believed that Model-based reinforcement learning (MBRL) is more efficient than model-free algorithms due to its ability to learn a predictive model of the environment. However, the quality of the learned model is crucial to the performance of MBRL, and it is typically built in a black-box manner, leading to poor predictive accuracy outside of the data distribution. This limitation can prevent policy optimization, despite proposed uncertainty analysis-based solutions. In this study, we propose using prior knowledge of the underlying physics of the environment to inform the model learning and policy search. Our physics-informed MBRL framework uses governing equations and physical constraints to improve the quality of the learned model and reduce interactions with the environment, resulting in better sample efficiency and learning performance. We demonstrate the effectiveness of our approach on several classic control problems governed by ordinary/partial differential equations.",1
"Reinforcement learning has recently shown promise as a technique for training an artificial neural network to parse sentences in some unknown format. A key aspect of this approach is that rather than explicitly inferring a grammar that describes the format, the neural network learns to perform various parsing actions (such as merging two tokens) over a corpus of sentences, with the goal of maximizing the total reward, which is roughly based on the estimated frequency of the resulting parse structures. This can allow the learning process to more easily explore different action choices, since a given choice may change the optimality of the parse (as expressed by the total reward), but will not result in the failure to parse a sentence. However, the approach also exhibits limitations: first, the neural network does not provide production rules for the grammar that it uses during parsing; second, because this neural network can successfully parse any sentence, it cannot be directly used to identify sentences that deviate from the format of the training sentences, i.e., that are anomalous. In this paper, we address these limitations by presenting procedures for extracting production rules from the neural network, and for using these rules to determine whether a given sentence is nominal or anomalous, when compared to structures observed within training data. In the latter case, an attempt is made to identify the location of the anomaly. Additionally, a two pass mechanism is presented for dealing with formats containing high-entropy information. We empirically evaluate the approach on artificial formats, demonstrating effectiveness, but also identifying limitations. By further improving parser learning, and leveraging rule extraction and anomaly detection, one might begin to understand common errors, either benign or malicious, in practical formats.",0
"The use of reinforcement learning has shown potential in training an artificial neural network to understand sentences in an unknown format. Instead of explicitly determining a grammar that describes the format, the neural network learns to perform various parsing actions to maximize the total reward based on the estimated frequency of resulting parse structures. This approach allows for easier exploration of different action choices, but has limitations as the neural network does not provide production rules for the grammar and cannot identify anomalous sentences. This paper addresses these limitations by presenting procedures for extracting production rules and determining whether a sentence is nominal or anomalous, as well as presenting a two-pass mechanism for dealing with high-entropy information. The approach is evaluated on artificial formats, demonstrating effectiveness and identifying limitations. By improving the parser learning and utilizing rule extraction and anomaly detection, it may be possible to understand common errors in practical formats.",1
"As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning.",0
"As questions addressed by Gaussian processes become more complex, it becomes increasingly difficult to find analytic solutions. Monte Carlo methods provide a convenient way to connect unmanageable mathematical expressions with practical estimates by means of sampling. Traditional methods for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This approach results in generative strategies that scale cubically in the size of the desired random vector, making them impractical for drawing high-dimensional vectors or continuous sample paths. In this study, we propose an alternative approach that focuses on Gaussian conditionals at the level of random variables rather than distributions. We demonstrate how this pathwise interpretation of conditioning generates a broad range of approximations that allow efficient sampling of Gaussian process posteriors. We derive these methods from first principles and analyze the approximation errors they introduce. We then investigate the practical implications of pathwise conditioning in various applied settings, including global optimization and reinforcement learning.",1
"High sample complexity remains a barrier to the application of reinforcement learning (RL), particularly in multi-agent systems. A large body of work has demonstrated that exploration mechanisms based on the principle of optimism under uncertainty can significantly improve the sample efficiency of RL in single agent tasks. This work seeks to understand the role of optimistic exploration in non-cooperative multi-agent settings. We will show that, in zero-sum games, optimistic exploration can cause the learner to waste time sampling parts of the state space that are irrelevant to strategic play, as they can only be reached through cooperation between both players. To address this issue, we introduce a formal notion of strategically efficient exploration in Markov games, and use this to develop two strategically efficient learning algorithms for finite Markov games. We demonstrate that these methods can be significantly more sample efficient than their optimistic counterparts.",0
"The application of reinforcement learning (RL) in multi-agent systems is hindered by the need for a high sample complexity. Previous research has shown that exploration mechanisms based on optimism under uncertainty can enhance the sample efficiency of RL in single agent tasks. This study aims to examine how optimistic exploration can be applied in non-cooperative multi-agent settings. Our findings reveal that in zero-sum games, optimistic exploration can lead to the learner wasting time exploring irrelevant parts of the state space that can only be accessed through cooperation between both players. To address this issue, we introduce the concept of strategically efficient exploration in Markov games and use it to develop two learning algorithms that are more sample efficient than their optimistic counterparts.",1
"In recent years, there have been many deep structures for Reinforcement Learning, mainly for value function estimation and representations. These methods achieved great success in Atari 2600 domain. In this paper, we propose an improved architecture based upon Dueling Networks, in this architecture, there are two separate estimators, one approximate the state value function and the other, state advantage function. This improvement based on Maximum Entropy, shows better policy evaluation compared to the original network and other value-based architectures in Atari domain.",0
"Over the past few years, Reinforcement Learning has seen numerous advancements in deep structures pertaining to value function estimation and representations. These methods have proven highly successful in the Atari 2600 domain. This paper introduces an enhanced architecture, which builds upon the Dueling Networks approach, featuring two separate estimators: one for approximating state value function and the other for state advantage function. By incorporating Maximum Entropy, this improvement demonstrates superior policy evaluation when compared to the original network and other value-based architectures in the Atari domain.",1
"Learning to adapt and make real-time informed decisions in a dynamic and complex environment is a challenging problem. Monopoly is a popular strategic board game that requires players to make multiple decisions during the game. Decision-making in Monopoly involves many real-world elements such as strategizing, luck, and modeling of opponent's policies. In this paper, we present novel representations for the state and action space for the full version of Monopoly and define an improved reward function. Using these, we show that our deep reinforcement learning agent can learn winning strategies for Monopoly against different fixed-policy agents. In Monopoly, players can take multiple actions even if it is not their turn to roll the dice. Some of these actions occur more frequently than others, resulting in a skewed distribution that adversely affects the performance of the learning agent. To tackle the non-uniform distribution of actions, we propose a hybrid approach that combines deep reinforcement learning (for frequent but complex decisions) with a fixed policy approach (for infrequent but straightforward decisions). Experimental results show that our hybrid agent outperforms a standard deep reinforcement learning agent by 30% in the number of games won against fixed-policy agents.",0
"Mastering the skill of making informed decisions in a dynamic and complex environment poses a great challenge. The popular board game, Monopoly, requires players to make multiple decisions during the game, including strategizing, accounting for luck, and anticipating the opponent's policies. This study introduces innovative representations for the state and action space for the full version of Monopoly and proposes an improved reward function. The research shows that the deep reinforcement learning agent can learn how to win against different fixed-policy agents. However, the distribution of actions in Monopoly is non-uniform, which negatively impacts the performance of the learning agent. To address this issue, a hybrid method combining deep reinforcement learning for frequent but complex decisions and a fixed-policy approach for infrequent but straightforward decisions is proposed. Experimental results demonstrate that the hybrid agent outperforms the standard deep reinforcement learning agent by 30% in the number of games won against fixed-policy agents.",1
"This paper investigates the motion planning of autonomous dynamical systems modeled by Markov decision processes (MDP) with unknown transition probabilities over continuous state and action spaces. Linear temporal logic (LTL) is used to specify high-level tasks over infinite horizon, which can be converted into a limit deterministic generalized B\""uchi automaton (LDGBA) with several accepting sets. The novelty is to design an embedded product MDP (EP-MDP) between the LDGBA and the MDP by incorporating a synchronous tracking-frontier function to record unvisited accepting sets of the automaton, and to facilitate the satisfaction of the accepting conditions. The proposed LDGBA-based reward shaping and discounting schemes for the model-free reinforcement learning (RL) only depend on the EP-MDP states and can overcome the issues of sparse rewards. Rigorous analysis shows that any RL method that optimizes the expected discounted return is guaranteed to find an optimal policy whose traces maximize the satisfaction probability. A modular deep deterministic policy gradient (DDPG) is then developed to generate such policies over continuous state and action spaces. The performance of our framework is evaluated via an array of OpenAI gym environments.",0
"The aim of this study is to examine the planning of self-governing dynamical systems that are represented by Markov decision processes (MDP), where the transition probabilities are unknown and the state and action spaces are continuous. For this purpose, linear temporal logic (LTL) is utilized to determine high-level tasks that can be transformed into a limit deterministic generalized B\""uchi automaton (LDGBA) with various accepting sets. The novelty of this research lies in the development of an embedded product MDP (EP-MDP) that connects the LDGBA and the MDP by utilizing a synchronous tracking-frontier function to track unexplored accepting sets and facilitate the fulfillment of accepting conditions. The proposed LDGBA-based reward shaping and discounting methods for the model-free reinforcement learning (RL) depend solely on the EP-MDP states and can address the issue of sparse rewards. The thorough analysis of the research shows that any RL method that optimizes the expected discounted return will generate an optimal policy that maximizes the satisfaction probability. To generate these policies, a modular deep deterministic policy gradient (DDPG) is devised that can function over continuous state and action spaces. The effectiveness of this framework is examined using a range of OpenAI gym environments.",1
"Reinforcement learning (RL) has shown a promising performance in learning optimal policies for a variety of sequential decision-making tasks. However, in many real-world RL problems, besides optimizing the main objectives, the agent is expected to satisfy a certain level of safety (e.g., avoiding collisions in autonomous driving). While RL problems are commonly formalized as Markov decision processes (MDPs), safety constraints are incorporated via constrained Markov decision processes (CMDPs). Although recent advances in safe RL have enabled learning safe policies in CMDPs, these safety requirements should be satisfied during both training and in the deployment process. Furthermore, it is shown that in memory-based and partially observable environments, these methods fail to maintain safety over unseen out-of-distribution observations. To address these limitations, we propose a Lyapunov-based uncertainty-aware safe RL model. The introduced model adopts a Lyapunov function that converts trajectory-based constraints to a set of local linear constraints. Furthermore, to ensure the safety of the agent in highly uncertain environments, an uncertainty quantification method is developed that enables identifying risk-averse actions through estimating the probability of constraint violations. Moreover, a Transformers model is integrated to provide the agent with memory to process long time horizons of information via the self-attention mechanism. The proposed model is evaluated in grid-world navigation tasks where safety is defined as avoiding static and dynamic obstacles in fully and partially observable environments. The results of these experiments show a significant improvement in the performance of the agent both in achieving optimality and satisfying safety constraints.",0
"The use of reinforcement learning (RL) has demonstrated impressive results in determining optimal policies for various decision-making tasks that occur sequentially. However, in real-world RL scenarios, it is expected that agents not only achieve primary objectives but also adhere to safety requirements, such as preventing collisions in autonomous driving. To incorporate safety constraints, constrained Markov decision processes (CMDPs) are utilized, which are an extension of Markov decision processes (MDPs). While safe RL has made progress in learning safe policies for CMDPs, it is crucial to satisfy safety requirements during both training and deployment. Additionally, existing methods struggle to maintain safety in partially observable and memory-based environments when exposed to new, out-of-distribution observations. In response, we propose an uncertainty-aware safe RL model that employs a Lyapunov function to convert trajectory-based constraints into a set of local linear constraints. To ensure agent safety in uncertain environments, our model uses an uncertainty quantification method to identify risk-averse actions by estimating the probability of constraint violations. Furthermore, we integrate a Transformers model to provide the agent with memory to process long-term information via the self-attention mechanism. We evaluate our model in grid-world navigation tasks that require avoiding static and dynamic obstacles in partially and fully observable environments. Our experiments demonstrate a significant improvement in agent performance regarding optimality and safety constraint satisfaction.",1
"Reinforcement learning (RL) is a technique to learn the control policy for an agent that interacts with a stochastic environment. In any given state, the agent takes some action, and the environment determines the probability distribution over the next state as well as gives the agent some reward. Most RL algorithms typically assume that the environment satisfies Markov assumptions (i.e. the probability distribution over the next state depends only on the current state). In this paper, we propose a model-based RL technique for a system that has non-Markovian dynamics. Such environments are common in many real-world applications such as in human physiology, biological systems, material science, and population dynamics. Model-based RL (MBRL) techniques typically try to simultaneously learn a model of the environment from the data, as well as try to identify an optimal policy for the learned model. We propose a technique where the non-Markovianity of the system is modeled through a fractional dynamical system. We show that we can quantify the difference in the performance of an MBRL algorithm that uses bounded horizon model predictive control from the optimal policy. Finally, we demonstrate our proposed framework on a pharmacokinetic model of human blood glucose dynamics and show that our fractional models can capture distant correlations on real-world datasets.",0
"Reinforcement learning is a method used to teach an agent how to control its actions when interacting with a stochastic environment. Whenever the agent is in a particular state, it takes an action, and the environment assigns a probability distribution to the subsequent state and provides the agent with a reward. The majority of RL algorithms assume that the environment satisfies Markov assumptions, where the probability distribution over the next state is dependent solely on the current state. Nonetheless, in this article, we present a model-based RL method for systems that have non-Markovian dynamics. Such environments are frequently encountered in real-world scenarios, such as human physiology, biological systems, material science, and population dynamics. Typically, model-based RL techniques aim to learn both a model of the environment from the data and an optimal policy for the learned model simultaneously. We suggest a method that models the non-Markovianity of the system through a fractional dynamical system. Furthermore, we demonstrate our proposed approach on a pharmacokinetic model of human blood glucose dynamics and prove that it can detect distant correlations in real-world datasets. We also show that we can calculate the difference between the performance of an MBRL algorithm that employs bounded horizon model predictive control and the optimal policy.",1
"We consider the problem of Reinforcement Learning for nonlinear stochastic dynamical systems. We show that in the RL setting, there is an inherent ``Curse of Variance"" in addition to Bellman's infamous ``Curse of Dimensionality"", in particular, we show that the variance in the solution grows factorial-exponentially in the order of the approximation. A fundamental consequence is that this precludes the search for anything other than ``local"" feedback solutions in RL, in order to control the explosive variance growth, and thus, ensure accuracy. We further show that the deterministic optimal control has a perturbation structure, in that the higher order terms do not affect the calculation of lower order terms, which can be utilized in RL to get accurate local solutions.",0
"The focus of our study is on Reinforcement Learning applied to nonlinear stochastic dynamical systems. Our findings reveal that the RL approach is subject to an inherent drawback known as the ""Curse of Variance"" in addition to the well-known ""Curse of Dimensionality"" coined by Bellman. Specifically, our research shows that the solution's variance increases at an exponential rate equivalent to the factorial of the approximation's order. This exponential growth impedes the search for solutions beyond ""local"" feedback solutions in RL. This is necessary to avoid the explosive variance growth and ensure precision. However, we also demonstrate that the deterministic optimal control has a perturbation structure. Higher-order terms do not influence the calculation of lower-order terms, which can be leveraged in RL to obtain accurate local solutions.",1
"In recent years, researchers have achieved great success in applying Deep Reinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games, creating strong autonomous agents that could defeat professional players in StarCraft~II. However, existing approaches to tackle full games have high computational costs, usually requiring the use of thousands of GPUs and CPUs for weeks. This paper has two main contributions to address this issue: 1) We introduce Gym-$\mu$RTS (pronounced ""gym-micro-RTS"") as a fast-to-run RL environment for full-game RTS research and 2) we present a collection of techniques to scale DRL to play full-game $\mu$RTS as well as ablation studies to demonstrate their empirical importance. Our best-trained bot can defeat every $\mu$RTS bot we tested from the past $\mu$RTS competitions when working in a single-map setting, resulting in a state-of-the-art DRL agent while only taking about 60 hours of training using a single machine (one GPU, three vCPU, 16GB RAM). See the blog post at https://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg and the source code at https://github.com/vwxyzjn/gym-microrts-paper",0
"Recent advancements in Deep Reinforcement Learning (DRL) have led to the development of powerful autonomous agents that can outperform even professional players in StarCraft~II. However, the high computational costs of existing approaches have made it necessary to use thousands of GPUs and CPUs for several weeks to tackle full games. This paper addresses this issue by introducing Gym-$\mu$RTS, a fast-to-run RL environment for full-game RTS research, and presenting a collection of techniques to scale DRL to play full-game $\mu$RTS. Our best-trained bot can defeat every $\mu$RTS bot we tested from previous competitions within a single-map setting, resulting in a state-of-the-art DRL agent that requires only 60 hours of training using a single machine (one GPU, three vCPU, 16GB RAM). For more information, please see our blog post at https://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg and the source code at https://github.com/vwxyzjn/gym-microrts-paper.",1
"Arrhythmia detection from ECG is an important research subject in the prevention and diagnosis of cardiovascular diseases. The prevailing studies formulate arrhythmia detection from ECG as a time series classification problem. Meanwhile, early detection of arrhythmia presents a real-world demand for early prevention and diagnosis. In this paper, we address a problem of cardiovascular disease early classification, which is a varied-length and long-length time series early classification problem as well. For solving this problem, we propose a deep reinforcement learning-based framework, namely Snippet Policy Network (SPN), consisting of four modules, snippet generator, backbone network, controlling agent, and discriminator. Comparing to the existing approaches, the proposed framework features flexible input length, solves the dual-optimization solution of the earliness and accuracy goals. Experimental results demonstrate that SPN achieves an excellent performance of over 80\% in terms of accuracy. Compared to the state-of-the-art methods, at least 7% improvement on different metrics, including the precision, recall, F1-score, and harmonic mean, is delivered by the proposed SPN. To the best of our knowledge, this is the first work focusing on solving the cardiovascular early classification problem based on varied-length ECG data. Based on these excellent features from SPN, it offers a good exemplification for addressing all kinds of varied-length time series early classification problems.",0
"Detecting arrhythmia from ECG is crucial in preventing and diagnosing cardiovascular diseases. Most studies consider this as a time series classification problem, but detecting arrhythmia early is also crucial for real-world prevention and diagnosis. This paper proposes a framework called Snippet Policy Network (SPN) using deep reinforcement learning to solve the varied-length and long-length time series early classification problem. SPN has four modules: snippet generator, backbone network, controlling agent, and discriminator. Compared to existing approaches, SPN has flexible input length and solves the dual-optimization problem of earliness and accuracy. Experimental results show that SPN achieves over 80% accuracy and at least 7% improvement compared to state-of-the-art methods. This is the first work to focus on the cardiovascular early classification problem based on varied-length ECG data. SPN is a good example for addressing various varied-length time series early classification problems.",1
"Although learning from data is effective and has achieved significant milestones, it has many challenges and limitations. Learning from data starts from observations and then proceeds to broader generalizations. This framework is controversial in science, yet it has achieved remarkable engineering successes. This paper reflects on some epistemological issues and some of the limitations of the knowledge discovered in data. The document discusses the common perception that getting more data is the key to achieving better machine learning models from theoretical and practical perspectives. The paper sheds some light on the shortcomings of using generic mathematical theories to describe the process. It further highlights the need for theories specialized in learning from data. While more data leverages the performance of machine learning models in general, the relation in practice is shown to be logarithmic at its best; After a specific limit, more data stabilize or degrade the machine learning models. Recent work in reinforcement learning showed that the trend is shifting away from data-oriented approaches and relying more on algorithms. The paper concludes that learning from data is hindered by many limitations. Hence an approach that has an intensional orientation is needed.",0
"Despite its significant achievements, learning from data presents numerous challenges and limitations. This approach begins with observations and then attempts to make broader generalizations, which has sparked controversy in the scientific community. Nonetheless, learning from data has led to impressive engineering successes. This paper addresses some epistemological issues and limitations of knowledge acquired through data. It challenges the common belief that obtaining more data is the key to improving machine learning models, exposing the inadequacy of generic mathematical theories in describing the process and the need for specialized theories. Although increasing data can enhance machine learning models' performance, the relationship is logarithmic at best, and beyond a certain point, more data stabilizes or even degrades the models. Recently, reinforcement learning research has shifted away from data-centric approaches and prioritized algorithms. The paper concludes that learning from data has several limitations and requires an intentional orientation to overcome them.",1
"Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning (DRL) via combining deep learning (DL) with reinforcement learning (RL), which has noticed that the distribution of the acquired data would change during the training process. DQN found this property might cause instability for training, so it proposed effective methods to handle the downside of the property. Instead of focusing on the unfavourable aspects, we find it critical for RL to ease the gap between the estimated data distribution and the ground truth data distribution while supervised learning (SL) fails to do so. From this new perspective, we extend the basic paradigm of RL called the Generalized Policy Iteration (GPI) into a more generalized version, which is called the Generalized Data Distribution Iteration (GDI). We see massive RL algorithms and techniques can be unified into the GDI paradigm, which can be considered as one of the special cases of GDI. We provide theoretical proof of why GDI is better than GPI and how it works. Several practical algorithms based on GDI have been proposed to verify the effectiveness and extensiveness of it. Empirical experiments prove our state-of-the-art (SOTA) performance on Arcade Learning Environment (ALE), wherein our algorithm has achieved 9620.98% mean human normalized score (HNS), 1146.39% median HNS and 22 human world record breakthroughs (HWRB) using only 200M training frames. Our work aims to lead the RL research to step into the journey of conquering the human world records and seek real superhuman agents on both performance and efficiency.",0
"The Deep Q Network (DQN) was the first to introduce the concept of deep reinforcement learning (DRL) by integrating deep learning (DL) with reinforcement learning (RL). However, DQN discovered that the distribution of acquired data would change during the training process, which could lead to instability. Rather than focusing on this drawback, our team believes it is crucial for RL to bridge the gap between estimated and ground truth data distributions, unlike supervised learning (SL). With this in mind, we developed the Generalized Data Distribution Iteration (GDI), an extension of the basic RL paradigm, Generalized Policy Iteration (GPI). We have demonstrated that GDI outperforms GPI both theoretically and practically, with our algorithm achieving state-of-the-art performance on the Arcade Learning Environment (ALE) with just 200M training frames. Our goal is to push RL research towards developing superhuman agents that can break human world records in both performance and efficiency.",1
"With large-scale integration of renewable generation and distributed energy resources (DERs), modern power systems are confronted with new operational challenges, such as growing complexity, increasing uncertainty, and aggravating volatility. Meanwhile, more and more data are becoming available owing to the widespread deployment of smart meters, smart sensors, and upgraded communication networks. As a result, data-driven control techniques, especially reinforcement learning (RL), have attracted surging attention in recent years. In this paper, we provide a tutorial on various RL techniques and how they can be applied to decision-making in power systems. We illustrate RL-based models and solutions in three key applications, frequency regulation, voltage control, and energy management. We conclude with three critical issues in the application of RL, i.e., safety, scalability, and data. Several potential future directions are discussed as well.",0
"Modern power systems are facing new operational challenges due to the large-scale integration of renewable generation and distributed energy resources (DERs). These challenges include increasing complexity, uncertainty, and volatility. Additionally, the widespread deployment of smart meters, smart sensors, and upgraded communication networks has resulted in the availability of more data. As a result, data-driven control techniques, particularly reinforcement learning (RL), have gained popularity in recent years. This paper aims to provide a tutorial on various RL techniques and their application in decision-making for power systems. The authors illustrate the use of RL-based models and solutions in three critical applications: frequency regulation, voltage control, and energy management. The paper concludes by addressing three crucial issues in the application of RL, namely safety, scalability, and data. Furthermore, the authors discuss several potential future directions.",1
"Validating the safety of autonomous systems generally requires the use of high-fidelity simulators that adequately capture the variability of real-world scenarios. However, it is generally not feasible to exhaustively search the space of simulation scenarios for failures. Adaptive stress testing (AST) is a method that uses reinforcement learning to find the most likely failure of a system. AST with a deep reinforcement learning solver has been shown to be effective in finding failures across a range of different systems. This approach generally involves running many simulations, which can be very expensive when using a high-fidelity simulator. To improve efficiency, we present a method that first finds failures in a low-fidelity simulator. It then uses the backward algorithm, which trains a deep neural network policy using a single expert demonstration, to adapt the low-fidelity failures to high-fidelity. We have created a series of autonomous vehicle validation case studies that represent some of the ways low-fidelity and high-fidelity simulators can differ, such as time discretization. We demonstrate in a variety of case studies that this new AST approach is able to find failures with significantly fewer high-fidelity simulation steps than are needed when just running AST directly in high-fidelity. As a proof of concept, we also demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art high-fidelity simulator for finding failures in autonomous vehicles.",0
"To ensure the safety of autonomous systems, it is common practice to use high-fidelity simulators that mimic real-world scenarios. However, searching the vast space of simulation scenarios for potential failures is often impractical. Adaptive stress testing (AST) is a reinforcement learning method that can effectively identify the most likely system failure. This process requires running numerous simulations, which can be costly when using a high-fidelity simulator. To increase efficiency, we propose a technique that first detects failures in a low-fidelity simulator and then adapts them to high-fidelity using the backward algorithm. We conducted autonomous vehicle validation case studies to demonstrate the differences between low-fidelity and high-fidelity simulators and showcase the effectiveness of this new AST approach. Our results show that this method requires significantly fewer high-fidelity simulation steps than traditional AST. As a proof of concept, we successfully applied AST to NVIDIA's DriveSim simulator, a state-of-the-art high-fidelity tool for detecting autonomous vehicle failures.",1
"Reinforcement learning (RL) promises to enable autonomous acquisition of complex behaviors for diverse agents. However, the success of current reinforcement learning algorithms is predicated on an often under-emphasised requirement -- each trial needs to start from a fixed initial state distribution. Unfortunately, resetting the environment to its initial state after each trial requires substantial amount of human supervision and extensive instrumentation of the environment which defeats the purpose of autonomous reinforcement learning. In this work, we propose Value-accelerated Persistent Reinforcement Learning (VaPRL), which generates a curriculum of initial states such that the agent can bootstrap on the success of easier tasks to efficiently learn harder tasks. The agent also learns to reach the initial states proposed by the curriculum, minimizing the reliance on human interventions into the learning. We observe that VaPRL reduces the interventions required by three orders of magnitude compared to episodic RL while outperforming prior state-of-the art methods for reset-free RL both in terms of sample efficiency and asymptotic performance on a variety of simulated robotics problems.",0
"The potential of Reinforcement Learning (RL) is to facilitate complex behaviors acquisition of various agents independently. However, the current RL algorithms are successful only if a fixed initial state distribution is followed in each trial, which is often overlooked. The issue is that resetting the environment to its initial state after every trial requires significant human supervision and environment instrumentation, contradicting the goal of autonomous RL. To address this problem, we propose Value-accelerated Persistent Reinforcement Learning (VaPRL), which creates a curriculum of initial states. This allows the agent to build on the success of simpler tasks to learn more complex tasks effectively. Additionally, the agent also learns to reach the initial states specified by the curriculum, reducing the need for human intervention in the learning process. Our results show that VaPRL decreases the interventions required by three orders of magnitude compared to episodic RL, while also outperforming previous state-of-the-art methods for reset-free RL in terms of sample efficiency and asymptotic performance on various simulated robotics problems.",1
"Very deep Convolutional Neural Networks (CNNs) have greatly improved the performance on various image restoration tasks. However, this comes at a price of increasing computational burden, hence limiting their practical usages. We observe that some corrupted image regions are inherently easier to restore than others since the distortion and content vary within an image. To leverage this, we propose Path-Restore, a multi-path CNN with a pathfinder that can dynamically select an appropriate route for each image region. We train the pathfinder using reinforcement learning with a difficulty-regulated reward. This reward is related to the performance, complexity and ""the difficulty of restoring a region"". A policy mask is further investigated to jointly process all the image regions. We conduct experiments on denoising and mixed restoration tasks. The results show that our method achieves comparable or superior performance to existing approaches with less computational cost. In particular, Path-Restore is effective for real-world denoising, where the noise distribution varies across different regions on a single image. Compared to the state-of-the-art RIDNet, our method achieves comparable performance and runs 2.7x faster on the realistic Darmstadt Noise Dataset.",0
"The performance of image restoration tasks has greatly improved with the use of deep Convolutional Neural Networks (CNNs). However, this improvement comes with the drawback of increasing the computational burden, making them less practical. It has been observed that some parts of an image are easier to restore than others, due to variations in content and distortion. To address this, we propose Path-Restore, a multi-path CNN that includes a pathfinder capable of dynamically selecting the best route for each image region. The pathfinder is trained using reinforcement learning, with a reward that takes into account the performance, complexity, and difficulty of restoring a region. We also explore the use of a policy mask to process all image regions together. Our experiments on denoising and mixed restoration tasks demonstrate that Path-Restore achieves comparable or even superior performance to existing approaches, with less computational cost. This method is particularly effective for real-world denoising, where the noise distribution varies across different regions within a single image. Compared to the state-of-the-art RIDNet, our method achieves comparable performance and runs 2.7 times faster on the Darmstadt Noise Dataset.",1
"Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of Reinforcement Learning due to the non-stationarity of the environments and the large dimensionality of the combined action space. Deep MARL algorithms have been applied to solve different task offloading problems. However, in real-world applications, information required by the agents (i.e. rewards and states) are subject to noise and alterations. The stability and the robustness of deep MARL to practical challenges is still an open research problem. In this work, we apply state-of-the art MARL algorithms to solve task offloading with reward uncertainty. We show that perturbations in the reward signal can induce decrease in the performance compared to learning with perfect rewards. We expect this paper to stimulate more research in studying and addressing the practical challenges of deploying deep MARL solutions in wireless communications systems.",0
"Reinforcement Learning is a field that faces difficulties in Multi-Agent Reinforcement Learning (MARL) due to the non-stationary nature of the environments and the extensive dimensionality of the action spaces when combined. Various Deep MARL algorithms have been utilized to solve task offloading problems; however, real-world applications are often subject to noise and alterations, which affects the agents' required information such as rewards and states. There is still a need for research to establish the stability and robustness of deep MARL to practical challenges. This study applies state-of-the-art MARL algorithms to solve task offloading issues with reward uncertainty. The findings reveal that perturbations in the reward signal result in decreased performance compared to learning with perfect rewards. We anticipate this research will encourage further investigation into studying and addressing the practical challenges of deploying deep MARL solutions in wireless communications systems.",1
"We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input single-output (MISO) system for downlink transmission. We compare the performance of Deep Reinforcement Learning (DRL) and conventional optimization methods in finding optimal phase shifts of the IRS elements to maximize the user signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of these methods to channel impairments and changes in the system. We demonstrate numerically that DRL solutions show more robustness to noisy channels and user mobility.",0
A MISO system is aided by an Intelligent Reflecting Surface (IRS) for downlink transmission. The optimal phase shifts of the IRS elements to maximize the user's signal-to-noise ratio are compared between Deep Reinforcement Learning (DRL) and conventional optimization methods. The robustness of these methods to channel impairments and system changes is also evaluated. Numerical demonstrations show that DRL solutions are more robust to user mobility and noisy channels.,1
"We focus on reinforcement learning (RL) in relational problems that are naturally defined in terms of objects, their relations, and manipulations. These problems are characterized by variable state and action spaces, and finding a fixed-length representation, required by most existing RL methods, is difficult, if not impossible. We present a deep RL framework based on graph neural networks and auto-regressive policy decomposition that naturally works with these problems and is completely domain-independent. We demonstrate the framework in three very distinct domains and we report the method's competitive performance and impressive zero-shot generalization over different problem sizes. In goal-oriented BlockWorld, we demonstrate multi-parameter actions with pre-conditions. In SysAdmin, we show how to select multiple objects simultaneously. In the classical planning domain of Sokoban, the method trained exclusively on 10x10 problems with three boxes solves 89% of 15x15 problems with five boxes.",0
"Our focus is on utilizing reinforcement learning (RL) to solve relational problems that involve objects, their relationships, and manipulations. These problems are characterized by varying state and action spaces, making it challenging, if not impossible, to find a fixed-length representation using existing RL methods. To address this, we have developed a deep RL framework that utilizes graph neural networks and auto-regressive policy decomposition, which is adaptable to various domains. We have tested the framework in three different domains and have demonstrated its competitive performance and remarkable ability to generalize zero-shot across different problem sizes. In the BlockWorld domain, we have demonstrated multi-parameter actions with pre-conditions. In the SysAdmin domain, we have shown how to select multiple objects simultaneously. Lastly, in the Sokoban classical planning domain, our method trained exclusively on 10x10 problems with three boxes can solve 89% of 15x15 problems with five boxes.",1
"We extend the framework of Classification with Costly Features (CwCF) that works with samples of fixed dimensions to trees of varying depth and breadth (similar to a JSON/XML file). In this setting, the sample is a tree - sets of sets of features. Individually for each sample, the task is to sequentially select informative features that help the classification. Each feature has a real-valued cost, and the objective is to maximize accuracy while minimizing the total cost. The process is modeled as an MDP where the states represent the acquired features, and the actions select unknown features. We present a specialized neural network architecture trained through deep reinforcement learning that naturally fits the data and directly selects features in the tree. We demonstrate our method in seven datasets and compare it to two baselines.",0
"Our work expands on the Classification with Costly Features (CwCF) framework, which is designed for fixed-dimension sample sets, by applying it to trees with varying depth and breadth, similar to a JSON/XML file. In this new setting, each sample is a tree composed of sets of features. The objective is to select informative features that aid in classification, with each feature incurring a real-valued cost. We aim to maximize accuracy while minimizing the total cost, employing an MDP model where states represent acquired features, and actions select unknown features. To tackle this problem, we propose a specialized neural network architecture, trained via deep reinforcement learning, which seamlessly fits the data and directly picks features from the tree. Our approach is evaluated on seven datasets, and we compare its performance to two baselines.",1
"We consider model-free reinforcement learning (RL) in non-stationary Markov decision processes. Both the reward functions and the state transition functions are allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain variation budgets. We propose Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free algorithm for non-stationary RL, and show that it outperforms existing solutions in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret bound of $\widetilde{O}(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H T^{\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions, respectively, $\Delta>0$ is the variation budget, $H$ is the number of time steps per episode, and $T$ is the total number of time steps. We further present a parameter-free algorithm named Double-Restart Q-UCB that does not require prior knowledge of the variation budget. We show that our algorithms are \emph{nearly optimal} by establishing an information-theoretical lower bound of $\Omega(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H^{\frac{2}{3}} T^{\frac{2}{3}})$, the first lower bound in non-stationary RL. Numerical experiments validate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency. We demonstrate the power of our results in examples of multi-agent RL and inventory control across related products.",0
"Our focus is on non-stationary Markov decision processes, where both the state transition and reward functions can change over time, provided their cumulative variations do not exceed certain budgets. To address this problem, we propose Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), a model-free algorithm that outperforms existing solutions in terms of dynamic regret. Using Freedman-type bonus terms, RestartQ-UCB achieves a dynamic regret bound of $\widetilde{O}(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H T^{\frac{2}{3}})$. We also introduce a parameter-free algorithm called Double-Restart Q-UCB that does not require prior knowledge of the variation budget. Our algorithms are nearly optimal, as we establish an information-theoretical lower bound of $\Omega(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H^{\frac{2}{3}} T^{\frac{2}{3}})$, the first lower bound in non-stationary RL. Numerical experiments demonstrate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency, and we illustrate the applicability of our results through examples of multi-agent RL and inventory control across related products.",1
"Reinforcement Learning (RL) requires a large amount of exploration especially in sparse-reward settings. Imitation Learning (IL) can learn from expert demonstrations without exploration, but it never exceeds the expert's performance and is also vulnerable to distributional shift between demonstration and execution. In this paper, we radically unify RL and IL based on Free Energy Principle (FEP). FEP is a unified Bayesian theory of the brain that explains perception, action and model learning by a common fundamental principle. We present a theoretical extension of FEP and derive an algorithm in which an agent learns the world model that internalizes expert demonstrations and at the same time uses the model to infer the current and future states and actions that maximize rewards. The algorithm thus reduces exploration costs by partially imitating experts as well as maximizing its return in a seamless way, resulting in a higher performance than the suboptimal expert. Our experimental results show that this approach is promising in visual control tasks especially in sparse-reward environments.",0
"Exploration is a major requirement for Reinforcement Learning (RL), particularly in settings with sparse rewards. On the other hand, Imitation Learning (IL) can learn from expert demonstrations without any exploration, but it has limitations such as being incapable of surpassing the performance of the expert and being susceptible to distributional shifts between demonstration and execution. This paper proposes a novel approach that unifies RL and IL using the Free Energy Principle (FEP), which is a unified Bayesian theory of the brain that explains perception, action, and model learning based on a common fundamental principle. The authors introduce a theoretical extension of FEP and develop an algorithm that enables an agent to learn the world model by internalizing expert demonstrations while simultaneously inferring present and future states and actions that maximize rewards. This approach reduces exploration costs by partially imitating experts and maximizes performance by achieving seamless returns, which are higher than the suboptimal expert. The experimental results demonstrate that this approach is promising for visual control tasks, particularly in environments with sparse rewards.",1
"Maximum Entropy (MaxEnt) reinforcement learning is a powerful learning paradigm which seeks to maximize return under entropy regularization. However, action entropy does not necessarily coincide with state entropy, e.g., when multiple actions produce the same transition. Instead, we propose to maximize the transition entropy, i.e., the entropy of next states. We show that transition entropy can be described by two terms; namely, model-dependent transition entropy and action redundancy. Particularly, we explore the latter in both deterministic and stochastic settings and develop tractable approximation methods in a near model-free setup. We construct algorithms to minimize action redundancy and demonstrate their effectiveness on a synthetic environment with multiple redundant actions as well as contemporary benchmarks in Atari and Mujoco. Our results suggest that action redundancy is a fundamental problem in reinforcement learning.",0
"MaxEnt reinforcement learning is an effective learning approach that aims to maximize return with entropy regularization. However, state entropy and action entropy may not always align, such as in situations where various actions produce the same transition. Instead, our proposal is to enhance transition entropy, which pertains to the entropy of next states. We have identified two components of transition entropy, namely model-dependent transition entropy and action redundancy. Our focus is on the latter and we have developed useful approximation techniques in a nearly model-free setting for both deterministic and stochastic scenarios. We have created algorithms that reduce action redundancy and have tested them on synthetic environments with multiple redundant actions, as well as modern benchmarks such as Atari and Mujoco. Our findings indicate that action redundancy is a key issue in reinforcement learning.",1
"We contribute to micro-data model-based reinforcement learning (MBRL) by rigorously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counterparts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered",0
"Our study in micro-data model-based reinforcement learning (MBRL) involves a thorough comparison of different generative models using a fixed (random shooting) control agent. Our results show that mixture density nets outperform other models by a significant margin when dealing with environments that require multimodal posterior predictives. However, when multimodality is not a factor, deterministic models perform just as well as their probabilistic counterparts, with consistent but non-significant improvements. Additionally, we discovered that incorporating heteroscedasticity during training can improve predictions at longer horizons, acting as a useful regularizer. To enhance the evaluation of various models, we created metrics and a experimental protocol that can predict their asymptotic performance on control problems. Our findings led us to improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule outside of the standard hyperparameter interval.",1
"The policy gradient (PG) is one of the most popular methods for solving reinforcement learning (RL) problems. However, a solid theoretical understanding of even the ""vanilla"" PG has remained elusive for long time. In this paper, we apply recent tools developed for the analysis of SGD in non-convex optimization to obtain convergence guarantees for both REINFORCE and GPOMDP under smoothness assumption on the objective function and weak conditions on the second moment of the norm of the estimated gradient. When instantiated under common assumptions on the policy space, our general result immediately recovers existing $\widetilde{\mathcal{O}}(\epsilon^{-4})$ sample complexity guarantees, but for wider ranges of parameters (e.g., step size and batch size $m$) with respect to previous literature. Notably, our result includes the single trajectory case (i.e., $m=1$) and it provides a more accurate analysis of the dependency on problem-specific parameters by fixing previous results available in the literature. We believe that the integration of state-of-the-art tools from non-convex optimization may lead to identify a much broader range of problems where PG methods enjoy strong theoretical guarantees.",0
"Although the policy gradient (PG) is a popular method for solving reinforcement learning (RL) problems, understanding the ""vanilla"" PG has been difficult. In this study, we use recent tools developed for analyzing stochastic gradient descent (SGD) in non-convex optimization to derive convergence guarantees for both REINFORCE and GPOMDP under specific conditions. Our general result recovers existing sample complexity guarantees, but for broader ranges of parameters, including the single trajectory case. Additionally, our analysis provides a more precise understanding of the dependency on problem-specific parameters than previous literature. By utilizing advanced tools from non-convex optimization, we hope to identify more problems where PG methods have strong theoretical guarantees.",1
"Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at https://github.com/expectorlin/DR-Attacker.",0
"The role of language instruction is crucial in tasks involving natural language-based navigation. However, navigators who have been trained with limited human-annotated instructions may struggle to accurately comprehend complex instructions at different stages, which can result in poor navigation performance. To address this issue, we propose a method for training a more resilient navigator that can dynamically extract essential information from lengthy instructions using an adversarial attacking approach. Our Dynamic Reinforced Instruction Attacker (DR-Attacker) misleads the navigator by destroying the most informative information in instructions at different stages. DR-Attacker generates perturbed instructions sequentially during navigation, optimizing the reinforcement learning algorithm to formulate perturbation generation as a Markov Decision Process. These perturbed instructions serve as difficult examples, improving the navigator's robustness with effective adversarial training and an auxiliary self-supervised reasoning task. Our experimental results on Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks demonstrate the superiority of our method over existing approaches. Visualization analysis validates the efficacy of DR-Attacker in successfully attacking critical information in instructions at different stages. The code is available at https://github.com/expectorlin/DR-Attacker.",1
"Reinforcement learning (RL) can be used to learn treatment policies and aid decision making in healthcare. However, given the need for generalization over complex state/action spaces, the incorporation of function approximators (e.g., deep neural networks) requires model selection to reduce overfitting and improve policy performance at deployment. Yet a standard validation pipeline for model selection requires running a learned policy in the actual environment, which is often infeasible in a healthcare setting. In this work, we investigate a model selection pipeline for offline RL that relies on off-policy evaluation (OPE) as a proxy for validation performance. We present an in-depth analysis of popular OPE methods, highlighting the additional hyperparameters and computational requirements (fitting/inference of auxiliary models) when used to rank a set of candidate policies. We compare the utility of different OPE methods as part of the model selection pipeline in the context of learning to treat patients with sepsis. Among all the OPE methods we considered, fitted Q evaluation (FQE) consistently leads to the best validation ranking, but at a high computational cost. To balance this trade-off between accuracy of ranking and computational efficiency, we propose a simple two-stage approach to accelerate model selection by avoiding potentially unnecessary computation. Our work serves as a practical guide for offline RL model selection and can help RL practitioners select policies using real-world datasets. To facilitate reproducibility and future extensions, the code accompanying this paper is available online at https://github.com/MLD3/OfflineRL_ModelSelection.",0
"In healthcare, reinforcement learning (RL) is useful for determining treatment policies and aiding in decision-making. However, incorporating function approximators like deep neural networks requires model selection to prevent overfitting and improve policy performance during deployment. The standard validation pipeline for model selection involves running a learned policy in the actual environment, which is often impossible in healthcare settings. This paper explores a model selection pipeline for offline RL that relies on off-policy evaluation (OPE) as a proxy for validation performance. The authors analyze popular OPE methods, highlighting the additional hyperparameters and computational requirements when ranking a set of candidate policies. Fitted Q evaluation consistently provides the best validation ranking, although it is computationally expensive. To balance accuracy and efficiency, the authors propose a two-stage approach to accelerate model selection. This work serves as a practical guide for offline RL model selection and can help RL practitioners select policies using real-world datasets. The code for this study is available online to facilitate reproducibility and future extensions.",1
"From the ad network standpoint, a user's activity is a multi-type sequence of temporal events consisting of event types and time intervals. Understanding user patterns in ad networks has received increasing attention from the machine learning community. Particularly, the problems of fraud detection, Conversion Rate (CVR), and Click-Through Rate (CTR) prediction are of interest. However, the class imbalance between major and minor classes in these tasks can bias a machine learning model leading to poor performance. This study proposes using two multi-type (continuous and discrete) training approaches for GANs to deal with the limitations of traditional GANs in passing the gradient updates for discrete tokens. First, we used the Reinforcement Learning (RL)-based training approach and then, an approximation of the multinomial distribution parameterized in terms of the softmax function (Gumble-Softmax). Our extensive experiments based on synthetic data have shown the trained generator can generate sequences with desired properties measured by multiple criteria.",0
"The sequence of events and time intervals that make up a user's activity is considered a multi-type sequence from the perspective of ad networks. The machine learning community has taken an interest in understanding user patterns in ad networks, particularly in relation to fraud detection, Conversion Rate (CVR), and Click-Through Rate (CTR) prediction. However, the significant class imbalance between major and minor classes in these tasks can cause machine learning models to perform poorly. This study proposes using two multi-type (continuous and discrete) training approaches for GANs to address the limitations of traditional GANs in passing gradient updates for discrete tokens. Specifically, the Reinforcement Learning (RL)-based training approach and an approximation of the multinomial distribution parameterized in terms of the softmax function (Gumble-Softmax) were utilized. Results from extensive experiments on synthetic data indicate that the trained generator can produce sequences with desired properties based on multiple criteria.",1
"Intrinsic rewards are commonly applied to improve exploration in reinforcement learning. However, these approaches suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains separate policies for exploration and exploitation. DeRL can be applied with on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. We show that DeRL is more robust to scaling and speed of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically motivated baselines in fewer interactions.",0
"To enhance exploration in reinforcement learning, intrinsic rewards are often utilized. However, these methods are prone to instability due to non-stationary reward shaping and a strong reliance on hyperparameters. The proposed solution in this study is Decoupled RL (DeRL), which trains distinct policies for exploration and exploitation. DeRL can be incorporated into on-policy and off-policy RL algorithms, and we assessed its effectiveness in two sparse-reward settings with various types of intrinsic rewards. Our findings indicate that DeRL is more robust than intrinsically motivated baselines in terms of intrinsic reward scaling and decay rate, and achieves comparable evaluation returns with fewer interactions.",1
"First-order methods for quadratic optimization such as OSQP are widely used for large-scale machine learning and embedded optimal control, where many related problems must be rapidly solved. These methods face two persistent challenges: manual hyperparameter tuning and convergence time to high-accuracy solutions. To address these, we explore how Reinforcement Learning (RL) can learn a policy to tune parameters to accelerate convergence. In experiments with well-known QP benchmarks we find that our RL policy, RLQP, significantly outperforms state-of-the-art QP solvers by up to 3x. RLQP generalizes surprisingly well to previously unseen problems with varying dimension and structure from different applications, including the QPLIB, Netlib LP and Maros-Meszaros problems. Code for RLQP is available at https://github.com/berkeleyautomation/rlqp.",0
"OSQP, a first-order method utilized for quadratic optimization, is commonly employed for large-scale machine learning and embedded optimal control to solve multiple problems quickly. However, these methods encounter two persistent issues: the need for manual hyperparameter adjustment and the time required to achieve high-accuracy solutions. To address these challenges, we investigate how Reinforcement Learning (RL) can develop a policy to adjust parameters for faster convergence. Our RL policy, RLQP, proves to outperform current QP solvers up to 3x in well-known QP benchmarks. Additionally, RLQP generalizes exceptionally well to new problems with varying dimensions and structures from various applications such as QPLIB, Netlib LP, and Maros-Meszaros problems. Those interested in RLQP code can access it at https://github.com/berkeleyautomation/rlqp.",1
"State-of-the-art reinforcement learning algorithms mostly rely on being allowed to directly interact with their environment to collect millions of observations. This makes it hard to transfer their success to industrial control problems, where simulations are often very costly or do not exist, and exploring in the real environment can potentially lead to catastrophic events. Recently developed, model-free, offline RL algorithms, can learn from a single dataset (containing limited exploration) by mitigating extrapolation error in value functions. However, the robustness of the training process is still comparatively low, a problem known from methods using value functions. To improve robustness and stability of the learning process, we use dynamics models to assess policy performance instead of value functions, resulting in MOOSE (MOdel-based Offline policy Search with Ensembles), an algorithm which ensures low model bias by keeping the policy within the support of the data. We compare MOOSE with state-of-the-art model-free, offline RL algorithms { BRAC,} BEAR and BCQ on the Industrial Benchmark and MuJoCo continuous control tasks in terms of robust performance, and find that MOOSE outperforms its model-free counterparts in almost all considered cases, often even by far.",0
"Cutting-edge reinforcement learning algorithms typically rely on direct interaction with the environment to gather vast amounts of data. Unfortunately, this approach doesn't translate well to industrial control problems, where simulations can be prohibitively expensive or nonexistent. Real-world exploration can also be risky and lead to catastrophic outcomes. Recently, model-free, offline RL algorithms have emerged that can learn from limited exploration, but their training process is still relatively unstable. To address this problem, we've developed MOOSE, a model-based offline policy search algorithm that leverages dynamics models to evaluate policy performance instead of value functions. MOOSE ensures low model bias and outperforms its model-free counterparts in most cases, often by a wide margin, as demonstrated by our comparisons with other algorithms on the Industrial Benchmark and MuJoCo continuous control tasks.",1
"This paper presents an inverse reinforcement learning (IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function; then we construct set valued estimates of the cost function. To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. To illustrate our IRL scheme,we consider two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. Finally, for finite datasets, we propose an IRL detection algorithm and give finite sample bounds on its error probabilities. Also we discuss how to identify $\epsilon$-optimal Bayesian decision makers and perform IRL.",0
"This paper introduces a framework for inverse reinforcement learning (IRL) in Bayesian stopping time problems. Our approach involves observing the actions of a Bayesian decision maker and determining if they align with an optimized cost function. We then create a set of estimates for the cost function. Our IRL method uses innovative concepts from Bayesian revealed preferences in microeconomics. We demonstrate our IRL scheme with two critical examples of stopping time problems: sequential hypothesis testing and Bayesian search. Additionally, we propose an IRL detection algorithm for finite datasets and provide finite sample bounds for its error probabilities. We also explore how to identify $\epsilon$-optimal Bayesian decision makers and implement IRL.",1
"We study how an offline dataset of prior (possibly random) experience can be used to address two challenges that autonomous systems face when they endeavor to learn from, adapt to, and collaborate with humans : (1) identifying the human's intent and (2) safely optimizing the autonomous system's behavior to achieve this inferred intent. First, we use the offline dataset to efficiently infer the human's reward function via pool-based active preference learning. Second, given this learned reward function, we perform offline reinforcement learning to optimize a policy based on the inferred human intent. Crucially, our proposed approach does not require actual physical rollouts or an accurate simulator for either the reward learning or policy optimization steps, enabling both safe and efficient apprenticeship learning. We identify and evaluate our approach on a subset of existing offline RL benchmarks that are well suited for offline reward learning and also evaluate extensions of these benchmarks which allow more open-ended behaviors. Our experiments show that offline preference-based reward learning followed by offline reinforcement learning enables efficient and high-performing policies, while only requiring small numbers of preference queries. Videos available at https://sites.google.com/view/offline-prefs.",0
"The focus of our study is on how autonomous systems can overcome two challenges when interacting with humans: determining the human's intent and optimizing the autonomous system's behavior to achieve that intent. To address these challenges, we use an offline dataset of prior experiences, which may include random data. The first step is to infer the human's reward function using pool-based active preference learning, which is made possible by the offline dataset. Once the reward function is learned, we perform offline reinforcement learning to optimize the policy based on the inferred human intent. This approach does not require physical rollouts or an accurate simulator, making it both safe and efficient for apprenticeship learning. We evaluate our approach on a subset of existing offline RL benchmarks and also test extensions that allow for more open-ended behaviors. Our experiments demonstrate that this method of offline preference-based reward learning followed by offline reinforcement learning leads to efficient and high-performing policies with minimal preference queries. Videos of our findings are available at https://sites.google.com/view/offline-prefs.",1
"Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task demonstrations. Prior approaches for demonstration-guided RL treat every new task as an independent learning problem and attempt to follow the provided demonstrations step-by-step, akin to a human trying to imitate a completely unseen behavior by following the demonstrator's exact muscle movements. Naturally, such learning will be slow, but often new behaviors are not completely unseen: they share subtasks with behaviors we have previously learned. In this work, we aim to exploit this shared subtask structure to increase the efficiency of demonstration-guided RL. We first learn a set of reusable skills from large offline datasets of prior experience collected across many tasks. We then propose Skill-based Learning with Demonstrations (SkiLD), an algorithm for demonstration-guided RL that efficiently leverages the provided demonstrations by following the demonstrated skills instead of the primitive actions, resulting in substantial performance improvements over prior demonstration-guided RL approaches. We validate the effectiveness of our approach on long-horizon maze navigation and complex robot manipulation tasks.",0
"Demonstration-guided reinforcement learning (RL) is a promising method of learning complex behaviors using both reward feedback and target task demonstrations. Previous approaches treated each new task as a separate learning problem and attempted to follow demonstrations step-by-step, resulting in slow learning. However, new behaviors often share subtasks with previously learned behaviors. This study aims to exploit this shared subtask structure by learning reusable skills from prior experience datasets and proposing Skill-based Learning with Demonstrations (SkiLD), an algorithm that follows demonstrated skills instead of primitive actions, resulting in significant performance improvements. The effectiveness of SkiLD is validated on long-horizon maze navigation and complex robot manipulation tasks.",1
"This paper introduces the offline meta-reinforcement learning (offline meta-RL) problem setting and proposes an algorithm that performs well in this setting. Offline meta-RL is analogous to the widely successful supervised learning strategy of pre-training a model on a large batch of fixed, pre-collected data (possibly from various tasks) and fine-tuning the model to a new task with relatively little data. That is, in offline meta-RL, we meta-train on fixed, pre-collected data from several tasks in order to adapt to a new task with a very small amount (less than 5 trajectories) of data from the new task. By nature of being offline, algorithms for offline meta-RL can utilize the largest possible pool of training data available and eliminate potentially unsafe or costly data collection during meta-training. This setting inherits the challenges of offline RL, but it differs significantly because offline RL does not generally consider a) transfer to new tasks or b) limited data from the test task, both of which we face in offline meta-RL. Targeting the offline meta-RL setting, we propose Meta-Actor Critic with Advantage Weighting (MACAW), an optimization-based meta-learning algorithm that uses simple, supervised regression objectives for both the inner and outer loop of meta-training. On offline variants of common meta-RL benchmarks, we empirically find that this approach enables fully offline meta-reinforcement learning and achieves notable gains over prior methods.",0
"The paper presents the offline meta-reinforcement learning (meta-RL) problem and proposes an efficient algorithm for this concept. The offline meta-RL approach is similar to the supervised learning approach of pre-training a model on fixed, pre-collected data from various tasks and fine-tuning the model to a new task with minimal data. In offline meta-RL, the meta-training is done on a batch of fixed, pre-collected data from multiple tasks to adapt to a new task with less than 5 trajectories of new data. Offline meta-RL allows the use of the largest pool of training data available and avoids unsafe or expensive data collection during meta-training. While offline RL faces challenges, offline meta-RL faces additional challenges such as transfer to new tasks and limited data from the test task. To address this, the paper proposes an optimization-based meta-learning algorithm called Meta-Actor Critic with Advantage Weighting (MACAW) that uses simple, supervised regression objectives for both the inner and outer loops of meta-training. The empirical results on offline meta-RL benchmarks show that MACAW enables fully offline meta-reinforcement learning and outperforms existing methods.",1
"Many transfer problems require re-using previously optimal decisions for solving new tasks, which suggests the need for learning algorithms that can modify the mechanisms for choosing certain actions independently of those for choosing others. However, there is currently no formalism nor theory for how to achieve this kind of modular credit assignment. To answer this question, we define modular credit assignment as a constraint on minimizing the algorithmic mutual information among feedback signals for different decisions. We introduce what we call the modularity criterion for testing whether a learning algorithm satisfies this constraint by performing causal analysis on the algorithm itself. We generalize the recently proposed societal decision-making framework as a more granular formalism than the Markov decision process to prove that for decision sequences that do not contain cycles, certain single-step temporal difference action-value methods meet this criterion while all policy-gradient methods do not. Empirical evidence suggests that such action-value methods are more sample efficient than policy-gradient methods on transfer problems that require only sparse changes to a sequence of previously optimal decisions.",0
"The need for learning algorithms that can modify the mechanisms for choosing certain actions independently of others arises in many transfer problems. However, there is currently no theory or formalism for achieving modular credit assignment. To address this, we define modular credit assignment as a constraint on minimizing algorithmic mutual information among feedback signals for different decisions. We introduce the modularity criterion to test whether a learning algorithm satisfies this constraint by performing causal analysis on the algorithm. We use the societal decision-making framework to prove that certain single-step temporal difference action-value methods meet this criterion for decision sequences without cycles, while policy-gradient methods do not. Empirical evidence suggests that such action-value methods are more sample efficient than policy-gradient methods for transfer problems requiring sparse changes to previously optimal decisions.",1
"Zeroth-order (ZO) optimization is widely used to handle challenging tasks, such as query-based black-box adversarial attacks and reinforcement learning. Various attempts have been made to integrate prior information into the gradient estimation procedure based on finite differences, with promising empirical results. However, their convergence properties are not well understood. This paper makes an attempt to fill this gap by analyzing the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. We provide a convergence guarantee for the prior-guided random gradient-free (PRGF) algorithms. Moreover, to further accelerate over greedy descent methods, we present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. Finally, our theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks.",0
"The utilization of zeroth-order (ZO) optimization is widespread for handling difficult tasks, including reinforcement learning and query-based black-box adversarial attacks. Efforts have been made to integrate prior knowledge into the gradient estimation process utilizing finite differences, yielding promising empirical outcomes, yet their convergence properties remain unclear. This study aims to bridge this gap by examining the convergence of prior-guided ZO algorithms within a greedy descent framework employing various gradient estimators. We provide a convergence guarantee for the prior-guided random gradient-free (PRGF) algorithms. Additionally, to enhance over greedy descent methods, we propose an accelerated random search (ARS) algorithm that integrates prior knowledge, along with a convergence analysis. Lastly, our theoretical findings are verified by conducting experiments on various numerical benchmarks and adversarial attacks.",1
"Communication between agents in collaborative multi-agent settings is in general implicit or a direct data stream. This paper considers text-based natural language as a novel form of communication between multiple agents trained with reinforcement learning. This could be considered first steps toward a truly autonomous communication without the need to define a limited set of instructions, and natural collaboration between humans and robots. Inspired by the game of Blind Leads, we propose an environment where one agent uses natural language instructions to guide another through a maze. We test the ability of reinforcement learning agents to effectively communicate through discrete word-level symbols and show that the agents are able to sufficiently communicate through natural language with a limited vocabulary. Although the communication is not always perfect English, the agents are still able to navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of 0.61 over randomly generated sequences while maintaining a 100% maze completion rate. This is a 3.5 times the performance of the random baseline using our reference set.",0
"This article explores the idea of using natural language as a form of communication between agents in collaborative multi-agent settings. Instead of relying on implicit or direct data streams, the use of text-based communication could be a step towards autonomous communication without the need for limited sets of instructions. The study proposes an environment where one agent guides another through a maze using natural language instructions, inspired by the game of Blind Leads. The research finds that reinforcement learning agents can effectively communicate using discrete word-level symbols, even with a limited vocabulary. Although the language used might not be perfect, the agents are still able to navigate the maze. The study achieves a BLEU score of 0.85, which is a significant improvement over randomly generated sequences, with a 100% maze completion rate. This represents a 3.5 times increase in performance compared to the random baseline using the reference set.",1
"Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as ""one big sequence modeling"" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",0
"The typical focus of reinforcement learning (RL) is on determining single-step policies or models, which involves using the Markov property to break down the problem over time. However, it is also possible to view RL as a sequence modeling challenge, with the objective of predicting a series of actions that lead to a sequence of high rewards. This perspective raises the possibility of leveraging powerful sequence prediction models from other domains, such as natural-language processing, to tackle the RL problem in a straightforward and effective way. To explore this idea, we propose to reframe RL as a ""one big sequence modeling"" challenge, utilizing state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. This approach simplifies several design choices, such as the need for separate behavior policy constraints and epistemic uncertainty estimators. Our experiments demonstrate that this framework is flexible and effective across a range of RL tasks, including long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",1
"We present Megaverse, a new 3D simulation platform for reinforcement learning and embodied AI research. The efficient design of our engine enables physics-based simulation with high-dimensional egocentric observations at more than 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to 70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive objects. We achieve this high simulation performance by leveraging batched simulation, thereby taking full advantage of the massive parallelism of modern GPUs. We use Megaverse to build a new benchmark that consists of several single-agent and multi-agent tasks covering a variety of cognitive challenges. We evaluate model-free RL on this benchmark to provide baselines and facilitate future research. The source code is available at https://www.megaverse.info",0
"Our team has developed Megaverse, a novel 3D simulation platform that is ideal for research on reinforcement learning and embodied AI. Our engine is designed to be highly efficient, allowing for physics-based simulation with egocentric observations that exceed 1,000,000 actions per second on a single 8-GPU node. In fully-shaded 3D scenes with interactive objects, Megaverse is up to 70 times faster than DeepMind Lab. This remarkable simulation performance is achieved by using batched simulation and capitalizing on the parallelism of modern GPUs. Megaverse also includes a new benchmark consisting of several single-agent and multi-agent tasks that present various cognitive challenges. We used Megaverse to assess model-free RL on this benchmark, which establishes a baseline and supports future research. The source code is available at https://www.megaverse.info.",1
"Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks.",0
"In order to achieve sample efficient Reinforcement Learning (RL) in image-based environments, it is important to learn effective representations. However, in RL, representation learning is difficult because it is intertwined with the exploratory experience of the agent. In order to learn a useful representation, diverse data is necessary, but effective exploration requires coherent representations. Additionally, we want to learn representations that can generalize across tasks and speed up downstream exploration for efficient task-specific training. To tackle these challenges, we propose Proto-RL, which is a self-supervised framework that connects representation learning with exploration through prototypical representations. These prototypes summarize the exploratory experience of the agent and can be used to represent observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information, which leads to state-of-the-art downstream policy learning for difficult continuous control tasks.",1
"In recent years, reinforcement learning (RL) has gained increasing attention in control engineering. Especially, policy gradient methods are widely used. In this work, we improve the tracking performance of proximal policy optimization (PPO) for arbitrary reference signals by incorporating information about future reference values. Two variants of extending the argument of the actor and the critic taking future reference values into account are presented. In the first variant, global future reference values are added to the argument. For the second variant, a novel kind of residual space with future reference values applicable to model-free reinforcement learning is introduced. Our approach is evaluated against a PI controller on a simple drive train model. We expect our method to generalize to arbitrary references better than previous approaches, pointing towards the applicability of RL to control real systems.",0
"Control engineering has shown a growing interest in reinforcement learning (RL), particularly in policy gradient methods. This study aims to enhance the tracking performance of proximal policy optimization (PPO) for arbitrary reference signals by integrating knowledge about future reference values. We propose two variations for actor and critic models that consider future reference values. The initial variant involves adding global future reference values to the argument, while the second variant introduces a new residual space suitable for model-free reinforcement learning. We compare the results of our method against a PI controller on a basic drive train model and expect our approach to outperform previous methods in terms of generalization to arbitrary references. This highlights the potential of RL in controlling real systems.",1
"Most prior approaches to offline reinforcement learning (RL) have taken an iterative actor-critic approach involving off-policy evaluation. In this paper we show that simply doing one step of constrained/regularized policy improvement using an on-policy Q estimate of the behavior policy performs surprisingly well. This one-step algorithm beats the previously reported results of iterative algorithms on a large portion of the D4RL benchmark. The simple one-step baseline achieves this strong performance without many of the tricks used by previously proposed iterative algorithms and is more robust to hyperparameters. We argue that the relatively poor performance of iterative approaches is a result of the high variance inherent in doing off-policy evaluation and magnified by the repeated optimization of policies against those high-variance estimates. In addition, we hypothesize that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.",0
"Previous methods for offline reinforcement learning have typically utilized an iterative actor-critic approach with off-policy evaluation. However, our study demonstrates that a single step of constrained/regularized policy improvement using an on-policy Q estimate of the behavior policy can yield surprisingly positive results. This one-step approach outperforms previously reported results from iterative algorithms on a significant portion of the D4RL benchmark. Moreover, this simple baseline method achieves these outcomes without the need for the various techniques employed by past iterative algorithms, making it more robust to hyperparameters. We posit that the inferior performance of iterative methods stems from the high variance intrinsic in off-policy evaluation, which is further exacerbated by repeated policy optimization against these high-variance estimates. Additionally, we hypothesize that the one-step algorithm's strong performance is attributable to a combination of favorable environmental and behavioral policy structures.",1
"As a fundamental problem for Artificial Intelligence, multi-agent system (MAS) is making rapid progress, mainly driven by multi-agent reinforcement learning (MARL) techniques. However, previous MARL methods largely focused on grid-world like or game environments; MAS in visually rich environments has remained less explored. To narrow this gap and emphasize the crucial role of perception in MAS, we propose a large-scale 3D dataset, CollaVN, for multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed to cooperatively navigate across photo-realistic environments to reach target locations. Diverse MAVN variants are explored to make our problem more general. Moreover, a memory-augmented communication framework is proposed. Each agent is equipped with a private, external memory to persistently store communication information. This allows agents to make better use of their past communication information, enabling more efficient collaboration and robust long-term planning. In our experiments, several baselines and evaluation metrics are designed. We also empirically verify the efficacy of our proposed MARL approach across different MAVN task settings.",0
"The field of multi-agent system (MAS) poses a significant challenge for Artificial Intelligence, but is progressing rapidly due to the utilization of multi-agent reinforcement learning (MARL) techniques. However, previous MARL methods have focused mainly on grid-world or game environments, leaving visually rich environments for MAS underexplored. To address this gap and highlight the importance of perception in MAS, we have introduced CollaVN, a large-scale 3D dataset for multi-agent visual navigation (MAVN). In CollaVN, multiple agents cooperate to navigate through realistic environments towards a target location. Our problem is made more general by exploring different MAVN variants, and we have proposed a memory-augmented communication framework. Each agent has a private external memory to store communication information, enabling more efficient collaboration and robust long-term planning. We have designed several baselines and evaluation metrics for our experiments and have demonstrated the effectiveness of our proposed MARL approach across various MAVN task settings.",1
"Reinforcement learning methods for robotics are increasingly successful due to the constant development of better policy gradient techniques. A precise (low variance) and accurate (low bias) gradient estimator is crucial to face increasingly complex tasks. Traditional policy gradient algorithms use the likelihood-ratio trick, which is known to produce unbiased but high variance estimates. More modern approaches exploit the reparametrization trick, which gives lower variance gradient estimates but requires differentiable value function approximators. In this work, we study a different type of stochastic gradient estimator: the Measure-Valued Derivative. This estimator is unbiased, has low variance, and can be used with differentiable and non-differentiable function approximators. We empirically evaluate this estimator in the actor-critic policy gradient setting and show that it can reach comparable performance with methods based on the likelihood-ratio or reparametrization tricks, both in low and high-dimensional action spaces.",0
"Due to the continuous development of improved policy gradient techniques, reinforcement learning methods have become increasingly successful in robotics. A precise and accurate gradient estimator is critical for tackling complex tasks. While traditional policy gradient algorithms use the likelihood-ratio trick resulting in unbiased but high variance estimates, modern approaches employ the reparametrization trick leading to lower variance gradient estimates, but requiring differentiable value function approximators. This study explores a distinct type of stochastic gradient estimator, the Measure-Valued Derivative, which is unbiased, has low variance, and can be used with differentiable and non-differentiable function approximators. We conduct empirical evaluations in the actor-critic policy gradient setting and demonstrate that it can achieve comparable performance with methods based on the likelihood-ratio or reparametrization tricks, in both low and high-dimensional action spaces.",1
"Meta-reinforcement learning (RL) can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We do not want to remove this distributional shift by simply adopting a conservative exploration strategy, because learning an exploration strategy enables an agent to collect better data for faster adaptation. Instead, we propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",0
"Meta-reinforcement learning (RL) offers the ability to train policies that can adapt to new tasks with significantly less data than standard RL. However, the process of meta-training itself is time-consuming and costly. Offline meta-RL presents an opportunity to reuse a single static dataset, labeled once with rewards for different tasks, to meta-train policies that can adapt to a range of new tasks at meta-test time. Nevertheless, offline meta-RL comes with additional challenges that need to be overcome. Meta-RL needs to learn an exploration strategy to collect data for adapting and a policy that can quickly adapt to new tasks. However, when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data, the policy may behave unpredictably and induce distributional shift. We propose a hybrid offline meta-RL algorithm that uses offline data with rewards to meta-train an adaptive policy and then collects additional unsupervised online data to bridge this distribution shift. This additional data can be collected much more cheaply than labeled data. Our method shows a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",1
"Many machine learning strategies designed to automate mathematical tasks leverage neural networks to search large combinatorial spaces of mathematical symbols. In contrast to traditional evolutionary approaches, using a neural network at the core of the search allows learning higher-level symbolic patterns, providing an informed direction to guide the search. When no labeled data is available, such networks can still be trained using reinforcement learning. However, we demonstrate that this approach can suffer from an early commitment phenomenon and from initialization bias, both of which limit exploration. We present two exploration methods to tackle these issues, building upon ideas of entropy regularization and distribution initialization. We show that these techniques can improve the performance, increase sample efficiency, and lower the complexity of solutions for the task of symbolic regression.",0
"Several machine learning techniques have been developed to automate mathematical tasks by utilizing neural networks to browse through vast combinatory spaces of math symbols. Unlike traditional evolutionary methods, neural networks are capable of learning higher-level patterns of symbols, which can guide the search process. Even without labeled data, reinforcement learning can train such networks. However, we demonstrate that this approach can face limitations due to the early commitment phenomenon and initialization bias, which hinder exploration. To overcome these issues, we propose two exploration techniques based on entropy regularization and distribution initialization. Our findings suggest that these techniques can improve performance, sample efficiency, and solution complexity for symbolic regression tasks.",1
"This paper presents a constrained policy gradient algorithm. We introduce constraints for safe learning with the following steps. First, learning is slowed down (lazy learning) so that the episodic policy change can be computed with the help of the policy gradient theorem and the neural tangent kernel. Then, this enables us the evaluation of the policy at arbitrary states too. In the same spirit, learning can be guided, ensuring safety via augmenting episode batches with states where the desired action probabilities are prescribed. Finally, exogenous discounted sum of future rewards (returns) can be computed at these specific state-action pairs such that the policy network satisfies constraints. Computing the returns is based on solving a system of linear equations (equality constraints) or a constrained quadratic program (inequality constraints). Simulation results suggest that adding constraints (external information) to the learning can improve learning in terms of speed and safety reasonably if constraints are appropriately selected. The efficiency of the constrained learning was demonstrated with a shallow and wide ReLU network in the Cartpole and Lunar Lander OpenAI gym environments. The main novelty of the paper is giving a practical use of the neural tangent kernel in reinforcement learning.",0
"In this paper, we propose a policy gradient algorithm that incorporates constraints to ensure safe learning. Our approach involves several steps. First, we adopt a lazy learning strategy that slows down the learning process and relies on the policy gradient theorem and the neural tangent kernel to compute episodic policy changes. This allows us to evaluate the policy at arbitrary states as well. We also introduce guidance for learning by augmenting episode batches with states where desired action probabilities are prescribed to ensure safety. Finally, we compute the exogenous discounted sum of future rewards (returns) at specific state-action pairs, subject to constraints that are either equality or inequality-based. Our simulation results demonstrate that incorporating external constraints can improve learning in terms of speed and safety, especially when constraints are appropriately selected. We showcase the efficiency of our approach using a shallow and wide ReLU network in the Cartpole and Lunar Lander OpenAI gym environments. Our contribution is the practical use of the neural tangent kernel in reinforcement learning.",1
"In this paper, we propose a learning algorithm that enables a model to quickly exploit commonalities among related tasks from an unseen task distribution, before quickly adapting to specific tasks from that same distribution. We investigate how learning with different task distributions can first improve adaptability by meta-finetuning on related tasks before improving goal task generalization with finetuning. Synthetic regression experiments validate the intuition that learning to meta-learn improves adaptability and consecutively generalization. Experiments on more complex image classification, continual regression, and reinforcement learning tasks demonstrate that learning to meta-learn generally improves task-specific adaptation. The methodology, setup, and hypotheses in this proposal were positively evaluated by peer review before conclusive experiments were carried out.",0
"This paper proposes a learning algorithm that allows a model to quickly utilize shared attributes among related tasks from an unknown task distribution, before adapting to specific tasks from the same distribution. The study explores how learning with various task distributions can first enhance adaptability by meta-finetuning on related tasks, then improve goal task generalization with finetuning. Synthetic regression experiments confirm the idea that learning to meta-learn boosts adaptability and subsequently generalization. Experiments on more intricate tasks such as image classification, continual regression, and reinforcement learning tasks demonstrate that learning to meta-learn enhances task-specific adaptation. Peer review evaluated the methodology, setup, and hypotheses in this proposal positively before conclusive experiments were conducted.",1
"Reward-Weighted Regression (RWR) belongs to a family of widely known iterative Reinforcement Learning algorithms based on the Expectation-Maximization framework. In this family, learning at each iteration consists of sampling a batch of trajectories using the current policy and fitting a new policy to maximize a return-weighted log-likelihood of actions. Although RWR is known to yield monotonic improvement of the policy under certain circumstances, whether and under which conditions RWR converges to the optimal policy have remained open questions. In this paper, we provide for the first time a proof that RWR converges to a global optimum when no function approximation is used.",0
"Reward-Weighted Regression (RWR) is an iterative Reinforcement Learning algorithm that falls under the Expectation-Maximization framework. The algorithm samples a batch of trajectories using the current policy and fits a new policy to maximize a return-weighted log-likelihood of actions. While it has been observed that RWR improves the policy under specific circumstances, the conditions under which it converges to the optimal policy remain unclear. This paper presents the first-ever evidence that RWR achieves global convergence without any function approximation.",1
"We study the problem of safe offline reinforcement learning (RL), the goal is to learn a policy that maximizes long-term reward while satisfying safety constraints given only offline data, without further interaction with the environment. This problem is more appealing for real world RL applications, in which data collection is costly or dangerous. Enforcing constraint satisfaction is non-trivial, especially in offline settings, as there is a potential large discrepancy between the policy distribution and the data distribution, causing errors in estimating the value of safety constraints. We show that na\""ive approaches that combine techniques from safe RL and offline RL can only learn sub-optimal solutions. We thus develop a simple yet effective algorithm, Constraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits the use of data generated by mixed behavior policies. We present a theoretical analysis and demonstrate empirically that our approach can learn robustly across a variety of benchmark control tasks, outperforming several baselines.",0
"Our focus is on the challenge of safe offline reinforcement learning (RL), which involves finding an optimal policy that adheres to safety constraints using only offline data and no additional interaction with the environment. This is particularly important for real-world RL applications where data collection can be expensive or hazardous. However, enforcing these constraints is challenging, especially in offline settings where there may be significant differences between the policy distribution and the data distribution, leading to potential errors in constraint value estimation. We discovered that combining safe RL and offline RL techniques results in sub-optimal outcomes. As a result, we developed Constraints Penalized Q-Learning (CPQ), a straightforward yet effective algorithm that can use data generated by mixed behavior policies. We provide a theoretical analysis and demonstrate through empirical testing that our method can robustly learn across a variety of benchmark control tasks, surpassing several baselines.",1
"Previous work on policy learning for Malaria control has often formulated the problem as an optimization problem assuming the objective function and the search space have a specific structure. The problem has been formulated as multi-armed bandits, contextual bandits and a Markov Decision Process in isolation. Furthermore, an emphasis is put on developing new algorithms specific to an instance of Malaria control, while ignoring a plethora of simpler and general algorithms in the literature. In this work, we formally study the formulation of Malaria control and present a comprehensive analysis of several formulations used in the literature. In addition, we implement and analyze several reinforcement learning algorithms in all formulations and compare them to black box optimization. In contrast to previous work, our results show that simple algorithms based on Upper Confidence Bounds are sufficient for learning good Malaria policies, and tend to outperform their more advanced counterparts on the malaria OpenAI Gym environment.",0
"Past research on policy learning for Malaria prevention has typically approached the problem as an optimization problem with specific structures for the objective function and search space. The formulations have included multi-armed bandits, contextual bandits, and Markov Decision Processes, with a focus on creating novel algorithms tailored to Malaria control rather than utilizing existing, simpler algorithms. This study examines the Malaria control formulation in detail and analyzes various formulations found in the literature. Additionally, several reinforcement learning algorithms are implemented and compared to black box optimization. Unlike prior research, the findings indicate that basic algorithms, such as those based on Upper Confidence Bounds, are sufficient for developing effective Malaria policies and often outperform more advanced algorithms in the Malaria OpenAI Gym environment.",1
"A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model, a distance function, and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen.",0
"Autonomous agents should possess the ability to solve complex problems over a long period and apply this knowledge to new tasks. Recent improvements in data-driven skill learning have demonstrated that behavioral priors extracted from offline data can aid agents in solving challenging long-horizon tasks through reinforcement learning. However, the issue of generalizing to unseen tasks that were not included in the behavioral prior training persists. In response, we introduce the Few-shot Imitation with Skill Transition Models (FIST) algorithm, which extracts skills from offline data and uses them to adapt to new tasks with the help of a few demonstrations. FIST utilizes an inverse skill dynamics model, a distance function, and a semi-parametric imitation approach. We demonstrate that FIST can effectively generalize to new tasks and outperforms previous baselines in navigation experiments that require maneuvering through uncharted areas of a large maze and 7-DoF robotic arm experiments that require manipulating never-before-seen objects in a kitchen.",1
"Many sequential decision problems involve finding a policy that maximizes total reward while obeying safety constraints. Although much recent research has focused on the development of safe reinforcement learning (RL) algorithms that produce a safe policy after training, ensuring safety during training as well remains an open problem. A fundamental challenge is performing exploration while still satisfying constraints in an unknown Markov decision process (MDP). In this work, we address this problem for the chance-constrained setting. We propose a new algorithm, SAILR, that uses an intervention mechanism based on advantage functions to keep the agent safe throughout training and optimizes the agent's policy using off-the-shelf RL algorithms designed for unconstrained MDPs. Our method comes with strong guarantees on safety during both training and deployment (i.e., after training and without the intervention mechanism) and policy performance compared to the optimal safety-constrained policy. In our experiments, we show that SAILR violates constraints far less during training than standard safe RL and constrained MDP approaches and converges to a well-performing policy that can be deployed safely without intervention. Our code is available at https://github.com/nolanwagener/safe_rl.",0
"A common issue in sequential decision problems is determining a policy that maximizes overall reward while adhering to safety restrictions. While recent research has focused on developing safe reinforcement learning (RL) algorithms to produce a secure policy post-training, ensuring safety during the training process remains an unsolved problem. The main challenge is exploring while still meeting constraints in an unknown Markov decision process (MDP). This research aims to address this issue for the chance-constrained setting. The proposed algorithm, SAILR, utilizes an intervention mechanism based on advantage functions to ensure the agent's safety throughout training and optimizes the agent's policy using off-the-shelf RL algorithms designed for unconstrained MDPs. Our approach guarantees safety during both training and deployment, in addition to policy performance when compared with the optimal safety-constrained policy. Our experiments show that SAILR violates constraints less frequently during training than standard safe RL and constrained MDP approaches and leads to a well-performing policy that can be deployed safely without intervention. Our code is available at https://github.com/nolanwagener/safe_rl.",1
"Building autonomous machines that can explore open-ended environments, discover possible interactions and autonomously build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autonomous and intrinsically motivated learning agents that can generate, select and learn to solve their own problems. In recent years, we have seen a convergence of developmental approaches, and developmental robotics in particular, with deep reinforcement learning (RL) methods, forming the new domain of developmental machine learning. Within this new domain, we review here a set of methods where deep RL algorithms are trained to tackle the developmental robotics problem of the autonomous acquisition of open-ended repertoires of skills. Intrinsically motivated goal-conditioned RL algorithms train agents to learn to represent, generate and pursue their own goals. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions, which results in new challenges compared to traditional RL algorithms designed to tackle pre-defined sets of goals using external reward signals. This paper proposes a typology of these methods at the intersection of deep RL and developmental approaches, surveys recent approaches and discusses future avenues.",0
"Artificial intelligence aims to develop autonomous machines that can explore environments, discover interactions, and build skill sets. Developmental approaches contend that this requires learning agents that are self-motivated, autonomous, and capable of solving their own problems. Recently, developmental robotics has merged with deep reinforcement learning (RL) methods, giving rise to developmental machine learning. This paper examines a set of methods within this new domain that employ deep RL algorithms to address the developmental robotics problem of acquiring open-ended skill sets autonomously. Intrinsically motivated goal-conditioned RL algorithms are used to teach agents how to represent, generate, and pursue their own goals. This approach requires the learning of compact goal encodings and associated goal-achievement functions, which presents unique challenges compared to traditional RL algorithms that tackle predetermined sets of goals using external rewards. The paper proposes a typology of these methods at the intersection of deep RL and developmental approaches, surveys recent approaches, and discusses future directions.",1
"We study multi-task reinforcement learning (RL) in tabular episodic Markov decision processes (MDPs). We formulate a heterogeneous multi-player RL problem, in which a group of players concurrently face similar but not necessarily identical MDPs, with a goal of improving their collective performance through inter-player information sharing. We design and analyze an algorithm based on the idea of model transfer, and provide gap-dependent and gap-independent upper and lower bounds that characterize the intrinsic complexity of the problem.",0
"Our focus is on tabular episodic Markov decision processes (MDPs) in the field of multi-task reinforcement learning (RL). Our objective is to enhance collective performance by enabling inter-player information sharing, as a group of players tackle similar but not identical MDPs. To achieve this goal, we have developed an algorithm that utilizes model transfer. Our analysis includes gap-dependent and gap-independent upper and lower bounds that determine the complexity of the problem.",1
"Object-centric world models provide structured representation of the scene and can be an important backbone in reinforcement learning and planning. However, existing approaches suffer in partially-observable environments due to the lack of belief states. In this paper, we propose Structured World Belief, a model for learning and inference of object-centric belief states. Inferred by Sequential Monte Carlo (SMC), our belief states provide multiple object-centric scene hypotheses. To synergize the benefits of SMC particles with object representations, we also propose a new object-centric dynamics model that considers the inductive bias of object permanence. This enables tracking of object states even when they are invisible for a long time. To further facilitate object tracking in this regime, we allow our model to attend flexibly to any spatial location in the image which was restricted in previous models. In experiments, we show that object-centric belief provides a more accurate and robust performance for filtering and generation. Furthermore, we show the efficacy of structured world belief in improving the performance of reinforcement learning, planning and supervised reasoning.",0
"Structured world models that are object-centric can be a crucial foundation for reinforcement learning and planning as they offer a well-organized representation of the scene. Nevertheless, existing methods face difficulties in partially-observable settings due to the absence of belief states. In this study, we present the Structured World Belief, a model that can learn and infer object-centric belief states. Our belief states provide several hypotheses of the object-centric scene, which are inferred via Sequential Monte Carlo (SMC). To capitalize on the advantages of SMC particles and object representations, we propose a new object-centric dynamics model that accounts for the inductive bias of object permanence. This enables the tracking of object states even when they are out of sight for an extended period. Our model can also attend flexibly to any spatial location in the image, which was restricted in previous models, to further help with object tracking. Our experiments demonstrate that object-centric belief provides more precise and robust performance for filtering and generation. Additionally, we demonstrate that the structured world belief is effective in enhancing the performance of reinforcement learning, planning, and supervised reasoning.",1
"Exploration in reinforcement learning is a challenging problem: in the worst case, the agent must search for high-reward states that could be hidden anywhere in the state space. Can we define a more tractable class of RL problems, where the agent is provided with examples of successful outcomes? In this problem setting, the reward function can be obtained automatically by training a classifier to categorize states as successful or not. If trained properly, such a classifier can provide a well-shaped objective landscape that both promotes progress toward good states and provides a calibrated exploration bonus. In this work, we show that an uncertainty aware classifier can solve challenging reinforcement learning problems by both encouraging exploration and provided directed guidance towards positive outcomes. We propose a novel mechanism for obtaining these calibrated, uncertainty-aware classifiers based on an amortized technique for computing the normalized maximum likelihood (NML) distribution. To make this tractable, we propose a novel method for computing the NML distribution by using meta-learning. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions, while also providing more effective guidance towards the goal. We demonstrate that our algorithm solves a number of challenging navigation and robotic manipulation tasks which prove difficult or impossible for prior methods.",0
"The task of exploring in reinforcement learning is difficult as the agent must search for valuable states, which could be hidden in any part of the state space. Is it possible to simplify the RL problems by providing the agent with successful outcome examples? This approach allows for obtaining the reward function through training a classifier to categorize states as successful or not. A well-trained classifier can offer a structured objective landscape that promotes progress towards desired states and offers exploration bonuses. Our research demonstrates that an uncertainty-aware classifier can resolve complex RL problems by providing both exploration incentives and guidance towards positive outcomes. We propose a novel method for obtaining these calibrated, uncertainty-aware classifiers using an amortized technique for computing the normalized maximum likelihood distribution. To streamline this process, we propose a new approach for computing the NML distribution using meta-learning. Our approach has similarities to count-based exploration methods and prior reward function learning algorithms, but it offers more effective guidance towards achieving the goal. We prove the effectiveness of our algorithm by demonstrating its ability to solve challenging navigation and robotic manipulation tasks that have been difficult or impossible for prior methods.",1
"Transfer in reinforcement learning is usually achieved through generalisation across tasks. Whilst many studies have investigated transferring knowledge when the reward function changes, they have assumed that the dynamics of the environments remain consistent. Many real-world RL problems require transfer among environments with different dynamics. To address this problem, we propose an approach based on successor features in which we model successor feature functions with Gaussian Processes permitting the source successor features to be treated as noisy measurements of the target successor feature function. Our theoretical analysis proves the convergence of this approach as well as the bounded error on modelling successor feature functions with Gaussian Processes in environments with both different dynamics and rewards. We demonstrate our method on benchmark datasets and show that it outperforms current baselines.",0
"Typically, transfer in reinforcement learning is accomplished by generalizing across tasks. However, most studies focus on transferring knowledge when the reward function changes, assuming that the environment dynamics remain constant. Unfortunately, many real-world RL problems require transferring knowledge among environments with varying dynamics. To address this issue, we propose a successor feature-based approach that utilizes Gaussian Processes to model the successor feature functions. This allows for the source successor features to be treated as imprecise measurements of the target successor feature function. Our theoretical analysis demonstrates the convergence of this approach and the bounded error of modeling successor feature functions in environments with different dynamics and rewards. Our method is evaluated on benchmark datasets and outperforms current baselines.",1
"Policy optimization is a widely-used method in reinforcement learning. Due to its local-search nature, however, theoretical guarantees on global optimality often rely on extra assumptions on the Markov Decision Processes (MDPs) that bypass the challenge of global exploration. To eliminate the need of such assumptions, in this work, we develop a general solution that adds dilated bonuses to the policy update to facilitate global exploration. To showcase the power and generality of this technique, we apply it to several episodic MDP settings with adversarial losses and bandit feedback, improving and generalizing the state-of-the-art. Specifically, in the tabular case, we obtain $\widetilde{\mathcal{O}}(\sqrt{T})$ regret where $T$ is the number of episodes, improving the $\widetilde{\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al. (2020). When the number of states is infinite, under the assumption that the state-action values are linear in some low-dimensional features, we obtain $\widetilde{\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator, matching the result of Neu and Olkhovskaya (2020) while importantly removing the need of an exploratory policy that their algorithm requires. When a simulator is unavailable, we further consider a linear MDP setting and obtain $\widetilde{\mathcal{O}}({T}^{14/15})$ regret, which is the first result for linear MDPs with adversarial losses and bandit feedback.",0
"Reinforcement learning commonly employs policy optimization, which is effective but limited by its local-search nature. To achieve global optimality without relying on extra assumptions about Markov Decision Processes (MDPs), we propose a general solution that introduces dilated bonuses to policy updates, facilitating global exploration. This approach is demonstrated in several episodic MDP settings, including those with adversarial losses and bandit feedback, where it improves upon and generalizes existing methods. Our tabular case achieves $\widetilde{\mathcal{O}}(\sqrt{T})$ regret, surpassing the $\widetilde{\mathcal{O}}({T}^{2/3})$ regret bound of Shani et al. (2020). In infinite state scenarios, assuming state-action values are linear in low-dimensional features, our method matches Neu and Olkhovskaya's (2020) $\widetilde{\mathcal{O}}({T}^{2/3})$ regret result, while eliminating the need for an exploratory policy. For linear MDPs with adversarial losses and bandit feedback, where no simulator is available, we achieve the first result of $\widetilde{\mathcal{O}}({T}^{14/15})$ regret.",1
"Designing off-policy reinforcement learning algorithms is typically a very challenging task, because a desirable iteration update often involves an expectation over an on-policy distribution. Prior off-policy actor-critic (AC) algorithms have introduced a new critic that uses the density ratio for adjusting the distribution mismatch in order to stabilize the convergence, but at the cost of potentially introducing high biases due to the estimation errors of both the density ratio and value function. In this paper, we develop a doubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take advantage of learned nuisance functions to reduce estimation errors. Moreover, DR-Off-PAC adopts a single timescale structure, in which both actor and critics are updated simultaneously with constant stepsize, and is thus more sample efficient than prior algorithms that adopt either two timescale or nested-loop structure. We study the finite-time convergence rate and characterize the sample complexity for DR-Off-PAC to attain an $\epsilon$-accurate optimal policy. We also show that the overall convergence of DR-Off-PAC is doubly robust to the approximation errors that depend only on the expressive power of approximation functions. To the best of our knowledge, our study establishes the first overall sample complexity analysis for a single time-scale off-policy AC algorithm.",0
"Creating off-policy reinforcement learning algorithms is known to be a difficult task, as desirable updates often require an expectation over an on-policy distribution. Previous off-policy actor-critic (AC) algorithms have introduced a new critic that uses the density ratio to adjust distribution mismatch and stabilize convergence, but this can introduce high biases due to estimation errors of both the density ratio and value function. This paper presents a new approach called doubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which uses learned nuisance functions to reduce estimation errors. DR-Off-PAC has a single timescale structure, updating both actor and critic simultaneously with a constant stepsize, making it more sample efficient than previous algorithms. This study analyzes the finite-time convergence rate and sample complexity of DR-Off-PAC to attain an $\epsilon$-accurate optimal policy, and shows that its overall convergence is doubly robust to approximation errors dependent only on the expressive power of approximation functions. This is the first study to establish an overall sample complexity analysis for a single timescale off-policy AC algorithm.",1
"Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems from game playing and robotics have been solved with deep model-free methods. Unfortunately, the sample complexity of model-free methods is often high. To reduce the number of environment samples, model-based reinforcement learning creates an explicit model of the environment dynamics. Achieving high model accuracy is a challenge in high-dimensional problems. In recent years, a diverse landscape of model-based methods has been introduced to improve model accuracy, using methods such as uncertainty modeling, model-predictive control, latent models, and end-to-end learning and planning. Some of these methods succeed in achieving high accuracy at low sample complexity, most do so either in a robotics or in a games context. In this paper, we survey these methods; we explain in detail how they work and what their strengths and weaknesses are. We conclude with a research agenda for future work to make the methods more robust and more widely applicable to other applications.",0
"Over the last few years, deep reinforcement learning has achieved impressive results. It has solved highly complex sequential decision-making problems in game playing and robotics using deep model-free methods. However, the sample complexity of these methods is often high, which is a limitation. To overcome this, model-based reinforcement learning has been introduced, which creates an explicit model of the environment dynamics to reduce the number of environment samples. However, achieving high model accuracy is a challenge in high-dimensional problems. To improve model accuracy, a range of model-based methods have been developed, such as uncertainty modeling, model-predictive control, latent models, and end-to-end learning and planning. While some of these methods have succeeded in achieving high accuracy at low sample complexity, most have only been tested in the context of robotics or games. This paper surveys these methods, providing detailed explanations of how they work and their strengths and weaknesses. The paper concludes with a research agenda to make these methods more robust and applicable to a wider range of applications.",1
"The high-dimensional or sparse reward task of a reinforcement learning (RL) environment requires a superior potential controller such as hierarchical reinforcement learning (HRL) rather than an atomic RL because it absorbs the complexity of commands to achieve the purpose of the task in its hierarchical structure. One of the HRL issues is how to train each level policy with the optimal data collection from its experience. That is to say, how to synchronize adjacent level policies optimally. Our research finds that a HRL model through the off-policy correction technique of HRL, which trains a higher-level policy with the goal of reflecting a lower-level policy which is newly trained using the off-policy method, takes the critical role of synchronizing both level policies at all times while they are being trained. We propose a novel HRL model supporting the optimal level synchronization using the off-policy correction technique with a deep generative model. This uses the advantage of the inverse operation of a flow-based deep generative model (FDGM) to achieve the goal corresponding to the current state of the lower-level policy. The proposed model also considers the freedom of the goal dimension between HRL policies which makes it the generalized inverse model of the model-free RL in HRL with the optimal synchronization method. The comparative experiment results show the performance of our proposed model.",0
"To effectively tackle high-dimensional or sparse reward tasks in reinforcement learning (RL) environments, hierarchical reinforcement learning (HRL) is a better choice than atomic RL. This is because HRL structures the task commands in a hierarchical manner, simplifying their complexity. However, one challenge of HRL is training each level policy optimally with the right data collection. This requires synchronizing adjacent level policies effectively. Our study shows that using the off-policy correction technique of HRL, which trains higher-level policies to reflect newly trained lower-level policies, is an efficient way to synchronize both policies during training. We propose a novel HRL model that supports optimal level synchronization using the off-policy correction technique and a deep generative model. The proposed model leverages the benefits of the inverse operation of a flow-based deep generative model (FDGM) to achieve the desired outcome based on the current state of the lower-level policy. It also allows for freedom in the goal dimension between HRL policies, making it a generalized inverse model of the model-free RL in HRL with optimal synchronization. Our comparative experiment results demonstrate the effectiveness of our proposed model.",1
"Federated learning enables a cluster of decentralized mobile devices at the edge to collaboratively train a shared machine learning model, while keeping all the raw training samples on device. This decentralized training approach is demonstrated as a practical solution to mitigate the risk of privacy leakage. However, enabling efficient FL deployment at the edge is challenging because of non-IID training data distribution, wide system heterogeneity and stochastic-varying runtime effects in the field. This paper jointly optimizes time-to-convergence and energy efficiency of state-of-the-art FL use cases by taking into account the stochastic nature of edge execution. We propose AutoFL by tailor-designing a reinforcement learning algorithm that learns and determines which K participant devices and per-device execution targets for each FL model aggregation round in the presence of stochastic runtime variance, system and data heterogeneity. By considering the unique characteristics of FL edge deployment judiciously, AutoFL achieves 3.6 times faster model convergence time and 4.7 and 5.2 times higher energy efficiency for local clients and globally over the cluster of K participants, respectively.",0
"Federated learning is a method that allows decentralized mobile devices to collaborate in training a shared machine learning model, while preserving the raw training samples on each device. This approach is useful in reducing the risk of privacy breaches. Nonetheless, deploying efficient federated learning at the edge is a challenge due to the non-IID distribution of training data, system heterogeneity, and stochastic runtime effects. This paper proposes AutoFL, which optimizes the time-to-convergence and energy efficiency of state-of-the-art federated learning cases by taking into account the stochastic nature of edge execution. AutoFL employs a reinforcement learning algorithm that determines the K participant devices and per-device execution targets for each model aggregation round, given the presence of stochastic runtime variance, system, and data heterogeneity. By considering the unique features of federated learning edge deployment, AutoFL achieves 3.6 times faster model convergence time and 4.7 and 5.2 times higher energy efficiency for local clients and globally over the cluster of K participants, respectively.",1
"In this work, we propose a deep reinforcement learning (DRL) model for finding a feasible solution for (mixed) integer programming (MIP) problems. Finding a feasible solution for MIP problems is critical because many successful heuristics rely on a known initial feasible solution. However, it is in general NP-hard. Inspired by the feasibility pump (FP), a well-known heuristic for searching feasible MIP solutions, we develop a smart feasibility pump (SFP) method using DRL. In addition to multi-layer perception (MLP), we propose a novel convolution neural network (CNN) structure for the policy network to capture the hidden information of the constraint matrix of the MIP problem. Numerical experiments on various problem instances show that SFP significantly outperforms the classic FP in terms of the number of steps required to reach the first feasible solution. Moreover, the CNN structure works without the projection of the current solution as the input, which saves the computational effort at each step of the FP algorithms to find projections. This highlights the representational power of the CNN structure.",0
"Our research presents a model that uses deep reinforcement learning (DRL) to solve (mixed) integer programming (MIP) problems and find feasible solutions. MIP problems are challenging to solve, and many heuristics rely on a known initial feasible solution. Our smart feasibility pump (SFP) method, inspired by the feasibility pump (FP) heuristic, utilizes DRL to improve the search for feasible MIP solutions. We propose a novel convolution neural network (CNN) structure in addition to multi-layer perception (MLP) for the policy network to capture the hidden information of the MIP problem's constraint matrix. Numerical experiments demonstrate that SFP outperforms FP in terms of the number of steps required to reach the first feasible solution. Additionally, the CNN structure works without the projection of the current solution as input, which saves computational effort in each step of the FP algorithm. This highlights the superior representational power of the CNN structure.",1
"The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical system is equivalent to the search for an optimal feedback law utilizing the simulations/ rollouts of the unknown dynamical system. Most RL techniques search over a complex global nonlinear feedback parametrization making them suffer from high training times as well as variance. Instead, we advocate searching over a local feedback representation consisting of an open-loop sequence, and an associated optimal linear feedback law completely determined by the open-loop. We show that this alternate approach results in highly efficient training, the answers obtained are repeatable and hence reliable, and the resulting closed performance is superior to global state-of-the-art RL techniques. Finally, if we replan, whenever required, which is feasible due to the fast and reliable local solution, allows us to recover global optimality of the resulting feedback law.",0
"In an unknown nonlinear dynamical system, the challenge of Reinforcement Learning (RL) involves finding an optimal feedback law through simulations/rollouts of the system. Traditional RL methods search for a complex global nonlinear feedback parametrization, which leads to extended training times and variance. In contrast, our approach proposes a local feedback representation that includes an open-loop sequence and an optimal linear feedback law determined by the open-loop. This method offers efficient training, reliable and repeatable results, and superior closed performance compared to global state-of-the-art RL techniques. Furthermore, the fast and dependable local solution enables us to achieve global optimality by replanning as needed.",1
"Automatic surgical instruction generation is a prerequisite towards intra-operative context-aware surgical assistance. However, generating instructions from surgical scenes is challenging, as it requires jointly understanding the surgical activity of current view and modelling relationships between visual information and textual description. Inspired by the neural machine translation and imaging captioning tasks in open domain, we introduce a transformer-backboned encoder-decoder network with self-critical reinforcement learning to generate instructions from surgical images. We evaluate the effectiveness of our method on DAISI dataset, which includes 290 procedures from various medical disciplines. Our approach outperforms the existing baseline over all caption evaluation metrics. The results demonstrate the benefits of the encoder-decoder structure backboned by transformer in handling multimodal context.",0
"The ability to generate surgical instructions automatically is necessary for providing context-aware assistance during surgery. However, this is a difficult task as it involves understanding the surgical activity in the current view and establishing relationships between visual information and textual descriptions. To address this challenge, we have developed a transformer-based encoder-decoder network with self-critical reinforcement learning, drawing inspiration from neural machine translation and imaging captioning in the open domain. Our method was evaluated using the DAISI dataset, which contains 290 procedures from various medical disciplines. Our approach outperformed the current baseline across all caption evaluation metrics, demonstrating the advantages of using a transformer-based encoder-decoder structure to handle multimodal context.",1
"Trading markets represent a real-world financial application to deploy reinforcement learning agents, however, they carry hard fundamental challenges such as high variance and costly exploration. Moreover, markets are inherently a multiagent domain composed of many actors taking actions and changing the environment. To tackle these type of scenarios agents need to exhibit certain characteristics such as risk-awareness, robustness to perturbations and low learning variance. We take those as building blocks and propose a family of four algorithms. First, we contribute with two algorithms that use risk-averse objective functions and variance reduction techniques. Then, we augment the framework to multi-agent learning and assume an adversary which can take over and perturb the learning process. Our third and fourth algorithms perform well under this setting and balance theoretical guarantees with practical use. Additionally, we consider the multi-agent nature of the environment and our work is the first one extending empirical game theory analysis for multi-agent learning by considering risk-sensitive payoffs.",0
"Reinforcement learning agents can be applied in trading markets, but these markets pose significant challenges due to high variance and exploration costs. Markets are a multiagent domain with many actors changing the environment, and agents must exhibit risk-awareness, robustness to perturbations, and low learning variance to succeed. We propose a family of four algorithms that address these challenges. The first two algorithms use risk-averse objective functions and variance reduction techniques. We then expand the framework to multi-agent learning, assuming an adversary that can take over and perturb the learning process. Our third and fourth algorithms perform well in this setting, balancing theoretical guarantees with practical use. Our work is the first to extend empirical game theory analysis for multi-agent learning by considering risk-sensitive payoffs in the multi-agent environment.",1
"The policy improvement bound on the difference of the discounted returns plays a crucial role in the theoretical justification of the trust-region policy optimization (TRPO) algorithm. The existing bound leads to a degenerate bound when the discount factor approaches one, making the applicability of TRPO and related algorithms questionable when the discount factor is close to one. We refine the results in \cite{Schulman2015, Achiam2017} and propose a novel bound that is ""continuous"" in the discount factor. In particular, our bound is applicable for MDPs with the long-run average rewards as well.",0
"The theoretical justification of the trust-region policy optimization (TRPO) algorithm is heavily reliant on the policy improvement bound concerning the difference of the discounted returns. However, the current bound becomes problematic as the discount factor approaches one, rendering the applicability of TRPO and related algorithms uncertain in such cases. Our contribution builds upon the findings in \cite{Schulman2015, Achiam2017}, where we present a new bound that remains ""continuous"" even as the discount factor varies. Furthermore, our bound is suitable for Markov decision processes (MDPs) that involve long-run average rewards.",1
"Recently, neural network compression schemes like channel pruning have been widely used to reduce the model size and computational complexity of deep neural network (DNN) for applications in power-constrained scenarios such as embedded systems. Reinforcement learning (RL)-based auto-pruning has been further proposed to automate the DNN pruning process to avoid expensive hand-crafted work. However, the RL-based pruner involves a time-consuming training process and the high expense of each sample further exacerbates this problem. These impediments have greatly restricted the real-world application of RL-based auto-pruning. Thus, in this paper, we propose an efficient auto-pruning framework which solves this problem by taking advantage of the historical data from the previous auto-pruning process. In our framework, we first boost the convergence of the RL-pruner by transfer learning. Then, an augmented transfer learning scheme is proposed to further speed up the training process by improving the transferability. Finally, an assistant learning process is proposed to improve the sample efficiency of the RL agent. The experiments have shown that our framework can accelerate the auto-pruning process by 1.5-2.5 times for ResNet20, and 1.81-2.375 times for other neural networks like ResNet56, ResNet18, and MobileNet v1.",0
"The utilization of neural network compression methods, such as channel pruning, has become increasingly popular in reducing the size and complexity of deep neural networks (DNN) for low-power scenarios, like embedded systems. Reinforcement learning (RL) based auto-pruning has been introduced to automate the DNN pruning process, which eliminates the need for time-consuming manual work. However, the high cost per sample and the extensive training process of the RL-based pruner have limited its practical application. To address this issue, we propose an efficient auto-pruning framework that utilizes previous auto-pruning data to improve convergence during the RL-pruner training. Our framework includes transfer learning to enhance convergence, an augmented transfer learning approach to speed up the training process, and an assistant learning process to increase the sample efficiency of the RL agent. Our experiments show that our framework can accelerate the auto-pruning process by up to 2.5 times for ResNet20 and 2.375 times for other neural networks like ResNet56, ResNet18, and MobileNet v1.",1
"Finding the minimal structural assumptions that empower sample-efficient learning is one of the most important research directions in Reinforcement Learning (RL). This paper advances our understanding of this fundamental question by introducing a new complexity measure -- Bellman Eluder (BE) dimension. We show that the family of RL problems of low BE dimension is remarkably rich, which subsumes a vast majority of existing tractable RL problems including but not limited to tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems as well as low Eluder dimension problems. This paper further designs a new optimization-based algorithm -- GOLF, and reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang et al., 2017). We prove that both algorithms learn the near-optimal policies of low BE dimension problems in a number of samples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret and sample complexity results match or improve the best existing results for several well-known subclasses of low BE dimension problems.",0
"One of the main areas of research in Reinforcement Learning (RL) is to identify the structural assumptions required for efficient learning. This paper contributes to this topic by introducing a new measure of complexity, the Bellman Eluder (BE) dimension. The research shows that RL problems with low BE dimension are diverse and include many tractable problems, such as tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems, and low Eluder dimension problems. The paper also presents two new algorithms, GOLF and OLIVE (previously proposed by Jiang et al., 2017), that can learn near-optimal policies for low BE dimension problems with a polynomial number of samples, regardless of the state-action space size. The results of this research match or improve the current best results for several subclasses of low BE dimension problems in terms of regret and sample complexity.",1
"We convert the DeepMind Mathematics Dataset into a reinforcement learning environment by interpreting it as a program synthesis problem. Each action taken in the environment adds an operator or an input into a discrete compute graph. Graphs which compute correct answers yield positive reward, enabling the optimization of a policy to construct compute graphs conditioned on problem statements. Baseline models are trained using Double DQN on various subsets of problem types, demonstrating the capability to learn to correctly construct graphs despite the challenges of combinatorial explosion and noisy rewards.",0
"By interpreting the DeepMind Mathematics Dataset as a program synthesis problem, we transform it into a reinforcement learning environment. In this environment, every action taken involves the addition of an operator or an input into a compute graph that is discrete. If the graph computes accurate answers, positive rewards are obtained, allowing for the optimization of a policy that generates compute graphs based on problem statements. We trained Double DQN baseline models on different problem types, proving that it is possible to learn how to construct graphs correctly despite the challenges of noisy rewards and combinatorial explosion.",1
"The recent booming of entropy-regularized literature reveals that Kullback-Leibler (KL) regularization brings advantages to Reinforcement Learning (RL) algorithms by canceling out errors under mild assumptions. However, existing analyses focus on fixed regularization with a constant weighting coefficient and have not considered the case where the coefficient is allowed to change dynamically. In this paper, we study the dynamic coefficient scheme and present the first asymptotic error bound. Based on the dynamic coefficient error bound, we propose an effective scheme to tune the coefficient according to the magnitude of error in favor of more robust learning. On top of this development, we propose a novel algorithm: Geometric Value Iteration (GVI) that features a dynamic error-aware KL coefficient design aiming to mitigate the impact of errors on the performance. Our experiments demonstrate that GVI can effectively exploit the trade-off between learning speed and robustness over uniform averaging of constant KL coefficient. The combination of GVI and deep networks shows stable learning behavior even in the absence of a target network where algorithms with a constant KL coefficient would greatly oscillate or even fail to converge.",0
"The popularity of entropy-regularized literature has shown that Kullback-Leibler (KL) regularization is beneficial to Reinforcement Learning (RL) algorithms as it can eliminate errors with mild assumptions. However, previous analyses have only focused on a fixed regularization with a constant weighting coefficient and have not explored the potential of a dynamically changing coefficient. Our paper investigates the dynamic coefficient scheme and presents the first asymptotic error bound, proposing an effective method to adjust the coefficient based on the error magnitude for more robust learning. Building on this development, we introduce a novel algorithm, Geometric Value Iteration (GVI), which utilizes a dynamic error-aware KL coefficient design to reduce the impact of errors on performance. Our experiments reveal that GVI can effectively balance learning speed and robustness, outperforming uniform averaging of constant KL coefficient. Furthermore, combining GVI with deep networks results in stable learning behavior, even without a target network, where algorithms with a constant KL coefficient would oscillate or fail to converge.",1
"Reward function specification, which requires considerable human effort and iteration, remains a major impediment for learning behaviors through deep reinforcement learning. In contrast, providing visual demonstrations of desired behaviors often presents an easier and more natural way to teach agents. We consider a setting where an agent is provided a fixed dataset of visual demonstrations illustrating how to perform a task, and must learn to solve the task using the provided demonstrations and unsupervised environment interactions. This setting presents a number of challenges including representation learning for visual observations, sample complexity due to high dimensional spaces, and learning instability due to the lack of a fixed reward or learning signal. Towards addressing these challenges, we develop a variational model-based adversarial imitation learning (V-MAIL) algorithm. The model-based approach provides a strong signal for representation learning, enables sample efficiency, and improves the stability of adversarial training by enabling on-policy learning. Through experiments involving several vision-based locomotion and manipulation tasks, we find that V-MAIL learns successful visuomotor policies in a sample-efficient manner, has better stability compared to prior work, and also achieves higher asymptotic performance. We further find that by transferring the learned models, V-MAIL can learn new tasks from visual demonstrations without any additional environment interactions. All results including videos can be found online at \url{https://sites.google.com/view/variational-mail}.",0
"Learning behaviors through deep reinforcement learning is hindered by the substantial human effort and iteration required for reward function specification. In contrast, teaching agents through visual demonstrations of desired behaviors is often an easier and more natural approach. In our study, we explore a scenario where an agent must learn to complete a task using a set of fixed visual demonstrations and unsupervised interactions with the environment. However, this approach presents several challenges, such as learning instability due to the absence of a fixed reward signal, high dimensionality, and the need for representation learning for visual observations. To tackle these challenges, we introduce the variational model-based adversarial imitation learning (V-MAIL) algorithm, which employs a model-based strategy for representation learning and enables on-policy learning to improve adversarial training. Our experiments show that V-MAIL can efficiently learn successful visuomotor policies, exhibits better stability than previous approaches, and achieves higher asymptotic performance. Moreover, we demonstrate that V-MAIL can learn new tasks from visual demonstrations without requiring additional environment interactions. For more information, including videos, please visit \url{https://sites.google.com/view/variational-mail}.",1
This survey article has grown out of the RL4ED workshop organized by the authors at the Educational Data Mining (EDM) 2021 conference. We organized this workshop as part of a community-building effort to bring together researchers and practitioners interested in the broad areas of reinforcement learning (RL) and education (ED). This article aims to provide an overview of the workshop activities and summarize the main research directions in the area of RL for ED.,0
The authors of this survey article conducted the RL4ED workshop during the Educational Data Mining (EDM) 2021 conference with the objective of fostering a community of researchers and practitioners focused on the intersection of reinforcement learning (RL) and education (ED). The purpose of this article is to outline the workshop's activities and highlight the key research avenues in RL for ED.,1
"Black-box machine learning learning methods are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Distribution-free uncertainty quantification (distribution-free UQ) is a user-friendly paradigm for creating statistically rigorous confidence intervals/sets for such predictions. Critically, the intervals/sets are valid without distributional assumptions or model assumptions, with explicit guarantees with finitely many datapoints. Moreover, they adapt to the difficulty of the input; when the input example is difficult, the uncertainty intervals/sets are large, signaling that the model might be wrong. Without much work, one can use distribution-free methods on any underlying algorithm, such as a neural network, to produce confidence sets guaranteed to contain the ground truth with a user-specified probability, such as 90%. Indeed, the methods are easy-to-understand and general, applying to many modern prediction problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed at a reader interested in the practical implementation of distribution-free UQ, including conformal prediction and related methods, who is not necessarily a statistician. We will include many explanatory illustrations, examples, and code samples in Python, with PyTorch syntax. The goal is to provide the reader a working understanding of distribution-free UQ, allowing them to put confidence intervals on their algorithms, with one self-contained document.",0
"Machine learning methods that utilize black-box techniques are commonly employed in high-risk environments such as medical diagnostics, where the need for accuracy is paramount to avoid failures. Distribution-free uncertainty quantification (distribution-free UQ) is an accessible approach that ensures the creation of statistically sound confidence intervals/sets for these predictions. Importantly, these intervals/sets do not require any distributional or model assumptions and provide explicit guarantees even with limited data points. These intervals/sets also adjust to the complexity of the input, where challenging examples result in larger uncertainty intervals/sets that indicate potential flaws in the model. Distribution-free methods can be effortlessly utilized on any underlying algorithm, including neural networks, to generate confidence sets that contain the true value with a specified probability. This method is applicable to various prediction problems, such as natural language processing, computer vision, and deep reinforcement learning. This practical guide aims to provide individuals interested in implementing distribution-free UQ, including conformal prediction and similar methods, with an understanding of the process. The guide will feature numerous explanations, examples, and code samples in Python with PyTorch syntax to enable the reader to place confidence intervals on their algorithms using a single document.",1
"Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",0
"Model-based Reinforcement Learning (RL) is a well-liked learning approach that has the potential to be more sample efficient than model-free RL. However, current empirical model-based RL methods lack the ability to explore. This study examines a model-based algorithm that is both computationally and statistically efficient for Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). Our algorithm guarantees polynomial sample complexity for both models and only requires access to a planning oracle. Our experimental results first demonstrate the flexibility and effectiveness of the algorithm on challenging control tasks where current empirical model-based RL methods fail to perform. We then prove that our approach maintains excellent performance even on common dense reward control benchmarks that do not necessitate extensive exploration. Lastly, we illustrate that our method can efficiently conduct reward-free exploration. Our code is available at https://github.com/yudasong/PCMLP.",1
"Video streaming services strive to support high-quality videos at higher resolutions and frame rates to improve the quality of experience (QoE). However, high-quality videos consume considerable amounts of energy on mobile devices. This paper proposes NeuSaver, which reduces the power consumption of mobile devices when streaming videos by applying an adaptive frame rate to each video chunk without compromising user experience. NeuSaver generates an optimal policy that determines the appropriate frame rate for each video chunk using reinforcement learning (RL). The RL model automatically learns the policy that maximizes the QoE goals based on previous observations. NeuSaver also uses an asynchronous advantage actor-critic algorithm to reinforce the RL model quickly and robustly. Streaming servers that support NeuSaver preprocesses videos into segments with various frame rates, which is similar to the process of creating videos with multiple bit rates in dynamic adaptive streaming over HTTP. NeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in various experiments and a user study through four video categories along with the state-of-the-art model. Our experiments showed that NeuSaver effectively reduces the power consumption of mobile devices when streaming video by an average of 16.14% and up to 23.12% while achieving high QoE.",0
"The goal of video streaming services is to enhance the quality of experience (QoE) by supporting high-quality videos with higher resolutions and frame rates. However, this comes with a downside, as high-quality videos consume a lot of energy on mobile devices. To address this issue, NeuSaver proposes a solution that reduces power consumption by applying an adaptive frame rate to each video chunk. This is achieved without compromising the user experience. NeuSaver generates an optimal policy by utilizing reinforcement learning (RL) to determine the appropriate frame rate for each video chunk based on previous observations. The RL model learns automatically, maximizing QoE goals. NeuSaver uses the commonly used H.264 video codec, and streaming servers that support NeuSaver preprocess videos into segments with varying frame rates. This is similar to creating videos with multiple bit rates in dynamic adaptive streaming over HTTP. An asynchronous advantage actor-critic algorithm is used to reinforce the RL model quickly and robustly. In various experiments and a user study, NeuSaver effectively reduces power consumption by an average of 16.14% and up to 23.12% while achieving high QoE.",1
"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",0
"The effectiveness of different algorithms and methods in the empirical machine learning (ML) world is determined through benchmarks. This paper introduces the concept of a ""benchmark lottery"" to describe the fragility of the ML benchmarking process. According to the benchmark lottery, factors other than algorithmic superiority can lead to a method being perceived as superior. By conducting experiments on prevalent benchmark setups in the ML community, we demonstrate that the relative performance of algorithms can be significantly altered by choosing different benchmark tasks, which highlights the fragility of current paradigms and potential flawed interpretations derived from benchmarking ML methods. We argue that every benchmark makes a statement about what it deems important, which could result in biased progress in the community. To address this issue, we suggest using multiple machine learning domains and communities as examples, such as natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning. We also discuss the implications of the observed phenomena and provide recommendations for mitigating them.",1
"This paper presents a deep Inverse Reinforcement Learning (IRL) framework that can learn an a priori unknown number of nonlinear reward functions from unlabeled experts' demonstrations. For this purpose, we employ the tools from Dirichlet processes and propose an adaptive approach to simultaneously account for both complex and unknown number of reward functions. Using the conditional maximum entropy principle, we model the experts' multi-intention behaviors as a mixture of latent intention distributions and derive two algorithms to estimate the parameters of the deep reward network along with the number of experts' intentions from unlabeled demonstrations. The proposed algorithms are evaluated on three benchmarks, two of which have been specifically extended in this study for multi-intention IRL, and compared with well-known baselines. We demonstrate through several experiments the advantages of our algorithms over the existing approaches and the benefits of online inferring, rather than fixing beforehand, the number of expert's intentions.",0
"In this article, we introduce a robust Inverse Reinforcement Learning (IRL) framework that can identify multiple nonlinear reward functions from expert demonstrations without prior knowledge. To accomplish this, we employ Dirichlet processes and propose an adaptive method that can handle complex and unknown numbers of reward functions. We utilize the conditional maximum entropy principle to model multi-intention behaviors as a combination of latent intention distributions and develop two algorithms to estimate deep reward network parameters and the number of expert intentions from unlabeled demonstrations. We evaluate our algorithms on three benchmarks, including two that we expanded specifically for multi-intention IRL, and compare them to well-known baselines. Through various experiments, we demonstrate the advantages of our algorithms over existing methods and the benefits of inferring the number of expert intentions online rather than predetermined.",1
"Random exploration is one of the main mechanisms through which reinforcement learning (RL) finds well-performing policies. However, it can lead to undesirable or catastrophic outcomes when learning online in safety-critical environments. In fact, safe learning is one of the major obstacles towards real-world agents that can learn during deployment. One way of ensuring that agents respect hard limitations is to explicitly configure boundaries in which they can operate. While this might work in some cases, we do not always have clear a-priori information which states and actions can lead dangerously close to hazardous states. Here, we present an approach where an additional policy can override the main policy and offer a safer alternative action. In our instinct-regulated RL (IR^2L) approach, an ""instinctual"" network is trained to recognize undesirable situations, while guarding the learning policy against entering them. The instinct network is pre-trained on a single task where it is safe to make mistakes, and transferred to environments in which learning a new task safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain, in which it receives a significantly lower number of safety violations during training than a baseline RL approach while reaching similar task performance.",0
"Reinforcement learning (RL) employs random exploration as a primary means of finding effective policies. However, this approach can lead to unwanted or harmful outcomes when learning in safety-critical environments. Consequently, safe learning poses a significant challenge towards the development of real-world agents capable of learning while in operation. Although setting explicit boundaries within which the agents can operate can ensure adherence to limitations, the lack of clear a-priori knowledge regarding hazardous states and actions makes this approach unreliable. To circumvent this problem, we propose an instinct-regulated RL (IR^2L) approach that utilizes an additional policy to provide a safer alternative action. In the IR^2L approach, an instinctual network is trained to recognize undesirable situations and prevent the learning policy from entering them. The instinct network is pre-trained on a single task, where making mistakes is safe, and then transferred to environments that require safe learning of new tasks. We evaluate IR^2L in the OpenAI Safety gym domain, where it significantly outperforms the baseline RL approach in terms of safety violations while achieving similar task performance.",1
"A deep reinforcement learning (DRL) agent observes its states through observations, which may contain natural measurement errors or adversarial noises. Since the observations deviate from the true states, they can mislead the agent into making suboptimal actions. Several works have shown this vulnerability via adversarial attacks, but existing approaches on improving the robustness of DRL under this setting have limited success and lack for theoretical principles. We show that naively applying existing techniques on improving robustness for classification tasks, like adversarial training, is ineffective for many RL tasks. We propose the state-adversarial Markov decision process (SA-MDP) to study the fundamental properties of this problem, and develop a theoretically principled policy regularization which can be applied to a large family of DRL algorithms, including proximal policy optimization (PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for both discrete and continuous action control problems. We significantly improve the robustness of PPO, DDPG and DQN agents under a suite of strong white box adversarial attacks, including new attacks of our own. Additionally, we find that a robust policy noticeably improves DRL performance even without an adversary in a number of environments. Our code is available at https://github.com/chenhongge/StateAdvDRL.",0
"The observations received by a deep reinforcement learning (DRL) agent may not accurately represent the true states due to measurement errors or adversarial noises, leading to suboptimal actions. Previous attempts to improve the robustness of DRL have had limited success and lack theoretical grounding. Adversarial training, a technique used to improve robustness in classification tasks, is found to be ineffective for many RL tasks. To address this problem, we introduce the state-adversarial Markov decision process (SA-MDP) and develop a theoretically principled policy regularization that can be applied to a variety of DRL algorithms, including PPO, DDPG, and DQN, for both discrete and continuous action control tasks. Our approach significantly improves the robustness of DRL agents against strong adversarial attacks, and even improves DRL performance in the absence of an adversary in certain environments. Our code can be found at https://github.com/chenhongge/StateAdvDRL.",1
"We propose policy-gradient algorithms for solving the problem of control in a risk-sensitive reinforcement learning (RL) context. The objective of our algorithm is to maximize the distorted risk measure (DRM) of the cumulative reward in an episodic Markov decision process (MDP). We derive a variant of the policy gradient theorem that caters to the DRM objective. Using this theorem in conjunction with a likelihood ratio (LR) based gradient estimation scheme, we propose policy gradient algorithms for optimizing DRM in both on-policy and off-policy RL settings. We derive non-asymptotic bounds that establish the convergence of our algorithms to an approximate stationary point of the DRM objective.",0
"Our proposal involves utilizing policy-gradient algorithms to tackle the issue of control within a risk-sensitive reinforcement learning (RL) environment. Our algorithm aims to maximize the cumulative reward's distorted risk measure (DRM) in an episodic Markov decision process (MDP). We have created a policy gradient theorem variation that is specifically designed for the DRM objective. By utilizing this theorem and a gradient estimation scheme based on likelihood ratios (LR), we have developed policy gradient algorithms that optimize DRM in both on-policy and off-policy RL settings. We have established non-asymptotic bounds that prove the convergence of our algorithms to an approximate stationary point of the DRM objective.",1
"Deep Reinforcement Learning (RL) powered by neural net approximation of the Q function has had enormous empirical success. While the theory of RL has traditionally focused on linear function approximation (or eluder dimension) approaches, little is known about nonlinear RL with neural net approximations of the Q functions. This is the focus of this work, where we study function approximation with two-layer neural networks (considering both ReLU and polynomial activation functions). Our first result is a computationally and statistically efficient algorithm in the generative model setting under completeness for two-layer neural networks. Our second result considers this setting but under only realizability of the neural net function class. Here, assuming deterministic dynamics, the sample complexity scales linearly in the algebraic dimension. In all cases, our results significantly improve upon what can be attained with linear (or eluder dimension) methods.",0
"Neural net approximation of the Q function has proven to be very successful in Deep Reinforcement Learning (RL). However, while RL theory has typically focused on linear function approximation, there is limited knowledge about nonlinear RL using neural net approximations of the Q functions. This study aims to fill this gap by examining function approximation using two-layer neural networks with ReLU and polynomial activation functions. The first outcome is a highly efficient algorithm for the generative model setting, which is complete for two-layer neural networks. The second outcome focuses on the same setting but only assumes realizability of the neural net function class, with the sample complexity scaling linearly in the algebraic dimension assuming deterministic dynamics. In all cases, these results surpass what can be achieved using linear techniques.",1
"We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the agent's trajectory that improves the sample efficiency in sparse-reward MDPs. We show that any optimal policy necessarily satisfies the k-SP constraint. Notably, the k-SP constraint prevents the policy from exploring state-action pairs along the non-k-SP trajectories (e.g., going back and forth). However, in practice, excluding state-action pairs may hinder the convergence of RL algorithms. To overcome this, we propose a novel cost function that penalizes the policy violating SP constraint, instead of completely excluding it. Our numerical experiment in a tabular RL setting demonstrates that the SP constraint can significantly reduce the trajectory space of policy. As a result, our constraint enables more sample efficient learning by suppressing redundant exploration and exploitation. Our experiments on MiniGrid, DeepMind Lab, Atari, and Fetch show that the proposed method significantly improves proximal policy optimization (PPO) and outperforms existing novelty-seeking exploration methods including count-based exploration even in continuous control tasks, indicating that it improves the sample efficiency by preventing the agent from taking redundant actions.",0
"The k-Shortest-Path (k-SP) constraint is suggested as a fresh approach to improve sample efficiency in sparse-reward MDPs by restricting an agent's trajectory. We establish that the k-SP constraint is necessary for any optimal policy. The k-SP constraint restricts the policy from exploring state-action pairs along non-k-SP trajectories, such as going back and forth. However, excluding these pairs may impede the convergence of RL algorithms. To address this, we introduce a cost function that penalizes policy violation of the SP constraint, rather than excluding it entirely. Our tabular RL experiment shows that the SP constraint can significantly reduce policy trajectory space, leading to more sample efficient learning by suppressing redundant exploration and exploitation. Our MiniGrid, DeepMind Lab, Atari, and Fetch experiments demonstrate that the proposed method improves proximal policy optimization (PPO) and outperforms existing novelty-seeking exploration methods, including count-based exploration, even in continuous control tasks. This indicates that it enhances sample efficiency by preventing the agent from taking redundant actions.",1
"Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning, computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with a human performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.",0
"The task of Visual Question Answering (VQA) involves answering questions about an image, which requires a deep understanding of the question and the ability to associate it with various objects in the image. This task is challenging and requires multi-modal reasoning from both computer vision and natural language processing. Our proposed method, Graphhopper, integrates knowledge graph reasoning, computer vision, and natural language processing techniques. We start by creating a scene graph that describes the objects in the image and their relationships. Then, a reinforcement learning agent is trained to navigate through the scene graph to generate reasoning paths, which are used to derive answers. We conducted an experimental study on the GQA dataset, using both manually curated and automatically generated scene graphs. Our results show that Graphhopper performs on par with human performance on manually curated scene graphs and outperforms another state-of-the-art model on both manually curated and automatically generated scene graphs by a significant margin.",1
"Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite.",0
"Deploying reinforcement learning (RL) systems in the real world presents a significant challenge in terms of generalization. This paper proposes new approaches to address this challenge, focusing on the sequential structure of the RL problem. While supervised learning techniques can generalize effectively without accounting for epistemic uncertainty, this paper shows that this is not the case in RL. Generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, which turns fully-observed MDPs into POMDPs. To solve this problem, the paper proposes the epistemic POMDP, which is an induced partially observed Markov decision process. The paper demonstrates the failure modes of algorithms that do not handle this partial observability and suggests a simple ensemble-based technique for approximately solving the problem. Empirically, the paper shows that the proposed algorithm achieves significant gains in generalization over current methods on the Procgen benchmark suite.",1
"We study model-based offline Reinforcement Learning with general function approximation. We present an algorithm named Constrained Pessimistic Policy Optimization (CPPO) which leverages a general function class and uses a constraint to encode pessimism. Under the assumption that the ground truth model belongs to our function class, CPPO can learn with the offline data only providing partial coverage, i.e., it can learn a policy that completes against any policy that is covered by the offline data, in polynomial sample complexity with respect to the statistical complexity of the function class. We then demonstrate that this algorithmic framework can be applied to many specialized Markov Decision Processes where the additional structural assumptions can further refine the concept of partial coverage. One notable example is low-rank MDP with representation learning where the partial coverage is defined using the concept of relative condition number measured by the underlying unknown ground truth feature representation. Finally, we introduce and study the Bayesian setting in offline RL. The key benefit of Bayesian offline RL is that algorithmically, we do not need to explicitly construct pessimism or reward penalty which could be hard beyond models with linear structures. We present a posterior sampling-based incremental policy optimization algorithm (PS-PO) which proceeds by iteratively sampling a model from the posterior distribution and performing one-step incremental policy optimization inside the sampled model. Theoretically, in expectation with respect to the prior distribution, PS-PO can learn a near optimal policy under partial coverage with polynomial sample complexity.",0
"Our focus is on model-based offline Reinforcement Learning that uses general function approximation. To this end, we introduce the Constrained Pessimistic Policy Optimization (CPPO) algorithm that employs a general function class and a constraint to encode pessimism. Assuming that the ground truth model belongs to our function class, CPPO can learn through offline data with partial coverage. It can produce a policy that competes against any policy covered by the offline data, with polynomial sample complexity relative to the function class's statistical complexity. We apply this algorithmic framework to specialized Markov Decision Processes, including the low-rank MDP with representation learning, where partial coverage is defined by the relative condition number measured by the underlying unknown feature representation. We also explore the Bayesian setting in offline RL, which eliminates the need for explicit pessimism or reward penalty. We present the Posterior Sampling-based Incremental Policy Optimization (PS-PO) algorithm, which iteratively samples a model from the posterior distribution and performs one-step incremental policy optimization within it. Theoretically, PS-PO can learn a near-optimal policy under partial coverage, with polynomial sample complexity, as expected with respect to the prior distribution.",1
"This paper addresses the problem of decentralized spectrum sharing in vehicle-to-everything (V2X) communication networks. The aim is to provide resource-efficient coexistence of vehicle-to-infrastructure(V2I) and vehicle-to-vehicle(V2V) links. A recent work on the topic proposes a multi-agent reinforcement learning (MARL) approach based on deep Q-learning, which leverages a fingerprint-based deep Q-network (DQN) architecture. This work considers an extension of this framework by combining Double Q-learning (via Double DQN) and transfer learning. The motivation behind is that Double Q-learning can alleviate the problem of overestimation of the action values present in conventional Q-learning, while transfer learning can leverage knowledge acquired by an expert model to accelerate learning in the MARL setting. The proposed algorithm is evaluated in a realistic V2X setting, with synthetic data generated based on a geometry-based propagation model that incorporates location-specific geographical descriptors of the simulated environment(outlines of buildings, foliage, and vehicles). The advantages of the proposed approach are demonstrated via numerical simulations.",0
"The focus of this paper is on resolving the issue of decentralized spectrum sharing within vehicle-to-everything (V2X) communication networks to ensure efficient resource allocation for vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) links. A recent study suggests a multi-agent reinforcement learning (MARL) approach that employs deep Q-learning with a fingerprint-based deep Q-network (DQN) architecture. In this work, the authors extend this approach by combining Double Q-learning utilizing Double DQN and transfer learning. This is done to address the issue of overestimation of action values present in conventional Q-learning and to leverage knowledge acquired by an expert model for faster learning in the MARL setting. The proposed algorithm is tested in a realistic V2X environment, using synthetic data generated via a geometry-based propagation model that includes location-specific geographical descriptors of the simulated surroundings. Numerical simulations demonstrate the advantages of the proposed approach.",1
"Data augmentation is a commonly used approach to improving the generalization of deep learning models. Recent works show that learned data augmentation policies can achieve better generalization than hand-crafted ones. However, most of these works use unified augmentation policies for all samples in a dataset, which is observed not necessarily beneficial for all labels in multi-label classification tasks, i.e., some policies may have negative impacts on some labels while benefitting the others. To tackle this problem, we propose a novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios, where augmentation policies are generated with respect to labels by an augmentation-policy network. The policies are learned via reinforcement learning using policy gradient methods, providing a mapping from instance labels to their optimal augmentation policies. Numerical experiments show that our LB-Aug outperforms previous state-of-the-art augmentation methods by large margins in multiple benchmarks on image and video classification.",0
"The enhancement of deep learning models' generalization is frequently achieved through data augmentation. Recent studies have indicated that learned data augmentation policies can yield superior generalization than manually crafted ones. However, most of these studies employ standardized augmentation policies for all samples in a dataset, which may not be advantageous for all labels in multi-label classification tasks. In other words, certain policies may negatively impact some labels while benefiting others. To overcome this challenge, we introduce a new approach called Label-Based AutoAugmentation (LB-Aug) for multi-label scenarios. The LB-Aug generates augmentation policies with regard to labels using an augmentation-policy network. The policies are learned through reinforcement learning using policy gradient methods, establishing a mapping from instance labels to their optimal augmentation policies. Experimental results demonstrate that our LB-Aug significantly outperforms previous state-of-the-art augmentation methods in numerous benchmarks for image and video classification.",1
"In neural combinatorial optimization (CO), reinforcement learning (RL) can turn a deep neural net into a fast, powerful heuristic solver of NP-hard problems. This approach has a great potential in practical applications because it allows near-optimal solutions to be found without expert guides armed with substantial domain knowledge. We introduce Policy Optimization with Multiple Optima (POMO), an end-to-end approach for building such a heuristic solver. POMO is applicable to a wide range of CO problems. It is designed to exploit the symmetries in the representation of a CO solution. POMO uses a modified REINFORCE algorithm that forces diverse rollouts towards all optimal solutions. Empirically, the low-variance baseline of POMO makes RL training fast and stable, and it is more resistant to local minima compared to previous approaches. We also introduce a new augmentation-based inference method, which accompanies POMO nicely. We demonstrate the effectiveness of POMO by solving three popular NP-hard problems, namely, traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based on POMO shows a significant improvement in performance over all recent learned heuristics. In particular, we achieve the optimality gap of 0.14% with TSP100 while reducing inference time by more than an order of magnitude.",0
"Reinforcement learning can transform a deep neural network into a highly efficient and speedy heuristic solver of NP-hard problems in neural combinatorial optimization (CO). This technique is particularly promising for practical applications since it does not require domain experts to find near-optimal solutions. To create such a heuristic solver, we propose Policy Optimization with Multiple Optima (POMO), an end-to-end method that can be used in a wide range of CO problems. POMO utilizes the symmetries in the CO solution representation and employs a modified REINFORCE algorithm that encourages diverse rollouts towards all optimal solutions. Training with POMO is fast and stable due to its low-variance baseline, which also makes it less susceptible to local minima than previous methods. Additionally, we introduce an augmentation-based inference method that complements POMO well. We demonstrate the effectiveness of POMO by solving three popular NP-hard problems, including traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). Our solver based on POMO significantly outperforms all recent learned heuristics in all three problems, achieving an optimality gap of only 0.14% with TSP100 while reducing inference time by more than tenfold.",1
"We address the problem of model selection for the finite horizon episodic Reinforcement Learning (RL) problem where the transition kernel $P^*$ belongs to a family of models $\mathcal{P}^*$ with finite metric entropy. In the model selection framework, instead of $\mathcal{P}^*$, we are given $M$ nested families of transition kernels $\cP_1 \subset \cP_2 \subset \ldots \subset \cP_M$. We propose and analyze a novel algorithm, namely \emph{Adaptive Reinforcement Learning (General)} (\texttt{ARL-GEN}) that adapts to the smallest such family where the true transition kernel $P^*$ lies. \texttt{ARL-GEN} uses the Upper Confidence Reinforcement Learning (\texttt{UCRL}) algorithm with value targeted regression as a blackbox and puts a model selection module at the beginning of each epoch. Under a mild separability assumption on the model classes, we show that \texttt{ARL-GEN} obtains a regret of $\Tilde{\mathcal{O}}(d_{\mathcal{E}}^*H^2+\sqrt{d_{\mathcal{E}}^* \mathbb{M}^* H^2 T})$, with high probability, where $H$ is the horizon length, $T$ is the total number of steps, $d_{\mathcal{E}}^*$ is the Eluder dimension and $\mathbb{M}^*$ is the metric entropy corresponding to $\mathcal{P}^*$. Note that this regret scaling matches that of an oracle that knows $\mathcal{P}^*$ in advance. We show that the cost of model selection for \texttt{ARL-GEN} is an additive term in the regret having a weak dependence on $T$. Subsequently, we remove the separability assumption and consider the setup of linear mixture MDPs, where the transition kernel $P^*$ has a linear function approximation. With this low rank structure, we propose novel adaptive algorithms for model selection, and obtain (order-wise) regret identical to that of an oracle with knowledge of the true model class.",0
"The focus of our work is on the problem of selecting a suitable model for finite horizon episodic Reinforcement Learning (RL) where the transition kernel is an element of a family of models $\mathcal{P}^*$ with finite metric entropy. Instead of using $\mathcal{P}^*$, we are provided with $M$ nested families of transition kernels $\cP_1 \subset \cP_2 \subset \ldots \subset \cP_M$ in the model selection framework. To address this issue, we introduce a new algorithm called \emph{Adaptive Reinforcement Learning (General)} (\texttt{ARL-GEN}) that adjusts to the smallest family where the true transition kernel $P^*$ is located. The algorithm uses the Upper Confidence Reinforcement Learning (\texttt{UCRL}) algorithm with value targeted regression as a blackbox and includes a model selection module at the start of each epoch. We prove that under a mild separability assumption on the model classes, \texttt{ARL-GEN} achieves a regret of $\Tilde{\mathcal{O}}(d_{\mathcal{E}}^*H^2+\sqrt{d_{\mathcal{E}}^* \mathbb{M}^* H^2 T})$ with high probability, where $H$ represents the horizon length, $T$ is the total number of steps, $d_{\mathcal{E}}^*$ is the Eluder dimension, and $\mathbb{M}^*$ is the metric entropy corresponding to $\mathcal{P}^*$. This regret scaling is equivalent to that of an oracle that has prior knowledge of $\mathcal{P}^*$. We also demonstrate that the cost of model selection for \texttt{ARL-GEN} is an additive term in the regret, which weakly depends on $T$. Furthermore, we extend our analysis to the linear mixture MDPs setup, where the transition kernel $P^*$ has a low-rank structure of linear function approximation. We develop novel adaptive algorithms for model selection in this context and obtain regret scaling identical to that of an oracle with knowledge of the true model class.",1
"Low-light image enhancement (LLIE) is a pervasive yet challenging problem, since: 1) low-light measurements may vary due to different imaging conditions in practice; 2) images can be enlightened subjectively according to diverse preferences by each individual. To tackle these two challenges, this paper presents a novel deep reinforcement learning based method, dubbed ReLLIE, for customized low-light enhancement. ReLLIE models LLIE as a markov decision process, i.e., estimating the pixel-wise image-specific curves sequentially and recurrently. Given the reward computed from a set of carefully crafted non-reference loss functions, a lightweight network is proposed to estimate the curves for enlightening of a low-light image input. As ReLLIE learns a policy instead of one-one image translation, it can handle various low-light measurements and provide customized enhanced outputs by flexibly applying the policy different times. Furthermore, ReLLIE can enhance real-world images with hybrid corruptions, e.g., noise, by using a plug-and-play denoiser easily. Extensive experiments on various benchmarks demonstrate the advantages of ReLLIE, comparing to the state-of-the-art methods.",0
"Enhancing images in low-light conditions is a difficult task due to the variability of measurements and subjective preferences. To address these challenges, the authors propose a deep reinforcement learning-based method called ReLLIE. ReLLIE models low-light image enhancement as a markov decision process and estimates pixel-wise image-specific curves sequentially and recurrently. By using a set of non-reference loss functions, a lightweight network can estimate the curves for enhancing low-light images. ReLLIE learns a policy instead of a one-to-one image translation, allowing it to handle different low-light measurements and provide customized outputs. ReLLIE can also enhance real-world images with hybrid corruptions such as noise by incorporating a plug-and-play denoiser. Extensive experiments demonstrate the superiority of ReLLIE compared to existing methods.",1
"In offline reinforcement learning (RL) an optimal policy is learnt solely from a priori collected observational data. However, in observational data, actions are often confounded by unobserved variables. Instrumental variables (IVs), in the context of RL, are the variables whose influence on the state variables are all mediated through the action. When a valid instrument is present, we can recover the confounded transition dynamics through observational data. We study a confounded Markov decision process where the transition dynamics admit an additive nonlinear functional form. Using IVs, we derive a conditional moment restriction (CMR) through which we can identify transition dynamics based on observational data. We propose a provably efficient IV-aided Value Iteration (IVVI) algorithm based on a primal-dual reformulation of CMR. To the best of our knowledge, this is the first provably efficient algorithm for instrument-aided offline RL.",0
"Offline reinforcement learning involves learning an optimal policy solely from previously collected observational data, but the actions in this data are often influenced by unobserved variables. In the context of RL, instrumental variables (IVs) are variables that only influence state variables through the action. By using a valid instrument, we can recover the confounded transition dynamics from observational data. We examine a confounded Markov decision process where the transition dynamics have an additive nonlinear functional form. Using IVs, we establish a conditional moment restriction (CMR) that enables us to identify transition dynamics from observational data. We introduce an IV-aided Value Iteration (IVVI) algorithm that is provably efficient, based on a primal-dual reformulation of CMR. This is the first algorithm for instrument-aided offline RL that is provably efficient.",1
"Robots learning from observations in the real world using inverse reinforcement learning (IRL) may encounter objects or agents in the environment, other than the expert, that cause nuisance observations during the demonstration. These confounding elements are typically removed in fully-controlled environments such as virtual simulations or lab settings. When complete removal is impossible the nuisance observations must be filtered out. However, identifying the source of observations when large amounts of observations are made is difficult. To address this, we present a hierarchical Bayesian model that incorporates both the expert's and the confounding elements' observations thereby explicitly modeling the diverse observations a robot may receive. We extend an existing IRL algorithm originally designed to work under partial occlusion of the expert to consider the diverse observations. In a simulated robotic sorting domain containing both occlusion and confounding elements, we demonstrate the model's effectiveness. In particular, our technique outperforms several other comparative methods, second only to having perfect knowledge of the subject's trajectory.",0
"Inverse reinforcement learning (IRL) enables robots to learn from real-world observations. However, when using IRL, robots may encounter objects or agents in the environment that are not part of the expert demonstration, resulting in nuisance observations. Such elements are typically removed in fully-controlled environments but must be filtered out when this is not possible. This can be challenging, especially when large amounts of data are involved. To overcome this, we propose a hierarchical Bayesian model that explicitly models the diverse observations a robot may receive from both the expert and confounding elements. We extend an existing IRL algorithm to incorporate these observations. Our approach is effective in a simulated robotic sorting domain that includes both occlusion and confounding elements, outperforming several other comparative methods, second only to having perfect knowledge of the subject's trajectory.",1
"In this paper, we propose cautious policy programming (CPP), a novel value-based reinforcement learning (RL) algorithm that can ensure monotonic policy improvement during learning. Based on the nature of entropy-regularized RL, we derive a new entropy regularization-aware lower bound of policy improvement that only requires estimating the expected policy advantage function. CPP leverages this lower bound as a criterion for adjusting the degree of a policy update for alleviating policy oscillation. Different from similar algorithms that are mostly theory-oriented, we also propose a novel interpolation scheme that makes CPP better scale in high dimensional control problems. We demonstrate that the proposed algorithm can trade o? performance and stability in both didactic classic control problems and challenging high-dimensional Atari games.",0
"The paper introduces a new algorithm called cautious policy programming (CPP) that uses value-based reinforcement learning (RL) to ensure consistent policy improvement during the learning process. The algorithm is designed to address the issues of policy oscillation and instability that can arise in entropy-regularized RL. To achieve this, the authors derive a new lower bound of policy improvement that only requires estimating the expected policy advantage function. CPP uses this lower bound to adjust the degree of policy updates, and the authors also propose an interpolation scheme to improve scalability in high-dimensional control problems. The algorithm is demonstrated to perform well in classic control problems and challenging Atari games, balancing performance and stability.",1
"Learning data representations that are useful for various downstream tasks is a cornerstone of artificial intelligence. While existing methods are typically evaluated on downstream tasks such as classification or generative image quality, we propose to assess representations through their usefulness in downstream control tasks, such as reaching or pushing objects. By training over 10,000 reinforcement learning policies, we extensively evaluate to what extent different representation properties affect out-of-distribution (OOD) generalization. Finally, we demonstrate zero-shot transfer of these policies from simulation to the real world, without any domain randomization or fine-tuning. This paper aims to establish the first systematic characterization of the usefulness of learned representations for real-world OOD downstream tasks.",0
"The foundation of artificial intelligence lies in acquiring valuable data representations that can be applied to a range of downstream tasks. While current approaches are typically assessed based on classification or the quality of generated images, we suggest evaluating representations by their effectiveness in downstream control tasks, such as manipulating or moving objects. Through training more than 10,000 reinforcement learning policies, we thoroughly investigate the extent to which different representation properties influence generalization outside of a given distribution. Ultimately, we prove that these policies can be transferred from simulation to the physical world without domain randomization or fine-tuning. Our paper seeks to establish the first methodical analysis of the practicality of acquired representations for real-world downstream tasks beyond a given distribution.",1
"We tackle the problem of generalization to unseen configurations for dynamic tasks in the real world while learning from high-dimensional image input. The family of nonlinear dynamical system-based methods have successfully demonstrated dynamic robot behaviors but have difficulty in generalizing to unseen configurations as well as learning from image inputs. Recent works approach this issue by using deep network policies and reparameterize actions to embed the structure of dynamical systems but still struggle in domains with diverse configurations of image goals, and hence, find it difficult to generalize. In this paper, we address this dichotomy by leveraging embedding the structure of dynamical systems in a hierarchical deep policy learning framework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of fitting deep dynamical systems to diverse data directly, H-NDPs form a curriculum by learning local dynamical system-based policies on small regions in state-space and then distill them into a global dynamical system-based policy that operates only from high-dimensional images. H-NDPs additionally provide smooth trajectories, a strong safety benefit in the real world. We perform extensive experiments on dynamic tasks both in the real world (digit writing, scooping, and pouring) and simulation (catching, throwing, picking). We show that H-NDPs are easily integrated with both imitation as well as reinforcement learning setups and achieve state-of-the-art results. Video results are at https://shikharbahl.github.io/hierarchical-ndps/",0
"This paper addresses the challenge of generalizing to new situations in dynamic tasks in the real world when using high-dimensional images for learning. While nonlinear dynamical system-based methods have been successful in demonstrating robot behaviors, they face difficulties in generalizing to new situations and learning from image inputs. Recent approaches have used deep network policies and reparameterized actions to embed the structure of dynamical systems, but have struggled to generalize in domains with diverse configurations of image goals. To address this issue, we propose a hierarchical deep policy learning framework called Hierarchical Neural Dynamical Policies (H-NDPs) that embeds the structure of dynamical systems. H-NDPs learn local dynamical system-based policies on small regions in state-space and then distill them into a global dynamical system-based policy that operates only from high-dimensional images. H-NDPs provide smooth trajectories and strong safety benefits in the real world. We conduct experiments on dynamic tasks in both the real world (digit writing, scooping, and pouring) and simulation (catching, throwing, picking) and show that H-NDPs can be easily integrated with both imitation and reinforcement learning setups, achieving state-of-the-art results. Video results can be found at https://shikharbahl.github.io/hierarchical-ndps/.",1
"Reinforcement learning (RL) provides a framework for learning goal-directed policies given user-specified rewards. However, since designing rewards often requires substantial engineering effort, we are interested in the problem of learning without rewards, where agents must discover useful behaviors in the absence of task-specific incentives. Intrinsic motivation is a family of unsupervised RL techniques which develop general objectives for an RL agent to optimize that lead to better exploration or the discovery of skills. In this paper, we propose a new unsupervised RL technique based on an adversarial game which pits two policies against each other to compete over the amount of surprise an RL agent experiences. The policies each take turns controlling the agent. The Explore policy maximizes entropy, putting the agent into surprising or unfamiliar situations. Then, the Control policy takes over and seeks to recover from those situations by minimizing entropy. The game harnesses the power of multi-agent competition to drive the agent to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show empirically that our method leads to the emergence of complex skills by exhibiting clear phase transitions. Furthermore, we show both theoretically (via a latent state space coverage argument) and empirically that our method has the potential to be applied to the exploration of stochastic, partially-observed environments. We show that Adversarial Surprise learns more complex behaviors, and explores more effectively than competitive baselines, outperforming intrinsic motivation methods based on active inference, novelty-seeking (Random Network Distillation (RND)), and multi-agent unsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom environments.",0
"The RL framework enables the learning of policies to achieve user-defined objectives, but the task of designing rewards can be time-consuming. Therefore, we aim to address the challenge of learning without rewards, where agents must discover beneficial behaviors in the absence of task-specific incentives. Intrinsic motivation is a category of unsupervised RL techniques that establish general objectives for an RL agent to optimize, leading to superior exploration or skill discovery. Our paper introduces a novel unsupervised RL technique founded on an adversarial game in which two policies compete to generate the most surprise for an RL agent. The Explore policy maximizes entropy to expose the agent to unexpected or unfamiliar situations, while the Control policy seeks to minimize entropy to recover from those situations. By leveraging multi-agent competition, our method motivates agents to explore progressively more surprising aspects of the environment while developing mastery over them, resulting in the emergence of sophisticated skills. We demonstrate the effectiveness of our approach through empirical evidence, indicating clear phase transitions, and by showing theoretically and empirically that our method has the potential to explore stochastic, partially-observed environments. Our Adversarial Surprise technique exhibits more complex behaviors and explores more effectively than intrinsic motivation methods based on active inference, Random Network Distillation, and Asymmetric Self-Play in MiniGrid, Atari, and VizDoom environments.",1
"The Laplacian representation recently gains increasing attention for reinforcement learning as it provides succinct and informative representation for states, by taking the eigenvectors of the Laplacian matrix of the state-transition graph as state embeddings. Such representation captures the geometry of the underlying state space and is beneficial to RL tasks such as option discovery and reward shaping. To approximate the Laplacian representation in large (or even continuous) state spaces, recent works propose to minimize a spectral graph drawing objective, which however has infinitely many global minimizers other than the eigenvectors. As a result, their learned Laplacian representation may differ from the ground truth. To solve this problem, we reformulate the graph drawing objective into a generalized form and derive a new learning objective, which is proved to have eigenvectors as its unique global minimizer. It enables learning high-quality Laplacian representations that faithfully approximate the ground truth. We validate this via comprehensive experiments on a set of gridworld and continuous control environments. Moreover, we show that our learned Laplacian representations lead to more exploratory options and better reward shaping.",0
"Reinforcement learning has recently been focusing on the Laplacian representation, which offers a concise and informative depiction of states. This representation utilizes eigenvectors of the Laplacian matrix from the state-transition graph to create state embeddings, effectively capturing the geometry of the underlying state space and enhancing RL tasks such as option discovery and reward shaping. However, approximating the Laplacian representation in large or continuous state spaces poses challenges. Previous attempts to minimize a spectral graph drawing objective resulted in infinitely many global minimizers other than the eigenvectors. To address this issue, we have redefined the graph drawing objective to produce a new learning objective with eigenvectors as its exclusive global minimizer. Our approach enables the development of high-quality Laplacian representations that accurately approximate the ground truth. We have validated our method through extensive experiments on various gridworld and continuous control environments, demonstrating that our learned Laplacian representations lead to more exploratory options and better reward shaping.",1
"Many reinforcement learning (RL) problems in practice are offline, learning purely from observational data. A key challenge is how to ensure the learned policy is safe, which requires quantifying the risk associated with different actions. In the online setting, distributional RL algorithms do so by learning the distribution over returns (i.e., cumulative rewards) instead of the expected return; beyond quantifying risk, they have also been shown to learn better representations for planning. We propose Conservative Offline Distributional Actor Critic (CODAC), an offline RL algorithm suitable for both risk-neutral and risk-averse domains. CODAC adapts distributional RL to the offline setting by penalizing the predicted quantiles of the return for out-of-distribution actions. We prove that CODAC learns a conservative return distribution -- in particular, for finite MDPs, CODAC converges to an uniform lower bound on the quantiles of the return distribution; our proof relies on a novel analysis of the distributional Bellman operator. In our experiments, on two challenging robot navigation tasks, CODAC successfully learns risk-averse policies using offline data collected purely from risk-neutral agents. Furthermore, CODAC is state-of-the-art on the D4RL MuJoCo benchmark in terms of both expected and risk-sensitive performance.",0
"A majority of reinforcement learning (RL) problems are offline and rely solely on observational data for learning. The main obstacle in such scenarios is ensuring the safety of the learned policy by quantifying the risk associated with different actions. Distributional RL algorithms mitigate this concern in online settings by learning the distribution over returns instead of the expected return and have also been shown to improve planning capabilities. To address this issue in offline settings, we propose the Conservative Offline Distributional Actor Critic (CODAC), a versatile RL algorithm that caters to both risk-neutral and risk-averse domains. CODAC penalizes the predicted quantiles of the return for out-of-distribution actions, resulting in a conservative return distribution. Our proof of convergence relies on novel analysis of the distributional Bellman operator. CODAC successfully learns risk-averse policies in two challenging robot navigation tasks by utilizing offline data collected purely from risk-neutral agents. Furthermore, CODAC outperforms state-of-the-art algorithms on the D4RL MuJoCo benchmark in both expected and risk-sensitive performance.",1
"In offline reinforcement learning, a policy needs to be learned from a single pre-collected dataset. Typically, policies are thus regularized during training to behave similarly to the data generating policy, by adding a penalty based on a divergence between action distributions of generating and trained policy. We propose a new algorithm, which constrains the policy directly in its weight space instead, and demonstrate its effectiveness in experiments.",0
"The process of learning a policy in offline reinforcement learning involves using a pre-collected dataset. During training, policies are often regulated to mimic the data generating policy through a penalty based on the divergence between action distributions of the generating and trained policy. Our proposed algorithm takes a different approach by directly constraining the policy in its weight space, and our experiments prove its efficacy.",1
"Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (CoBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. CoBERL enables efficient, robust learning from pixels across a wide range of domains. We use bidirectional masked prediction in combination with a generalization of recent contrastive methods to learn better representations for transformers in RL, without the need of hand engineered data augmentations. We find that CoBERL consistently improves performance across the full Atari suite, a set of control tasks and a challenging 3D environment.",0
"To solve tasks, many RL agents require a significant amount of experience. Our proposed solution, CoBERL, combines a hybrid LSTM-transformer architecture and a new contrastive loss to improve data efficiency. CoBERL enables robust and efficient learning from pixels across various domains without the need for hand-engineered data augmentations. We use bidirectional masked prediction and a generalization of recent contrastive methods to develop better representations for transformers in RL. We find that CoBERL consistently enhances performance in the full Atari suite, control tasks, and a challenging 3D environment.",1
"To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of Reinforcement Learning has been proven potential. In Reinforcement Learning, it is crucial to represent states and assign rewards based on the action-caused transitions of states. However, the state representation in previous Visual Dialogue works uses the textual information only and its transitions are implicit. In this paper, we propose Explicit Concerning States (ECS) to represent what visual contents are concerned at each round and what have been concerned throughout the Visual Dialogue. ECS is modeled from multimodal information and is represented explicitly. Based on ECS, we formulate two intuitive and interpretable rewards to encourage the Visual Dialogue agents to converse on diverse and informative visual information. Experimental results on the VisDial v1.0 dataset show our method enables the Visual Dialogue agents to generate more visual coherent, less repetitive and more visual informative dialogues compared with previous methods, according to multiple automatic metrics, human study and qualitative analysis.",0
"The use of Reinforcement Learning has been shown to have potential in encouraging AI agents to conduct meaningful Visual Dialogue (VD). To effectively use Reinforcement Learning, it is important to represent states and assign rewards based on the action-caused transitions of states. However, previous works on Visual Dialogue have only used textual information and implicit transitions, which is not enough. In this study, we introduce Explicit Concerning States (ECS) that represent visual contents explicitly and are modeled from multimodal information. We propose two intuitive and interpretable rewards based on ECS to encourage the Visual Dialogue agents to converse on diverse and informative visual information. Our experimental results on the VisDial v1.0 dataset show that our method generates more visual coherent, less repetitive, and more visual informative dialogues compared to previous methods, based on multiple automatic metrics, human study, and qualitative analysis.",1
"In constrained reinforcement learning (RL), a learning agent seeks to not only optimize the overall reward but also satisfy the additional safety, diversity, or budget constraints. Consequently, existing constrained RL solutions require several new algorithmic ingredients that are notably different from standard RL. On the other hand, reward-free RL is independently developed in the unconstrained literature, which learns the transition dynamics without using the reward information, and thus naturally capable of addressing RL with multiple objectives under the common dynamics. This paper bridges reward-free RL and constrained RL. Particularly, we propose a simple meta-algorithm such that given any reward-free RL oracle, the approachability and constrained RL problems can be directly solved with negligible overheads in sample complexity. Utilizing the existing reward-free RL solvers, our framework provides sharp sample complexity results for constrained RL in the tabular MDP setting, matching the best existing results up to a factor of horizon dependence; our framework directly extends to a setting of tabular two-player Markov games, and gives a new result for constrained RL with linear function approximation.",0
"Constrained reinforcement learning involves optimizing the overall reward while also satisfying additional safety, diversity, or budget constraints. This requires novel algorithmic ingredients that differ from standard reinforcement learning. Reward-free reinforcement learning, on the other hand, learns transition dynamics without using reward information and can naturally address multiple objectives under common dynamics. This study connects reward-free reinforcement learning and constrained reinforcement learning by proposing a meta-algorithm that solves the approachability and constrained RL problems with minimal overheads in sample complexity, using any reward-free RL oracle. Our framework provides precise sample complexity results for constrained RL in the tabular MDP setting, matching the best existing results up to a horizon dependence factor. Furthermore, it extends to tabular two-player Markov games and offers a new result for constrained RL with linear function approximation.",1
"Many reinforcement learning (RL) environments in practice feature enormous state spaces that may be described compactly by a ""factored"" structure, that may be modeled by Factored Markov Decision Processes (FMDPs). We present the first polynomial-time algorithm for RL with FMDPs that does not rely on an oracle planner, and instead of requiring a linear transition model, only requires a linear value function with a suitable local basis with respect to the factorization. With this assumption, we can solve FMDPs in polynomial time by constructing an efficient separation oracle for convex optimization. Importantly, and in contrast to prior work, we do not assume that the transitions on various factors are independent.",0
"RL environments often have large state spaces that can be described compactly using a ""factored"" structure. These structures can be modeled using FMDPs. We have developed a new algorithm for RL with FMDPs that does not rely on an oracle planner and only requires a linear value function with a local basis. By making this assumption, we can solve FMDPs in polynomial time using an efficient separation oracle for convex optimization. In contrast to previous work, we do not assume that transitions on various factors are independent.",1
"We design a simple reinforcement learning (RL) agent that implements an optimistic version of $Q$-learning and establish through regret analysis that this agent can operate with some level of competence in any environment. While we leverage concepts from the literature on provably efficient RL, we consider a general agent-environment interface and provide a novel agent design and analysis. This level of generality positions our results to inform the design of future agents for operation in complex real environments. We establish that, as time progresses, our agent performs competitively relative to policies that require longer times to evaluate. The time it takes to approach asymptotic performance is polynomial in the complexity of the agent's state representation and the time required to evaluate the best policy that the agent can represent. Notably, there is no dependence on the complexity of the environment. The ultimate per-period performance loss of the agent is bounded by a constant multiple of a measure of distortion introduced by the agent's state representation. This work is the first to establish that an algorithm approaches this asymptotic condition within a tractable time frame.",0
"We created a reinforcement learning (RL) agent using an optimistic approach to $Q$-learning, which we found to be competent in any environment based on regret analysis. Our agent design and analysis consider a general agent-environment interface, drawing from prior literature on provably efficient RL. Our results have broad applicability for designing future agents to operate in complex environments. Over time, our agent's performance competes well with policies that require longer evaluation times. The time it takes for our agent to approach asymptotic performance is polynomial and independent of the environment's complexity. We bound the ultimate per-period performance loss of the agent by a constant multiple of a measure of distortion introduced by its state representation. This work is the first to show that the algorithm can achieve this asymptotic condition within a feasible timeframe.",1
"This work introduces Bilinear Classes, a new structural framework, which permit generalization in reinforcement learning in a wide variety of settings through the use of function approximation. The framework incorporates nearly all existing models in which a polynomial sample complexity is achievable, and, notably, also includes new models, such as the Linear $Q^*/V^*$ model in which both the optimal $Q$-function and the optimal $V$-function are linear in some known feature space. Our main result provides an RL algorithm which has polynomial sample complexity for Bilinear Classes; notably, this sample complexity is stated in terms of a reduction to the generalization error of an underlying supervised learning sub-problem. These bounds nearly match the best known sample complexity bounds for existing models. Furthermore, this framework also extends to the infinite dimensional (RKHS) setting: for the the Linear $Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample complexities that have no explicit dependence on the explicit feature dimension (which could be infinite), but instead depends only on information theoretic quantities.",0
"In this work, a new structural framework called Bilinear Classes is presented which allows for generalization in reinforcement learning across a wide range of settings through the use of function approximation. The framework encompasses almost all existing models that achieve polynomial sample complexity, and also includes new models like the Linear $Q^*/V^*$ model, where the optimal $Q$-function and $V$-function are both linear in a known feature space. The main contribution of this work is the introduction of an RL algorithm, with polynomial sample complexity for Bilinear Classes, which is linked to the generalization error of a supervised learning sub-problem. These bounds are almost as good as the best-known sample complexity bounds for existing models. Additionally, the framework extends to the infinite dimensional setting, where sample complexities are provided for the Linear $Q^*/V^*$ model, linear MDPs, and linear mixture MDPs that do not depend explicitly on the feature dimension but instead depend on information theoretic quantities.",1
"We consider distributed machine learning (ML) through unmanned aerial vehicles (UAVs) for geo-distributed device clusters. We propose five new technologies/techniques: (i) stratified UAV swarms with leader, worker, and coordinator UAVs, (ii) hierarchical nested personalized federated learning (HN-PFL): a holistic distributed ML framework for personalized model training across the worker-leader-core network hierarchy, (iii) cooperative UAV resource pooling for distributed ML using the UAVs' local computational capabilities, (iv) aerial data caching and relaying for efficient data relaying to conduct ML, and (v) concept/model drift, capturing online data variations at the devices. We split the UAV-enabled model training problem as two parts. (a) Network-aware HN-PFL, where we optimize a tradeoff between energy consumption and ML model performance by configuring data offloading among devices-UAVs and UAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to communication/computation network heterogeneity. We tackle this optimization problem via the method of posynomial condensation and propose a distributed algorithm with a performance guarantee. (b) Macro-trajectory and learning duration design, which we formulate as a sequential decision making problem, tackled via deep reinforcement learning. Our simulations demonstrate the superiority of our methodology with regards to the distributed ML performance, the optimization of network resources, and the swarm trajectory efficiency.",0
"Our focus is on using unmanned aerial vehicles (UAVs) for distributed machine learning (ML) in a geo-distributed device cluster. To achieve this, we propose five new technologies/techniques. Firstly, we suggest stratified UAV swarms consisting of leader, worker, and coordinator UAVs. Secondly, we propose hierarchical nested personalized federated learning (HN-PFL), which is a comprehensive framework for personalized model training across the worker-leader-core network hierarchy. Thirdly, we suggest cooperative UAV resource pooling for distributed ML, which utilizes the local computational capabilities of UAVs. Fourthly, we propose aerial data caching and relaying to efficiently relay data for ML. Lastly, we introduce the concept/model drift, which captures online data variations at the devices. We divide the UAV-enabled model training problem into two parts. Firstly, we optimize the tradeoff between energy consumption and ML model performance via the Network-aware HN-PFL, which configures data offloading among devices-UAVs and UAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes, subject to communication/computation network heterogeneity. We tackle this optimization problem using the method of posynomial condensation and propose a distributed algorithm with a performance guarantee. Secondly, we formulate the macro-trajectory and learning duration design as a sequential decision-making problem tackled via deep reinforcement learning. Our simulations demonstrate the superiority of our methodology in terms of distributed ML performance, network resource optimization, and swarm trajectory efficiency.",1
"We study the statistical theory of offline reinforcement learning (RL) with deep ReLU network function approximation. We analyze a variant of fitted-Q iteration (FQI) algorithm under a new dynamic condition that we call Besov dynamic closure, which encompasses the conditions from prior analyses for deep neural network function approximation. Under Besov dynamic closure, we prove that the FQI-type algorithm enjoys the sample complexity of $\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 - 2d/\alpha} \right)$ where $\kappa$ is a distribution shift measure, $d$ is the dimensionality of the state-action space, $\alpha$ is the (possibly fractional) smoothness parameter of the underlying MDP, and $\epsilon$ is a user-specified precision. This is an improvement over the sample complexity of $\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 - d/\alpha} \right)$ in the prior result [Yang et al., 2019] where $K$ is an algorithmic iteration number which is arbitrarily large in practice. Importantly, our sample complexity is obtained under the new general dynamic condition and a data-dependent structure where the latter is either ignored in prior algorithms or improperly handled by prior analyses. This is the first comprehensive analysis for offline RL with deep ReLU network function approximation under a general setting.",0
"The focus of our study is the statistical theory of offline reinforcement learning (RL) with deep ReLU network function approximation. Our analysis centers on a modified version of the fitted-Q iteration (FQI) algorithm, which we subject to a new dynamic condition that we term Besov dynamic closure. This condition encompasses the conditions previously established for deep neural network function approximation. We demonstrate that under Besov dynamic closure, the FQI-type algorithm achieves sample complexity of $\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 - 2d/\alpha} \right)$, where $\kappa$ represents the distribution shift measure, $d$ denotes the dimensionality of the state-action space, $\alpha$ is the smoothness parameter of the underlying MDP (which may be fractional), and $\epsilon$ stands for user-specified precision. Our sample complexity improvement is significant, as it surpasses that of prior results, which was $\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 - d/\alpha} \right)$, where $K$ represents an algorithmic iteration number that can be arbitrarily large in practice. Moreover, our sample complexity is derived from a new general dynamic condition and a data-dependent structure that has previously been ignored or improperly handled by prior algorithms and analyses. Our study represents the first comprehensive analysis of offline RL with deep ReLU network function approximation under a general setting.",1
"Recently, many plug-and-play self-attention modules are proposed to enhance the model generalization by exploiting the internal information of deep convolutional neural networks (CNNs). Previous works lay an emphasis on the design of attention module for specific functionality, e.g., light-weighted or task-oriented attention. However, they ignore the importance of where to plug in the attention module since they connect the modules individually with each block of the entire CNN backbone for granted, leading to incremental computational cost and number of parameters with the growth of network depth. Thus, we propose a framework called Efficient Attention Network (EAN) to improve the efficiency for the existing attention modules. In EAN, we leverage the sharing mechanism (Huang et al. 2020) to share the attention module within the backbone and search where to connect the shared attention module via reinforcement learning. Finally, we obtain the attention network with sparse connections between the backbone and modules, while (1) maintaining accuracy (2) reducing extra parameter increment and (3) accelerating inference. Extensive experiments on widely-used benchmarks and popular attention networks show the effectiveness of EAN. Furthermore, we empirically illustrate that our EAN has the capacity of transferring to other tasks and capturing the informative features. The code is available at https://github.com/gbup-group/EAN-efficient-attention-network.",0
"Numerous plug-and-play self-attention modules have recently been proposed to improve model generalization by utilizing internal information from deep convolutional neural networks (CNNs). Prior research has focused on designing attention modules for specific functionalities, such as light-weighted or task-oriented attention, but has neglected to consider where to connect the attention module. This has resulted in increased computational costs and parameter numbers as network depth grows, due to individual module connections with each block of the entire CNN backbone. To address this issue, we introduce the Efficient Attention Network (EAN) framework, which enhances the efficiency of existing attention modules. EAN utilizes a sharing mechanism to share attention modules within the backbone and employs reinforcement learning to determine where to connect the shared attention module. As a result, EAN achieves an attention network with sparse connections between the backbone and modules, while maintaining accuracy, reducing parameter increment, and accelerating inference. Extensive experiments on widely-used benchmarks and popular attention networks demonstrate the effectiveness of EAN, which can also transfer to other tasks and capture informative features. The code is available at https://github.com/gbup-group/EAN-efficient-attention-network.",1
"A core element in decision-making under uncertainty is the feedback on the quality of the performed actions. However, in many applications, such feedback is restricted. For example, in recommendation systems, repeatedly asking the user to provide feedback on the quality of recommendations will annoy them. In this work, we formalize decision-making problems with querying budget, where there is a (possibly time-dependent) hard limit on the number of reward queries allowed. Specifically, we consider multi-armed bandits, linear bandits, and reinforcement learning problems. We start by analyzing the performance of `greedy' algorithms that query a reward whenever they can. We show that in fully stochastic settings, doing so performs surprisingly well, but in the presence of any adversity, this might lead to linear regret. To overcome this issue, we propose the Confidence-Budget Matching (CBM) principle that queries rewards when the confidence intervals are wider than the inverse square root of the available budget. We analyze the performance of CBM based algorithms in different settings and show that they perform well in the presence of adversity in the contexts, initial states, and budgets.",0
"Feedback on executed actions is a crucial component when making decisions under uncertain circumstances. However, there are instances where such feedback is limited, such as in recommendation systems, where asking for constant feedback from users can be bothersome. In this study, we address decision-making challenges with restricted querying budgets, where there is a fixed limit on the number of reward queries permitted. We examine multi-armed bandits, linear bandits, and reinforcement learning problems, starting with the analysis of ""greedy"" algorithms that query rewards whenever possible. While this works well in fully stochastic settings, it results in linear regret when faced with adversity. To address this issue, we propose the Confidence-Budget Matching (CBM) principle, which queries rewards only when the confidence intervals are wider than the inverse square root of the available budget. We assess the performance of CBM-based algorithms in various settings and demonstrate that they are effective in challenging contexts, initial states, and budgets.",1
"We study the problem of out-of-distribution dynamics (OODD) detection, which involves detecting when the dynamics of a temporal process change compared to the training-distribution dynamics. This is relevant to applications in control, reinforcement learning (RL), and multi-variate time-series, where changes to test time dynamics can impact the performance of learning controllers/predictors in unknown ways. This problem is particularly important in the context of deep RL, where learned controllers often overfit to the training environment. Currently, however, there is a lack of established OODD benchmarks for the types of environments commonly used in RL research. Our first contribution is to design a set of OODD benchmarks derived from common RL environments with varying types and intensities of OODD. Our second contribution is to design a strong OODD baseline approach based on recurrent implicit quantile networks (RIQNs), which monitors autoregressive prediction errors for OODD detection. Our final contribution is to evaluate the RIQN approach on the benchmarks to provide baseline results for future comparison.",0
"Our research focuses on detecting changes in the dynamics of a temporal process, known as out-of-distribution dynamics (OODD), which is critical in applications such as control, reinforcement learning, and multi-variate time-series. Changes in test time dynamics can significantly impact the performance of learning controllers and predictors in unknown ways, especially in deep RL, where controllers tend to overfit to the training environment. However, there is currently a lack of established OODD benchmarks for RL environments. To address this gap, we have designed a set of OODD benchmarks derived from common RL environments with varying types and intensities of OODD. Additionally, we have designed a robust OODD baseline approach using recurrent implicit quantile networks (RIQNs), which monitors autoregressive prediction errors for OODD detection. Finally, we have evaluated the RIQN approach on the benchmarks to provide baseline results for future comparison.",1
"In humans, perceptual awareness facilitates the fast recognition and extraction of information from sensory input. This awareness largely depends on how the human agent interacts with the environment. In this work, we propose active neural generative coding, a computational framework for learning action-driven generative models without backpropagation of errors (backprop) in dynamic environments. Specifically, we develop an intelligent agent that operates even with sparse rewards, drawing inspiration from the cognitive theory of planning as inference. We demonstrate on several control problems, in the online learning setting, that our proposed modeling framework performs competitively with deep Q-learning models. The robust performance of our agent offers promising evidence that a backprop-free approach for neural inference and learning can drive goal-directed behavior.",0
"Humans rely on perceptual awareness to quickly recognize and extract information from sensory input, which is influenced by their interaction with the environment. This study proposes a computational framework called active neural generative coding, which allows for learning action-driven generative models without the use of backpropagation of errors in dynamic environments. The framework includes an intelligent agent that can operate even with sparse rewards, inspired by the cognitive theory of planning as inference. Through several control problems in the online learning setting, the framework was found to perform competitively with deep Q-learning models. This suggests that a backpropagation-free approach for neural inference and learning can lead to goal-directed behavior. The agent's robust performance offers promising evidence for this approach.",1
"Millimeter wave (mmWave) beam-tracking based on machine learning enables the development of accurate tracking policies while obviating the need to periodically solve beam-optimization problems. However, its applicability is still arguable when training-test gaps exist in terms of environmental parameters that affect the node dynamics. From this skeptical point of view, the contribution of this study is twofold. First, by considering an example scenario, we confirm that the training-test gap adversely affects the beam-tracking performance. More specifically, we consider nodes placed on overhead messenger wires, where the node dynamics are affected by several environmental parameters, e.g, the wire mass and tension. Although these are particular scenarios, they yield insight into the validation of the training-test gap problems. Second, we demonstrate the feasibility of \textit{zero-shot adaptation} as a solution, where a learning agent adapts to environmental parameters unseen during training. This is achieved by leveraging a robust adversarial reinforcement learning (RARL) technique, where such training-and-test gaps are regarded as disturbances by adversaries that are jointly trained with a legitimate beam-tracking agent. Numerical evaluations demonstrate that the beam-tracking policy learned via RARL can be applied to a wide range of environmental parameters without severely degrading the received power.",0
"Machine learning-based millimeter wave (mmWave) beam-tracking allows for the development of precise tracking policies, removing the need to frequently solve beam-optimization problems. Nevertheless, its effectiveness is subject to debate when there are discrepancies between training and testing with regard to environmental factors that impact node dynamics. This study offers two contributions from a skeptical perspective. Firstly, we validate that the beam-tracking performance is negatively affected by the training-test gap in specific scenarios, such as overhead messenger wires, where the node dynamics are impacted by environmental parameters like wire mass and tension. Secondly, we demonstrate the feasibility of zero-shot adaptation as a solution, where an adaptive learning agent can adjust to previously unseen environmental factors during training. We achieve this via a robust adversarial reinforcement learning technique, treating training-and-test gaps as disturbances that are jointly trained with a legitimate beam-tracking agent. Numerical assessments indicate that the beam-tracking policy obtained through this approach can be applied to various environmental parameters without significantly compromising received power.",1
"We consider the problem of reinforcement learning when provided with (1) a baseline control policy and (2) a set of constraints that the learner must satisfy. The baseline policy can arise from demonstration data or a teacher agent and may provide useful cues for learning, but it might also be sub-optimal for the task at hand, and is not guaranteed to satisfy the specified constraints, which might encode safety, fairness or other application-specific requirements. In order to safely learn from baseline policies, we propose an iterative policy optimization algorithm that alternates between maximizing expected return on the task, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. We analyze our algorithm theoretically and provide a finite-time convergence guarantee. In our experiments on five different control tasks, our algorithm consistently outperforms several state-of-the-art baselines, achieving 10 times fewer constraint violations and 40% higher reward on average.",0
"The problem of reinforcement learning is addressed in this study, with the presence of a baseline control policy and a set of constraints that must be followed by the learner. The baseline policy may be beneficial for learning, but it may not be the best for the task and may not comply with the specified constraints, which may include safety or fairness requirements. An iterative policy optimization algorithm is proposed to safely learn from baseline policies, by maximizing expected return, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. The algorithm is analyzed theoretically and has a finite-time convergence guarantee. Experimental results on five control tasks demonstrate that the proposed algorithm consistently outperforms several state-of-the-art baselines, with 10 times fewer constraint violations and 40% higher reward on average.",1
"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the ""Rashomon set"" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.",0
"In the field of machine learning, the ability to interpret results is crucial for making important decisions and resolving issues. This paper outlines the fundamental principles of interpretable machine learning while addressing common misunderstandings that undermine its importance. Additionally, it identifies ten technical challenges that arise in interpretable machine learning, both old and new, such as optimizing sparse logical models and incorporating physics into machine learning models. This survey is an excellent resource for statisticians and computer scientists looking to get started in the field of interpretable machine learning.",1
"Exploration in unknown environments is a fundamental problem in reinforcement learning and control. In this work, we study task-guided exploration and determine what precisely an agent must learn about their environment in order to complete a particular task. Formally, we study a broad class of decision-making problems in the setting of linear dynamical systems, a class that includes the linear quadratic regulator problem. We provide instance- and task-dependent lower bounds which explicitly quantify the difficulty of completing a task of interest. Motivated by our lower bound, we propose a computationally efficient experiment-design based exploration algorithm. We show that it optimally explores the environment, collecting precisely the information needed to complete the task, and provide finite-time bounds guaranteeing that it achieves the instance- and task-optimal sample complexity, up to constant factors. Through several examples of the LQR problem, we show that performing task-guided exploration provably improves on exploration schemes which do not take into account the task of interest. Along the way, we establish that certainty equivalence decision making is instance- and task-optimal, and obtain the first algorithm for the linear quadratic regulator problem which is instance-optimal. We conclude with several experiments illustrating the effectiveness of our approach in practice.",0
"The challenge of exploring unknown environments is a fundamental issue in reinforcement learning and control. In this study, we focus on task-guided exploration and aim to determine the specific knowledge an agent must acquire about its surroundings to achieve a specific objective. To accomplish this, we examine a broad range of decision-making problems in the context of linear dynamical systems, which encompasses the linear quadratic regulator problem. We establish lower bounds that vary based on the instance and task and that explicitly quantify the level of difficulty involved in completing a desired task. Drawing inspiration from these lower bounds, we propose an experiment-design based exploration algorithm that is computationally efficient. Our algorithm optimizes the exploration of the environment, gathering the necessary information to complete the task while providing finite-time bounds that ensure it achieves the optimal sample complexity for the instance and task, up to constant factors. Using various examples of the LQR problem, we demonstrate that task-guided exploration outperforms exploration schemes that don't consider the task at hand. We also establish that certainty equivalence decision making is instance- and task-optimal and present the first algorithm for the linear quadratic regulator problem that is instance-optimal. We conclude by demonstrating the effectiveness of our approach through several experiments.",1
"The success of reinforcement learning in typical settings is, in part, predicated on underlying Markovian assumptions on the reward signal by which an agent learns optimal policies. In recent years, the use of reward machines has relaxed this assumption by enabling a structured representation of non-Markovian rewards. In particular, such representations can be used to augment the state space of the underlying decision process, thereby facilitating non-Markovian reinforcement learning. However, these reward machines cannot capture the semantics of stochastic reward signals. In this paper, we make progress on this front by introducing probabilistic reward machines (PRMs) as a representation of non-Markovian stochastic rewards. We present an algorithm to learn PRMs from the underlying decision process as well as to learn the PRM representation of a given decision-making policy.",0
"Reinforcement learning's success in standard scenarios is partly based on the assumption that the reward signal follows Markovian principles, allowing an agent to learn optimal policies. Reward machines have been introduced in recent years to address non-Markovian reward representation in a structured way, extending the state space of the decision process and enabling non-Markovian reinforcement learning. However, reward machines lack the ability to capture stochastic reward signal semantics. This paper introduces probabilistic reward machines (PRMs), which represent non-Markovian stochastic rewards. We present an algorithm to learn PRMs from the decision process and to learn the PRM representation of a given decision-making policy.",1
High quality standard cell layout automation in advanced technology nodes is still challenging in the industry today because of complex design rules. In this paper we introduce an automatic standard cell layout generator called NVCell that can generate layouts with equal or smaller area for over 90% of single row cells in an industry standard cell library on an advanced technology node. NVCell leverages reinforcement learning (RL) to fix design rule violations during routing and to generate efficient placements.,0
"Despite the advancements in technology, the industry still faces challenges in achieving high-quality standard cell layout automation due to intricate design rules. This article presents NVCell, an innovative standard cell layout generator that utilizes reinforcement learning (RL) to address design rule violations during routing and optimize placements, ultimately producing layouts that are equal to or smaller than 90% of single row cells in an industry-standard cell library for an advanced technology node.",1
"We consider the problem of controlling a partially-observed dynamic process on a graph by a limited number of interventions. This problem naturally arises in contexts such as scheduling virus tests to curb an epidemic; targeted marketing in order to promote a product; and manually inspecting posts to detect fake news spreading on social networks.   We formulate this setup as a sequential decision problem over a temporal graph process. In face of an exponential state space, combinatorial action space and partial observability, we design a novel tractable scheme to control dynamical processes on temporal graphs. We successfully apply our approach to two popular problems that fall into our framework: prioritizing which nodes should be tested in order to curb the spread of an epidemic, and influence maximization on a graph.",0
"The problem at hand involves managing a dynamic process on a graph with limited interventions, which is commonly seen in scenarios like virus testing scheduling to control outbreaks, targeted advertising for product promotions, and manual inspection of posts to detect fake news on social networks. Our approach involves formulating the problem as a sequential decision process over a temporal graph, despite the challenges posed by a large state space, complex action space, and partial observability. To address this, we have developed a new approach for controlling dynamic processes on temporal graphs that is both feasible and effective. We have validated our methodology by successfully addressing two prominent problems that fit our framework: prioritizing nodes for virus testing to prevent further spread and maximizing influence on a graph.",1
"Safe exploration is crucial for the real-world application of reinforcement learning (RL). Previous works consider the safe exploration problem as Constrained Markov Decision Process (CMDP), where the policies are being optimized under constraints. However, when encountering any potential dangers, human tends to stop immediately and rarely learns to behave safely in danger. Motivated by human learning, we introduce a new approach to address safe RL problems under the framework of Early Terminated MDP (ET-MDP). We first define the ET-MDP as an unconstrained MDP with the same optimal value function as its corresponding CMDP. An off-policy algorithm based on context models is then proposed to solve the ET-MDP, which thereby solves the corresponding CMDP with better asymptotic performance and improved learning efficiency. Experiments on various CMDP tasks show a substantial improvement over previous methods that directly solve CMDP.",0
"Ensuring safe exploration is crucial for the practical use of reinforcement learning (RL). Past studies have viewed the safe exploration problem as a Constrained Markov Decision Process (CMDP), wherein policies are optimized under constraints. However, in the face of potential dangers, humans typically halt immediately and seldom learn to act safely in perilous situations. Inspired by human learning, we introduce a novel approach for dealing with safe RL issues using the framework of Early Terminated MDP (ET-MDP). We begin by defining the ET-MDP as an unconstrained MDP with the same optimal value function as its corresponding CMDP. Subsequently, we propose an off-policy algorithm based on context models to solve the ET-MDP, which in turn solves the corresponding CMDP with superior asymptotic performance and enhanced learning efficiency. Our experiments on various CMDP tasks demonstrate significant improvement over previous methods that directly address CMDP.",1
"Learning in multi-agent systems is highly challenging due to the inherent complexity introduced by agents' interactions. We tackle systems with a huge population of interacting agents (e.g., swarms) via Mean-Field Control (MFC). MFC considers an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. Specifically, we consider the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm $\text{M}^3\text{-UCRL}$ that runs in episodes and provably solves this problem. $\text{M}^3\text{-UCRL}$ uses upper-confidence bounds to balance exploration and exploitation during policy learning. Our main theoretical contributions are the first general regret bounds for model-based RL for MFC, obtained via a novel mean-field type analysis. $\text{M}^3\text{-UCRL}$ can be instantiated with different models such as neural networks or Gaussian Processes, and effectively combined with neural network policy learning. We empirically demonstrate the convergence of $\text{M}^3\text{-UCRL}$ on the swarm motion problem of controlling an infinite population of agents seeking to maximize location-dependent reward and avoid congested areas.",0
"The complexity of agents' interactions makes learning in multi-agent systems a difficult task. To address this challenge, we utilize Mean-Field Control (MFC) to handle large populations of interacting agents, such as swarms. MFC involves an asymptotically infinite group of identical agents working together to maximize collective rewards. Our focus is on optimizing rewards and learning from experience in systems with unknown dynamics. To solve this problem, we propose an efficient model-based reinforcement learning algorithm called $\text{M}^3\text{-UCRL}$, which uses upper-confidence bounds to balance exploration and exploitation during policy learning. Our novel mean-field type analysis provides the first general regret bounds for model-based RL for MFC. The algorithm can be combined with different models, including neural networks and Gaussian Processes. We demonstrate the effectiveness of $\text{M}^3\text{-UCRL}$ on a swarm motion problem, where an infinite number of agents aim to maximize location-dependent rewards while avoiding congested areas.",1
"We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method for quadrupedal locomotion that leverages a Transformer-based model for fusing proprioceptive states and visual observations. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We show that our method obtains significant improvements over policies with only proprioceptive state inputs, and that Transformer-based models further improve generalization across environments. Our project page with videos is at https://RchalYang.github.io/LocoTransformer .",0
"Our proposal involves using Reinforcement Learning (RL) and a Transformer-based model to tackle quadrupedal locomotion tasks. Previous RL-based methods have relied on domain randomization to train agents that can navigate challenging terrains. However, our approach combines proprioceptive information with high-dimensional depth sensor inputs to enable agents to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. We introduce LocoTransformer, an end-to-end RL method that fuses proprioceptive states and visual observations, and evaluate it in challenging simulated environments with different obstacles and terrain. Our results show that our method significantly outperforms policies that only use proprioceptive state inputs, and that Transformer-based models improve generalization across environments. Check out our project page with videos at https://RchalYang.github.io/LocoTransformer.",1
"Complex sensors like video cameras include tens of configurable parameters, which can be set by end-users to customize the sensors to specific application scenarios. Although parameter settings significantly affect the quality of the sensor output and the accuracy of insights derived from sensor data, most end-users use a fixed parameter setting because they lack the skill or understanding to appropriately configure these parameters. We propose CamTuner, which is a system to automatically, and dynamically adapt the complex sensor to changing environments. CamTuner includes two key components. First, a bespoke analytics quality estimator, which is a deep-learning model to automatically and continuously estimate the quality of insights from an analytics unit as the environment around a sensor change. Second, a reinforcement learning (RL) module, which reacts to the changes in quality, and automatically adjusts the camera parameters to enhance the accuracy of insights. We improve the training time of the RL module by an order of magnitude by designing virtual models to mimic essential behavior of the camera: we design virtual knobs that can be set to different values to mimic the effects of assigning different values to the camera's configurable parameters, and we design a virtual camera model that mimics the output from a video camera at different times of the day. These virtual models significantly accelerate training because (a) frame rates from a real camera are limited to 25-30 fps while the virtual models enable processing at 300 fps, (b) we do not have to wait until the real camera sees different environments, which could take weeks or months, and (c) virtual knobs can be updated instantly, while it can take 200-500 ms to change the camera parameter settings. Our dynamic tuning approach results in up to 12% improvement in the accuracy of insights from several video analytics tasks.",0
"Video cameras are complex sensors with numerous configurable parameters that can be customized by end-users to suit specific application scenarios. However, most end-users do not have the necessary skills or understanding to configure these parameters and consequently rely on fixed settings. To address this, we introduce CamTuner, a system that automatically and dynamically adapts the camera to changing environments. CamTuner comprises two key components: a bespoke analytics quality estimator and a reinforcement learning module. The former is a deep-learning model that continuously estimates the quality of insights from an analytics unit as the environment around the camera changes. The latter reacts to changes in quality and automatically adjusts the camera parameters to enhance the accuracy of insights. To speed up the training time of the reinforcement learning module, we designed virtual models that mimic the behavior of the camera. These models enable processing at 300 fps, are not limited by the frame rates of a real camera, and can be instantly updated. Our dynamic tuning approach leads to up to a 12% improvement in the accuracy of insights from various video analytics tasks.",1
"With few exceptions, neural networks have been relying on backpropagation and gradient descent as the inference engine in order to learn the model parameters, because the closed-form Bayesian inference for neural networks has been considered to be intractable. In this paper, we show how we can leverage the tractable approximate Gaussian inference's (TAGI) capabilities to infer hidden states, rather than only using it for inferring the network's parameters. One novel aspect it allows is to infer hidden states through the imposition of constraints designed to achieve specific objectives, as illustrated through three examples: (1) the generation of adversarial-attack examples, (2) the usage of a neural network as a black-box optimization method, and (3) the application of inference on continuous-action reinforcement learning. These applications showcase how tasks that were previously reserved to gradient-based optimization approaches can now be approached with analytically tractable inference",0
"Traditionally, neural networks have relied on backpropagation and gradient descent for learning model parameters due to the belief that closed-form Bayesian inference is too difficult. However, this study demonstrates the usefulness of tractable approximate Gaussian inference (TAGI) for inferring hidden states. TAGI enables the imposition of constraints for specific objectives, as shown through three examples: (1) generating adversarial-attack examples, (2) using a neural network as a black-box optimization method, and (3) applying inference on continuous-action reinforcement learning. These examples illustrate how TAGI can tackle tasks that were previously limited to gradient-based optimization methods.",1
"Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-to-real gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.",0
"In the field of robotics, deep reinforcement learning has achieved significant success in various areas. Due to the difficulties of obtaining real-world data, including sample inefficiency and high collection costs, simulation environments are utilized to train agents. This approach not only offers an almost inexhaustible source of data, but also addresses safety concerns associated with real robots. However, the transfer of models from simulation to reality results in a decline in policy performance due to the gap between the simulated and real worlds. To achieve more efficient policy transfer, multiple research efforts are now focused on closing this gap. Although various methods have emerged in recent years, there is a lack of a comprehensive review that summarizes and contextualizes these approaches. This survey paper provides an overview of the fundamental background of sim-to-real transfer in deep reinforcement learning and outlines the main methods in use: domain randomization, domain adaptation, imitation learning, meta-learning, and knowledge distillation. Relevant recent works are categorized, and the main application scenarios are presented. Finally, the paper discusses the opportunities and challenges of the different approaches and identifies the most promising areas for future research.",1
"We present a new practical framework based on deep reinforcement learning and decision-time planning for real-world vehicle repositioning on ride-hailing (a type of mobility-on-demand, MoD) platforms. Our approach learns the spatiotemporal state-value function using a batch training algorithm with deep value networks. The optimal repositioning action is generated on-demand through value-based policy search, which combines planning and bootstrapping with the value networks. For the large-fleet problems, we develop several algorithmic features that we incorporate into our framework and that we demonstrate to induce coordination among the algorithmically-guided vehicles. We benchmark our algorithm with baselines in a ride-hailing simulation environment to demonstrate its superiority in improving income efficiency meausred by income-per-hour. We have also designed and run a real-world experiment program with regular drivers on a major ride-hailing platform. We have observed significantly positive results on key metrics comparing our method with experienced drivers who performed idle-time repositioning based on their own expertise.",0
"Our novel framework utilizes deep reinforcement learning and decision-time planning to facilitate real-world vehicle repositioning on mobility-on-demand platforms. We employ batch training algorithms and deep value networks to learn the spatiotemporal state-value function, which allows us to generate optimal repositioning actions on-demand using value-based policy search. We have incorporated algorithmic features to promote coordination among the vehicles, especially for large-fleet problems. Our algorithm outperforms baselines in a ride-hailing simulation environment, as demonstrated by its superior income efficiency measured by income-per-hour. In addition, we have conducted a real-world experiment program with regular drivers on a major ride-hailing platform, and our method has yielded significantly positive results on key metrics compared to experienced drivers who performed idle-time repositioning based on their own expertise.",1
"A major goal of materials design is to find material structures with desired properties and in a second step to find a processing path to reach one of these structures. In this paper, we propose and investigate a deep reinforcement learning approach for the optimization of processing paths. The goal is to find optimal processing paths in the material structure space that lead to target-structures, which have been identified beforehand to result in desired material properties. There exists a target set containing one or multiple different structures. Our proposed methods can find an optimal path from a start structure to a single target structure, or optimize the processing paths to one of the equivalent target-structures in the set. In the latter case, the algorithm learns during processing to simultaneously identify the best reachable target structure and the optimal path to it. The proposed methods belong to the family of model-free deep reinforcement learning algorithms. They are guided by structure representations as features of the process state and by a reward signal, which is formulated based on a distance function in the structure space. Model-free reinforcement learning algorithms learn through trial and error while interacting with the process. Thereby, they are not restricted to information from a priori sampled processing data and are able to adapt to the specific process. The optimization itself is model-free and does not require any prior knowledge about the process itself. We instantiate and evaluate the proposed methods by optimizing paths of a generic metal forming process. We show the ability of both methods to find processing paths leading close to target structures and the ability of the extended method to identify target-structures that can be reached effectively and efficiently and to focus on these targets for sample efficient processing path optimization.",0
"The aim of materials design is to discover material structures with specific properties and then determine a processing route to achieve these structures. This study suggests and explores a deep reinforcement learning method for optimizing processing paths. The objective is to identify optimal processing paths in the material structure space that lead to pre-defined target structures, which result in desired material properties. The target set consists of one or more different structures. The proposed approach can find an optimal path from the starting structure to a single target structure or optimize the processing paths to one of the equivalent target structures in the set. During processing, the algorithm learns to identify the best reachable target structure and the optimal path to it. The proposed methods belong to the family of model-free deep reinforcement learning algorithms, which are guided by structure representations and a reward signal based on a distance function in the structure space. These algorithms learn through trial and error while interacting with the process, allowing them to adapt to the specific process without prior knowledge. The optimization itself is model-free and does not require prior knowledge of the process. The proposed methods are evaluated by optimizing paths of a generic metal forming process, demonstrating their ability to find processing paths close to target structures and to identify reachable target-structures efficiently and focus on these targets for sample efficient processing path optimization.",1
"The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM: https://ezliu.github.io/dream/",0
"Meta-reinforcement learning (meta-RL) aims to create agents that can quickly learn new tasks by utilizing prior experience on related tasks. When learning a new task, it's essential to both explore and exploit the information to solve it. The optimal exploration and exploitation can be learned end-to-end by maximizing task performance. However, this approach faces the challenge of local optima due to a chicken-and-egg problem. To overcome this, separate objectives for exploration and exploitation are optimized, but the existing exploration objectives yield suboptimal policies. To tackle this issue, an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information are constructed. This eliminates local optima in end-to-end training, without sacrificing optimal exploration. The DREAM approach outperforms existing methods on complex meta-RL tasks like sparse-reward 3D visual navigation. Check out the videos of DREAM: https://ezliu.github.io/dream/.",1
"We formulate an efficient approximation for multi-agent batch reinforcement learning, the approximate multi-agent fitted Q iteration (AMAFQI). We present a detailed derivation of our approach. We propose an iterative policy search and show that it yields a greedy policy with respect to multiple approximations of the centralized, standard Q-function. In each iteration and policy evaluation, AMAFQI requires a number of computations that scales linearly with the number of agents whereas the analogous number of computations increase exponentially for the fitted Q iteration (FQI), one of the most commonly used approaches in batch reinforcement learning. This property of AMAFQI is fundamental for the design of a tractable multi-agent approach. We evaluate the performance of AMAFQI and compare it to FQI in numerical simulations. Numerical examples illustrate the significant computation time reduction when using AMAFQI instead of FQI in multi-agent problems and corroborate the similar decision-making performance of both approaches.",0
"The approximate multi-agent fitted Q iteration (AMAFQI) is a highly efficient method for multi-agent batch reinforcement learning. Our approach is derived in great detail and includes an iterative policy search that produces a policy that is greedy with respect to multiple approximations of the centralized, standard Q-function. What sets AMAFQI apart is that in each iteration and policy evaluation, the number of computations scales linearly with the number of agents. This is in contrast to the fitted Q iteration (FQI), which is a commonly used approach in batch reinforcement learning and requires computations that increase exponentially with the number of agents. The scalability of AMAFQI is a fundamental aspect of this approach, making it a practical solution for multi-agent problems. We demonstrate the effectiveness of AMAFQI through numerical simulations, which show that it significantly reduces computation time while maintaining similar decision-making performance to FQI.",1
"In this research, some of the issues that arise from the scalarization of the multi-objective optimization problem in the Advantage Actor Critic (A2C) reinforcement learning algorithm are investigated. The paper shows how a naive scalarization can lead to gradients overlapping. Furthermore, the possibility that the entropy regularization term can be a source of uncontrolled noise is discussed. With respect to the above issues, a technique to avoid gradient overlapping is proposed, while keeping the same loss formulation. Moreover, a method to avoid the uncontrolled noise, by sampling the actions from distributions with a desired minimum entropy, is investigated. Pilot experiments have been carried out to show how the proposed method speeds up the training. The proposed approach can be applied to any Advantage-based Reinforcement Learning algorithm.",0
"This study examines the concerns arising from the scalarization of the multi-objective optimization problem in the Advantage Actor Critic (A2C) reinforcement learning algorithm. The article demonstrates how the simple scalarization method can result in overlapping gradients. Additionally, the potential for the entropy regularization term to generate uncontrollable noise is discussed. To address these issues, a method for avoiding gradient overlap is proposed while maintaining the same loss formulation. Additionally, a technique for preventing uncontrolled noise by sampling actions from distributions with a minimum entropy is explored. Pilot experiments are conducted to demonstrate the efficacy of the proposed method in speeding up training. It can be applied to any Advantage-based Reinforcement Learning algorithm.",1
"Designing agents, capable of learning autonomously a wide range of skills is critical in order to increase the scope of reinforcement learning. It will both increase the diversity of learned skills and reduce the burden of manually designing reward functions for each skill. Self-supervised agents, setting their own goals, and trying to maximize the diversity of those goals have shown great promise towards this end. However, a currently known limitation of agents trying to maximize the diversity of sampled goals is that they tend to get attracted to noise or more generally to parts of the environments that cannot be controlled (distractors). When agents have access to predefined goal features or expert knowledge, absolute Learning Progress (ALP) provides a way to distinguish between regions that can be controlled and those that cannot. However, those methods often fall short when the agents are only provided with raw sensory inputs such as images. In this work we extend those concepts to unsupervised image-based goal exploration. We propose a framework that allows agents to autonomously identify and ignore noisy distracting regions while searching for novelty in the learnable regions to both improve overall performance and avoid catastrophic forgetting. Our framework can be combined with any state-of-the-art novelty seeking goal exploration approaches. We construct a rich 3D image based environment with distractors. Experiments on this environment show that agents using our framework successfully identify interesting regions of the environment, resulting in drastically improved performances. The source code is available at https://sites.google.com/view/grimgep.",0
"To expand the potential of reinforcement learning, it is crucial to develop agents with the ability to independently learn a wide range of skills. This will increase the variety of acquired skills and eliminate the need for manual reward function design for each skill. Self-supervised agents that establish their own objectives and strive to maximize goal diversity have demonstrated great potential in achieving this goal. However, these agents are prone to being drawn towards noise or uncontrollable components of the environment (distractors). To address this issue, we propose the use of absolute Learning Progress (ALP) to distinguish between controllable and uncontrollable regions when agents have access to predefined goal features or expert knowledge. Unfortunately, this method falls short when agents are presented with raw sensory inputs such as images. In this study, we extend these concepts to unsupervised image-based goal exploration, introducing a framework that enables agents to autonomously recognize and disregard distracting regions while searching for novelty in learnable regions. Our framework can be combined with any state-of-the-art novelty-seeking goal exploration approach and is tested in a 3D image-based environment with distractors. The results of our experiments demonstrate that agents utilizing our framework successfully identify interesting regions of the environment, leading to significant improvements in performance. The source code for our framework is available at https://sites.google.com/view/grimgep.",1
"How sensitive should machine learning models be to input changes? We tackle the question of model smoothness and show that it is a useful inductive bias which aids generalization, adversarial robustness, generative modeling and reinforcement learning. We explore current methods of imposing smoothness constraints and observe they lack the flexibility to adapt to new tasks, they don't account for data modalities, they interact with losses, architectures and optimization in ways not yet fully understood. We conclude that new advances in the field are hinging on finding ways to incorporate data, tasks and learning into our definitions of smoothness.",0
"The level of sensitivity of machine learning models to input changes is a crucial consideration. In this regard, we investigate the notion of model smoothness, which serves as a helpful inductive bias for enhancing generalization, adversarial robustness, generative modeling, and reinforcement learning. Our examination of the existing techniques for enforcing smoothness constraints reveals their inflexibility in adapting to new tasks and lack of consideration for data modalities. Furthermore, they interact with losses, architectures, and optimization in ways that require deeper understanding. We therefore conclude that further advancements in this field depend on integrating data, tasks, and learning into our smoothness definitions.",1
"Most approaches in reinforcement learning (RL) are data-hungry and specific to fixed environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably to changes across domains. Specifically, we construct a generative environment model for the structural relationships among variables in the system and embed the changes in a compact way, which provides a clear and interpretable picture for locating what and where the changes are and how to adapt. Based on the environment model, we characterize a minimal set of representations, including both domain-specific factors and domain-shared state representations, that suffice for reliable and low-cost transfer. Moreover, we show that by explicitly leveraging a compact representation to encode changes, we can adapt the policy with only a few samples without further policy optimization in the target domain. We illustrate the efficacy of AdaRL through a series of experiments that allow for changes in different components of Cartpole and Atari games.",0
"The majority of reinforcement learning (RL) methods require a significant amount of data and are limited to particular environments. This paper introduces AdaRL, a systematic approach to adaptive RL that can effectively adapt to changes in various domains. AdaRL employs a generative environment model to capture the connections between system variables and incorporates changes in a concise manner, resulting in a clear and interpretable depiction of where and what changes have occurred, as well as how to adjust accordingly. We identify a fundamental set of representations, including domain-specific factors and shared state representations, that are sufficient for efficient and reliable transfer. By utilizing a compact representation to encode changes, we demonstrate that we can adjust the policy with minimal samples without any additional policy optimization in the target domain. We validate AdaRL's effectiveness by conducting experiments on Cartpole and Atari games, with the ability to adjust different elements.",1
"Deep Reinforcement Learning (DRL) is considered a potential framework to improve many real-world autonomous systems; it has attracted the attention of multiple and diverse fields. Nevertheless, the successful deployment in the real world is a test most of DRL models still need to pass. In this work we focus on this issue by reviewing and evaluating the research efforts from both domain-agnostic and domain-specific communities. On one hand, we offer a comprehensive summary of DRL challenges and summarize the different proposals to mitigate them; this helps identifying five gaps of domain-agnostic research. On the other hand, from the domain-specific perspective, we discuss different success stories and argue why other models might fail to be deployed. Finally, we take up on ways to move forward accounting for both perspectives.",0
"Many fields have shown interest in Deep Reinforcement Learning (DRL) due to its potential to enhance various autonomous systems. However, the successful implementation of DRL models in real-world scenarios is still an untested challenge. To address this issue, we conduct a review and evaluation of research efforts from both domain-specific and domain-agnostic communities. Our work provides a comprehensive summary of DRL challenges and potential solutions, identifying five gaps in domain-agnostic research. Additionally, we discuss successful DRL models from a domain-specific perspective and highlight potential reasons for failure. Ultimately, we propose ways to move forward by considering both perspectives.",1
"The classic DQN algorithm is limited by the overestimation bias of the learned Q-function. Subsequent algorithms have proposed techniques to reduce this problem, without fully eliminating it. Recently, the Maxmin and Ensemble Q-learning algorithms have used different estimates provided by the ensembles of learners to reduce the overestimation bias. Unfortunately, these learners can converge to the same point in the parametric or representation space, falling back to the classic single neural network DQN. In this paper, we describe a regularization technique to maximize ensemble diversity in these algorithms. We propose and compare five regularization functions inspired from economics theory and consensus optimization. We show that the regularized approach significantly outperforms the Maxmin and Ensemble Q-learning algorithms as well as non-ensemble baselines.",0
"The overestimation bias of the learned Q-function limits the effectiveness of the classic DQN algorithm. Although subsequent algorithms have attempted to reduce this issue, it has not been fully eliminated. Recently, the Maxmin and Ensemble Q-learning algorithms have implemented ensembles of learners to address the overestimation bias using different estimates. However, these ensembles can converge to the same point, returning to the classic single neural network DQN. This study proposes a regularization technique to increase ensemble diversity in these algorithms. The study introduces five regularization functions based on economics theory and consensus optimization, and demonstrates that the regularized approach outperforms Maxmin and Ensemble Q-learning algorithms, as well as non-ensemble baselines.",1
"General Value Function (GVF) is a powerful tool to represent both the {\em predictive} and {\em retrospective} knowledge in reinforcement learning (RL). In practice, often multiple interrelated GVFs need to be evaluated jointly with pre-collected off-policy samples. In the literature, the gradient temporal difference (GTD) learning method has been adopted to evaluate GVFs in the off-policy setting, but such an approach may suffer from a large estimation error even if the function approximation class is sufficiently expressive. Moreover, none of the previous work have formally established the convergence guarantee to the ground truth GVFs under the function approximation settings. In this paper, we address both issues through the lens of a class of GVFs with causal filtering, which cover a wide range of RL applications such as reward variance, value gradient, cost in anomaly detection, stationary distribution gradient, etc. We propose a new algorithm called GenTD for off-policy GVFs evaluation and show that GenTD learns multiple interrelated multi-dimensional GVFs as efficiently as a single canonical scalar value function. We further show that unlike GTD, the learned GVFs by GenTD are guaranteed to converge to the ground truth GVFs as long as the function approximation power is sufficiently large. To our best knowledge, GenTD is the first off-policy GVF evaluation algorithm that has global optimality guarantee.",0
"The General Value Function (GVF) is a useful tool in reinforcement learning (RL) for representing both predictive and retrospective knowledge. When evaluating multiple interrelated GVFs with pre-collected off-policy samples, the gradient temporal difference (GTD) learning method is often used. However, this approach can lead to a large estimation error, even with expressive function approximation. Additionally, previous work has not established convergence guarantees to ground truth GVFs under function approximation settings. This paper addresses both issues by focusing on GVFs with causal filtering, which are used in a variety of RL applications. The proposed algorithm, GenTD, efficiently learns multiple interrelated multi-dimensional GVFs as well as a single scalar value function. Unlike GTD, GenTD guarantees convergence to ground truth GVFs with sufficient function approximation power. To the best of our knowledge, GenTD is the first off-policy GVF evaluation algorithm with a global optimality guarantee.",1
"Clinical finding summaries from an orthopantomogram, or a dental panoramic radiograph, have significant potential to improve patient communication and speed up clinical judgments. While orthopantomogram is a first-line tool for dental examinations, no existing work has explored the summarization of findings from it. A finding summary has to find teeth in the imaging study and label the teeth with several types of past treatments. To tackle the problem, we developDeepOPG that breaks the summarization process into functional segmentation and tooth localization, the latter of which is further refined by a novel dental coherence module. We also leverage weak supervision labels to improve detection results in a reinforcement learning scenario. Experiments show high efficacy of DeepOPG on finding summarization, achieving an overall AUC of 88.2% in detecting six types of findings. The proposed dental coherence and weak supervision both are shown to improve DeepOPG by adding 5.9% and 0.4% to AP@IoU=0.5.",0
"The summarization of clinical findings from an orthopantomogram, also known as a dental panoramic radiograph, has the potential to enhance communication with patients and expedite clinical decisions. Despite being a primary dental examination tool, no previous research has explored the summarization of findings from an orthopantomogram. To address this gap, we developed DeepOPG, which divides the summarization process into functional segmentation and tooth localization. Our novel dental coherence module refines tooth localization, while weak supervision labels improve detection results through reinforcement learning. Our experiments demonstrate that DeepOPG is highly effective, achieving an overall AUC of 88.2% in detecting six types of findings. In addition, our proposed dental coherence and weak supervision techniques improve DeepOPG by 5.9% and 0.4% respectively on AP@IoU=0.5.",1
"Though convolutional neural networks are widely used in different tasks, lack of generalization capability in the absence of sufficient and representative data is one of the challenges that hinder their practical application. In this paper, we propose a simple, effective, and plug-and-play training strategy named Knowledge Distillation for Domain Generalization (KDDG) which is built upon a knowledge distillation framework with the gradient filter as a novel regularization term. We find that both the ``richer dark knowledge"" from the teacher network, as well as the gradient filter we proposed, can reduce the difficulty of learning the mapping which further improves the generalization ability of the model. We also conduct experiments extensively to show that our framework can significantly improve the generalization capability of deep neural networks in different tasks including image classification, segmentation, reinforcement learning by comparing our method with existing state-of-the-art domain generalization techniques. Last but not the least, we propose to adopt two metrics to analyze our proposed method in order to better understand how our proposed method benefits the generalization capability of deep neural networks.",0
"The practical application of convolutional neural networks is impeded by their lack of generalization ability when faced with insufficient and non-representative data, despite their frequent usage in various tasks. This paper presents a training strategy called Knowledge Distillation for Domain Generalization (KDDG) that is uncomplicated, efficient, and readily applicable, based on a knowledge distillation framework enhanced by the inclusion of a novel regularization term called the gradient filter. Our investigation reveals that both the ""richer dark knowledge"" from the teacher network and the gradient filter can alleviate the mapping learning challenge, ultimately enhancing the model's generalization ability. Furthermore, we extensively conduct experiments to demonstrate that our framework can considerably enhance the generalization ability of deep neural networks in different tasks, such as image classification, segmentation, and reinforcement learning, by comparing it to existing state-of-the-art domain generalization techniques. Finally, we suggest employing two metrics to analyze our proposed method to obtain a better understanding of how it enhances the generalization ability of deep neural networks.",1
"Instance segmentation is an important computer vision problem which remains challenging despite impressive recent advances due to deep learning-based methods. Given sufficient training data, fully supervised methods can yield excellent performance, but annotation of ground-truth data remains a major bottleneck, especially for biomedical applications where it has to be performed by domain experts. The amount of labels required can be drastically reduced by using rules derived from prior knowledge to guide the segmentation. However, these rules are in general not differentiable and thus cannot be used with existing methods. Here, we relax this requirement by using stateless actor critic reinforcement learning, which enables non-differentiable rewards. We formulate the instance segmentation problem as graph partitioning and the actor critic predicts the edge weights driven by the rewards, which are based on the conformity of segmented instances to high-level priors on object shape, position or size. The experiments on toy and real datasets demonstrate that we can achieve excellent performance without any direct supervision based only on a rich set of priors.",0
"Despite remarkable progress in deep learning-based methods, instance segmentation continues to be a challenging computer vision problem. While fully supervised methods can deliver excellent results with sufficient training data, annotating ground-truth data remains a major obstacle, particularly in biomedical applications that require domain experts. Using prior knowledge-derived rules to guide segmentation can significantly reduce the number of labels required. However, existing methods cannot use these rules because they are generally not differentiable. To address this, we apply stateless actor critic reinforcement learning, which enables non-differentiable rewards. We approach instance segmentation as graph partitioning, where the actor critic predicts edge weights based on rewards derived from the conformity of segmented instances to high-level priors on object shape, position, or size. Our experiments on both toy and real datasets demonstrate that our approach can achieve outstanding performance without direct supervision based solely on a rich set of priors.",1
"With the arrival of next generation wireless communication, a growing number of new applications like internet of things, autonomous driving systems, and drone are crowding the unlicensed spectrum. Licensed network such as the long-term evolution (LTE) also comes to the unlicensed spectrum for better providing high-capacity contents with low cost. However, LTE was not designed to share resources with others. Previous solutions usually work on fixed scenarios. This work features a Nonparametric Bayesian reinforcement learning algorithm to cope with the coexistence between Wi-Fi and LTE licensed assisted access (LTE-LAA) agents in 5 GHz unlicensed spectrum. The coexistence problem is modeled as a decentralized partially-observable Markov decision process (Dec-POMDP) and Bayesian inference is adopted for policy learning with nonparametric prior to accommodate the uncertainty of policy for different agents. A fairness measure is introduced in the reward function to encourage fair sharing between agents. Variational inference for posterior model approximation is considered to make the algorithm computationally efficient. Simulation results demonstrate that this algorithm can reach high value with compact policy representations in few learning iterations.",0
"The unlicensed spectrum is becoming increasingly congested with new applications such as autonomous driving systems, drones, and the internet of things, all utilizing next generation wireless communication. To provide high-capacity content at a low cost, licensed networks like long-term evolution (LTE) are also entering the unlicensed spectrum. However, LTE was not designed to share resources with other systems, and previous solutions only worked in fixed scenarios. This study proposes a Nonparametric Bayesian reinforcement learning algorithm to address coexistence issues between Wi-Fi and LTE licensed assisted access (LTE-LAA) agents in the 5 GHz unlicensed spectrum. The coexistence problem is modeled as a decentralized partially-observable Markov decision process (Dec-POMDP), and Bayesian inference is employed for policy learning with nonparametric prior to account for policy uncertainty among agents. A fairness measure is incorporated in the reward function to encourage equitable resource sharing. Variational inference is used for posterior model approximation to enhance computational efficiency. Simulation results indicate that this algorithm can achieve high value with concise policy representations in a few learning iterations.",1
"Eluder dimension and information gain are two widely used methods of complexity measures in bandit and reinforcement learning. Eluder dimension was originally proposed as a general complexity measure of function classes, but the common examples of where it is known to be small are function spaces (vector spaces). In these cases, the primary tool to upper bound the eluder dimension is the elliptic potential lemma. Interestingly, the elliptic potential lemma also features prominently in the analysis of linear bandits/reinforcement learning and their nonparametric generalization, the information gain. We show that this is not a coincidence -- eluder dimension and information gain are equivalent in a precise sense for reproducing kernel Hilbert spaces.",0
"In the realm of bandit and reinforcement learning, two widely utilized methods for measuring complexity are eluder dimension and information gain. Although originally proposed as a general measure of function class complexity, eluder dimension is typically applied to function spaces. To estimate the upper bound of eluder dimension for these spaces, the elliptic potential lemma is commonly employed. Interestingly, the same lemma plays a significant role in analyzing linear bandits/reinforcement learning and their nonparametric form, information gain. We demonstrate that this connection is not coincidental, as eluder dimension and information gain are precisely equivalent in reproducing kernel Hilbert spaces.",1
"Ensemble and auxiliary tasks are both well known to improve the performance of machine learning models when data is limited. However, the interaction between these two methods is not well studied, particularly in the context of deep reinforcement learning. In this paper, we study the effects of ensemble and auxiliary tasks when combined with the deep Q-learning algorithm. We perform a case study on ATARI games under limited data constraint. Moreover, we derive a refined bias-variance-covariance decomposition to analyze the different ways of learning ensembles and using auxiliary tasks, and use the analysis to help provide some understanding of the case study. Our code is open source and available at https://github.com/NUS-LID/RENAULT.",0
"The utilization of both ensemble and auxiliary tasks has proven to enhance the effectiveness of machine learning models in situations where data is scarce. However, the interplay between these two techniques has not been extensively explored, especially in the realm of deep reinforcement learning. This research delves into the effects of combining ensemble and auxiliary tasks with the deep Q-learning algorithm. The study focuses on ATARI games while operating under limited data conditions. Additionally, the researchers develop a refined bias-variance-covariance decomposition to scrutinize the various approaches to learning ensembles and integrating auxiliary tasks. They employ this analysis to gain insight into the case study. The open-source code for this experiment is available at https://github.com/NUS-LID/RENAULT.",1
"We address the problem of solving complex bimanual robot manipulation tasks on multiple objects with sparse rewards. Such complex tasks can be decomposed into sub-tasks that are accomplishable by different robots concurrently or sequentially for better efficiency. While previous reinforcement learning approaches primarily focus on modeling the compositionality of sub-tasks, two fundamental issues are largely ignored particularly when learning cooperative strategies for two robots: (i) domination, i.e., one robot may try to solve a task by itself and leaves the other idle; (ii) conflict, i.e., one robot can easily interrupt another's workspace when executing different sub-tasks simultaneously. To tackle these two issues, we propose a novel technique called disentangled attention, which provides an intrinsic regularization for two robots to focus on separate sub-tasks and objects. We evaluate our method on four bimanual manipulation tasks. Experimental results show that our proposed intrinsic regularization successfully avoids domination and reduces conflicts for the policies, which leads to significantly more effective cooperative strategies than all the baselines. Our project page with videos is at https://mehooz.github.io/bimanual-attention.",0
"Our focus is on solving the challenge of achieving sparse rewards for complex bimanual robot manipulation tasks involving multiple objects. To enhance efficiency, these tasks can be divided into sub-tasks that can be completed concurrently or sequentially by different robots. Existing reinforcement learning methods mainly concentrate on modeling the compositionality of sub-tasks. However, when learning cooperative strategies for two robots, two key issues are often disregarded: domination and conflict. Domination occurs when one robot attempts to solve a task alone, leaving the other robot idle. Conflict arises when one robot interrupts another's workspace while executing different sub-tasks simultaneously. To address these two problems, we introduce a new technique called disentangled attention, which provides intrinsic regularization to enable the two robots to concentrate on separate sub-tasks and objects. We evaluate our approach using four bimanual manipulation tasks, and our experimental outcomes indicate that our proposed intrinsic regularization successfully eliminates domination and reduces conflicts, resulting in significantly more effective cooperative strategies than all the baselines. Visit our project page, which includes videos, at https://mehooz.github.io/bimanual-attention.",1
"This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations.",0
"The objective of this study is to develop strong and practical deep world models that utilize multiple sensors, despite their potential unreliability. Our research indicates that current methods lack the necessary emphasis on a shared representation between modalities, leading to inadequate performance in downstream tasks and dependence on specific sensors. To address this issue, we propose a novel multi-modal deep latent state-space model that employs a mutual information lower-bound during training. Our approach introduces a unique density ratio estimator that promotes consistency among the latent codes of each modality. We evaluate our method on various multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task, where it learns policies through self-supervision. Our experiments demonstrate that our approach surpasses state-of-the-art deep reinforcement learning methods, particularly when dealing with incomplete observations.",1
"Proximal Policy Optimization (PPO) is a popular on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due the belief that on-policy methods are significantly less sample efficient than their off-policy counterparts in multi-agent problems. In this work, we investigate Multi-Agent PPO (MAPPO), a variant of PPO which is specialized for multi-agent settings. Using a 1-GPU desktop, we show that MAPPO achieves surprisingly strong performance in three popular multi-agent testbeds: the particle-world environments, the Starcraft multi-agent challenge, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. In the majority of environments, we find that compared to off-policy baselines, MAPPO achieves strong results while exhibiting comparable sample efficiency. Finally, through ablation studies, we present the implementation and algorithmic factors which are most influential to MAPPO's practical performance.",0
"The on-policy reinforcement learning algorithm, Proximal Policy Optimization (PPO), is not as commonly used as off-policy learning algorithms in multi-agent settings because it is believed that on-policy methods are less sample efficient. However, in this study, Multi-Agent PPO (MAPPO), a specialized variant of PPO for multi-agent settings, is investigated. The study shows that MAPPO achieves strong performance in three popular multi-agent testbeds without domain-specific algorithmic modifications or architectures and with minimal hyperparameter tuning. Compared to off-policy baselines, MAPPO achieves strong results while exhibiting comparable sample efficiency in most environments. The study also presents the most influential factors to MAPPO's practical performance through ablation studies.",1
"Autonomous systems such as self-driving cars and general-purpose robots are safety-critical systems that operate in highly uncertain and dynamic environments. We propose an interactive multi-agent framework where the system-under-design is modeled as an ego agent and its environment is modeled by a number of adversarial (ado) agents. For example, a self-driving car is an ego agent whose behavior is influenced by ado agents such as pedestrians, bicyclists, traffic lights, road geometry etc. Given a logical specification of the correct behavior of the ego agent, and a set of constraints that encode reasonable adversarial behavior, our framework reduces the adversarial testing problem to the problem of synthesizing controllers for (constrained) ado agents that cause the ego agent to violate its specifications. Specifically, we explore the use of tabular and deep reinforcement learning approaches for synthesizing adversarial agents. We show that ado agents trained in this fashion are better than traditional falsification or testing techniques because they can generalize to ego agents and environments that differ from the original ego agent. We demonstrate the efficacy of our technique on two real-world case studies from the domain of self-driving cars.",0
"Our proposed framework addresses the safety concerns of autonomous systems like self-driving cars and general-purpose robots that function in complex and unpredictable surroundings. In this approach, we portray the system-under-design as an ego agent and its environment as a group of adversarial (ado) agents. For instance, a self-driving car is an ego agent that is influenced by ado agents such as pedestrians, traffic lights, road geometry, and bicyclists. By defining the correct behavior of the ego agent and a set of restrictions that reflect rational adversarial behavior, we simplify the adversarial testing issue by designing controllers for (restricted) ado agents that cause the ego agent to breach its specifications. We employ tabular and deep reinforcement learning techniques to synthesize adversarial agents and prove their superiority over conventional falsification or testing methods because they can extend to various ego agents and environments. We validate the effectiveness of our approach through two realistic case studies related to self-driving cars.",1
"Humans and other intelligent animals evolved highly sophisticated perception systems that combine multiple sensory modalities. On the other hand, state-of-the-art artificial agents rely mostly on visual inputs or structured low-dimensional observations provided by instrumented environments. Learning to act based on combined visual and auditory inputs is still a new topic of research that has not been explored beyond simple scenarios. To facilitate progress in this area we introduce a new version of VizDoom simulator to create a highly efficient learning environment that provides raw audio observations. We study the performance of different model architectures in a series of tasks that require the agent to recognize sounds and execute instructions given in natural language. Finally, we train our agent to play the full game of Doom and find that it can consistently defeat a traditional vision-based adversary. We are currently in the process of merging the augmented simulator with the main ViZDoom code repository. Video demonstrations and experiment code can be found at https://sites.google.com/view/sound-rl.",0
"Sophisticated perception systems combining multiple sensory modalities have evolved in humans and other intelligent animals. However, current artificial agents mostly rely on visual inputs or structured low-dimensional observations from instrumented environments. Although research on learning to act based on combined visual and auditory inputs is a new area, progress can be facilitated with the introduction of a new version of VizDoom simulator that offers raw audio observations. We evaluated different model architectures in various tasks requiring the agent to recognize sounds and execute instructions given in natural language. Our agent was trained to play the full game of Doom and consistently outperformed a traditional vision-based adversary. The augmented simulator is being merged with the main ViZDoom code repository. For video demonstrations and experiment code, please visit https://sites.google.com/view/sound-rl.",1
"Convolutional neural networks (CNNs) often have poor generalization performance under domain shift. One way to improve domain generalization is to collect diverse source data from multiple relevant domains so that a CNN model is allowed to learn more domain-invariant, and hence generalizable representations. In this work, we address domain generalization with MixStyle, a plug-and-play, parameter-free module that is simply inserted to shallow CNN layers and requires no modification to training objectives. Specifically, MixStyle probabilistically mixes feature statistics between instances. This idea is inspired by the observation that visual domains can often be characterized by image styles which are in turn encapsulated within instance-level feature statistics in shallow CNN layers. Therefore, inserting MixStyle modules in effect synthesizes novel domains albeit in an implicit way. MixStyle is not only simple and flexible, but also versatile -- it can be used for problems whereby unlabeled images are available, such as semi-supervised domain generalization and unsupervised domain adaptation, with a simple extension to mix feature statistics between labeled and pseudo-labeled instances. We demonstrate through extensive experiments that MixStyle can significantly boost the out-of-distribution generalization performance across a wide range of tasks including object recognition, instance retrieval, and reinforcement learning.",0
"CNNs often struggle with generalization when faced with domain shifts. To overcome this challenge, one solution is to gather diverse source data from multiple relevant domains. This approach enables a CNN model to learn more domain-invariant and generalizable representations. The present study introduces MixStyle, a plug-and-play module that can enhance domain generalization without requiring any parameter adjustments or modifications to the training objectives. MixStyle works by probabilistically mixing feature statistics between instances, inspired by the observation that image styles encapsulate instance-level feature statistics in shallow CNN layers. MixStyle modules create new domains implicitly, making this approach simple, flexible, and versatile. It can be used for problems where unlabeled images are available, such as semi-supervised domain generalization and unsupervised domain adaptation, with an extension to mix feature statistics between labeled and pseudo-labeled instances. Extensive experimentation demonstrates that MixStyle can significantly improve out-of-distribution generalization performance for a wide range of tasks, including object recognition, instance retrieval, and reinforcement learning.",1
"We consider the problem of building a state representation model for control, in a continual learning setting. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge, and then use Reinforcement Learning on the resulting features for efficient policy learning. To this end, we propose S-TRIGGER, a general method for Continual State Representation Learning applicable to Variational Auto-Encoders and its many variants. The method is based on Generative Replay, i.e. the use of generated samples to maintain past knowledge. It comes along with a statistically sound method for environment change detection, which self-triggers the Generative Replay. Our experiments on VAEs show that S-TRIGGER learns state representations that allows fast and high-performing Reinforcement Learning, while avoiding catastrophic forgetting. The resulting system is capable of autonomously learning new information without using past data and with a bounded system size. Code for our experiments is attached in Appendix.",0
"In a continual learning scenario, we aim to construct a state representation model for control that can effectively compress sensory state information as the environment changes. This must be done without sacrificing past knowledge, and the resulting features can then be utilized for efficient policy learning through Reinforcement Learning. Our proposed solution, S-TRIGGER, is a general method for Continual State Representation Learning that can be applied to Variational Auto-Encoders and its various forms. S-TRIGGER implements Generative Replay, which involves using generated samples to retain past knowledge, along with a reliable method for detecting changes in the environment that self-triggers the Generative Replay. Our experiments show that S-TRIGGER is capable of learning state representations that enable fast and high-performing Reinforcement Learning while avoiding catastrophic forgetting. Additionally, the system can learn new information independently without the need for past data and with a limited system size. The code for our experiments is enclosed in the Appendix.",1
"Although recent works have developed methods that can generate estimations (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks. This is particularly true if the data have large missingness rates or a small population. More importantly, the imputation error could be propagated into the prediction step that follows, causing the gradients used to train the prediction models to be biased. Consequently, in this work, we introduce the importance guided stochastic gradient descent (IGSGD) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train the models via back-propagation. This not only reduces bias but allows the model to exploit the underlying information behind missingness patterns. We test the proposed approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods.",0
"Although some recent methodologies have been developed to generate approximations or imputations for missing entries in datasets in order to simplify downstream analysis, most of them rely on assumptions that may not be applicable to real-world scenarios and could result in poor performance in subsequent tasks. This is especially true when data have high rates of missingness or a small population. Additionally, inaccurate imputations may propagate into the prediction step, leading to biased gradients used to train prediction models. Therefore, this study introduces the importance guided stochastic gradient descent (IGSGD) approach to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Reinforcement learning (RL) is employed to adjust the gradients used for model training through back-propagation, which not only reduces bias but also enables the model to exploit the underlying information behind missingness patterns. The proposed method is tested on real-world time-series (MIMIC-III), tabular data from an eye clinic, and a standard dataset (MNIST), where the imputation-free predictions outperform traditional two-step imputation-based predictions using state-of-the-art imputation methods.",1
"In problem-solving, we humans can come up with multiple novel solutions to the same problem. However, reinforcement learning algorithms can only produce a set of monotonous policies that maximize the cumulative reward but lack diversity and novelty. In this work, we address the problem of generating novel policies in reinforcement learning tasks. Instead of following the multi-objective framework used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We first introduce a new metric to evaluate the difference between policies and then design two practical novel policy generation methods following the new perspective. The two proposed methods, namely the Constrained Task Novel Bisector (CTNB) and the Interior Policy Differentiation (IPD), are derived from the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJoCo control suite show our methods can achieve substantial improvement over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.",0
"While humans can come up with various innovative solutions to a problem, reinforcement learning algorithms can only generate monotonous policies that aim to maximize the cumulative reward. This study is focused on addressing the issue of creating new policies in reinforcement learning tasks. Rather than using the multi-objective framework that existing methods rely on, we propose a new approach based on constrained optimization. We introduce a new metric to measure policy differences and utilize two new practical policy generation methods - the Constrained Task Novel Bisector (CTNB) and the Interior Policy Differentiation (IPD), which are derived from the feasible direction and interior point methods used in constrained optimization literature. Our experimental findings using the MuJoCo control suite demonstrate that these methods improve novelty-seeking methods significantly, with enhanced policy novelty and performance in the primary task.",1
"Model-based deep reinforcement learning has achieved success in various domains that require high sample efficiencies, such as Go and robotics. However, there are some remaining issues, such as planning efficient explorations to learn more accurate dynamic models, evaluating the uncertainty of the learned models, and more rational utilization of models. To mitigate these issues, we present MEEE, a model-ensemble method that consists of optimistic exploration and weighted exploitation. During exploration, unlike prior methods directly selecting the optimal action that maximizes the expected accumulative return, our agent first generates a set of action candidates and then seeks out the optimal action that takes both expected return and future observation novelty into account. During exploitation, different discounted weights are assigned to imagined transition tuples according to their model uncertainty respectively, which will prevent model predictive error propagation in agent training. Experiments on several challenging continuous control benchmark tasks demonstrated that our approach outperforms other model-free and model-based state-of-the-art methods, especially in sample complexity.",0
"Successful implementation of model-based deep reinforcement learning has been observed in various domains that require better sample efficiencies, such as Go and robotics. However, there are still some unresolved issues such as efficient planning of explorations to learn more accurate dynamic models, assessing the uncertainty of the learned models, and utilizing models more rationally. To address these challenges, we propose MEEE, a model-ensemble method that comprises optimistic exploration and weighted exploitation. During exploration, instead of directly selecting the optimal action that maximizes the expected accumulative return, our agent generates a set of action candidates and then identifies the best action that takes into account both expected return and future observation novelty. During exploitation, we assign different discounted weights to imagined transition tuples based on their model uncertainty, which helps prevent the propagation of model predictive errors during agent training. Our approach outperforms other model-free and model-based state-of-the-art methods, especially in sample complexity, as demonstrated in experiments conducted on several challenging continuous control benchmark tasks.",1
"Policy gradient (PG) algorithms have been widely used in reinforcement learning (RL). However, PG algorithms rely on exploiting the value function being learned with the first-order update locally, which results in limited sample efficiency. In this work, we propose an alternative method called Zeroth-Order Supervised Policy Improvement (ZOSPI). ZOSPI exploits the estimated value function $Q$ globally while preserving the local exploitation of the PG methods based on zeroth-order policy optimization. This learning paradigm follows Q-learning but overcomes the difficulty of efficiently operating argmax in continuous action space. It finds max-valued action within a small number of samples. The policy learning of ZOSPI has two steps: First, it samples actions and evaluates those actions with a learned value estimator, and then it learns to perform the action with the highest value through supervised learning. We further demonstrate such a supervised learning framework can learn multi-modal policies. Experiments show that ZOSPI achieves competitive results on the continuous control benchmarks with a remarkable sample efficiency.",0
"Reinforcement learning (RL) commonly employs Policy Gradient (PG) algorithms, but their dependence on exploiting the value function locally through first-order updates leads to limited sample efficiency. This paper presents an alternate approach, Zeroth-Order Supervised Policy Improvement (ZOSPI), which utilizes the globally estimated value function $Q$ while maintaining local exploitation through zeroth-order policy optimization. ZOSPI overcomes the challenge of efficiently operating argmax in continuous action space by identifying the maximum-valued action within a limited number of samples, following the Q-learning paradigm. ZOSPI's policy learning comprises two steps: first, it samples actions and evaluates them using a learned value estimator, and second, it uses supervised learning to perform the action with the highest value. Additionally, we demonstrate that this supervised learning framework can learn multi-modal policies. Our experiments show that ZOSPI performs competitively on continuous control benchmarks with impressive sample efficiency.",1
"Reinforcement learning in large-scale environments is challenging due to the many possible actions that can be taken in specific situations. We have previously developed a means of constraining, and hence speeding up, the search process through the use of motion primitives; motion primitives are sequences of pre-specified actions taken across a state series. As a byproduct of this work, we have found that if the motion primitives' motions and actions are labeled, then the search can be sped up further. Since motion primitives may initially lack such details, we propose a theoretically viewpoint-insensitive and speed-insensitive means of automatically annotating the underlying motions and actions. We do this through a differential-geometric, spatio-temporal kinematics descriptor, which analyzes how the poses of entities in two motion sequences change over time. We use this descriptor in conjunction with a weighted-nearest-neighbor classifier to label the primitives using a limited set of training examples. In our experiments, we achieve high motion and action annotation rates for human-action-derived primitives with as few as one training sample. We also demonstrate that reinforcement learning using accurately labeled trajectories leads to high-performing policies more quickly than standard reinforcement learning techniques. This is partly because motion primitives encode prior domain knowledge and preempt the need to re-discover that knowledge during training. It is also because agents can leverage the labels to systematically ignore action classes that do not facilitate task objectives, thereby reducing the action space.",0
"Large-scale environments pose a challenge for reinforcement learning due to the vast number of potential actions in specific situations. Our team has previously developed a way to speed up the search process by using motion primitives, which are predetermined action sequences across a state series. We discovered that labeling the motions and actions of these primitives can further enhance the search process. However, motion primitives often lack these details initially. To address this, we propose a theoretically viewpoint-insensitive and speed-insensitive method for automatically annotating underlying motions and actions using a differential-geometric, spatio-temporal kinematics descriptor. With the help of a weighted-nearest-neighbor classifier, we can label the primitives using a limited set of training examples. Our experiments showed high motion and action annotation rates for human-action-derived primitives with as few as one training sample. We also demonstrated that reinforcement learning using accurately labeled trajectories leads to high-performing policies faster than standard reinforcement learning methods. This is because motion primitives contain prior domain knowledge and eliminate the need to re-discover that knowledge during training. Additionally, agents can use the labels to systematically ignore irrelevant action classes, reducing the action space.",1
"Offline reinforcement learning proposes to learn policies from large collected datasets without interacting with the physical environment. These algorithms have made it possible to learn useful skills from data that can then be deployed in the environment in real-world settings where interactions may be costly or dangerous, such as autonomous driving or factories. However, current algorithms overfit to the dataset they are trained on and exhibit poor out-of-distribution generalization to the environment when deployed. In this paper, we study the effectiveness of performing data augmentations on the state space, and study 7 different augmentation schemes and how they behave with existing offline RL algorithms. We then combine the best data performing augmentation scheme with a state-of-the-art Q-learning technique, and improve the function approximation of the Q-networks by smoothening out the learned state-action space. We experimentally show that using this Surprisingly Simple Self-Supervision technique in RL (S4RL), we significantly improve over the current state-of-the-art algorithms on offline robot learning environments such as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",0
"The idea behind offline reinforcement learning is to acquire policies from large datasets without physically engaging with the environment. This approach has enabled the development of valuable skills that can be utilized in real-world settings where interactions are expensive or dangerous, such as in factories or autonomous driving. However, the current algorithms tend to overfit to the dataset they are trained on and exhibit poor generalization when deployed in different environments. To address this issue, we explore the effectiveness of data augmentation on the state space by examining seven different augmentation schemes and their impact on existing offline RL algorithms. We then integrate the most effective augmentation scheme with a state-of-the-art Q-learning technique and enhance the function approximation of Q-networks by smoothing out the learned state-action space. Through our Surprisingly Simple Self-Supervision technique in RL (S4RL), we have demonstrated significant improvement over current state-of-the-art algorithms in offline robot learning environments such as MetaWorld and RoboSuite, as well as benchmark datasets like D4RL.",1
"Many practical applications of reinforcement learning (RL) constrain the agent to learn from a fixed offline dataset of logged interactions, which has already been gathered, without offering further possibility for data collection. However, commonly used off-policy RL algorithms, such as the Deep Q Network and the Deep Deterministic Policy Gradient, are incapable of learning without data correlated to the distribution under the current policy, making them ineffective for this offline setting. As the first step towards useful offline RL algorithms, we analysis the reason of instability in standard off-policy RL algorithms. It is due to the bootstrapping error. The key to avoiding this error, is ensuring that the agent's action space does not go out of the fixed offline dataset. Based on our consideration, a creative offline RL framework, the Least Restriction (LR), is proposed in this paper. The LR regards selecting an action as taking a sample from the probability distribution. It merely set a little limit for action selection, which not only avoid the action being out of the offline dataset but also remove all the unreasonable restrictions in earlier approaches (e.g. Batch-Constrained Deep Q-Learning). In the further, we will demonstrate that the LR, is able to learn robustly from different offline datasets, including random and suboptimal demonstrations, on a range of practical control tasks.",0
"In many cases, reinforcement learning (RL) is limited to learning from a pre-existing dataset of interactions, without the option of collecting further data. However, popular off-policy RL algorithms such as Deep Q Network and Deep Deterministic Policy Gradient are ineffective in this offline setting as they require data correlated to the current policy. The instability of standard off-policy RL algorithms is caused by the bootstrapping error, which can be avoided by ensuring that the agent's action space remains within the fixed offline dataset. To address this issue, we propose the Least Restriction (LR) framework, which allows the agent to select actions by sampling from a probability distribution with minimal limitations. This approach not only prevents actions outside of the offline dataset but also removes unnecessary constraints from previous methods like Batch-Constrained Deep Q-Learning. Our experiments demonstrate that the LR framework can effectively learn from various offline datasets, including random and suboptimal demonstrations, for different control tasks.",1
"We introduce Robust Restless Bandits, a challenging generalization of restless multi-arm bandits (RMAB). RMABs have been widely studied for intervention planning with limited resources. However, most works make the unrealistic assumption that the transition dynamics are known perfectly, restricting the applicability of existing methods to real-world scenarios. To make RMABs more useful in settings with uncertain dynamics: (i) We introduce the Robust RMAB problem and develop solutions for a minimax regret objective when transitions are given by interval uncertainties; (ii) We develop a double oracle algorithm for solving Robust RMABs and demonstrate its effectiveness on three experimental domains; (iii) To enable our double oracle approach, we introduce RMABPPO, a novel deep reinforcement learning algorithm for solving RMABs. RMABPPO hinges on learning an auxiliary ""$\lambda$-network"" that allows each arm's learning to decouple, greatly reducing sample complexity required for training; (iv) Under minimax regret, the adversary in the double oracle approach is notoriously difficult to implement due to non-stationarity. To address this, we formulate the adversary oracle as a multi-agent reinforcement learning problem and solve it with a multi-agent extension of RMABPPO, which may be of independent interest as the first known algorithm for this setting. Code is available at https://github.com/killian-34/RobustRMAB.",0
"The article introduces a more challenging version of restless multi-arm bandits (RMAB), called Robust Restless Bandits, which aims to address the unrealistic assumption that transition dynamics are perfectly known in most existing RMAB studies. To tackle uncertainties in transition dynamics, the authors present the Robust RMAB problem and offer solutions for minimizing regret objectives. They also introduce a double oracle algorithm to solve Robust RMABs and demonstrate its effectiveness through experiments. Additionally, they present RMABPPO, a novel deep reinforcement learning algorithm that simplifies the learning process for each arm. Finally, the article addresses the challenge of implementing the adversary oracle in the double oracle approach and proposes a multi-agent extension of RMABPPO to solve it. The code is available at https://github.com/killian-34/RobustRMAB.",1
"Deep Reinforcement Learning has shown its ability in solving complicated problems directly from high-dimensional observations. However, in end-to-end settings, Reinforcement Learning algorithms are not sample-efficient and requires long training times and quantities of data. In this work, we proposed a framework for sample-efficient Reinforcement Learning that take advantage of state and action representations to transform a high-dimensional problem into a low-dimensional one. Moreover, we seek to find the optimal policy mapping latent states to latent actions. Because now the policy is learned on abstract representations, we enforce, using auxiliary loss functions, the lifting of such policy to the original problem domain. Results show that the novel framework can efficiently learn low-dimensional and interpretable state and action representations and the optimal latent policy.",0
"The capability of Deep Reinforcement Learning to tackle complex problems using high-dimensional observations is well-established. Nonetheless, Reinforcement Learning algorithms in end-to-end scenarios often demand extensive training periods and extensive data to be efficient. Our study proposes a sample-efficient Reinforcement Learning framework that leverages state and action representations to transform a high-dimensional challenge into a low-dimensional one. Furthermore, we aim to discover the best policy that maps latent states to latent actions. Given that the policy is now trained on theoretical representations, we utilize auxiliary loss functions to ensure that the policy is transferred to the original problem domain. We demonstrate that this innovative framework can effectively discover low-dimensional and clear state and action representations and the optimal latent policy.",1
"Breakthrough advances in reinforcement learning (RL) research have led to a surge in the development and application of RL. To support the field and its rapid growth, several frameworks have emerged that aim to help the community more easily build effective and scalable agents. However, very few of these frameworks exclusively support multi-agent RL (MARL), an increasingly active field in itself, concerned with decentralised decision-making problems. In this work, we attempt to fill this gap by presenting Mava: a research framework specifically designed for building scalable MARL systems. Mava provides useful components, abstractions, utilities and tools for MARL and allows for simple scaling for multi-process system training and execution, while providing a high level of flexibility and composability. Mava is built on top of DeepMind's Acme \citep{hoffman2020acme}, and therefore integrates with, and greatly benefits from, a wide range of already existing single-agent RL components made available in Acme. Several MARL baseline systems have already been implemented in Mava. These implementations serve as examples showcasing Mava's reusable features, such as interchangeable system architectures, communication and mixing modules. Furthermore, these implementations allow existing MARL algorithms to be easily reproduced and extended. We provide experimental results for these implementations on a wide range of multi-agent environments and highlight the benefits of distributed system training.",0
"Reinforcement learning (RL) research has made significant advancements, leading to a rise in the development and application of RL. To facilitate the field's growth, various frameworks have emerged to assist the community in creating effective and scalable agents. However, only a few of these frameworks are exclusively designed for multi-agent RL (MARL), a rapidly growing field concerned with decentralized decision-making problems. This paper aims to address this gap by introducing Mava, a research framework specifically tailored for building scalable MARL systems. Mava offers valuable components, abstractions, utilities, and tools for MARL, allowing for easy scaling of multi-process system training and execution while maintaining a high level of flexibility and composability. Mava is built on top of DeepMind's Acme, which integrates with and benefits from a wide range of current single-agent RL components. Mava features several MARL baseline systems, which demonstrate its reusable features, such as interchangeable system architectures, communication, and mixing modules. These implementations enable existing MARL algorithms to be easily reproduced and extended. We provide experimental results for these implementations on various multi-agent environments and emphasize the advantages of distributed system training.",1
"The performance of state-of-the-art baselines in the offline RL regime varies widely over the spectrum of dataset qualities, ranging from ""far-from-optimal"" random data to ""close-to-optimal"" expert demonstrations. We re-implement these under a fair, unified, and highly factorized framework, and show that when a given baseline outperforms its competing counterparts on one end of the spectrum, it never does on the other end. This consistent trend prevents us from naming a victor that outperforms the rest across the board. We attribute the asymmetry in performance between the two ends of the quality spectrum to the amount of inductive bias injected into the agent to entice it to posit that the behavior underlying the offline dataset is optimal for the task. The more bias is injected, the higher the agent performs, provided the dataset is close-to-optimal. Otherwise, its effect is brutally detrimental. Adopting an advantage-weighted regression template as base, we conduct an investigation which corroborates that injections of such optimality inductive bias, when not done parsimoniously, makes the agent subpar in the datasets it was dominant as soon as the offline policy is sub-optimal. In an effort to design methods that perform well across the whole spectrum, we revisit the generalized policy iteration scheme for the offline regime, and study the impact of nine distinct newly-introduced proposal distributions over actions, involved in proposed generalization of the policy evaluation and policy improvement update rules. We show that certain orchestrations strike the right balance and can improve the performance on one end of the spectrum without harming it on the other end.",0
"The effectiveness of state-of-the-art baselines in the offline RL regime varies greatly depending on the quality of the dataset, ranging from random data to expert demonstrations. To provide a fair and unified comparison, we re-implement these baselines within a highly factorized framework. Our analysis reveals that when a baseline performs well on one end of the spectrum, it does not necessarily perform well on the other end. This consistent trend makes it impossible to identify a clear winner that outperforms the rest across the board. We attribute this asymmetry in performance to the level of inductive bias injected into the agent to encourage it to assume that the behavior in the offline dataset is optimal for the task. The more bias injected, the better the agent performs, provided the dataset is close to optimal. However, if the dataset is suboptimal, excessive bias can harm the agent's performance. To address this issue, we explore the generalized policy iteration scheme for the offline regime and study the impact of nine newly proposed proposal distributions over actions. Our findings demonstrate that certain combinations strike a balance and can improve performance on one end of the spectrum without compromising it on the other.",1
"Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward shaping methods, which makes the base method it is plugged into provably more robust, as shown in several additional theoretical guarantees. We then discuss these through a fine-grained lens and share our insights. Crucially, the guarantees derived and reported in this work are valid for any reward satisfying the Lipschitzness condition, nothing is specific to imitation. As such, these may be of independent interest.",0
"Despite the success of reinforcement learning in various fields, these approaches are often sensitive to hyper-parameters and require significant engineering feats for success. Our focus is on off-policy generative adversarial imitation learning, and we conduct a thorough qualitative and quantitative review of the method. We demonstrate that a necessary condition for the method's success is to enforce a locally Lipschitz-continuous learned reward function. We then examine the effects of this condition and provide theoretical results regarding the state-value function's local Lipschitzness. We support these findings with empirical evidence showing that satisfying the Lipschitzness constraint on the reward has a significant positive impact on imitation performance. Additionally, we explore a generic pessimistic reward preconditioning add-on that enhances the base method's robustness, as evidenced by several theoretical guarantees. Our discussion provides a detailed analysis of these guarantees and our insights. Importantly, the guarantees we derive are applicable to any reward that satisfies the Lipschitzness condition, not just imitation, making them of independent interest.",1
"We consider an improper reinforcement learning setting where a learner is given $M$ base controllers for an unknown Markov decision process, and wishes to combine them optimally to produce a potentially new controller that can outperform each of the base ones. This can be useful in tuning across controllers, learnt possibly in mismatched or simulated environments, to obtain a good controller for a given target environment with relatively few trials.   \par We propose a gradient-based approach that operates over a class of improper mixtures of the controllers. We derive convergence rate guarantees for the approach assuming access to a gradient oracle. The value function of the mixture and its gradient may not be available in closed-form; however, we show that we can employ rollouts and simultaneous perturbation stochastic approximation (SPSA) for explicit gradient descent optimization. Numerical results on (i) the standard control theoretic benchmark of stabilizing an inverted pendulum and (ii) a constrained queueing task show that our improper policy optimization algorithm can stabilize the system even when the base policies at its disposal are unstable\footnote{Under review. Please do not distribute.}.",0
"In this study, we explore the concept of improper reinforcement learning where a learner has access to $M$ base controllers for an unknown Markov decision process. The goal is to combine these controllers in an optimal manner to create a new controller that performs better than each individual base controller. This approach is particularly useful for fine-tuning controllers learned in different environments to suit a specific target environment. We introduce a gradient-based method that works with a class of improper mixtures of the controllers. While closed-form expressions of the value function and its gradient may not be available, we illustrate how we can use rollouts and simultaneous perturbation stochastic approximation (SPSA) for explicit gradient descent optimization. We provide convergence rate guarantees for the approach, assuming access to a gradient oracle. Our numerical experiments with an inverted pendulum and a constrained queueing task show that this improper policy optimization algorithm can stabilize systems even with unstable base policies. \footnote{This paper is currently under review and should not be distributed.}",1
"In reinforcement learning (RL), the goal is to obtain an optimal policy, for which the optimality criterion is fundamentally important. Two major optimality criteria are average and discounted rewards, where the later is typically considered as an approximation to the former. While the discounted reward is more popular, it is problematic to apply in environments that have no natural notion of discounting. This motivates us to revisit a) the progression of optimality criteria in dynamic programming, b) justification for and complication of an artificial discount factor, and c) benefits of directly maximizing the average reward. Our contributions include a thorough examination of the relationship between average and discounted rewards, as well as a discussion of their pros and cons in RL. We emphasize that average-reward RL methods possess the ingredient and mechanism for developing the general discounting-free optimality criterion (Veinott, 1969) in RL.",0
"The main objective in reinforcement learning (RL) is to obtain the best policy, and the criteria for optimality play a crucial role in achieving this. The two major optimality criteria are average and discounted rewards, with the latter being viewed as an approximation of the former. However, the discounted reward criterion is not suitable for environments without a natural discounting mechanism. In view of this, we investigate a) the evolution of optimality criteria in dynamic programming, b) the rationale and complexity of using an artificial discount factor, and c) the advantages of maximizing average reward directly. Our research delves deeply into the relationship between average and discounted rewards, and we present a comprehensive discussion of their pros and cons in RL. We highlight that average-reward RL techniques have the potential for developing a general discounting-free optimality criterion (Veinott, 1969) in RL.",1
"Despite the fact that deep reinforcement learning (RL) has surpassed human-level performances in various tasks, it still has several fundamental challenges. First, most RL methods require intensive data from the exploration of the environment to achieve satisfactory performance. Second, the use of neural networks in RL renders it hard to interpret the internals of the system in a way that humans can understand. To address these two challenges, we propose a framework that enables an RL agent to reason over its exploration process and distill high-level knowledge for effectively guiding its future explorations. Specifically, we propose a novel RL algorithm that learns high-level knowledge in the form of a finite reward automaton by using the L* learning algorithm. We prove that in episodic RL, a finite reward automaton can express any non-Markovian bounded reward functions with finitely many reward values and approximate any non-Markovian bounded reward function (with infinitely many reward values) with arbitrary precision. We also provide a lower bound for the episode length such that the proposed RL approach almost surely converges to an optimal policy in the limit. We test this approach on two RL environments with non-Markovian reward functions, choosing a variety of tasks with increasing complexity for each environment. We compare our algorithm with the state-of-the-art RL algorithms for non-Markovian reward functions, such as Joint Inference of Reward machines and Policies for RL (JIRP), Learning Reward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show that our algorithm converges to an optimal policy faster than other baseline methods.",0
"Even though deep reinforcement learning (RL) has achieved superior performance to humans in various tasks, it still faces significant challenges. Firstly, most RL techniques require extensive data from environment exploration for satisfactory performance. Secondly, the use of neural networks in RL makes it difficult to understand the system's internals in a way that humans can interpret. To tackle these challenges, we propose a framework that allows an RL agent to reason about its exploration process and distill high-level knowledge for guiding future explorations effectively. Our novel RL algorithm learns high-level knowledge in the form of a finite reward automaton using the L* learning algorithm. We demonstrate that a finite reward automaton can express any non-Markovian bounded reward functions and approximate any non-Markovian bounded reward function with infinite reward values with arbitrary precision in episodic RL. We also provide a lower bound for the episode length, ensuring that our proposed RL approach almost certainly converges to an optimal policy. We evaluate our approach on two RL environments with non-Markovian reward functions, selecting a range of tasks with increasing complexity. We compare our algorithm with state-of-the-art RL algorithms for non-Markovian reward functions, such as Joint Inference of Reward machines and Policies for RL (JIRP), Learning Reward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results demonstrate that our algorithm converges to an optimal policy more efficiently than other baseline methods.",1
"We provide improved gap-dependent regret bounds for reinforcement learning in finite episodic Markov decision processes. Compared to prior work, our bounds depend on alternative definitions of gaps. These definitions are based on the insight that, in order to achieve a favorable regret, an algorithm does not need to learn how to behave optimally in states that are not reached by an optimal policy. We prove tighter upper regret bounds for optimistic algorithms and accompany them with new information-theoretic lower bounds for a large class of MDPs. Our results show that optimistic algorithms can not achieve the information-theoretic lower bounds even in deterministic MDPs unless there is a unique optimal policy.",0
"Our research offers enhanced regret bounds for reinforcement learning in finite episodic Markov decision processes. In contrast to previous studies, our bounds utilize different gap definitions. We have discovered that to attain favorable regret, algorithms do not necessarily need to learn how to behave optimally in states that an optimal policy does not reach. Our findings demonstrate tighter upper regret bounds for optimistic algorithms and are accompanied by new information-theoretic lower bounds for a wide range of MDPs. Our results indicate that, even in deterministic MDPs, optimistic algorithms cannot meet the information-theoretic lower bounds unless there is a sole optimal policy.",1
"Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT's 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students' average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.",0
"Is it possible for a machine to learn the intricacies of Machine Learning? This study focuses on teaching a machine learning model to tackle problems from an undergraduate-level course on Machine Learning. A new set of questions and answers was curated from exercises, homework and quizzes from MIT's 6.036 Introduction to Machine Learning course, which was used to train the model to answer these questions. The system achieved an accuracy of 96% for open-response questions and 97% for multiple-choice questions, surpassing the average grade of MIT students by achieving a grade A performance in the course, in real-time. The questions spanned 12 topics taught in the course, excluding coding questions and image-based questions. The topics included basic machine learning principles, perceptrons, feature extraction and selection, logistic regression, regression, neural networks, advanced neural networks, convolutional neural networks, recurrent neural networks, state machines and MDPs, reinforcement learning, and decision trees. The Transformer models were utilized in an encoder-decoder architecture with graph and tree representations. The study also incorporated a data-augmentation technique to generate new example problems and trained another machine learning model to provide hints for problems. Hence, the system not only generates new questions across topics and answers open-response and multiple-choice questions but also classifies problems and generates problem hints, leading the way in AI for STEM education.",1
"Besides accuracy, the model size of convolutional neural networks (CNN) models is another important factor considering limited hardware resources in practical applications. For example, employing deep neural networks on mobile systems requires the design of accurate yet fast CNN for low latency in classification and object detection. To fulfill the need, we aim at obtaining CNN models with both high testing accuracy and small size to address resource constraints in many embedded devices. In particular, this paper focuses on proposing a generic reinforcement learning-based model compression approach in a two-stage compression pipeline: pruning and quantization. The first stage of compression, i.e., pruning, is achieved via exploiting deep reinforcement learning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise channel pruning and element-wise variational pruning via information dropout. The second stage, i.e., quantization, is achieved via a similar DRL approach but focuses on obtaining the optimal bits representation for individual layers. We further conduct experimental results on CIFAR-10 and ImageNet datasets. For the CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x from 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet dataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to 4.14MB with no accuracy loss.",0
"In practical applications, the size of convolutional neural networks (CNN) models is an essential consideration due to limited hardware resources, in addition to accuracy. For instance, the use of deep neural networks on mobile systems necessitates the creation of precise yet rapid CNN to achieve low latency in object detection and classification. Our goal is to develop CNN models that possess high testing accuracy and a small size to meet resource limitations on embedded devices. In this paper, we propose a two-stage compression pipeline, including pruning and quantization, using a generic reinforcement learning-based model compression approach. The first compression stage entails deep reinforcement learning (DRL) to co-learn the accuracy and FLOPs by layer-wise channel pruning and element-wise variational pruning via information dropout. The second stage uses a similar DRL approach to obtain the best bits representation for individual layers. We also conducted experiments on CIFAR-10 and ImageNet datasets. Our proposed method reduces the VGGNet size by 9x from 20.04MB to 2.2MB, with a slight increase in accuracy, for the CIFAR-10 dataset. For the ImageNet dataset, our method reduces the VGG-16 size by 33x from 138MB to 4.14MB, with no loss in accuracy.",1
"This paper reviews the current state of the art in Artificial Intelligence (AI) technologies and applications in the context of the creative industries. A brief background of AI, and specifically Machine Learning (ML) algorithms, is provided including Convolutional Neural Network (CNNs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement Learning (DRL). We categorise creative applications into five groups related to how AI technologies are used: i) content creation, ii) information analysis, iii) content enhancement and post production workflows, iv) information extraction and enhancement, and v) data compression. We critically examine the successes and limitations of this rapidly advancing technology in each of these areas. We further differentiate between the use of AI as a creative tool and its potential as a creator in its own right. We foresee that, in the near future, machine learning-based AI will be adopted widely as a tool or collaborative assistant for creativity. In contrast, we observe that the successes of machine learning in domains with fewer constraints, where AI is the `creator', remain modest. The potential of AI (or its developers) to win awards for its original creations in competition with human creatives is also limited, based on contemporary technologies. We therefore conclude that, in the context of creative industries, maximum benefit from AI will be derived where its focus is human centric -- where it is designed to augment, rather than replace, human creativity.",0
"In this paper, we evaluate the current state of Artificial Intelligence (AI) technologies and their applications in the creative industries. We provide a brief overview of AI and Machine Learning (ML) algorithms, such as Convolutional Neural Network (CNNs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), and Deep Reinforcement Learning (DRL). We categorize creative applications into five groups based on how AI technologies are utilized: content creation, information analysis, content enhancement and post-production workflows, information extraction and enhancement, and data compression. We analyze the successes and limitations of AI in each of these areas and differentiate between its use as a creative tool versus its potential as a creator in its own right. We predict that AI will soon be widely adopted as a tool or collaborative assistant for creativity, but its success as a creator remains limited. We conclude that AI should be designed to augment, rather than replace, human creativity to maximize its benefits in the context of the creative industries.",1
"This paper presents a new model-free algorithm for episodic finite-horizon Markov Decision Processes (MDP), Adaptive Multi-step Bootstrap (AMB), which enjoys a stronger gap-dependent regret bound. The first innovation is to estimate the optimal $Q$-function by combining an optimistic bootstrap with an adaptive multi-step Monte Carlo rollout. The second innovation is to select the action with the largest confidence interval length among admissible actions that are not dominated by any other actions. We show when each state has a unique optimal action, AMB achieves a gap-dependent regret bound that only scales with the sum of the inverse of the sub-optimality gaps. In contrast, Simchowitz and Jamieson (2019) showed all upper-confidence-bound (UCB) algorithms suffer an additional $\Omega\left(\frac{S}{\Delta_{min}}\right)$ regret due to over-exploration where $\Delta_{min}$ is the minimum sub-optimality gap and $S$ is the number of states. We further show that for general MDPs, AMB suffers an additional $\frac{|Z_{mul}|}{\Delta_{min}}$ regret, where $Z_{mul}$ is the set of state-action pairs $(s,a)$'s satisfying $a$ is a non-unique optimal action for $s$. We complement our upper bound with a lower bound showing the dependency on $\frac{|Z_{mul}|}{\Delta_{min}}$ is unavoidable for any consistent algorithm. This lower bound also implies a separation between reinforcement learning and contextual bandits.",0
"In this article, a novel model-free algorithm for episodic finite-horizon Markov Decision Processes (MDP) is presented, called Adaptive Multi-step Bootstrap (AMB). This algorithm offers a stronger gap-dependent regret bound. AMB estimates the optimal Q-function by combining an optimistic bootstrap with an adaptive multi-step Monte Carlo rollout. It selects the optimal action by choosing the one with the largest confidence interval length among admissible actions that are not dominated by any other actions. When each state has a unique optimal action, AMB achieves a gap-dependent regret bound that scales only with the sum of the inverse of the sub-optimality gaps. In contrast, Simchowitz and Jamieson (2019) have demonstrated that all upper-confidence-bound (UCB) algorithms suffer from additional regret due to over-exploration. Furthermore, it has been shown that for general MDPs, AMB suffers from additional regret that scales with the number of state-action pairs that have non-unique optimal actions. An upper bound is complemented by a lower bound that demonstrates that this dependence is unavoidable for any consistent algorithm. This lower bound also highlights the difference between reinforcement learning and contextual bandits.",1
"Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.",0
"The problem of determining causal relationships from observations has long been a challenge in machine learning. Most research in this area assumes that the causal variables are directly observable, but for AI agents like robots, only low-level variables such as image pixels can be observed. To be effective, an agent must identify high-level variables that are causally related or affected by causal variables. Therefore, the goal for AI and causality is to discover abstract representations and causal structure together. However, existing environments for studying causal induction are not ideal because they are task-specific and cannot be parametrically manipulated. The authors aim to support research in learning high-level variables and causal structures and have designed a set of benchmarking RL environments to test various representation learning algorithms. Their findings suggest that models that incorporate structure and modularity can improve causal induction in model-based reinforcement learning.",1
"A reinforcement-learning-based non-uniform compressed sensing (NCS) framework for time-varying signals is introduced. The proposed scheme, referred to as RL-NCS, aims to boost the performance of signal recovery through an optimal and adaptive distribution of sensing energy among two groups of coefficients of the signal, referred to as the region of interest (ROI) coefficients and non-ROI coefficients. The coefficients in ROI usually have greater importance and need to be reconstructed with higher accuracy compared to non-ROI coefficients. In order to accomplish this task, the ROI is predicted at each time step using two specific approaches. One of these approaches incorporates a long short-term memory (LSTM) network for the prediction. The other approach employs the previous ROI information for predicting the next step ROI. Using the exploration-exploitation technique, a Q-network learns to choose the best approach for designing the measurement matrix. Furthermore, a joint loss function is introduced for the efficient training of the Q-network as well as the LSTM network. The result indicates a significant performance gain for our proposed method, even for rapidly varying signals and a reduced number of measurements.",0
"Introducing a framework for time-varying signals called RL-NCS, this article proposes a reinforcement-learning-based non-uniform compressed sensing approach to enhance signal recovery. The scheme strategically distributes sensing energy among two groups of coefficients: the region of interest (ROI) coefficients and non-ROI coefficients, to improve the accuracy of reconstructing the former. Two methods are used to predict the ROI at each time step: one utilizing a long short-term memory (LSTM) network, and the other employing previous ROI information. Using the exploration-exploitation technique, a Q-network is trained to select the best approach for designing the measurement matrix. Additionally, a joint loss function is introduced to efficiently train both the Q-network and LSTM network. The results demonstrate significant performance improvements, even for rapidly varying signals and with a reduced number of measurements.",1
