"Reinforcement learning has steadily improved and outperform human in lots of traditional games since the resurgence of deep neural network. However, these success is not easy to be copied to autonomous driving because the state spaces in real world are extreme complex and action spaces are continuous and fine control is required. Moreover, the autonomous driving vehicles must also keep functional safety under the complex environments. To deal with these challenges, we first adopt the deep deterministic policy gradient (DDPG) algorithm, which has the capacity to handle complex state and action spaces in continuous domain. We then choose The Open Racing Car Simulator (TORCS) as our environment to avoid physical damage. Meanwhile, we select a set of appropriate sensor information from TORCS and design our own rewarder. In order to fit DDPG algorithm to TORCS, we design our network architecture for both actor and critic inside DDPG paradigm. To demonstrate the effectiveness of our model, We evaluate on different modes in TORCS and show both quantitative and qualitative results.",0
"Since the resurgence of deep neural networks, reinforcement learning has made significant progress and surpassed human performance in many traditional games. However, transferring these achievements to autonomous driving is not easy due to the extreme complexity of real-world state spaces, continuous action spaces, and the need for precise control to maintain functional safety in complex environments. To tackle these challenges, we have employed the deep deterministic policy gradient (DDPG) algorithm, which can handle complex state and action spaces in the continuous domain. We have chosen The Open Racing Car Simulator (TORCS) as our environment to avoid physical damage, and we have carefully selected a set of appropriate sensor information from TORCS and developed our own rewarder. To adapt the DDPG algorithm to TORCS, we have designed network architectures for both the actor and critic within the DDPG paradigm. To demonstrate the effectiveness of our approach, we have evaluated it on different TORCS modes and presented both quantitative and qualitative results.",1
"The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the Bellman operator. Surprisingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when combined with Deep Q-learning, leads to Q-functions with superior policies in practice, even outperforming its double Q-learning counterpart. To better understand how and why this occurs, we revisit theoretical properties of the softmax Bellman operator, and prove that $(i)$ it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and $(ii)$ the distance of its Q function from the optimal one can be bounded. These alone do not explain its superior performance, so we also show that the softmax operator can reduce the overestimation error, which may give some insight into why a sub-optimal operator leads to better performance in the presence of value function approximation. A comparison among different Bellman operators is then presented, showing the trade-offs when selecting them.",0
"The use of softmax in reinforcement learning (RL) is often seen as problematic because it can result in suboptimal value functions and interfere with the contraction properties of the Bellman operator. Despite these concerns, when combined with Deep Q-learning, the softmax Bellman operator has been shown to produce superior policies in practice, even outperforming its double Q-learning counterpart. To better understand why this occurs, the theoretical properties of the softmax Bellman operator have been revisited, and it has been proven that it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and the distance of its Q function from the optimal one can be bounded. However, these properties alone do not explain its superior performance. It has also been found that the softmax operator can reduce overestimation error, which may provide some insight into why a suboptimal operator leads to better performance when using value function approximation. A comparison of different Bellman operators is presented, showing the trade-offs involved in selecting them.",1
"In this paper, a review of model-free reinforcement learning for learning of dynamical systems in uncertain environments has discussed. For this purpose, the Markov Decision Process (MDP) will be reviewed. Furthermore, some learning algorithms such as Temporal Difference (TD) learning, Q-Learning, and Approximate Q-learning as model-free algorithms which constitute the main part of this article have been investigated, and benefits and drawbacks of each algorithm will be discussed. The discussed concepts in each section are explaining with details and examples.",0
"This paper provides an overview of model-free reinforcement learning in uncertain environments for learning dynamical systems. The Markov Decision Process (MDP) is reviewed for this purpose. Additionally, the main focus is on some learning algorithms, including Temporal Difference (TD) learning, Q-Learning, and Approximate Q-learning, as model-free algorithms. The advantages and disadvantages of each algorithm are also discussed. Furthermore, each section is explained in detail with examples.",1
"This paper provides a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.",0
"By defining functionals on probability measures, this paper presents a comprehensive perspective on various machine learning problems. Our framework reveals that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be interpreted within it. Additionally, we introduce a universal optimization algorithm, Probability Functional Descent (PFD), and demonstrate how it can recover methods previously developed in the mentioned contexts.",1
"Many continuous control tasks have easily formulated objectives, yet using them directly as a reward in reinforcement learning (RL) leads to suboptimal policies. Therefore, many classical control tasks guide RL training using complex rewards, which require tedious hand-tuning. We automate the reward search with AutoRL, an evolutionary layer over standard RL that treats reward tuning as hyperparameter optimization and trains a population of RL agents to find a reward that maximizes the task objective. AutoRL, evaluated on four Mujoco continuous control tasks over two RL algorithms, shows improvements over baselines, with the the biggest uplift for more complex tasks. The video can be found at: \url{https://youtu.be/svdaOFfQyC8}.",0
"Although continuous control tasks may have objectives that are easy to define, using them as a reward in reinforcement learning (RL) can result in suboptimal policies. As a result, many classical control tasks require complex rewards that need to be painstakingly hand-tuned to guide RL training. To simplify this process, we have developed AutoRL, an evolutionary layer that treats reward tuning as hyperparameter optimization and trains a population of RL agents to find a reward that maximizes the task objective. Our evaluation of AutoRL on four Mujoco continuous control tasks using two RL algorithms showed improvements over baselines, particularly for more complex tasks. A video of our work can be found at: \url{https://youtu.be/svdaOFfQyC8}.",1
"We assume that we are given a time series of data from a dynamical system and our task is to learn the flow map of the dynamical system. We present a collection of results on how to enforce constraints coming from the dynamical system in order to accelerate the training of deep neural networks to represent the flow map of the system as well as increase their predictive ability. In particular, we provide ways to enforce constraints during training for all three major modes of learning, namely supervised, unsupervised and reinforcement learning. In general, the dynamic constraints need to include terms which are analogous to memory terms in model reduction formalisms. Such memory terms act as a restoring force which corrects the errors committed by the learned flow map during prediction.   For supervised learning, the constraints are added to the objective function. For the case of unsupervised learning, in particular generative adversarial networks, the constraints are introduced by augmenting the input of the discriminator. Finally, for the case of reinforcement learning and in particular actor-critic methods, the constraints are added to the reward function. In addition, for the reinforcement learning case, we present a novel approach based on homotopy of the action-value function in order to stabilize and accelerate training. We use numerical results for the Lorenz system to illustrate the various constructions.",0
"Our objective is to learn the flow map of a given dynamical system using a time series of data. To enhance the predictive ability of deep neural networks and expedite their training to represent the system's flow map, we suggest enforcing constraints based on the dynamical system. To achieve this, we provide methods to enforce constraints during the three major modes of learning: supervised, unsupervised, and reinforcement learning. These dynamic constraints need to include memory terms that act as a restoring force to correct errors committed during prediction. We incorporate constraints in the objective function for supervised learning and augment the discriminator's input for unsupervised learning, particularly generative adversarial networks. For reinforcement learning, we add constraints to the reward function and introduce a new approach based on homotopy of the action-value function to stabilize and expedite training. We present numerical results for the Lorenz system to illustrate the various techniques.",1
"Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.",0
"Model-agnostic meta-learning (MAML) is a technique that enables a model to be trained on multiple learning tasks, preparing it for the few-shot learning of new tasks. Although the MAML algorithm performs well on classification, regression, and fine-tuning of policy gradients in reinforcement learning, it requires costly hyperparameter tuning for training stability. To address this issue, we introduce a new extension to MAML known as Alpha MAML, which includes an online hyperparameter adaptation scheme that eliminates the need for meta-learning and learning rates tuning. Our experiments with the Omniglot database demonstrate a significant reduction in the requirement for MAML training hyperparameters tuning, and improved training stability with less sensitivity to hyperparameter selection.",1
"As global greenhouse gas emissions continue to rise, the use of stratospheric aerosol injection (SAI), a form of solar geoengineering, is increasingly considered in order to artificially mitigate climate change effects. However, initial research in simulation suggests that naive SAI can have catastrophic regional consequences, which may induce serious geostrategic conflicts. Current geo-engineering research treats SAI control in low-dimensional approximation only. We suggest treating SAI as a high-dimensional control problem, with policies trained according to a context-sensitive reward function within the Deep Reinforcement Learning (DRL) paradigm. In order to facilitate training in simulation, we suggest to emulate HadCM3, a widely used General Circulation Model, using deep learning techniques. We believe this is the first application of DRL to the climate sciences.",0
"The use of stratospheric aerosol injection (SAI) as a solar geoengineering solution is being considered to mitigate the effects of climate change in light of increasing global greenhouse gas emissions. However, initial simulations indicate that this approach can have serious regional consequences, leading to potential geostrategic conflicts. Current geo-engineering research focuses on low-dimensional SAI control, but we propose a high-dimensional control problem that utilizes deep reinforcement learning (DRL) to train policies based on a context-sensitive reward function. To facilitate simulation training, we suggest using deep learning techniques to emulate HadCM3, a widely used General Circulation Model, representing the first application of DRL to the climate sciences.",1
"Off-policy reinforcement learning with eligibility traces is challenging because of the discrepancy between target policy and behavior policy. One common approach is to measure the difference between two policies in a probabilistic way, such as importance sampling and tree-backup. However, existing off-policy learning methods based on probabilistic policy measurement are inefficient when utilizing traces under a greedy target policy, which is ineffective for control problems. The traces are cut immediately when a non-greedy action is taken, which may lose the advantage of eligibility traces and slow down the learning process. Alternatively, some non-probabilistic measurement methods such as General Q($\lambda$) and Naive Q($\lambda$) never cut traces, but face convergence problems in practice. To address the above issues, this paper introduces a new method named TBQ($\sigma$), which effectively unifies the tree-backup algorithm and Naive Q($\lambda$). By introducing a new parameter $\sigma$ to illustrate the \emph{degree} of utilizing traces, TBQ($\sigma$) creates an effective integration of TB($\lambda$) and Naive Q($\lambda$) and continuous role shift between them. The contraction property of TB($\sigma$) is theoretically analyzed for both policy evaluation and control settings. We also derive the online version of TBQ($\sigma$) and give the convergence proof. We empirically show that, for $\epsilon\in(0,1]$ in $\epsilon$-greedy policies, there exists some degree of utilizing traces for $\lambda\in[0,1]$, which can improve the efficiency in trace utilization for off-policy reinforcement learning, to both accelerate the learning process and improve the performance.",0
"The use of eligibility traces in off-policy reinforcement learning poses a challenge due to the mismatch between target policy and behavior policy. Various methods, including importance sampling and tree-backup, have been employed to measure policy differences in a probabilistic manner. However, these methods prove to be inefficient and ineffective for control problems when utilizing traces under a greedy target policy. On the other hand, non-probabilistic measurement methods, such as General Q($\lambda$) and Naive Q($\lambda$), never cut traces, but face convergence problems in practice. This paper proposes a new method called TBQ($\sigma$) that effectively combines the tree-backup algorithm and Naive Q($\lambda$) by introducing a new parameter $\sigma$ to illustrate the degree of utilizing traces. TBQ($\sigma$) unifies TB($\lambda$) and Naive Q($\lambda$) and allows for a continuous role shift between them, while maintaining the contraction property for both policy evaluation and control settings. The online version of TBQ($\sigma$) is derived and its convergence proof is established. Empirical results show that utilizing traces to a certain degree can improve the efficiency in off-policy reinforcement learning and accelerate the learning process while improving performance, particularly for $\epsilon$-greedy policies with $\lambda\in[0,1]$.",1
"Shaping in humans and animals has been shown to be a powerful tool for learning complex tasks as compared to learning in a randomized fashion. This makes the problem less complex and enables one to solve the easier sub task at hand first. Generating a curriculum for such guided learning involves subjecting the agent to easier goals first, and then gradually increasing their difficulty. This paper takes a similar direction and proposes a dual curriculum scheme for solving robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro curriculum scheme which divides the task into multiple sub-tasks followed by a micro curriculum scheme which enables the agent to learn between such discovered sub-tasks. We show how combining macro and micro curriculum strategies help in overcoming major exploratory constraints considered in robot manipulation tasks without having to engineer any complex rewards. We also illustrate the meaning of the individual curricula and how they can be used independently based on the task. The performance of such a dual curriculum scheme is analyzed on the Fetch environments.",0
"Learning through shaping has been proven to be a more effective method for tackling complex tasks, as opposed to randomized learning. This approach simplifies the problem, allowing for easier completion of sub-tasks. To create a guided learning curriculum, the agent is first subjected to simpler goals that gradually increase in difficulty. This paper introduces the MaMiC scheme, which utilizes a dual curriculum approach to solve robotic manipulation tasks with sparse rewards. The macro curriculum divides the task into sub-tasks, while the micro curriculum enables learning between these sub-tasks. By combining these strategies, MaMiC overcomes exploratory constraints in robot manipulation tasks without requiring complex rewards. The curricula can also be used independently based on the task. The performance of the dual curriculum is evaluated on Fetch environments.",1
"Recent work in reinforcement learning demonstrated that learning solely through self-play is not only possible, but could also result in novel strategies that humans never would have thought of. However, optimization methods cast as a game between two players require careful tuning to prevent suboptimal results. Hence, we look at random play as an alternative method. In this paper, we train a DQN agent to play Sungka, a two-player turn-based board game wherein the players compete to obtain more stones than the other. We show that even with purely random play, our training algorithm converges very fast and is stable. Moreover, we test our trained agent against several baselines and show its ability to consistently win against these.",0
"Recent research on reinforcement learning has shown that self-play can lead to the development of innovative strategies that humans may not have considered. However, when optimization methods are framed as a two-player game, careful calibration is necessary to avoid suboptimal outcomes. Consequently, we explore random play as an alternative approach. Our study involves training a DQN agent to play Sungka, a two-player board game where the goal is to acquire more stones than the opponent. We demonstrate that our training algorithm converges quickly and remains stable even with purely random play. Furthermore, we assess our agent's performance against various baselines and prove that it consistently outperforms them.",1
"Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used---e.g., dilated convolutions---to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with state-of-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/drsleep/nas-segm-pytorch",0
"Exploring the avenue of automated design of neural network architectures tailored for specific tasks is highly promising, yet challenging. While previous studies have focused on image classification and language modelling problems, our research centers on dense per-pixel tasks, specifically semantic image segmentation through fully convolutional networks. Compared to previous areas, designing a fully convolutional network requires various changes, including the type of operations used, such as dilated convolutions, and solving more challenging optimization problems. Our main focus is to search for high-performance compact segmentation architectures that can run in real-time with limited resources. To achieve this, we intentionally over-parameterize the architecture during training with auxiliary cells that provide intermediate supervisory signals and can be disregarded during evaluation. The design of the auxiliary cell is generated by a controller, which is a neural network with a fixed structure trained using reinforcement learning. Our research demonstrates an efficient approach to searching for these architectures within limited time and computational budgets, using a progressive strategy to terminate non-promising architectures and Polyak averaging with knowledge distillation to hasten convergence. Our approach discovers a set of architectures that perform on-par with the state-of-the-art among compact models in semantic segmentation, pose estimation, and depth prediction tasks in just 8 GPU-days. We will make the code available in the following link: https://github.com/drsleep/nas-segm-pytorch.",1
"We consider a model-based approach to perform batch off-policy evaluation in reinforcement learning. Our method takes a mixture-of-experts approach to combine parametric and non-parametric models of the environment such that the final value estimate has the least expected error. We do so by first estimating the local accuracy of each model and then using a planner to select which model to use at every time step as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based approach outperforms the individual models alone as well as state-of-the-art importance sampling-based estimators.",0
"In reinforcement learning, we adopt a model-centered strategy to execute batch off-policy evaluation. Our technique merges parametric and non-parametric models of the surroundings using a mixture-of-experts approach to obtain the final value estimate with minimal anticipated error. Initially, we assess the accuracy of each model locally and then apply a planner to determine the model to be used at each time step to minimize the return error estimate throughout trajectories. Our mixture-based method surpasses the individual models and importance sampling-based estimators in various domains.",1
"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.",0
"The integration of deep neural network models and reinforcement learning algorithms can enable the learning of policies for robotic behaviors that can directly interpret raw sensory inputs, such as camera images. This approach efficiently combines estimation and control into a single model. However, applying reinforcement learning in real-world scenarios necessitates manual programming of a reward function to specify the task's goal. This practice requires designing a perception pipeline that end-to-end reinforcement learning aims to bypass or introducing additional sensors in the environment to determine task completion. In this study, we propose a method that eliminates the need for manual reward engineering by allowing a robot to learn from a small number of successful examples and actively requesting input from the user to label states that indicate task completion. Our approach only requires labels for a small fraction of the states observed during training, making it a practical and efficient way of learning skills without manually programmed rewards. We evaluated our approach on real-world robotic manipulation tasks using images captured by the robot's camera. Our experiments demonstrate that the proposed method can successfully arrange objects, place books, and drape cloth directly from images without any predetermined reward functions, and with only a few hours of interaction with the real world.",1
"We consider an agent who is involved in a Markov decision process and receives a vector of outcomes every round. Her objective is to maximize a global concave reward function on the average vectorial outcome. The problem models applications such as multi-objective optimization, maximum entropy exploration, and constrained optimization in Markovian environments. In our general setting where a stationary policy could have multiple recurrent classes, the agent faces a subtle yet consequential trade-off in alternating among different actions for balancing the vectorial outcomes. In particular, stationary policies are in general sub-optimal. We propose a no-regret algorithm based on online convex optimization (OCO) tools (Agrawal and Devanur 2014) and UCRL2 (Jaksch et al. 2010). Importantly, we introduce a novel gradient threshold procedure, which carefully controls the switches among actions to handle the subtle trade-off. By delaying the gradient updates, our procedure produces a non-stationary policy that diversifies the outcomes for optimizing the objective. The procedure is compatible with a variety of OCO tools.",0
"The article discusses an agent who operates within a Markov decision process and receives a series of outcome vectors during each round. The agent's primary aim is to maximize a global reward function that is concave, focusing on the average vectorial outcome. This type of problem is useful for modelling a range of applications, including multi-objective optimization, Markovian constrained optimization, and maximum entropy exploration. However, in situations where stationary policies have multiple recurrent classes, the agent must balance the vectorial outcomes by alternating between different actions. The study proposes a no-regret algorithm that utilizes online convex optimization and UCRL2 tools. Additionally, the authors introduce a new gradient threshold procedure that controls the switching of actions to achieve the optimal outcome. By delaying the gradient updates, the procedure produces a non-stationary policy that diversifies the outcomes for optimizing the objective. The approach is compatible with various OCO tools.",1
"We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective to introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.",0
"In our research, we analyze the geometric and topological features of the value function space in Markov decision processes with finite states and actions. Our primary achievement is identifying its shape as a general polytope, as previously described by Aigner et al. (2010). We showcase various aspects of the structural interplay between policies and value functions, such as the line theorem, which reveals that value functions of policies limited to all but one state depict a line segment. Ultimately, we utilize this innovative approach to introduce visual aids that improve comprehension of the mechanics of reinforcement learning algorithms.",1
"Model-free reinforcement learning based methods such as Proximal Policy Optimization, or Q-learning typically require thousands of interactions with the environment to approximate the optimum controller which may not always be feasible in robotics due to safety and time consumption. Model-based methods such as PILCO or BlackDrops, while data-efficient, provide solutions with limited robustness and complexity. To address this tradeoff, we introduce active uncertainty reduction-based virtual environments, which are formed through limited trials conducted in the original environment. We provide an efficient method for uncertainty management, which is used as a metric for self-improvement by identification of the points with maximum expected improvement through adaptive sampling. Capturing the uncertainty also allows for better mimicking of the reward responses of the original system. Our approach enables the use of complex policy structures and reward functions through a unique combination of model-based and model-free methods, while still retaining the data efficiency. We demonstrate the validity of our method on several classic reinforcement learning problems in OpenAI gym. We prove that our approach offers a better modeling capacity for complex system dynamics as compared to established methods.",0
"Reinforcement learning methods, such as Proximal Policy Optimization or Q-learning, that rely on model-free approaches usually require a significant number of interactions with the environment to approximate the optimal controller. However, this may not always be feasible in robotics due to safety and time constraints. On the other hand, model-based methods like PILCO or BlackDrops are data-efficient but may provide limited robustness and complexity. To balance this tradeoff, we propose using active uncertainty reduction-based virtual environments, which are created through a few trials in the original environment. Our approach efficiently manages uncertainty by identifying the points with the highest expected improvement through adaptive sampling, which also allows for better mimicry of reward responses. By combining model-based and model-free methods, our approach enables the use of complex policy structures and reward functions while retaining data efficiency. We validate our method on several reinforcement learning problems in OpenAI gym and demonstrate its superior modeling capacity for complex system dynamics compared to established methods.",1
"A long-standing challenge in Reinforcement Learning is enabling agents to learn a model of their environment which can be transferred to solve other problems in a world with the same underlying rules. One reason this is difficult is the challenge of learning accurate models of an environment. If such a model is inaccurate, the agent's plans and actions will likely be sub-optimal, and likely lead to the wrong outcomes. Recent progress in model-based reinforcement learning has improved the ability for agents to learn and use predictive models. In this paper, we extend a recent deep learning architecture which learns a predictive model of the environment that aims to predict only the value of a few key measurements, which are be indicative of an agent's performance. Predicting only a few measurements rather than the entire future state of an environment makes it more feasible to learn a valuable predictive model. We extend this predictive model with a small, evolving neural network that suggests the best goals to pursue in the current state. We demonstrate that this allows the predictive model to transfer to new scenarios where goals are different, and that the adaptive goals can even adjust agent behavior on-line, changing its strategy to fit the current context.",0
"Developing a model of an environment that can be applied to solve other problems with similar rules has been a persistent challenge in Reinforcement Learning. The difficulty lies in the ability to learn an accurate model, as an erroneous model can lead to sub-optimal plans and actions by the agent, resulting in unfavorable outcomes. Recent advancements in model-based reinforcement learning have improved the agent's ability to learn and use predictive models. This paper extends a deep learning architecture that aims to predict only a few key measurements indicative of an agent's performance, making it more feasible to learn a valuable predictive model. The extension involves the integration of a small, evolving neural network that suggests the best goals to pursue in the current state. This approach enables the predictive model to transfer to new scenarios with different goals and adapt agent behavior on-line to fit the current context.",1
"Recently, the state-of-the-art models for image captioning have overtaken human performance based on the most popular metrics, such as BLEU, METEOR, ROUGE, and CIDEr. Does this mean we have solved the task of image captioning? The above metrics only measure the similarity of the generated caption to the human annotations, which reflects its accuracy. However, an image contains many concepts and multiple levels of detail, and thus there is a variety of captions that express different concepts and details that might be interesting for different humans. Therefore only evaluating accuracy is not sufficient for measuring the performance of captioning models --- the diversity of the generated captions should also be considered. In this paper, we proposed a new metric for measuring the diversity of image captions, which is derived from latent semantic analysis and kernelized to use CIDEr similarity. We conduct extensive experiments to re-evaluate recent captioning models in the context of both diversity and accuracy. We find that there is still a large gap between the model and human performance in terms of both accuracy and diversity and the models that have optimized accuracy (CIDEr) have low diversity. We also show that balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy of the generated captions.",0
"The latest image captioning models have surpassed human performance on common metrics like BLEU, METEOR, ROUGE, and CIDEr. However, this does not mean that the task of image captioning has been fully solved. These metrics only evaluate the accuracy of the generated captions, but images have multiple concepts and levels of detail that can be expressed in various ways. Therefore, measuring only accuracy is insufficient, and diversity of the generated captions should also be taken into account. To address this, we introduce a new metric that uses latent semantic analysis and CIDEr similarity to measure diversity. Our experiments show that there is still a significant gap between model and human performance in terms of both accuracy and diversity. Models that prioritize accuracy tend to have lower diversity, but balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy.",1
"In an episodic Markov Decision Process (MDP) problem, an online algorithm chooses from a set of actions in a sequence of $H$ trials, where $H$ is the episode length, in order to maximize the total payoff of the chosen actions. Q-learning, as the most popular model-free reinforcement learning (RL) algorithm, directly parameterizes and updates value functions without explicitly modeling the environment. Recently, [Jin et al. 2018] studies the sample complexity of Q-learning with finite states and actions. Their algorithm achieves nearly optimal regret, which shows that Q-learning can be made sample efficient. However, MDPs with large discrete states and actions [Silver et al. 2016] or continuous spaces [Mnih et al. 2013] cannot learn efficiently in this way. Hence, it is critical to develop new algorithms to solve this dilemma with provable guarantee on the sample complexity. With this motivation, we propose a novel algorithm that works for MDPs with a more general setting, which has infinitely many states and actions and assumes that the payoff function and transition kernel are Lipschitz continuous. We also provide corresponding theory justification for our algorithm. It achieves the regret $\tilde{\mathcal{O}}(K^{\frac{d+1}{d+2}}\sqrt{H^3}),$ where $K$ denotes the number of episodes and $d$ denotes the dimension of the joint space. To the best of our knowledge, this is the first analysis in the model-free setting whose established regret matches the lower bound up to a logarithmic factor.",0
"An online algorithm is utilized in an episodic Markov Decision Process (MDP) problem to select actions in a sequence of $H$ trials, with the objective of maximizing the total payoff of the chosen actions. Q-learning, which is the most widely used model-free reinforcement learning (RL) algorithm, updates value functions without explicitly modeling the environment. Recent research by Jin et al. (2018) examines the sample complexity of Q-learning with finite states and actions, which shows that Q-learning can be made sample efficient. However, MDPs with large discrete states and actions or continuous spaces cannot be learned efficiently using this method. Therefore, it is important to develop new algorithms with provable guarantees on sample complexity. Our proposed algorithm works for MDPs with infinitely many states and actions, assuming that the payoff function and transition kernel are Lipschitz continuous. We provide theoretical justification for our algorithm, which achieves the regret $\tilde{\mathcal{O}}(K^{\frac{d+1}{d+2}}\sqrt{H^3})$, where $K$ denotes the number of episodes and $d$ denotes the dimension of the joint space. This is the first analysis in the model-free setting whose established regret matches the lower bound up to a logarithmic factor.",1
"In most machine learning training paradigms a fixed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sample efficient reinforcement learning approach for adapting the loss dynamically during training. We empirically show how this formulation improves performance by simultaneously optimizing the evaluation metric and smoothing the loss landscape. We verify our method in metric learning and classification scenarios, showing considerable improvements over the state-of-the-art on a diverse set of tasks. Importantly, our method is applicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned policies are transferable across tasks and data, demonstrating the versatility of the method.",0
"The common practice in machine learning training is to use a fixed loss function as a substitute for the evaluation metric. However, we aim to challenge this assumption by meta-learning an adaptable loss function that directly optimizes the evaluation metric. Our proposed approach involves using reinforcement learning to dynamically adjust the loss during training, leading to better performance and smoother loss landscape. We have tested our method in metric learning and classification scenarios and have observed significant improvements over the current state-of-the-art in various tasks. It is worth noting that our method is not limited to specific loss functions or evaluation metrics and can be applied to a wide range of scenarios. Additionally, the policies learned can be transferred across different tasks and data, indicating the versatility of our approach.",1
"Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.",0
"Many reinforcement learning methods without a model use state representations for generalization, but they do not consider the structure of the action space or assume that structure is already known. Our study shows how a policy can be broken down into two parts: one that operates in a low-dimensional space of action representations and another that converts these representations into actual actions. These representations enhance generalization over large, finite action sets by allowing the agent to deduce the outcomes of actions that are similar to those already taken. We present an algorithm that can both learn and use action representations, along with convergence criteria. The effectiveness of this approach is demonstrated in real-world problems of significant scale.",1
"The options framework in reinforcement learning models the notion of a skill or a temporally extended sequence of actions. The discovery of a reusable set of skills has typically entailed building options, that navigate to bottleneck states. This work adopts a complementary approach, where we attempt to discover options that navigate to landmark states. These states are prototypical representatives of well-connected regions and can hence access the associated region with relative ease. In this work, we propose Successor Options, which leverages Successor Representations to build a model of the state space. The intra-option policies are learnt using a novel pseudo-reward and the model scales to high-dimensional spaces easily. Additionally, we also propose an Incremental Successor Options model that iterates between constructing Successor Representations and building options, which is useful when robust Successor Representations cannot be built solely from primitive actions. We demonstrate the efficacy of our approach on a collection of grid-worlds, and on the high-dimensional robotic control environment of Fetch.",0
"Reinforcement learning's options framework represents skills or sequences of actions over time. Traditionally, discovering reusable skills meant creating options that navigate to bottleneck states. However, this approach is supplemented by the discovery of options that navigate to landmark states, which represent well-connected regions and can access associated regions with ease. This work introduces Successor Options, which uses Successor Representations to model the state space. Intra-option policies are learned via a novel pseudo-reward, and the model is scalable to high-dimensional spaces. Additionally, the Incremental Successor Options model alternates between constructing Successor Representations and building options when robust representations cannot be built from primitive actions alone. The approach's effectiveness is demonstrated in a grid-world collection and the high-dimensional robotic control environment of Fetch.",1
"Policy gradient methods are powerful reinforcement learning algorithms and have been demonstrated to solve many complex tasks. However, these methods are also data-inefficient, afflicted with high variance gradient estimates, and frequently get stuck in local optima. This work addresses these weaknesses by combining recent improvements in the reuse of off-policy data and exploration in parameter space with deterministic behavioral policies. The resulting objective is amenable to standard neural network optimization strategies like stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. Incorporation of previous rollouts via importance sampling greatly improves data-efficiency, whilst stochastic optimization schemes facilitate the escape from local optima. We evaluate the proposed approach on a series of continuous control benchmark tasks. The results show that the proposed algorithm is able to successfully and reliably learn solutions using fewer system interactions than standard policy gradient methods.",0
"Although policy gradient methods are effective reinforcement learning algorithms, they suffer from inefficiencies in data utilization, high variance in gradient estimates, and the tendency to become trapped in local optima. In response to these limitations, we present a novel approach that combines recent advancements in off-policy data reuse and parameter space exploration with deterministic behavioral policies. This approach enables us to optimize the objective using conventional neural network optimization strategies, such as stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. The incorporation of previous rollouts through importance sampling enhances data efficiency, while stochastic optimization techniques facilitate the avoidance of local optima. Our proposed method is evaluated through a series of continuous control benchmark tasks, which demonstrate its ability to learn solutions with fewer system interactions than traditional policy gradient methods, while maintaining reliability and success.",1
"In this paper, we propose TauRieL and target Traveling Salesman Problem (TSP) since it has broad applicability in theoretical and applied sciences. TauRieL utilizes an actor-critic inspired architecture that adopts ordinary feedforward nets to obtain a policy update vector $v$. Then, we use $v$ to improve the state transition matrix from which we generate the policy. Also, the state transition matrix allows the solver to initialize from precomputed solutions such as nearest neighbors. In an online learning setting, TauRieL unifies the training and the search where it can generate near-optimal results in seconds. The input to the neural nets in the actor-critic architecture are raw 2-D inputs, and the design idea behind this decision is to keep neural nets relatively smaller than the architectures with wide embeddings with the tradeoff of omitting any distributed representations of the embeddings. Consequently, TauRieL generates TSP solutions two orders of magnitude faster per TSP instance as compared to state-of-the-art offline techniques with a performance impact of 6.1\% in the worst case.",0
"TauRieL is proposed in this paper to target the Traveling Salesman Problem (TSP) due to its wide applicability in theoretical and applied sciences. The proposed architecture of TauRieL is inspired by actor-critic and uses ordinary feedforward nets to obtain a policy update vector, which is then utilized to improve the state transition matrix. This matrix enables the solver to initialize from precomputed solutions, such as nearest neighbors. In an online learning setting, TauRieL unifies the training and search processes and can generate near-optimal results in seconds. The neural nets in the actor-critic architecture take raw 2-D inputs, which keeps them relatively smaller than architectures with wide embeddings, at the expense of omitting any distributed representations of the embeddings. Consequently, TauRieL generates TSP solutions two orders of magnitude faster per TSP instance than state-of-the-art offline techniques, with a worst-case performance impact of 6.1%.",1
"We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.",0
"In this article, we investigate solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime. While VDN and QMIX are commonly used examples that factorize the joint action-value function into individual ones for decentralized execution, they have structural constraints such as additivity and monotonicity, limiting their ability to address all factorizable MARL tasks. To overcome this limitation, we propose a new method called QTRAN, which transforms the original joint action-value function into an easily factorizable one without any structural constraints. QTRAN guarantees more general factorization than previous methods, covering a wider range of MARL tasks. Our experiments demonstrate the superior performance of QTRAN in multi-domain Gaussian-squeeze and modified predator-prey tasks, especially in games where non-cooperative behavior is heavily penalized.",1
"Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. Focusing on problems arising in continuous control, we propose a functional regularization approach to augmenting model-free RL. In particular, we regularize the behavior of the deep policy to be similar to a policy prior, i.e., we regularize in function space. We show that functional regularization yields a bias-variance trade-off, and propose an adaptive tuning strategy to optimize this trade-off. When the policy prior has control-theoretic stability guarantees, we further show that this regularization approximately preserves those stability guarantees throughout learning. We validate our approach empirically on a range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone.",0
"A major obstacle in model-free reinforcement learning (RL) is managing high variance. Current techniques display inconsistency, leading to varying performance with different seed initializations. Our focus is on the issues that arise in continuous control, and we suggest an approach to augmenting model-free RL with functional regularization. Specifically, we regulate the deep policy's behavior to be similar to a policy prior in function space. We observe that functional regularization provides a bias-variance trade-off and present an adaptive tuning method to optimize it. When the policy prior has control-theoretic stability guarantees, we demonstrate that this regularization approximately maintains those stability guarantees during learning. We validate our approach empirically on various settings and show significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone.",1
"Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.",0
"Simulation serves as a valuable tool in situations where the annotation of training data for machine learning models is costly or difficult to obtain. Our proposed method employs reinforcement learning to automatically adjust the parameters of any non-differentiable simulator, thereby regulating the distribution of synthesized data to enhance the accuracy of a model trained on said data. Unlike previous methods that manually manipulate simulation parameters or only alter select parameters, our approach fully controls the simulator with the ultimate goal of maximizing accuracy, rather than simply mimicking real data distribution or generating copious amounts of data at random. Our findings indicate that our approach efficiently converges to optimal simulation parameters in controlled experiments and effectively identifies good parameter sets for an image rendering simulator in computer vision applications.",1
"In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",0
"A new exploration method for deep RL is proposed, which incorporates the estimated distribution of value function used in distributional RL to model both parametric and intrinsic uncertainties. The method is comprised of two components: a decaying schedule to suppress intrinsic uncertainty, and an exploration bonus based on the upper quantiles of the learned distribution. In the Atari 2600 games, the proposed method outperforms QR-DQN in 12 out of 14 hard games, resulting in an average gain of 483% across 49 games in cumulative rewards over QR-DQN, with a significant win in Venture. In a challenging 3D driving simulator (CARLA), the proposed algorithm achieves near-optimal safety rewards twice as fast as QR-DQN, as demonstrated by the results of a comparison between the two algorithms.",1
"Many real-world decision problems are characterized by multiple conflicting objectives which must be balanced based on their relative importance. In the dynamic weights setting the relative importance changes over time and specialized algorithms that deal with such change, such as a tabular Reinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are required. However, this earlier work is not feasible for RL settings that necessitate the use of function approximators. We generalize across weight changes and high-dimensional inputs by proposing a multi-objective Q-network whose outputs are conditioned on the relative importance of objectives and we introduce Diverse Experience Replay (DER) to counter the inherent non-stationarity of the Dynamic Weights setting. We perform an extensive experimental evaluation and compare our methods to adapted algorithms from Deep Multi-Task/Multi-Objective Reinforcement Learning and show that our proposed network in combination with DER dominates these adapted algorithms across weight change scenarios and problem domains.",0
"Decision problems in the real-world often involve multiple objectives that conflict with one another. Balancing these objectives based on their relative importance is crucial. When the relative importance changes over time, specialized algorithms are required to manage this change, such as the tabular Reinforcement Learning (RL) algorithm proposed by Natarajan and Tadepalli in 2005. However, this approach is not suitable for RL settings that require the use of function approximators. To address this issue, we propose a multi-objective Q-network that can generalize across weight changes and high-dimensional inputs. This network's outputs are conditioned on the relative importance of objectives, and we introduce Diverse Experience Replay (DER) to handle the non-stationarity of the Dynamic Weights setting. Our extensive experimental evaluation shows that our proposed approach outperforms adapted algorithms from Deep Multi-Task/Multi-Objective Reinforcement Learning across various weight change scenarios and problem domains.",1
"Physical construction---the ability to compose objects, subject to physical dynamics, to serve some function---is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.",0
"Human intelligence relies on the ability to create objects based on physical dynamics in order to fulfill a purpose. In this study, we present a set of intricate physical construction tasks that are inspired by how children play with blocks. These tasks include matching configurations, connecting objects through stacking blocks, and creating shelter-like structures over target objects. We evaluate the performance of deep reinforcement learning agents on these challenges and introduce new approaches that demonstrate superior performance. Our findings indicate that agents utilizing structured representations and policies that revolve around objects outperform those that do not. Additionally, our research reveals that model-based agents that use Monte-Carlo Tree Search outperform strictly model-free agents in the most challenging construction problems. Our results suggest that agents that combine structured representations and reasoning with potent learning are a promising route towards developing agents that have a profound understanding of physics, scene interpretation, and planning.",1
"The Exploration-Exploitation tradeoff arises in Reinforcement Learning when one cannot tell if a policy is optimal. Then, there is a constant need to explore new actions instead of exploiting past experience. In practice, it is common to resolve the tradeoff by using a fixed exploration mechanism, such as $\epsilon$-greedy exploration or by adding Gaussian noise, while still trying to learn an optimal policy. In this work, we take a different approach and study exploration-conscious criteria, that result in optimal policies with respect to the exploration mechanism. Solving these criteria, as we establish, amounts to solving a surrogate Markov Decision Process. We continue and analyze properties of exploration-conscious optimal policies and characterize two general approaches to solve such criteria. Building on the approaches, we apply simple changes in existing tabular and deep Reinforcement Learning algorithms and empirically demonstrate superior performance relatively to their non-exploration-conscious counterparts, both for discrete and continuous action spaces.",0
"When it's unclear whether a policy is optimal, the Exploration-Exploitation tradeoff becomes relevant in Reinforcement Learning. This means that instead of relying solely on past experience, it's necessary to frequently explore new actions. In practice, a fixed exploration mechanism like $\epsilon$-greedy exploration or added Gaussian noise is often used to address the tradeoff while still striving for an optimal policy. However, our approach is different. We focus on exploration-conscious criteria that lead to optimal policies considering the exploration mechanism. By solving these criteria, we establish a surrogate Markov Decision Process. We analyze the characteristics of exploration-conscious optimal policies and identify two general methods to solve the criteria. Using these methods, we modify existing tabular and deep Reinforcement Learning algorithms and demonstrate superior performance compared to non-exploration-conscious counterparts for both discrete and continuous action spaces.",1
"Hierarchical Reinforcement Learning (HRL) exploits temporally extended actions, or options, to make decisions from a higher-dimensional perspective to alleviate the sparse reward problem, one of the most challenging problems in reinforcement learning. The majority of existing HRL algorithms require either significant manual design with respect to the specific environment or enormous exploration to automatically learn options from data. To achieve fast exploration without using manual design, we devise a multi-goal HRL algorithm, consisting of a high-level policy Manager and a low-level policy Worker. The Manager provides the Worker multiple subgoals at each time step. Each subgoal corresponds to an option to control the environment. Although the agent may show some confusion at the beginning of training since it is guided by three diverse subgoals, the agent's behavior policy will quickly learn how to respond to multiple subgoals from the high-level controller on different occasions. By exploiting multiple subgoals, the exploration efficiency is significantly improved. We conduct experiments in Atari's Montezuma's Revenge environment, a well-known sparse reward environment, and in doing so achieve the same performance as state-of-the-art HRL methods with substantially reduced training time cost.",0
"To solve the challenging issue of sparse reward in reinforcement learning, Hierarchical Reinforcement Learning (HRL) uses options to make decisions from a higher-level perspective. However, existing HRL algorithms require either manual design or extensive exploration to learn options from data. To overcome this, we have developed a multi-goal HRL algorithm with a high-level policy Manager and a low-level policy Worker. The Manager provides the Worker with multiple subgoals, each corresponding to an option to control the environment. Although the agent may initially struggle with the diverse subgoals, it quickly learns to respond to them and improve exploration efficiency. We conducted experiments in the sparse reward environment of Atari's Montezuma's Revenge and achieved state-of-the-art performance with significantly reduced training time.",1
"Increasingly available city data and advanced learning techniques have empowered people to improve the efficiency of our city functions. Among them, improving the urban transportation efficiency is one of the most prominent topics. Recent studies have proposed to use reinforcement learning (RL) for traffic signal control. Different from traditional transportation approaches which rely heavily on prior knowledge, RL can learn directly from the feedback. On the other side, without a careful model design, existing RL methods typically take a long time to converge and the learned models may not be able to adapt to new scenarios. For example, a model that is trained well for morning traffic may not work for the afternoon traffic because the traffic flow could be reversed, resulting in a very different state representation. In this paper, we propose a novel design called FRAP, which is based on the intuitive principle of phase competition in traffic signal control: when two traffic signals conflict, priority should be given to one with larger traffic movement (i.e., higher demand). Through the phase competition modeling, our model achieves invariance to symmetrical cases such as flipping and rotation in traffic flow. By conducting comprehensive experiments, we demonstrate that our model finds better solutions than existing RL methods in the complicated all-phase selection problem, converges much faster during training, and achieves superior generalizability for different road structures and traffic conditions.",0
"City data and advanced learning techniques have given people the ability to enhance the efficiency of urban functions, with a focus on improving transportation. Reinforcement learning (RL) has been recommended as a means of traffic signal control, as it can learn from feedback rather than relying solely on prior knowledge. However, existing RL methods can be slow to converge and may not adapt well to new situations. The paper proposes a new approach called FRAP, which utilizes phase competition in traffic signal control to prioritize larger traffic movements. This design achieves invariance to symmetrical cases and outperforms existing RL methods by finding better solutions, training faster, and achieving superior generalizability for different road structures and traffic conditions.",1
"With the increasing availability of traffic data and advance of deep reinforcement learning techniques, there is an emerging trend of employing reinforcement learning (RL) for traffic signal control. A key question for applying RL to traffic signal control is how to define the reward and state. The ultimate objective in traffic signal control is to minimize the travel time, which is difficult to reach directly. Hence, existing studies often define reward as an ad-hoc weighted linear combination of several traffic measures. However, there is no guarantee that the travel time will be optimized with the reward. In addition, recent RL approaches use more complicated state (e.g., image) in order to describe the full traffic situation. However, none of the existing studies has discussed whether such a complex state representation is necessary. This extra complexity may lead to significantly slower learning process but may not necessarily bring significant performance gain.   In this paper, we propose to re-examine the RL approaches through the lens of classic transportation theory. We ask the following questions: (1) How should we design the reward so that one can guarantee to minimize the travel time? (2) How to design a state representation which is concise yet sufficient to obtain the optimal solution? Our proposed method LIT is theoretically supported by the classic traffic signal control methods in transportation field. LIT has a very simple state and reward design, thus can serve as a building block for future RL approaches to traffic signal control. Extensive experiments on both synthetic and real datasets show that our method significantly outperforms the state-of-the-art traffic signal control methods.",0
"Reinforcement learning (RL) is increasingly being used for traffic signal control due to the availability of traffic data and advances in deep reinforcement learning techniques. However, defining the reward and state for RL in traffic signal control is a key challenge. The goal is to minimize travel time, but existing studies use ad-hoc weighted linear combinations of traffic measures as rewards, which may not optimize travel time. Moreover, recent RL approaches use complex states, such as images, without exploring whether such complexity is necessary. In this paper, we propose a method called LIT that re-examines RL approaches through classic transportation theory. We design a concise yet sufficient state representation and a reward that guarantees the minimization of travel time. LIT outperforms existing methods and can serve as a foundation for future RL approaches in traffic signal control.",1
"Assemblies of modular subsystems are being pressed into service to perform sensing, reasoning, and decision making in high-stakes, time-critical tasks in such areas as transportation, healthcare, and industrial automation. We address the opportunity to maximize the utility of an overall computing system by employing reinforcement learning to guide the configuration of the set of interacting modules that comprise the system. The challenge of doing system-wide optimization is a combinatorial problem. Local attempts to boost the performance of a specific module by modifying its configuration often leads to losses in overall utility of the system's performance as the distribution of inputs to downstream modules changes drastically. We present metareasoning techniques which consider a rich representation of the input, monitor the state of the entire pipeline, and adjust the configuration of modules on-the-fly so as to maximize the utility of a system's operation. We show significant improvement in both real-world and synthetic pipelines across a variety of reinforcement learning techniques.",0
"Modular subsystems are being utilized for sensing, reasoning, and decision-making in crucial and time-sensitive tasks, such as transportation, healthcare, and industrial automation. Our focus is on optimizing the overall computing system by employing reinforcement learning to guide the configuration of the interacting modules that make up the system. The difficulty lies in the combinatorial problem of system-wide optimization. Attempts to improve the performance of individual modules by modifying their configuration can result in decreased overall utility as the input distribution to downstream modules changes significantly. To overcome this challenge, we present metareasoning techniques that consider a comprehensive representation of input, monitor the entire pipeline's state, and adjust the module configuration dynamically to maximize system operation's utility. Our approach results in significant improvements in both real-world and synthetic pipelines across multiple reinforcement learning techniques.",1
"This paper studies accelerations in Q-learning algorithms. We propose an accelerated target update scheme by incorporating the historical iterates of Q functions. The idea is conceptually inspired by the momentum-based accelerated methods in the optimization theory. Conditions under which the proposed accelerated algorithms converge are established. The algorithms are validated using commonly adopted testing problems in reinforcement learning, including the FrozenLake grid world game, two discrete-time LQR problems from the Deepmind Control Suite, and the Atari 2600 games. Simulation results show that the proposed accelerated algorithms can improve the convergence performance compared with the vanilla Q-learning algorithm.",0
"The focus of this paper is on exploring the acceleration of Q-learning algorithms. Our proposal involves incorporating the past iterations of Q functions to create an accelerated target update scheme, inspired by the momentum-based accelerated methods in optimization theory. We have established the conditions necessary for the convergence of the accelerated algorithms. To test our algorithms, we have used popular problems in reinforcement learning, such as the FrozenLake grid world game, two discrete-time LQR problems from the Deepmind Control Suite, and the Atari 2600 games. Our simulation results demonstrate that the proposed accelerated algorithms are more effective than the vanilla Q-learning algorithm in terms of convergence performance.",1
"Parameterised actions in reinforcement learning are composed of discrete actions with continuous action-parameters. This provides a framework for solving complex domains that require combining high-level actions with flexible control. The recent P-DQN algorithm extends deep Q-networks to learn over such action spaces. However, it treats all action-parameters as a single joint input to the Q-network, invalidating its theoretical foundations. We analyse the issues with this approach and propose a novel method, multi-pass deep Q-networks, or MP-DQN, to address them. We empirically demonstrate that MP-DQN significantly outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.",0
"Reinforcement learning utilises parameterised actions that consist of discrete actions with continuous action-parameters, enabling the resolution of intricate domains that necessitate the fusion of high-level actions with adaptable control. The P-DQN algorithm, which extends deep Q-networks to learn over such action spaces, has been introduced recently. However, it considers all action-parameters as a single joint input to the Q-network, hence undermining its theoretical foundations. We scrutinise the downsides of this approach and offer a new method, called multi-pass deep Q-networks or MP-DQN, to address them. We demonstrate through empirical evidence that MP-DQN outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.",1
"Mapping states to actions in deep reinforcement learning is mainly based on visual information. The commonly used approach for dealing with visual information is to extract pixels from images and use them as state representation for reinforcement learning agent. But, any vision only agent is handicapped by not being able to sense audible cues. Using hearing, animals are able to sense targets that are outside of their visual range. In this work, we propose the use of audio as complementary information to visual only in state representation. We assess the impact of such multi-modal setup in reach-the-goal tasks in ViZDoom environment. Results show that the agent improves its behavior when visual information is accompanied with audio features.",0
"In deep reinforcement learning, states are linked to actions primarily through visual input. The conventional method of processing visual data involves extracting pixels from images and using them as a state representation for the reinforcement learning agent. However, this approach leaves vision-only agents at a disadvantage, as they are unable to perceive auditory cues. Animals, on the other hand, use their sense of hearing to detect targets that are beyond their line of sight. In this study, we suggest augmenting the state representation of the agent with audio information to complement the visual data. We evaluate the impact of this multi-modal approach on the agent's performance in reaching its goals in the ViZDoom environment. Our findings reveal that the agent's behavior is enhanced when it is provided with audio features alongside visual input.",1
"Reinforcement learning (RL) is capable of managing wireless, energy-harvesting IoT nodes by solving the problem of autonomous management in non-stationary, resource-constrained settings. We show that the state-of-the-art policy-gradient approaches to RL are appropriate for the IoT domain and that they outperform previous approaches. Due to the ability to model continuous observation and action spaces, as well as improved function approximation capability, the new approaches are able to solve harder problems, permitting reward functions that are better aligned with the actual application goals. We show such a reward function and use policy-gradient approaches to learn capable policies, leading to behavior more appropriate for IoT nodes with less manual design effort, increasing the level of autonomy in IoT.",0
"By solving the challenge of autonomous management in non-stationary, resource-constrained settings, reinforcement learning (RL) proves to be effective in handling wireless, energy-harvesting IoT nodes. Our research demonstrates that state-of-the-art policy-gradient approaches in RL are suitable for the IoT domain and yield better results than previous methods. With the ability to model continuous observation and action spaces, as well as enhanced function approximation capability, these new approaches can tackle tougher problems and achieve reward functions that better align with the application goals. We present an example of such a reward function and utilize policy-gradient approaches to acquire proficient policies, resulting in more suitable behavior for IoT nodes with less manual design effort, ultimately increasing the level of autonomy in IoT.",1
"In order perform a large variety of tasks and to achieve human-level performance in complex real-world environments, Artificial Intelligence (AI) Agents must be able to learn from their past experiences and gain both knowledge and an accurate representation of their environment from raw sensory inputs. Traditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy. Deep reinforcement learning algorithms have provided a solution to this issue. In this study, the performance of different conventional and novel deep reinforcement learning algorithms was analysed. The proposed method utilises two types of algorithms, one trained with a variant of Q-learning (DQN) and another trained with SARSA learning (DSN) to assess the feasibility of using direct feedback alignment, a novel biologically plausible method for back-propagating the error. These novel agents, alongside two similar agents trained with the conventional backpropagation algorithm, were tested by using the OpenAI Gym toolkit on several classic control theory problems and Atari 2600 video games. The results of this investigation open the way into new, biologically-inspired deep reinforcement learning algorithms, and their implementation on neuromorphic hardware.",0
"For Artificial Intelligence (AI) Agents to achieve human-like performance in complex real-world environments, they must learn from past experiences and gain accurate knowledge of their environment from raw sensory inputs. However, traditional AI agents have struggled to use sensory inputs to map an efficient control policy. Fortunately, deep reinforcement learning algorithms have provided a solution to this problem. This study analyzed the performance of various conventional and novel deep reinforcement learning algorithms that utilize Q-learning and SARSA learning to assess the feasibility of using direct feedback alignment. The agents were tested on classic control theory problems and Atari 2600 games using the OpenAI Gym toolkit. These results pave the way for new, biologically-inspired deep reinforcement learning algorithms and their implementation on neuromorphic hardware.",1
"Partial domain adaptation aims to transfer knowledge from a label-rich source domain to a label-scarce target domain which relaxes the fully shared label space assumption across different domains. In this more general and practical scenario, a major challenge is how to select source instances in the shared classes across different domains for positive transfer. To address this issue, we propose a Domain Adversarial Reinforcement Learning (DARL) framework to automatically select source instances in the shared classes for circumventing negative transfer as well as to simultaneously learn transferable features between domains by reducing the domain shift. Specifically, in this framework, we employ deep Q-learning to learn policies for an agent to make selection decisions by approximating the action-value function. Moreover, domain adversarial learning is introduced to learn domain-invariant features for the selected source instances by the agent and the target instances, and also to determine rewards for the agent based on how relevant the selected source instances are to the target domain. Experiments on several benchmark datasets demonstrate that the superior performance of our DARL method over existing state of the arts for partial domain adaptation.",0
"The objective of partial domain adaptation is to transfer knowledge from a source domain with abundant labels to a target domain with scarce labels, which relaxes the assumption of fully shared label space across different domains. In this situation, a critical issue is how to choose source instances in the shared classes across different domains to achieve positive transfer. To overcome this challenge, we propose a Domain Adversarial Reinforcement Learning (DARL) framework that selects source instances in the shared classes to avoid negative transfer and simultaneously learns transferable features between domains by reducing the domain shift. The DARL framework utilizes deep Q-learning to train an agent to make selection decisions by approximating the action-value function. Additionally, domain adversarial learning is used to train the agent to select domain-invariant features for the source instances and target instances and to determine rewards based on the relevance of the selected source instances to the target domain. Our experiments on multiple benchmark datasets demonstrate that the DARL method outperforms existing state-of-the-art methods for partial domain adaptation.",1
"In reinforcement learning algorithms, it is a common practice to account for only a single view of the environment to make the desired decisions; however, utilizing multiple views of the environment can help to promote the learning of complicated policies. Since the views may frequently suffer from partial observability, their provided observation can have different levels of importance. In this paper, we present a novel attention-based deep reinforcement learning method in a multi-view environment in which each view can provide various representative information about the environment. Specifically, our method learns a policy to dynamically attend to views of the environment based on their importance in the decision-making process. We evaluate the performance of our method on TORCS racing car simulator and three other complex 3D environments with obstacles.",0
"It is typical for reinforcement learning algorithms to consider just one perspective of the environment when making decisions. However, incorporating multiple perspectives can facilitate the acquisition of complex policies. Since these perspectives may be subject to partial observability, their observations may vary in significance. This study introduces a new deep reinforcement learning approach that employs attention mechanisms in a multi-view setting, where each perspective provides distinctive environmental information. The method adapts by learning to allocate attention to each perspective dynamically, according to its relevance to the decision-making process. We assess our approach's effectiveness in the TORCS racing car simulator and three other intricate 3D obstacle environments.",1
"The game of Tetris is an important benchmark for research in artificial intelligence and machine learning. This paper provides a historical account of the algorithmic developments in Tetris and discusses open challenges. Handcrafted controllers, genetic algorithms, and reinforcement learning have all contributed to good solutions. However, existing solutions fall far short of what can be achieved by expert players playing without time pressure. Further study of the game has the potential to contribute to important areas of research, including feature discovery, autonomous learning of action hierarchies, and sample-efficient reinforcement learning.",0
"This paper offers a historical overview of the algorithmic advancements made in Tetris and explores unresolved obstacles, emphasizing its significance as a benchmark for artificial intelligence and machine learning research. Although handcrafted controllers, genetic algorithms, and reinforcement learning have produced favorable outcomes, they pale in comparison to the proficiency of skilled players operating without time restraints. Further exploration of Tetris could spur important research in feature discovery, independent acquisition of action hierarchies, and efficient reinforcement learning techniques.",1
"Pretraining reinforcement learning methods with demonstrations has been an important concept in the study of reinforcement learning since a large amount of computing power is spent on online simulations with existing reinforcement learning algorithms. Pretraining reinforcement learning remains a significant challenge in exploiting expert demonstrations whilst keeping exploration potentials, especially for value based methods. In this paper, we propose a pretraining method for soft Q-learning. Our work is inspired by pretraining methods for actor-critic algorithms since soft Q-learning is a value based algorithm that is equivalent to policy gradient. The proposed method is based on $\gamma$-discounted biased policy evaluation with entropy regularization, which is also the updating target of soft Q-learning. Our method is evaluated on various tasks from Atari 2600. Experiments show that our method effectively learns from imperfect demonstrations, and outperforms other state-of-the-art methods that learn from expert demonstrations.",0
"Since a considerable amount of computing power is utilized in online simulations using existing reinforcement learning algorithms, pretraining reinforcement learning techniques with demonstrations has become a crucial concept in the field of reinforcement learning. However, pretraining reinforcement learning while maintaining exploration potential, particularly for value-based methods, remains a significant challenge. In this study, we propose a pretraining approach for soft Q-learning, which is a value-based algorithm comparable to policy gradient and has been inspired by actor-critic algorithm pretraining methods. Our approach is based on $\gamma$-discounted biased policy evaluation with entropy regularization, which is also the updating target of soft Q-learning. We have evaluated our method on numerous tasks from Atari 2600, and the experiments demonstrate that our approach effectively learns from suboptimal demonstrations and outperforms other state-of-the-art techniques that learn from expert demonstrations.",1
"Learning an effective representation for high-dimensional data is a challenging problem in reinforcement learning (RL). Deep reinforcement learning (DRL) such as Deep Q networks (DQN) achieves remarkable success in computer games by learning deeply encoded representation from convolution networks. In this paper, we propose a simple yet very effective method for representation learning with DRL algorithms. Our key insight is that features learned by DRL algorithms are highly correlated, which interferes with learning. By adding a regularized loss that penalizes correlation in latent features (with only slight computation), we decorrelate features represented by deep neural networks incrementally. On 49 Atari games, with the same regularization factor, our decorrelation algorithms perform $70\%$ in terms of human-normalized scores, which is $40\%$ better than DQN. In particular, ours performs better than DQN on 39 games with 4 close ties and lost only slightly on $6$ games. Empirical results also show that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly boosts performance. Further experiments on the losing games show that our decorelation algorithms can win over DQN and QR-DQN with a fined tuned regularization factor.",0
"Reinforcement learning (RL) faces a difficult challenge in generating an effective representation for data with high dimensions. Deep reinforcement learning (DRL), such as Deep Q networks (DQN), has achieved remarkable success in computer games by utilizing convolution networks to learn a deeply encoded representation. This paper proposes a straightforward yet highly effective method for representation learning with DRL algorithms. By recognizing that DRL algorithms learn highly correlated features, which hinders learning, we introduce a regularized loss that penalizes correlation in latent features. This addition incrementally decorrelates deep neural network features with minimal computation. Our decorrelation algorithms achieved a 70% human-normalized score on 49 Atari games, which is 40% better than DQN with the same regularization factor. Specifically, our method outperformed DQN on 39 games, with only slight losses on six games. Empirical results demonstrate that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly enhances performance. Further experiments on losing games demonstrate that our decorrelation algorithms can outperform DQN and QR-DQN with a fine-tuned regularization factor.",1
"While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.",0
"Despite the growing interest in Bayesian neural networks (BNNs), the challenge of conducting posterior inference still persists, mainly due to their high-dimensional and over-parameterized nature. To address this challenge, various flexible and scalable variational inference procedures based on particle optimization have been introduced. These procedures involve the direct optimization of a group of particles to approximate the target posterior. However, their application to BNNs often results in sub-optimal performance as they have a tendency to fail on over-parameterized models. This paper suggests a solution to this problem by conducting particle optimization directly in the space of regression functions. Extensive experiments have demonstrated that our method has successfully overcome this issue and outperformed strong baselines in various tasks, including prediction, defense against adversarial examples, and reinforcement learning.",1
"Policy gradient algorithms are among the best candidates for the much anticipated application of reinforcement learning to real-world control tasks, such as the ones arising in robotics. However, the trial-and-error nature of these methods introduces safety issues whenever the learning phase itself must be performed on a physical system. In this paper, we address a specific safety formulation, where danger is encoded in the reward signal and the learning agent is constrained to never worsen its performance. By studying actor-only policy gradient from a stochastic optimization perspective, we establish improvement guarantees for a wide class of parametric policies, generalizing existing results on Gaussian policies. This, together with novel upper bounds on the variance of policy gradient estimators, allows to identify those meta-parameter schedules that guarantee monotonic improvement with high probability. The two key meta-parameters are the step size of the parameter updates and the batch size of the gradient estimators. By a joint, adaptive selection of these meta-parameters, we obtain a safe policy gradient algorithm.",0
"Reinforcement learning has the potential to be used in real-world control tasks, particularly in robotics, and policy gradient algorithms are considered to be some of the best options. However, the trial-and-error approach of these methods can create safety concerns when the learning phase takes place on a physical system. This paper focuses on a specific safety formulation where danger is reflected in the reward signal and the learning agent must not worsen its performance. By examining actor-only policy gradient from a stochastic optimization perspective, we establish improvement guarantees for various types of parametric policies, building on existing results on Gaussian policies. Additionally, we introduce new upper bounds on the variance of policy gradient estimators, enabling us to determine meta-parameter schedules that ensure monotonic improvement with a high probability. Two key meta-parameters are identified: the step size of the parameter updates and the batch size of the gradient estimators. Using an adaptive selection of these meta-parameters, we can create a safe policy gradient algorithm.",1
"In this paper, we propose a novel meta-learning method in a reinforcement learning setting, based on evolution strategies (ES), exploration in parameter space and deterministic policy gradients. ES methods are easy to parallelize, which is desirable for modern training architectures; however, such methods typically require a huge number of samples for effective training. We use deterministic policy gradients during adaptation and other techniques to compensate for the sample-efficiency problem while maintaining the inherent scalability of ES methods. We demonstrate that our method achieves good results compared to gradient-based meta-learning in high-dimensional control tasks in the MuJoCo simulator. In addition, because of gradient-free methods in the meta-training phase, which do not need information about gradients and policies in adaptation training, we predict and confirm our algorithm performs better in tasks that need multi-step adaptation.",0
"This paper presents a new approach to meta-learning in a reinforcement learning context, utilizing evolution strategies (ES), exploration in parameter space, and deterministic policy gradients. Although ES methods are easy to parallelize, they typically require a large number of samples for effective training. To address this, we incorporate deterministic policy gradients during adaptation and other strategies to improve sample efficiency while maintaining scalability. Our method outperforms gradient-based meta-learning in high-dimensional control tasks within the MuJoCo simulator. Additionally, due to the use of gradient-free methods in the meta-training phase, our algorithm is better suited for tasks that require multi-step adaptation.",1
"Evaluation of deep reinforcement learning (RL) is inherently challenging. In particular, learned policies are largely opaque, and hypotheses about the behavior of deep RL agents are difficult to test in black-box environments. Considerable effort has gone into addressing opacity, but almost no effort has been devoted to producing high quality environments for experimental evaluation of agent behavior. We present TOYBOX, a new high-performance, open-source* subset of Atari environments re-designed for the experimental evaluation of deep RL. We show that TOYBOX enables a wide range of experiments and analyses that are impossible in other environments.   *https://kdl-umass.github.io/Toybox/",0
"The assessment of deep reinforcement learning (RL) is a complex task due to the opaqueness of learned policies and the difficulty of testing hypotheses about deep RL agents' behavior in black-box environments. Although attempts have been made to address opacity, there has been almost no emphasis on developing top-quality environments for experimental evaluation of agent behavior. TOYBOX is a new, high-performance, open-source subset of Atari environments, designed specifically for the experimental evaluation of deep RL. With TOYBOX, a wide range of experiments and analyses are possible that are not feasible in other environments.",1
"Although deep reinforcement learning has advanced significantly over the past several years, sample efficiency remains a major challenge. Careful choice of input representations can help improve efficiency depending on the structure present in the problem. In this work, we present an attention-based method to project inputs into an efficient representation space that is invariant under changes to input ordering. We show that our proposed representation results in a search space that is a factor of m! smaller for inputs of m objects. Our experiments demonstrate improvements in sample efficiency for policy gradient methods on a variety of tasks. We show that our representation allows us to solve problems that are otherwise intractable when using naive approaches.",0
"Despite significant advancements in deep reinforcement learning, sample efficiency remains a major hurdle. Depending on the problem structure, careful selection of input representations can enhance efficiency. This study introduces an attention-based approach to project inputs into a space that is invariant to alterations in input ordering, resulting in an efficient representation. The proposed representation results in a search space that is a factor of m! smaller for inputs consisting of m objects. Our experiments indicate that policy gradient methods on various tasks benefit from improved sample efficiency with our representation. Furthermore, our approach allows us to tackle problems that are intractable using naive techniques.",1
"A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Specifically, we consider two scenarios in which the agent attempts to perform an action $a$, and (i) with probability $\alpha$, an alternative adversarial action $\bar a$ is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.",0
"The concept of robustness in a policy refers to its ability to obtain the maximum reward in the presence of a bad or adversarial model. In this study, we introduce two new criteria for robustness to address action uncertainty. The first scenario involves an agent attempting to perform an action $a$, but with a probability of $\alpha$, an alternative adversarial action $\bar a$ is taken. The second scenario occurs in a continuous action space, where an adversary adds a perturbation to the selected action. These criteria are relevant to robotics domains where abrupt forces may occur. We propose algorithms for the tabular case and extend them to deep reinforcement learning (DRL), conducting experiments across various MuJoCo domains. Our findings demonstrate that our approach not only produces robust policies but also enhances performance in the absence of perturbations. Therefore, we conclude that action-robustness can serve as implicit regularization in RL problems.",1
"Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyperparameters. One reason is that most approaches use a noisy version of their operating policy to explore - thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners - typically proven algorithms like TD3 - optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient - notably solving the Mujoco Humanoid benchmark where all of its composite learners (TD3) fail entirely in isolation.",0
"A variety of complex control tasks have been successfully tackled using deep reinforcement learning algorithms. However, these methods often encounter difficulties in achieving effective exploration and are highly sensitive to the choice of hyperparameters. This is partly because most approaches employ a noisy version of their operating policy for exploration, which limits the range of exploration. To address this issue, we present Collaborative Evolutionary Reinforcement Learning (CERL), which is a scalable framework that comprises a portfolio of policies that explore and exploit diverse regions of the solution space simultaneously. A group of learners, such as proven algorithms like TD3, optimize over different time-horizons, leading to a diverse portfolio. All learners use a shared replay buffer to achieve greater sample efficiency, and computational resources are flexibly allocated to favor the best learners as a form of online algorithm selection. Neuroevolution integrates this entire process to produce a single emergent learner that surpasses the capabilities of any individual learner. Experiments in various continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while also being more sample-efficient overall. Notably, it solves the Mujoco Humanoid benchmark, where all of its composite learners (TD3) fail in isolation.",1
"Recent success in deep reinforcement learning for continuous control has been dominated by model-free approaches which, unlike model-based approaches, do not suffer from representational limitations in making assumptions about the world dynamics and model errors inevitable in complex domains. However, they require a lot of experiences compared to model-based approaches that are typically more sample-efficient. We propose to combine the benefits of the two approaches by presenting an integrated approach called Curious Meta-Controller. Our approach alternates adaptively between model-based and model-free control using a curiosity feedback based on the learning progress of a neural model of the dynamics in a learned latent space. We demonstrate that our approach can significantly improve the sample efficiency and achieve near-optimal performance on learning robotic reaching and grasping tasks from raw-pixel input in both dense and sparse reward settings.",0
"Current progress in deep reinforcement learning for continuous control has mainly been led by model-free techniques. Unlike model-based methods, they do not encounter limitations in representing the world dynamics and inevitable model errors in complex environments. However, they do require more experiences compared to model-based techniques, which are typically more efficient in terms of sample usage. To combine the advantages of both methods, we introduce an integrated approach called Curious Meta-Controller. Our approach alternates between model-based and model-free control using a curiosity feedback dependent on the neural model's learning progress of the dynamics in a learned latent space. We demonstrate that our approach can significantly enhance sample efficiency and attain near-optimal performance when learning robotic reaching and grasping tasks from raw-pixel input in dense and sparse reward settings.",1
"We study online reinforcement learning for finite-horizon deterministic control systems with {\it arbitrary} state and action spaces. Suppose that the transition dynamics and reward function is unknown, but the state and action space is endowed with a metric that characterizes the proximity between different states and actions. We provide a surprisingly simple upper-confidence reinforcement learning algorithm that uses a function approximation oracle to estimate optimistic Q functions from experiences. We show that the regret of the algorithm after $K$ episodes is $O(HL(KH)^{\frac{d-1}{d}}) $ where $L$ is a smoothness parameter, and $d$ is the doubling dimension of the state-action space with respect to the given metric. We also establish a near-matching regret lower bound. The proposed method can be adapted to work for more structured transition systems, including the finite-state case and the case where value functions are linear combinations of features, where the method also achieve the optimal regret.",0
"Our focus is on finite-horizon deterministic control systems with unrestricted state and action spaces, where the transition dynamics and reward function are unknown. However, we assume that a metric is in place to gauge the proximity of different states and actions. To estimate optimistic Q functions from experiences, we propose a straightforward upper-confidence reinforcement learning algorithm that utilizes a function approximation oracle. Our analysis reveals that after $K$ episodes, the regret of the algorithm is $O(HL(KH)^{\frac{d-1}{d}})$, where $L$ denotes the smoothness parameter and $d$ represents the doubling dimension of the state-action space with respect to the given metric. We also provide a nearly-matching regret lower bound. With the proposed approach, we can adapt our method for more structured transition systems, such as the finite-state case and the linear combination of features case, where it achieves the optimal regret.",1
"Face hallucination is a domain-specific super-resolution problem that aims to generate a high-resolution (HR) face image from a low-resolution~(LR) input. In contrast to the existing patch-wise super-resolution models that divide a face image into regular patches and independently apply LR to HR mapping to each patch, we implement deep reinforcement learning and develop a novel attention-aware face hallucination (Attention-FH) framework, which recurrently learns to attend a sequence of patches and performs facial part enhancement by fully exploiting the global interdependency of the image. Specifically, our proposed framework incorporates two components: a recurrent policy network for dynamically specifying a new attended region at each time step based on the status of the super-resolved image and the past attended region sequence, and a local enhancement network for selected patch hallucination and global state updating. The Attention-FH model jointly learns the recurrent policy network and local enhancement network through maximizing a long-term reward that reflects the hallucination result with respect to the whole HR image. Extensive experiments demonstrate that our Attention-FH significantly outperforms the state-of-the-art methods on in-the-wild face images with large pose and illumination variations.",0
"The objective of face hallucination is to create a high-resolution image of a face from a low-resolution input. While existing models divide the face image into patches and apply a low-to-high resolution mapping independently to each patch, we have developed a new approach called Attention-FH. This uses deep reinforcement learning to learn how to attend to a sequence of patches and enhance facial features by utilizing the global interdependency of the image. Attention-FH comprises two components: a recurrent policy network to dynamically specify the attended region at each time step, and a local enhancement network for patch hallucination and global state updating. The model learns both networks jointly by maximizing a long-term reward that reflects the hallucination result with respect to the entire high-resolution image. Our experiments show that Attention-FH outperforms state-of-the-art methods on face images captured in the wild, including those with significant pose and illumination variations.",1
"In this letter, we address the problem of controlling energy storage systems (ESSs) for arbitrage in real-time electricity markets under price uncertainty. We first formulate this problem as a Markov decision process, and then develop a deep reinforcement learning based algorithm to learn a stochastic control policy that maps a set of available information processed by a recurrent neural network to ESSs' charging/discharging actions. Finally, we verify the effectiveness of our algorithm using real-time electricity prices from PJM.",0
The focus of this correspondence is on the management of energy storage systems (ESSs) for arbitrage in electricity markets with fluctuating prices. Our approach involves formulating the issue as a Markov decision process and utilizing a deep reinforcement learning algorithm to develop a stochastic control policy. This policy maps processed information from a recurrent neural network to the charging and discharging actions of ESSs. We conclude by demonstrating the efficacy of our algorithm through the application of real-time electricity prices from PJM.,1
"Motor control is a set of time-varying muscle excitations which generate desired motions for a biomechanical system. Muscle excitations cannot be directly measured from live subjects. An alternative approach is to estimate muscle activations using inverse motion-driven simulation. In this article, we propose a deep reinforcement learning method to estimate the muscle excitations in simulated biomechanical systems. Here, we introduce a custom-made reward function which incentivizes faster point-to-point tracking of target motion. Moreover, we deploy two new techniques, namely, episode-based hard update and dual buffer experience replay, to avoid feedback training loops. The proposed method is tested in four simulated 2D and 3D environments with 6 to 24 axial muscles. The results show that the models were able to learn muscle excitations for given motions after nearly 100,000 simulated steps. Moreover, the root mean square error in point-to-point reaching of the target across experiments was less than 1% of the length of the domain of motion. Our reinforcement learning method is far from the conventional dynamic approaches as the muscle control is derived functionally by a set of distributed neurons. This can open paths for neural activity interpretation of this phenomenon.",0
"The generation of desired movements for a biomechanical system is accomplished through a set of muscle excitations that vary over time. However, it is not possible to measure these excitations directly from live subjects. Instead, an alternative approach is to estimate muscle activations through inverse motion-driven simulation. In this article, we propose a deep reinforcement learning technique to estimate muscle excitations in simulated biomechanical systems. Our method includes a customized reward function that encourages faster target motion tracking and introduces two new techniques, episode-based hard update and dual buffer experience replay, to prevent feedback training loops. We tested our approach in four simulated 2D and 3D environments with 6 to 24 axial muscles. The results demonstrate that the models could learn muscle excitations for given motions after almost 100,000 simulated steps. Furthermore, the root mean square error in point-to-point target reaching was less than 1% of the motion domain length across experiments. Our proposed reinforcement learning method differs from conventional dynamic approaches as the muscle control is functionally derived by a set of distributed neurons, which may pave the way for interpreting neural activity in this phenomenon.",1
"Meta-learning is a tool that allows us to build sample-efficient learning systems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just faster learners than their sample-inefficient deep learning (DL) and reinforcement learning (RL) brethren, but that they actually pursue fundamentally different learning trajectories. We study their learning dynamics on three sets of structured tasks for which the corresponding learning dynamics of DL and RL systems have been previously described: linear regression (Saxe et al., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and contextual bandits (Schaul et al., 2019). In each case, while sample-inefficient DL and RL Learners uncover the task structure in a staggered manner, meta-trained LSTM Meta-Learners uncover almost all task structure concurrently, congruent with the patterns expected from Bayes-optimal inference algorithms. This has implications for research areas wherever the learning behaviour itself is of interest, such as safety, curriculum design, and human-in-the-loop machine learning.",0
"Meta-learning is a useful tool that enables the development of learning systems that are efficient with samples. Our research demonstrates that LSTM Meta-Learners, once trained, not only learn faster than their inefficient counterparts in deep learning (DL) and reinforcement learning (RL), but also follow distinct learning trajectories. We conducted an analysis of their learning dynamics on three sets of structured tasks, which have previously been studied in the context of DL and RL: linear regression, nonlinear regression, and contextual bandits. In each scenario, while the sample-inefficient DL and RL Learners gradually uncover the task structure, the meta-trained LSTM Meta-Learners efficiently uncover almost all task structure concurrently, which aligns with the expected patterns of Bayes-optimal inference algorithms. This finding has significant implications for research areas that focus on learning behavior, including safety, curriculum design, and human-in-the-loop machine learning.",1
"Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.",0
"One of the biggest challenges in neural network learning is the inability to perform consistently in non-stationary data distributions, which limits the ability to scale to more human-like settings. To address this challenge, we propose a new approach that considers the trade-off between transfer and interference in continual learning, and optimizes it through gradient alignment. Our proposed algorithm, Meta-Experience Replay (MER), combines experience replay with optimization-based meta-learning to learn parameters that reduce interference and increase transfer based on future gradients. Through experiments in lifelong supervised learning and non-stationary reinforcement learning, we demonstrate that MER outperforms recent baselines for continual learning. Moreover, the performance gap between our approach and baseline algorithms increases with greater non-stationarity and lower fraction of stored experiences.",1
"Air traffic control is a real-time safety-critical decision making process in highly dynamic and stochastic environments. In today's aviation practice, a human air traffic controller monitors and directs many aircraft flying through its designated airspace sector. With the fast growing air traffic complexity in traditional (commercial airliners) and low-altitude (drones and eVTOL aircraft) airspace, an autonomous air traffic control system is needed to accommodate high density air traffic and ensure safe separation between aircraft. We propose a deep multi-agent reinforcement learning framework that is able to identify and resolve conflicts between aircraft in a high-density, stochastic, and dynamic en-route sector with multiple intersections and merging points. The proposed framework utilizes an actor-critic model, A2C that incorporates the loss function from Proximal Policy Optimization (PPO) to help stabilize the learning process. In addition we use a centralized learning, decentralized execution scheme where one neural network is learned and shared by all agents in the environment. We show that our framework is both scalable and efficient for large number of incoming aircraft to achieve extremely high traffic throughput with safety guarantee. We evaluate our model via extensive simulations in the BlueSky environment. Results show that our framework is able to resolve 99.97% and 100% of all conflicts both at intersections and merging points, respectively, in extreme high-density air traffic scenarios.",0
"Air traffic control is a high-stakes decision-making process that takes place in environments that are constantly changing and unpredictable. Typically, a human air traffic controller is responsible for directing numerous aircraft within a designated airspace sector. However, with the rise of air traffic in both traditional (commercial airliners) and low-altitude (drones and eVTOL aircraft) airspace, there is a need for an autonomous air traffic control system to manage the high volume of traffic and ensure aircraft stay safe.  To address this need, we propose a deep multi-agent reinforcement learning framework that can identify and resolve conflicts between aircraft in complex, dynamic, and stochastic en-route sectors with multiple intersections and merging points. Our framework uses the actor-critic model, A2C, with the loss function from Proximal Policy Optimization (PPO) to stabilize the learning process. We also adopt a centralized learning, decentralized execution approach where one neural network is learned and shared by all agents in the environment.  Our simulations in the BlueSky environment demonstrate that our framework is scalable and efficient for handling large numbers of incoming aircraft, achieving high traffic throughput while maintaining safety. Specifically, our framework resolves 99.97% and 100% of conflicts at intersections and merging points, respectively, in extreme high-density air traffic scenarios.",1
"Reinforcement learning (RL) is about sequential decision making and is traditionally opposed to supervised learning (SL) and unsupervised learning (USL). In RL, given the current state, the agent makes a decision that may influence the next state as opposed to SL (and USL) where, the next state remains the same, regardless of the decisions taken, either in batch or online learning. Although this difference is fundamental between SL and RL, there are connections that have been overlooked. In particular, we prove in this paper that gradient policy method can be cast as a supervised learning problem where true label are replaced with discounted rewards. We provide a new proof of policy gradient methods (PGM) that emphasizes the tight link with the cross entropy and supervised learning. We provide a simple experiment where we interchange label and pseudo rewards. We conclude that other relationships with SL could be made if we modify the reward functions wisely.",0
"Reinforcement learning (RL) involves making sequential decisions, which contrasts with supervised learning (SL) and unsupervised learning (USL). In RL, the agent's current state influences the decision made, whereas in SL and USL, the next state is unaffected by the decisions taken in batch or online learning. Despite this fundamental difference between SL and RL, there are overlooked connections between the two. This paper demonstrates that the gradient policy method can be seen as a supervised learning problem, where the true labels are replaced with discounted rewards. The policy gradient methods (PGM) are proven in a new way that highlights their close relationship with cross entropy and supervised learning. An experiment is presented where label and pseudo rewards are interchanged, and it is concluded that further connections between SL and RL are possible by intelligently modifying the reward functions.",1
"Model-free Reinforcement Learning (RL) algorithms such as Q-learning [Watkins, Dayan 92] have been widely used in practice and can achieve human level performance in applications such as video games [Mnih et al. 15]. Recently, equipped with the idea of optimism in the face of uncertainty, Q-learning algorithms [Jin, Allen-Zhu, Bubeck, Jordan 18] can be proven to be sample efficient for discrete tabular Markov Decision Processes (MDPs) which have finite number of states and actions. In this work, we present an efficient model-free Q-learning based algorithm in MDPs with a natural metric on the state-action space--hence extending efficient model-free Q-learning algorithms to continuous state-action space. Compared to previous model-based RL algorithms for metric spaces [Kakade, Kearns, Langford 03], our algorithm does not require access to a black-box planning oracle.",0
"Q-learning, a model-free reinforcement learning (RL) algorithm, has been widely utilized in various applications, including video games, and is capable of achieving human-level performance. Recent advancements in Q-learning algorithms, incorporating the concept of optimism in uncertain situations, have made them more sample efficient for discrete tabular Markov Decision Processes (MDPs) with a finite number of states and actions. This study introduces a novel model-free Q-learning-based algorithm that is efficient in MDPs with a natural metric on the state-action space, enabling the extension of efficient model-free Q-learning algorithms to continuous state-action space. Unlike previous model-based RL algorithms for metric spaces, our algorithm does not necessitate access to a black-box planning oracle.",1
"Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (""why do we need them?"") and the naturalness (""when do they hold?"") of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",0
"Reinforcement learning (RL) heavily relies on batch mode value-function approximation methods. The effectiveness of these methods is often dependent on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the importance and likelihood of these assumptions have not been extensively explored in literature. This paper aims to address these questions by revisiting the assumptions and providing theoretical results to deepen our understanding of value-function approximation.",1
"Vision-based deep reinforcement learning (RL) typically obtains performance benefit by using high capacity and relatively large convolutional neural networks (CNN). However, a large network leads to higher inference costs (power, latency, silicon area, MAC count). Many inference optimizations have been developed for CNNs. Some optimization techniques offer theoretical efficiency, such as sparsity, but designing actual hardware to support them is difficult. On the other hand, distillation is a simple general-purpose optimization technique which is broadly applicable for transferring knowledge from a trained, high capacity teacher network to an untrained, low capacity student network. DQN distillation extended the original distillation idea to transfer information stored in a high performance, high capacity teacher Q-function trained via the Deep Q-Learning (DQN) algorithm. Our work adapts the DQN distillation work to the actor-critic Proximal Policy Optimization algorithm. PPO is simple to implement and has much higher performance than the seminal DQN algorithm. We show that a distilled PPO student can attain far higher performance compared to a DQN teacher. We also show that a low capacity distilled student is generally able to outperform a low capacity agent that directly trains in the environment. Finally, we show that distillation, followed by ""fine-tuning"" in the environment, enables the distilled PPO student to achieve parity with teacher performance. In general, the lessons learned in this work should transfer to other modern actor-critic RL algorithms.",0
"Usually, vision-based deep reinforcement learning (RL) employs large convolutional neural networks (CNN) to achieve high performance. Nevertheless, this approach entails higher inference costs, such as power, latency, silicon area, and MAC count. Although several optimization techniques for CNNs have been developed, some of them, such as sparsity, are challenging to design into hardware. Conversely, distillation is a straightforward optimization technique that enables the transfer of knowledge from a trained, high capacity teacher network to an untrained, low capacity student network. DQN distillation is an extension of the original distillation concept that transfers information from a high-performance, high-capacity teacher Q-function trained via the Deep Q-Learning (DQN) algorithm. Our study adapts the DQN distillation approach to the actor-critic Proximal Policy Optimization (PPO) algorithm, which is easy to implement and outperforms the seminal DQN algorithm. Our findings show that a distilled PPO student can achieve significantly higher performance than a DQN teacher. Furthermore, we demonstrate that a low-capacity distilled student can generally outperform a low-capacity agent that directly trains in the environment. Finally, we reveal that distillation followed by ""fine-tuning"" in the environment enables the distilled PPO student to achieve parity with the teacher's performance. In general, the lessons learned in this study could apply to other modern actor-critic RL algorithms.",1
"As an efficient and scalable graph neural network, GraphSAGE has enabled an inductive capability for inferring unseen nodes or graphs by aggregating subsampled local neighborhoods and by learning in a mini-batch gradient descent fashion. The neighborhood sampling used in GraphSAGE is effective in order to improve computing and memory efficiency when inferring a batch of target nodes with diverse degrees in parallel. Despite this advantage, the default uniform sampling suffers from high variance in training and inference, leading to sub-optimum accuracy. We propose a new data-driven sampling approach to reason about the real-valued importance of a neighborhood by a non-linear regressor, and to use the value as a criterion for subsampling neighborhoods. The regressor is learned using a value-based reinforcement learning. The implied importance for each combination of vertex and neighborhood is inductively extracted from the negative classification loss output of GraphSAGE. As a result, in an inductive node classification benchmark using three datasets, our method enhanced the baseline using the uniform sampling, outperforming recent variants of a graph neural network in accuracy.",0
"GraphSAGE is a graph neural network that is both efficient and scalable. It allows for inductive inference of unseen nodes or graphs by aggregating subsampled local neighborhoods and learning in mini-batch gradient descent. The neighborhood sampling used in GraphSAGE is effective for improving computing and memory efficiency when inferring a batch of target nodes with diverse degrees in parallel. Despite this advantage, the default uniform sampling suffers from high variance in training and inference, resulting in sub-optimal accuracy. Our proposed solution is a new data-driven sampling approach that uses a non-linear regressor to reason about the real-valued importance of a neighborhood and then subsamples based on this value. The regressor is trained using value-based reinforcement learning. The importance for each combination of vertex and neighborhood is inductively extracted from the negative classification loss output of GraphSAGE. As a result, our method outperformed recent variants of a graph neural network in accuracy in an inductive node classification benchmark using three datasets, improving upon the baseline that used uniform sampling.",1
"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",0
"Reinforcement learning (RL) has demonstrated its value in various artificial environments and is starting to exhibit some accomplishments in real-life situations. However, many of the advancements in RL research are difficult to apply in practical systems because they rely on assumptions that are rarely met. To address this issue, we identify nine unique challenges that need to be tackled to facilitate the implementation of RL in real-world problems. For each challenge, we provide a definition, review some relevant approaches from the literature, and establish metrics for evaluating its effectiveness. An approach that overcomes all nine challenges would be suitable for a broad range of real-world issues. In addition, we introduce an example domain that has been adapted to present these challenges as a platform for practical RL research.",1
"We present RL-GAN-Net, where a reinforcement learning (RL) agent provides fast and robust control of a generative adversarial network (GAN). Our framework is applied to point cloud shape completion that converts noisy, partial point cloud data into a high-fidelity completed shape by controlling the GAN. While a GAN is unstable and hard to train, we circumvent the problem by (1) training the GAN on the latent space representation whose dimension is reduced compared to the raw point cloud input and (2) using an RL agent to find the correct input to the GAN to generate the latent space representation of the shape that best fits the current input of incomplete point cloud. The suggested pipeline robustly completes point cloud with large missing regions. To the best of our knowledge, this is the first attempt to train an RL agent to control the GAN, which effectively learns the highly nonlinear mapping from the input noise of the GAN to the latent space of point cloud. The RL agent replaces the need for complex optimization and consequently makes our technique real time. Additionally, we demonstrate that our pipelines can be used to enhance the classification accuracy of point cloud with missing data.",0
"Our article introduces RL-GAN-Net, a framework that employs a reinforcement learning (RL) agent to effectively control a generative adversarial network (GAN) for quick and stable point cloud shape completion. By converting noisy, partial point cloud data into a high-quality, finished shape, the GAN is able to overcome its inherent instability and training difficulties. We achieve this by training the GAN on a reduced-dimension latent space representation of the raw point cloud input and using the RL agent to identify the optimal input for the GAN to generate the correct latent space representation for the current incomplete point cloud input. The proposed pipeline efficiently addresses point clouds with significant missing regions. Our approach is novel in that it is the first to employ an RL agent to control the GAN, thereby eliminating the need for complicated optimization and enabling real-time operation. Furthermore, we demonstrate that our framework can improve the classification accuracy of point clouds with missing data.",1
"Due to burdensome data requirements, learning from demonstration often falls short of its promise to allow users to quickly and naturally program robots. Demonstrations are inherently ambiguous and incomplete, making correct generalization to unseen situations difficult without a large number of demonstrations in varying conditions. By contrast, humans are often able to learn complex tasks from a single demonstration (typically observations without action labels) by leveraging context learned over a lifetime. Inspired by this capability, our goal is to enable robots to perform one-shot learning of multi-step tasks from observation by leveraging auxiliary video data as context. Our primary contribution is a novel system that achieves this goal by: (1) using a single user-segmented demonstration to define the primitive actions that comprise a task, (2) localizing additional examples of these actions in unsegmented auxiliary videos via a metalearning-based approach, (3) using these additional examples to learn a reward function for each action, and (4) performing reinforcement learning on top of the inferred reward functions to learn action policies that can be combined to accomplish the task. We empirically demonstrate that a robot can learn multi-step tasks more effectively when provided auxiliary video, and that performance greatly improves when localizing individual actions, compared to learning from unsegmented videos.",0
"Learning from demonstration often fails to live up to its potential of allowing users to easily and intuitively program robots due to the demanding data requirements. Demonstrations are inherently ambiguous and incomplete, making it challenging to correctly generalize to unforeseen circumstances without a significant number of demonstrations in varying conditions. On the other hand, humans can often learn complicated tasks from a single demonstration by utilizing context acquired over their lifetime. Our aim is to enable robots to learn multi-step tasks from observation using auxiliary video data as context. Our innovative system achieves this by: (1) using a single user-segmented demonstration to define primitive actions, (2) localizing additional examples of these actions in unsegmented auxiliary videos, (3) learning a reward function for each action using these additional examples, and (4) using reinforcement learning to learn action policies that can be combined to accomplish the task. We demonstrate through experiments that robots learn multi-step tasks more effectively when provided with auxiliary video, and that performance significantly improves when localizing individual actions compared to learning from unsegmented videos.",1
"By the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature that does not need too much cooperation and can significantly improve the user's experience at the same time. Face recognition is one of the main functions of electronic equipment propaganda. Hence it's virtually worth researching in computer vision. Previous work in this field has focused on two directions: converting loss function to improve recognition accuracy in traditional deep convolution neural networks (Resnet); combining the latest loss function with the lightweight system (MobileNet) to reduce network size at the minimal expense of accuracy. But none of these has changed the network structure. With the development of AutoML, neural architecture search (NAS) has shown excellent performance in the benchmark of image classification. In this paper, we integrate NAS technology into face recognition to customize a more suitable network. We quote the framework of neural architecture search which trains child and controller network alternately. At the same time, we mutate NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize policy gradient algorithm to search the architecture automatically with the most classical cross-entropy loss. The network architectures we searched out have got state-of-the-art accuracy in the large-scale face dataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with relatively small network size. To the best of our knowledge, this proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.",0
"Biometric technology has brought convenience to user authentication, surpassing traditional password and mode unlocking due to the widespread popularity of electronic devices. Among various biological characteristics, the face is a universal and irreplaceable feature that requires minimal cooperation and enhances the user's experience. Face recognition is a fundamental function of electronic devices, making it a worthwhile area of research in computer vision. Previous research has focused on improving recognition accuracy using traditional deep convolution neural networks or reducing network size with the latest loss function and lightweight systems. However, none of these approaches have altered the network structure. Neural architecture search (NAS) has shown excellent performance in image classification benchmarks, and this paper integrates NAS technology into face recognition to create a more suitable network. The proposed method trains child and controller networks alternately and mutates NAS by incorporating evaluation latency into reinforcement learning rewards. The policy gradient algorithm searches the architecture automatically with the classical cross-entropy loss. The searched network architectures achieve state-of-the-art accuracy in large-scale face datasets, with a top-1 accuracy of 98.77% in MS-Celeb-1M and 99.89% in LFW, despite having a relatively small network size. This proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.",1
"Supervised learning is widely used in training autonomous driving vehicle. However, it is trained with large amount of supervised labeled data. Reinforcement learning can be trained without abundant labeled data, but we cannot train it in reality because it would involve many unpredictable accidents. Nevertheless, training an agent with good performance in virtual environment is relatively much easier. Because of the huge difference between virtual and real, how to fill the gap between virtual and real is challenging. In this paper, we proposed a novel framework of reinforcement learning with image semantic segmentation network to make the whole model adaptable to reality. The agent is trained in TORCS, a car racing simulator.",0
"The training of autonomous driving vehicles commonly involves supervised learning, which necessitates a vast amount of labeled data. On the other hand, reinforcement learning does not require abundant labeled data, but its usage in reality is not feasible due to the potential for unpredictable accidents. However, it is more straightforward to train an agent to perform well in a virtual environment. Despite the significant contrast between virtual and real, bridging the gap between the two is a complex task. In this study, we present a new framework for reinforcement learning that incorporates an image semantic segmentation network to enhance the model's adaptability to reality. The agent is trained in TORCS, a car racing simulator.",1
"Rather than proposing a new method, this paper investigates an issue present in existing learning algorithms. We study the learning dynamics of reinforcement learning (RL), specifically a characteristic coupling between learning and data generation that arises because RL agents control their future data distribution. In the presence of function approximation, this coupling can lead to a problematic type of 'ray interference', characterized by learning dynamics that sequentially traverse a number of performance plateaus, effectively constraining the agent to learn one thing at a time even when learning in parallel is better. We establish the conditions under which ray interference occurs, show its relation to saddle points and obtain the exact learning dynamics in a restricted setting. We characterize a number of its properties and discuss possible remedies.",0
"Instead of introducing a new approach, this article examines a challenge that exists in current learning algorithms. Specifically, we explore the learning dynamics of reinforcement learning (RL) and the interplay between learning and data generation, which is unique to RL since agents can control their future data distribution. This coupling can result in a problematic phenomenon called 'ray interference' when function approximation is present. Ray interference causes learning dynamics to move sequentially through several performance plateaus, which restricts the agent to learn one thing at a time, even when parallel learning would be more effective. We identify the conditions that lead to ray interference, explain its relationship with saddle points, and provide the exact learning dynamics in a limited context. We describe several of its attributes and suggest potential solutions.",1
"We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning (Ashok et al., 2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training. Code is publicly available here: https://github.com/Friedrich1006/ESNAC .",0
"Our proposal involves a technique for progressively acquiring knowledge of an embedding space across the network architecture domain. This facilitates the careful selection of architectures for assessment during the compressed architecture search. Utilizing Bayesian Optimization (BO) with a kernel function over the proposed embedding space, we search for a compressed network architecture given a teacher network. Our search algorithm significantly outperforms various baseline methods, including random search and reinforcement learning (Ashok et al., 2018). The compressed architectures discovered by our method even surpass the state-of-the-art manually-designed compact architecture, ShuffleNet (Zhang et al., 2018). Additionally, we demonstrate that the learned embedding space can be transferred to new settings for architecture search without any training, such as a larger teacher network or a teacher network in a different architecture family. Code is available publicly at https://github.com/Friedrich1006/ESNAC.",1
"Learning to take actions based on observations is a core requirement for artificial agents to be able to be successful and robust at their task. Reinforcement Learning (RL) is a well-known technique for learning such policies. However, current RL algorithms often have to deal with reward shaping, have difficulties generalizing to other environments and are most often sample inefficient. In this paper, we explore active inference and the free energy principle, a normative theory from neuroscience that explains how self-organizing biological systems operate by maintaining a model of the world and casting action selection as an inference problem. We apply this concept to a typical problem known to the RL community, the mountain car problem, and show how active inference encompasses both RL and learning from demonstrations.",0
"To achieve success and resilience in their tasks, artificial agents must learn to act based on their observations. Reinforcement Learning (RL) is a commonly used approach for acquiring such policies. However, existing RL algorithms face issues such as reward shaping, limited generalization to new environments, and low sample efficiency. This study explores the free energy principle and active inference, a neuroscience-based theory that explains how self-organizing biological systems function by maintaining a model of the world and treating action selection as an inference matter. We apply this concept to the mountain car problem, a well-known challenge in the RL community, and demonstrate how active inference integrates both RL and learning from demonstrations.",1
"Opioids are the preferred medications for the treatment of pain in the intensive care unit. While undertreatment leads to unrelieved pain and poor clinical outcomes, excessive use of opioids puts patients at risk of experiencing multiple adverse effects. In this work, we present a sequential decision making framework for opioid dosing based on deep reinforcement learning. It provides real-time clinically interpretable dosing recommendations, personalized according to each patient's evolving pain and physiological condition. We focus on morphine, one of the most commonly prescribed opioids. To train and evaluate the model, we used retrospective data from the publicly available MIMIC-3 database. Our results demonstrate that reinforcement learning may be used to aid decision making in the intensive care setting by providing personalized pain management interventions.",0
"The intensive care unit prefers opioids as the go-to medication for pain relief. However, the use of opioids must be monitored as underutilization may lead to untreated pain and poor patient outcomes, while overuse can cause multiple adverse effects. To address this, we developed a deep reinforcement learning-based framework for opioid dosing, which offers real-time and clinically relevant dosing recommendations that are personalized to the evolving pain and physiological condition of each patient. Our focus is on morphine, a commonly prescribed opioid, and we utilized retrospective data from the publicly available MIMIC-3 database to train and evaluate the model. Our findings show that personalized pain management interventions can be achieved using reinforcement learning and can aid decision-making in the intensive care setting.",1
"Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study. To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.",0
"Various applications have widely utilized machine learning, some of which involve training with data that requires privacy protection. Although a few data breaches have been examined, such as credit card information in natural language data and identities from facial datasets, most studies have concentrated on supervised learning models. With the deployment of deep reinforcement learning (DRL) in actual systems, including indoor robot navigation, there is a need for a thorough investigation of whether trained DRL policies can leak sensitive information. To address this issue, we propose two primary methods: genetic algorithm-based environment dynamics search and shadow policy-based candidate inference. We perform extensive experiments to demonstrate privacy vulnerabilities in DRL under different circumstances. We employ the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The algorithm successfully infers most of the floor plans, with an average recovery rate of 95.83% using policy gradient-trained agents. Additionally, we accurately recover the robot configuration in continuous control environments and an autonomous driving simulator. This research is the first to investigate privacy leakage in DRL settings, demonstrating that DRL-based agents can potentially leak sensitive information from trained policies.",1
"This paper presents a novel approach to synthesize automatically age-progressed facial images in video sequences using Deep Reinforcement Learning. The proposed method models facial structures and the longitudinal face-aging process of given subjects coherently across video frames. The approach is optimized using a long-term reward, Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network. Unlike previous age-progression methods that are only able to synthesize an aged likeness of a face from a single input image, the proposed approach is capable of age-progressing facial likenesses in videos with consistently synthesized facial features across frames. In addition, the deep reinforcement learning method guarantees preservation of the visual identity of input faces after age-progression. Results on videos of our new collected aging face AGFW-v2 database demonstrate the advantages of the proposed solution in terms of both quality of age-progressed faces, temporal smoothness, and cross-age face verification.",0
"Using Deep Reinforcement Learning, this paper introduces a new method for generating age-progressed facial images in video sequences automatically. By modeling facial structures and the aging process of subjects in a coherent manner across video frames, the proposed approach optimizes a Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network using long-term rewards. Unlike previous methods that could only synthesize aged facial images from a single input image, this approach can consistently synthesize facial features across frames in videos. Furthermore, it ensures the preservation of visual identity after age-progression. Results from our new aging face AGFW-v2 database show the proposed solution's benefits in terms of quality, temporal smoothness, and cross-age face verification.",1
"In this paper, we point out a fundamental property of the objective in reinforcement learning, with which we can reformulate the policy gradient objective into a perceptron-like loss function, removing the need to distinguish between on and off policy training. Namely, we posit that it is sufficient to only update a policy $\pi$ for cases that satisfy the condition $A(\frac{\pi}{\mu}-1)\leq0$, where $A$ is the advantage, and $\mu$ is another policy. Furthermore, we show via theoretic derivation that a perceptron-like loss function matches the clipped surrogate objective for PPO. With our new formulation, the policies $\pi$ and $\mu$ can be arbitrarily apart in theory, effectively enabling off-policy training. To examine our derivations, we can combine the on-policy PPO clipped surrogate (which we show to be equivalent with one instance of the new reformation) with the off-policy IMPALA method. We first verify the combined method on the OpenAI Gym pendulum toy problem. Next, we use our method to train a quadrotor position controller in a simulator. Our trained policy is efficient and lightweight enough to perform in a low cost micro-controller at a minimum update rate of 500 Hz. For the quadrotor, we show two experiments to verify our method and demonstrate performance: 1) hovering at a fixed position, and 2) tracking along a specific trajectory. In preliminary trials, we are also able to apply the method to a real-world quadrotor.",0
"The objective of this research is to present a crucial characteristic of reinforcement learning. By doing so, we can modify the policy gradient objective and convert it into a perceptron-like loss function, eliminating the need for distinguishing between on and off policy training. Our hypothesis suggests that it is necessary to update the policy $\pi$ only for cases that fulfill the condition $A(\frac{\pi}{\mu}-1)\leq0$, where $A$ represents the advantage, and $\mu$ is another policy. Additionally, our theoretical derivation demonstrates that the perceptron-like loss function corresponds to the clipped surrogate objective for PPO. With this new formulation, policies $\pi$ and $\mu$ can be distant from each other, which facilitates off-policy training. We evaluate our theory by combining the on-policy PPO clipped surrogate with the off-policy IMPALA method and implementing the method on the OpenAI Gym pendulum toy problem and a quadrotor position controller in a simulator. Our trained policy is efficient and lightweight enough to perform on a low-cost micro-controller at a minimum update rate of 500 Hz. Furthermore, we demonstrate two experiments to verify our method for the quadrotor: hovering at a fixed position and tracking along a specific trajectory. In preliminary trials, we were also able to test the method on a real-world quadrotor.",1
"Model-free learning for multi-agent stochastic games is an active area of research. Existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings. Here, we develop a new data efficient Deep-Q-learning methodology for model-free learning of Nash equilibria for general-sum stochastic games. The algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. The expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs. We study symmetry properties of the algorithm stemming from label-invariant stochastic games and as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.",0
"The study of model-free learning for multi-agent stochastic games is an active area of research. However, current reinforcement learning algorithms are often limited to zero-sum games and can only be used in simplified settings or small state-action spaces. In this study, we introduce a new Deep-Q-learning methodology that efficiently learns Nash equilibria for general-sum stochastic games without the need to experience all state-action pairs. Our algorithm is based on a local linear-quadratic expansion of the game, which allows for analytically solvable optimal actions. We use deep neural networks to parameterize the expansion and provide the algorithm with enough flexibility to learn the environment. Additionally, we investigate symmetry properties of the algorithm, which are derived from label-invariant stochastic games. As a proof of concept, we apply our algorithm to learn optimal trading strategies in competitive electronic markets.",1
"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",0
"In this paper, the issue of scalability in architecture search is addressed through a differentiable approach. Rather than traditional methods that involve the use of evolution or reinforcement learning on a non-differentiable and discrete search space, our technique involves a continuous relaxation of the architecture representation. This allows for efficient architecture search using gradient descent. Our algorithm has been extensively tested on various datasets, including CIFAR-10, ImageNet, Penn Treebank, and WikiText-2, and has proven to be highly effective in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling. Furthermore, our method is significantly faster than current non-differentiable techniques. We have made our implementation publicly available to encourage further research on efficient architecture search algorithms.",1
"Decomposition methods have been proposed to approximate solutions to large sequential decision making problems. In contexts where an agent interacts with multiple entities, utility decomposition can be used to separate the global objective into local tasks considering each individual entity independently. An arbitrator is then responsible for combining the individual utilities and selecting an action in real time to solve the global problem. Although these techniques can perform well empirically, they rely on strong assumptions of independence between the local tasks and sacrifice the optimality of the global solution. This paper proposes an approach that improves upon such approximate solutions by learning a correction term represented by a neural network. We demonstrate this approach on a fisheries management problem where multiple boats must coordinate to maximize their catch over time as well as on a pedestrian avoidance problem for autonomous driving. In each problem, decomposition methods can scale to multiple boats or pedestrians by using strategies involving one entity. We verify empirically that the proposed correction method significantly improves the decomposition method and outperforms a policy trained on the full scale problem without utility decomposition.",0
"To tackle large sequential decision making problems involving multiple entities, decomposition methods have been proposed to approximate solutions. Utility decomposition is utilized in such scenarios, which helps divide the global objective into local tasks, considering each entity individually. An arbitrator then combines the individual utilities and chooses an action in real time to solve the global problem. While these methods perform well, they rely on the assumption of independence between local tasks, which might compromise the optimality of the global solution. This paper introduces a novel approach, where a neural network learns a correction term to improve the accuracy of the approximation. The approach is tested on two different problems: fisheries management, where multiple boats must coordinate to maximize their catch over time, and pedestrian avoidance, a problem related to autonomous driving. The proposed correction method significantly improves the decomposition method and outperforms a policy trained on the full-scale problem without utility decomposition.",1
"Abstracting complex 3D shapes with parsimonious part-based representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.",0
"The goal of computer vision has long been to abstract complex 3D shapes into parsimonious part-based representations. This paper proposes a solution to this problem that utilizes superquadrics as atomic elements rather than the traditional 3D cuboid representation. Our research shows that superquadrics are more expressive and easier to learn than cuboid representations, and we have developed an analytical solution to the Chamfer loss that eliminates the need for computationally expensive reinforcement learning or iterative prediction. Our model can learn to parse 3D objects into consistent superquadric representations without supervision, and it has proven its flexibility in capturing fine details and complex poses that cannot be modeled with cuboids. We have demonstrated this through our experiments on various ShapeNet categories and the SURREAL human body dataset.",1
"Power system emergency control is generally regarded as the last safety net for grid security and resiliency. Existing emergency control schemes are usually designed off-line based on either the conceived ""worst"" case scenario or a few typical operation scenarios. These schemes are facing significant adaptiveness and robustness issues as increasing uncertainties and variations occur in modern electrical grids. To address these challenges, for the first time, this paper developed novel adaptive emergency control schemes using deep reinforcement learning (DRL), by leveraging the high-dimensional feature extraction and non-linear generalization capabilities of DRL for complex power systems. Furthermore, an open-source platform named RLGC has been designed for the first time to assist the development and benchmarking of DRL algorithms for power system control. Details of the platform and DRL-based emergency control schemes for generator dynamic braking and under-voltage load shedding are presented. Extensive case studies performed in both two-area four-machine system and IEEE 39-Bus system have demonstrated the excellent performance and robustness of the proposed schemes.",0
"The last resort for ensuring the security and resilience of the power grid is commonly known as power system emergency control. Currently, emergency control strategies are typically created offline and are based on either a few typical operation scenarios or the perceived worst-case scenario. However, these schemes face significant adaptiveness and robustness challenges due to the increased uncertainties and variations present in modern electrical grids. To tackle these issues, this paper introduces innovative adaptive emergency control schemes that utilize deep reinforcement learning (DRL). DRL's high-dimensional feature extraction and non-linear generalization capabilities are leveraged to address complex power systems. Additionally, an open-source platform called RLGC has been designed specifically for developing and benchmarking DRL algorithms for power system control. The paper provides detailed information on the platform and DRL-based emergency control schemes, including generator dynamic braking and under-voltage load shedding. The proposed schemes' superior performance and robustness were demonstrated through extensive case studies in both two-area four-machine systems and IEEE 39-Bus systems.",1
"How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. delivering a package)? We study this question by integrating a generic perceptual skill set (e.g. a distance estimator, an edge detector, etc.) within a reinforcement learning framework--see Figure 1. This skill set (hereafter mid-level perception) provides the policy with a more processed state of the world compared to raw images.   We find that using a mid-level perception confers significant advantages over training end-to-end from scratch (i.e. not leveraging priors) in navigation-oriented tasks. Agents are able to generalize to situations where the from-scratch approach fails and training becomes significantly more sample efficient. However, we show that realizing these gains requires careful selection of the mid-level perceptual skills. Therefore, we refine our findings into an efficient max-coverage feature set that can be adopted in lieu of raw images. We perform our study in completely separate buildings for training and testing and compare against visually blind baseline policies and state-of-the-art feature learning methods.",0
"Our study aims to determine the extent to which visual priors, such as the world being three-dimensional, aid in learning downstream motor tasks like package delivery. We achieve this by incorporating a set of generic perceptual skills, such as an edge detector and distance estimator, into a reinforcement learning framework. This set of skills, referred to as mid-level perception, provides the policy with a more refined view of the world than raw images. Our research shows that using mid-level perception offers significant benefits over training from scratch, particularly in navigation-related tasks, enabling agents to generalize to scenarios where the from-scratch approach fails and reducing the amount of training required. However, we also discovered that the selection of mid-level perceptual skills is crucial to realizing these advantages. As a result, we have developed an efficient max-coverage feature set that can be used instead of raw images. Our study was conducted in separate buildings for training and testing and compared against visually impaired baseline policies and state-of-the-art feature learning techniques.",1
"An important facet of reinforcement learning (RL) has to do with how the agent goes about exploring the environment. Traditional exploration strategies typically focus on efficiency and ignore safety. However, for practical applications, ensuring safety of the agent during exploration is crucial since performing an unsafe action or reaching an unsafe state could result in irreversible damage to the agent. The main challenge of safe exploration is that characterizing the unsafe states and actions is difficult for large continuous state or action spaces and unknown environments. In this paper, we propose a novel approach to incorporate estimations of safety to guide exploration and policy search in deep reinforcement learning. By using a cost function to capture trajectory-based safety, our key idea is to formulate the state-action value function of this safety cost as a candidate Lyapunov function and extend control-theoretic results to approximate its derivative using online Gaussian Process (GP) estimation. We show how to use these statistical models to guide the agent in unknown environments to obtain high-performance control policies with provable stability certificates.",0
"Exploration in reinforcement learning (RL) is a crucial aspect that requires attention. The traditional approach focuses on efficiency and ignores safety, which is essential for practical applications. Unsafe actions or reaching hazardous states can cause irreversible harm to the agent. However, determining unsafe states and actions is challenging in large continuous state or action spaces and unknown environments. In this study, we propose a new method to incorporate safety estimates to guide exploration and policy search in deep RL. Our approach involves using a cost function to capture trajectory-based safety, where we formulate the state-action value function of this safety cost as a candidate Lyapunov function. To approximate its derivative, we extend control-theoretic outcomes using online Gaussian Process (GP) estimation. We demonstrate how these statistical models can guide the agent in unknown environments to obtain high-performance control policies with provable stability certificates.",1
"In decision making problems for continuous state and action spaces, linear dynamical models are widely employed. Specifically, policies for stochastic linear systems subject to quadratic cost functions capture a large number of applications in reinforcement learning. Selected randomized policies have been studied in the literature recently that address the trade-off between identification and control. However, little is known about policies based on bootstrapping observed states and actions. In this work, we show that bootstrap-based policies achieve a square root scaling of regret with respect to time. We also obtain results on the accuracy of learning the model's dynamics. Corroborative numerical analysis that illustrates the technical results is also provided.",0
"Linear dynamical models are commonly utilized in solving decision-making problems with continuous state and action spaces. Stochastic linear systems with quadratic cost functions are especially applicable in reinforcement learning. Randomized policies have been explored in literature to balance identification and control trade-offs. However, policies based on bootstrapping observed states and actions have not been thoroughly investigated. Our research demonstrates that bootstrap-based policies result in a regret scaling of square root with respect to time. Additionally, we present findings on the accuracy of learning the model's dynamics, along with numerical analysis to support our results.",1
"Deep neural networks have become commonplace in the domain of reinforcement learning, but are often expensive in terms of the number of parameters needed. While compressing deep neural networks has of late assumed great importance to overcome this drawback, little work has been done to address this problem in the context of reinforcement learning agents. This work aims at making first steps towards model compression in an RL agent. In particular, we compress networks to drastically reduce the number of parameters in them (to sizes less than 3% of their original size), further facilitated by applying a global max pool after the final convolution layer, and propose using Actor-Mimic in the context of compression. Finally, we show that this global max-pool allows for weakly supervised object localization, improving the ability to identify the agent's points of focus.",0
"Reinforcement learning is a domain where deep neural networks are commonly used, but they require a large number of parameters which can be expensive. Although compressing these networks has recently become important, there is little research on this topic for reinforcement learning agents. This study aims to take the first steps towards compressing models in RL agents by significantly reducing the number of parameters (to less than 3% of their original size) through network compression and global max pooling. Additionally, Actor-Mimic is proposed for use in compression. Finally, the global max-pooling technique improves weakly supervised object localization, which enhances the agent's ability to focus.",1
"Boltzmann exploration is widely used in reinforcement learning to provide a trade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et al., 2017) it has been shown that pure Boltzmann exploration does not perform well from a regret perspective, even in the simplest setting of stochastic multi-armed bandit (MAB) problems. In this paper, we show that a simple modification to Boltzmann exploration, motivated by a variation of the standard doubling trick, achieves $O(K\log^{1+\alpha} T)$ regret for a stochastic MAB problem with $K$ arms, where $\alpha>0$ is a parameter of the algorithm. This improves on the result in (Cesa-Bianchi et al., 2017), where an algorithm inspired by the Gumbel-softmax trick achieves $O(K\log^2 T)$ regret. We also show that our algorithm achieves $O(\beta(G) \log^{1+\alpha} T)$ regret in stochastic MAB problems with graph-structured feedback, without knowledge of the graph structure, where $\beta(G)$ is the independence number of the feedback graph. Additionally, we present extensive experimental results on real datasets and applications for multi-armed bandits with both traditional bandit feedback and graph-structured feedback. In all cases, our algorithm performs as well or better than the state-of-the-art.",0
"Reinforcement learning often utilizes Boltzmann exploration to balance exploration and exploitation. However, recent research (Cesa-Bianchi et al., 2017) revealed that pure Boltzmann exploration falls short in terms of regret, even in basic stochastic multi-armed bandit (MAB) problems. This study proposes a modified Boltzmann exploration algorithm, inspired by a variation of the doubling trick, that achieves $O(K\log^{1+\alpha} T)$ regret for a MAB task with $K$ arms and $\alpha>0$. This outperforms the Gumbel-softmax trick in Cesa-Bianchi et al. (2017), which only achieves $O(K\log^2 T)$ regret. The study also demonstrates that the modified algorithm obtains $O(\beta(G) \log^{1+\alpha} T)$ regret in MAB problems with feedback graphs, without prior knowledge of the graph structure. The independence number of the feedback graph, $\beta(G)$, is used in this context. Furthermore, the study presents experimental findings on actual datasets and MAB applications with both traditional and graph-structured feedback, indicating that the modified algorithm's performance is on par with or exceeds that of the current state-of-the-art.",1
"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",0
"The optimization process of reinforcement learning agents is limited to the features stated in the reward function, disregarding anything not included. Therefore, it is crucial to specify not only what actions to take, but also what actions should be avoided. It is easy to overlook these preferences, as they are already satisfied within the environment. This led to the realization that when a robot operates in a human-inhabited environment, the state of the environment is already optimized for human preferences. Thus, we can use the implicit preference information from the state to fill in any missing information. To test this idea, we created an algorithm based on Maximum Causal Entropy IRL and evaluated it in a set of proof-of-concept environments. Our findings indicate that initial state information can be used to identify both unwanted side effects and preferred environmental organization. Our code is available at https://github.com/HumanCompatibleAI/rlsp.",1
"Within Reinforcement Learning, there is a growing collection of research which aims to express all of an agent's knowledge of the world through predictions about sensation, behaviour, and time. This work can be seen not only as a collection of architectural proposals, but also as the beginnings of a theory of machine knowledge in reinforcement learning. Recent work has expanded what can be expressed using predictions, and developed applications which use predictions to inform decision-making on a variety of synthetic and real-world problems. While promising, we here suggest that the notion of predictions as knowledge in reinforcement learning is as yet underdeveloped: some work explicitly refers to predictions as knowledge, what the requirements are for considering a prediction to be knowledge have yet to be well explored. This specification of the necessary and sufficient conditions of knowledge is important; even if claims about the nature of knowledge are left implicit in technical proposals, the underlying assumptions of such claims have consequences for the systems we design. These consequences manifest in both the way we choose to structure predictive knowledge architectures, and how we evaluate them. In this paper, we take a first step to formalizing predictive knowledge by discussing the relationship of predictive knowledge learning methods to existing theories of knowledge in epistemology. Specifically, we explore the relationships between Generalized Value Functions and epistemic notions of Justification and Truth.",0
"The field of Reinforcement Learning is producing an increasing amount of research aimed at expressing an agent's knowledge of the world through predictions about sensation, behavior, and time. This work not only proposes new architectures but also establishes a theory of machine knowledge in reinforcement learning. Recent studies have expanded the range of applications by using predictions to inform decision-making on both real-world and synthetic problems. However, the idea of predictions being knowledge is still in its early stages, and the requirements for considering a prediction to be knowledge have not been well explored. Therefore, it is essential to specify the necessary and sufficient conditions of knowledge because it affects the design and evaluation of predictive knowledge architectures. In this paper, we take the first step in formalizing predictive knowledge by examining the relationship between predictive knowledge learning methods and existing theories of knowledge in epistemology. We specifically explore the connections between Generalized Value Functions and epistemic concepts of Justification and Truth.",1
"We present a Reinforcement Learning (RL) methodology to bypass Google reCAPTCHA v3. We formulate the problem as a grid world where the agent learns how to move the mouse and click on the reCAPTCHA button to receive a high score. We study the performance of the agent when we vary the cell size of the grid world and show that the performance drops when the agent takes big steps toward the goal. Finally, we used a divide and conquer strategy to defeat the reCAPTCHA system for any grid resolution. Our proposed method achieves a success rate of 97.4% on a 100x100 grid and 96.7% on a 1000x1000 screen resolution.",0
"Our approach employs Reinforcement Learning (RL) to circumvent Google reCAPTCHA v3. We treat the problem as a grid world, where the agent is trained to navigate the mouse and click the reCAPTCHA button for a high score. By experimenting with different grid cell sizes, we observe that the agent's performance decreases with larger steps towards the objective. To overcome this, we employ a divide and conquer technique that enables us to defeat the reCAPTCHA system at any grid resolution. Our method attains a success rate of 97.4% for a 100x100 grid and 96.7% for a 1000x1000 screen resolution.",1
"Optimism about the poorly understood states and actions is the main driving force of exploration for many provably-efficient reinforcement learning algorithms. We propose optimism in the face of sensible value functions (OFVF)- a novel data-driven Bayesian algorithm to constructing Plausibility sets for MDPs to explore robustly minimizing the worst case exploration cost. The method computes policies with tighter optimistic estimates for exploration by introducing two new ideas. First, it is based on Bayesian posterior distributions rather than distribution-free bounds. Second, OFVF does not construct plausibility sets as simple confidence intervals. Confidence intervals as plausibility sets are a sufficient but not a necessary condition. OFVF uses the structure of the value function to optimize the location and shape of the plausibility set to guarantee upper bounds directly without necessarily enforcing the requirement for the set to be a confidence interval. OFVF proceeds in an episodic manner, where the duration of the episode is fixed and known. Our algorithm is inherently Bayesian and can leverage prior information. Our theoretical analysis shows the robustness of OFVF, and the empirical results demonstrate its practical promise.",0
"Many reinforcement learning algorithms rely on optimism regarding the unknown states and actions to drive their exploration efforts. To this end, we propose a new Bayesian algorithm called Optimism in the Face of Sensible Value Functions (OFVF) that constructs Plausibility sets for MDPs to enable robust exploration while minimizing worst-case exploration costs. OFVF introduces two key ideas to achieve tighter optimistic estimates for exploration. Firstly, it uses Bayesian posterior distributions instead of distribution-free bounds. Secondly, it does not create plausibility sets as simple confidence intervals, as this is not a necessary condition. Instead, OFVF optimizes the location and shape of the plausibility set based on the structure of the value function, thus guaranteeing upper bounds directly without necessarily enforcing the requirement for the set to be a confidence interval. OFVF operates in an episodic manner with a fixed, known episode duration, and can leverage prior information due to its Bayesian nature. Our theoretical analysis demonstrates the robustness of OFVF, and our empirical results show its practical promise.",1
"Efficient exploration is one of the key challenges for reinforcement learning (RL) algorithms. Most traditional sample efficiency bounds require strategic exploration. Recently many deep RL algorithms with simple heuristic exploration strategies that have few formal guarantees, achieve surprising success in many domains. These results pose an important question about understanding these exploration strategies such as $e$-greedy, as well as understanding what characterize the difficulty of exploration in MDPs. In this work we propose problem specific sample complexity bounds of $Q$ learning with random walk exploration that rely on several structural properties. We also link our theoretical results to some empirical benchmark domains, to illustrate if our bound gives polynomial sample complexity in these domains and how that is related with the empirical performance.",0
"Reinforcement learning (RL) algorithms face the challenge of efficient exploration, which has been traditionally addressed through strategic exploration and sample efficiency bounds. However, recent successes of deep RL algorithms with simple heuristic exploration strategies, such as $e$-greedy, raise questions about the nature of these strategies and the difficulty of exploration in Markov Decision Processes (MDPs). In this study, we introduce problem-specific sample complexity bounds for $Q$ learning with random walk exploration, based on various structural properties. We also analyze the empirical performance of these bounds in benchmark domains to determine their polynomial sample complexity and relationship with real-world situations.",1
"This paper addresses the problem of learning the optimal control policy for a nonlinear stochastic dynamical system with continuous state space, continuous action space and unknown dynamics. This class of problems are typically addressed in stochastic adaptive control and reinforcement learning literature using model-based and model-free approaches respectively. Both methods rely on solving a dynamic programming problem, either directly or indirectly, for finding the optimal closed loop control policy. The inherent `curse of dimensionality' associated with dynamic programming method makes these approaches also computationally difficult.   This paper proposes a novel decoupled data-based control (D2C) algorithm that addresses this problem using a decoupled, `open loop - closed loop', approach. First, an open-loop deterministic trajectory optimization problem is solved using a black-box simulation model of the dynamical system. Then, a closed loop control is developed around this open loop trajectory by linearization of the dynamics about this nominal trajectory. By virtue of linearization, a linear quadratic regulator based algorithm can be used for this closed loop control. We show that the performance of D2C algorithm is approximately optimal. Moreover, simulation performance suggests significant reduction in training time compared to other state of the art algorithms.",0
"The focus of this paper is on finding the optimal control policy for a nonlinear stochastic dynamical system with continuous state and action spaces and unknown dynamics. Traditionally, stochastic adaptive control and reinforcement learning literature has tackled this problem using model-based and model-free approaches, both of which involve solving a dynamic programming problem to determine the optimal closed loop control policy. However, these methods are computationally challenging due to the `curse of dimensionality' associated with dynamic programming. To overcome this, the paper proposes a decoupled data-based control (D2C) algorithm that takes an open loop-closed loop approach. The algorithm first solves an open-loop deterministic trajectory optimization problem using a black-box simulation model, followed by developing a closed loop control around this trajectory by linearizing the dynamics. This allows for the use of a linear quadratic regulator based algorithm for closed loop control, resulting in approximately optimal performance. Simulation results show that the D2C algorithm significantly reduces training time compared to existing state of the art algorithms.",1
"We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage-a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.",0
"The focus of our research is on generating responses in visual dialogues over multiple rounds, where the response is based on a conversation history that is grounded in visual cues. The current methods for this task follow a supervised learning approach, using a codec model to encode the visual triplet of image, Q&A history, and current question, before decoding the current answer based on ground-truth supervision. However, this approach neglects the impact of imperfect history on the conversation, leading to bias towards history rather than contextual reasoning. To address this, we present a new training paradigm called History Advantage Sequence Training (HAST), inspired by actor-critic policy gradient in reinforcement learning. HAST intentionally introduces incorrect answers into the history, creating an adverse critic to evaluate how historic errors affect the codec's future behavior. The proposed approach also includes a novel attention network called History-Aware Co-Attention Network (HACAN) to improve the codec's sensitivity to the conversation history. Experimental results on three benchmark datasets demonstrate that HAST outperforms current supervised methods consistently.",1
"Reinforcement Learning (RL) algorithms allow artificial agents to improve their action selections so as to increase rewarding experiences in their environments. Deep Reinforcement Learning algorithms require solving a nonconvex and nonlinear unconstrained optimization problem. Methods for solving the optimization problems in deep RL are restricted to the class of first-order algorithms, such as stochastic gradient descent (SGD). The major drawback of the SGD methods is that they have the undesirable effect of not escaping saddle points and their performance can be seriously obstructed by ill-conditioning. Furthermore, SGD methods require exhaustive trial and error to fine-tune many learning parameters. Using second derivative information can result in improved convergence properties, but computing the Hessian matrix for large-scale problems is not practical. Quasi-Newton methods require only first-order gradient information, like SGD, but they can construct a low rank approximation of the Hessian matrix and result in superlinear convergence. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular quasi-Newton methods that construct positive definite Hessian approximations. In this paper, we introduce an efficient optimization method, based on the limited memory BFGS quasi-Newton method using line search strategy -- as an alternative to SGD methods. Our method bridges the disparity between first order methods and second order methods by continuing to use gradient information to calculate a low-rank Hessian approximations. We provide formal convergence analysis as well as empirical results on a subset of the classic ATARI 2600 games. Our results show a robust convergence with preferred generalization characteristics, as well as fast training time and no need for the experience replaying mechanism.",0
"Artificial agents can enhance their action selections and increase rewarding experiences in their environments through Reinforcement Learning (RL) algorithms. However, Deep Reinforcement Learning algorithms face a nonconvex and nonlinear unconstrained optimization problem that requires first-order algorithms, such as stochastic gradient descent (SGD), to be used for optimization. Unfortunately, SGD methods have limitations such as being unable to escape saddle points and being obstructed by ill-conditioning. Additionally, they require exhaustive trial and error to fine-tune many learning parameters. Although second derivative information can improve convergence properties, computing the Hessian matrix for large-scale problems is not practical. Quasi-Newton methods, however, can construct low rank approximations of the Hessian matrix using only first-order gradient information, resulting in superlinear convergence. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one popular quasi-Newton method that constructs positive definite Hessian approximations. In this paper, we propose an alternative to SGD methods, an efficient optimization method based on the limited memory BFGS quasi-Newton method, using a line search strategy. Our method uses gradient information to calculate a low-rank Hessian approximation, bridging the gap between first-order and second-order methods. We provide formal convergence analysis and empirical results on a subset of classic ATARI 2600 games. Our results show that our method has robust convergence with preferred generalization characteristics, fast training time, and no need for the experience replaying mechanism.",1
"We present Simion Zoo, a Reinforcement Learning (RL) workbench that provides a complete set of tools to design, run, and analyze the results,both statistically and visually, of RL control applications. The main features that set apart Simion Zoo from similar software packages are its easy-to-use GUI, its support for distributed execution including deployment over graphics processing units (GPUs) , and the possibility to explore concurrently the RL metaparameter space, which is key to successful RL experimentation.",0
"Simion Zoo is a Reinforcement Learning (RL) workbench that offers a comprehensive range of instruments for creating, executing, and evaluating the outcomes of RL control applications, both statistically and visually. What distinguishes Simion Zoo from other software packages is its user-friendly graphical user interface (GUI), its ability to operate on distributed systems such as graphics processing units (GPUs), and the capacity to concurrently examine the RL metaparameter space, which is critical for successful RL experimentation.",1
"In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks such as machine translation, headline generation, text summarization, speech to text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder-decoder models produce competitive results, many researchers have proposed additional improvements over these sequence-to-sequence models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with sequence-to-sequence models that enable remembering long-term memories. We present some of the most recent frameworks that combine concepts from RL and deep neural networks and explain how these two areas could benefit from each other in solving complex seq2seq tasks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization.",0
"Sequence-to-sequence (seq2seq) models have become increasingly popular and are now widely used in tasks such as machine translation, headline generation, text summarization, speech to text conversion, and image caption generation. These models typically consist of a deep neural network with an encoder and a decoder, though researchers have proposed various improvements such as attention-based models, pointer-generation models, and self-attention models. Despite their usefulness, seq2seq models still face two common problems: exposure bias and inconsistency between train/test measurement. Recently, researchers have begun to address these problems by using reinforcement learning (RL) methods. In this survey, we explore how RL can be used to improve seq2seq models and provide a formulation that combines the strengths of both approaches. We examine recent frameworks that combine RL and deep neural networks and demonstrate how they can solve complex seq2seq tasks. Our work aims to provide insights into the challenges associated with existing approaches and how RL models can be used to overcome them. We also provide source code for implementing RL models in abstractive text summarization.",1
"When watching omnidirectional images (ODIs), subjects can access different viewports by moving their heads. Therefore, it is necessary to predict subjects' head fixations on ODIs. Inspired by generative adversarial imitation learning (GAIL), this paper proposes a novel approach to predict saliency of head fixations on ODIs, named SalGAIL. First, we establish a dataset for attention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset is large-scale, which contains the head fixations of 30 subjects viewing 600 ODIs. Next, we mine our AOI dataset and determine three findings: (1) The consistency of head fixations are consistent among subjects, and it grows alongside the increased subject number; (2) The head fixations exist with a front center bias (FCB); and (3) The magnitude of head movement is similar across subjects. According to these findings, our SalGAIL approach applies deep reinforcement learning (DRL) to predict the head fixations of one subject, in which GAIL learns the reward of DRL, rather than the traditional human-designed reward. Then, multi-stream DRL is developed to yield the head fixations of different subjects, and the saliency map of an ODI is generated via convoluting predicted head fixations. Finally, experiments validate the effectiveness of our approach in predicting saliency maps of ODIs, significantly better than 10 state-of-the-art approaches.",0
"To access different viewports while watching omnidirectional images (ODIs), viewers must move their heads, making it crucial to predict their head fixations. In this paper, we introduce a new method called SalGAIL, inspired by generative adversarial imitation learning (GAIL), to predict the saliency of head fixations on ODIs. Our approach involves creating a large-scale attention on ODIs (AOI) dataset, consisting of head fixations from 30 subjects viewing 600 ODIs. After analyzing the dataset, we discovered three key findings: (1) head fixations are consistent among subjects, (2) they exhibit a front center bias (FCB), and (3) the magnitude of head movement is similar across subjects. To predict the head fixations of one subject, we utilize deep reinforcement learning (DRL), in which GAIL learns the reward of DRL. Additionally, we develop multi-stream DRL to yield the head fixations of different subjects, and a saliency map of an ODI is generated via convoluting predicted head fixations. Our experiments show that SalGAIL is significantly more effective in predicting saliency maps of ODIs compared to 10 state-of-the-art approaches.",1
"In reinforcement learning (RL), temporal abstraction still remains as an important and unsolved problem. The options framework provided clues to temporal abstraction in the RL, and the option-critic architecture elegantly solved the two problems of finding options and learning RL agents in an end-to-end manner. However, it is necessary to examine whether the options learned through this method play a mutually exclusive role. In this paper, we propose a Hellinger distance regularizer, a method for disentangling options. In addition, we will shed light on various indicators from the statistical point of view to compare with the options learned through the existing option-critic architecture.",0
"Temporal abstraction remains a significant yet unresolved issue in reinforcement learning (RL). The options framework offers insights into temporal abstraction in RL, and the option-critic architecture successfully addresses the challenge of identifying options and training RL agents end-to-end. However, it is essential to investigate whether the options learned with this approach serve a mutually exclusive purpose. Thus, this paper introduces a Hellinger distance regularizer, which is a technique for disentangling options. Moreover, we will use several statistical indicators to compare the options learned with the existing option-critic architecture.",1
"Existing methods for image captioning are usually trained by cross entropy loss, which leads to exposure bias and the inconsistency between the optimizing function and evaluation metrics. Recently it has been shown that these two issues can be addressed by incorporating techniques from reinforcement learning, where one of the popular techniques is the advantage actor-critic algorithm that calculates per-token advantage by estimating state value with a parametrized estimator at the cost of introducing estimation bias. In this paper, we estimate state value without using a parametrized value estimator. With the properties of image captioning, namely, the deterministic state transition function and the sparse reward, state value is equivalent to its preceding state-action value, and we reformulate advantage function by simply replacing the former with the latter. Moreover, the reformulated advantage is extended to n-step, which can generally increase the absolute value of the mean of reformulated advantage while lowering variance. Then two kinds of rollout are adopted to estimate state-action value, which we call self-critical n-step training. Empirically we find that our method can obtain better performance compared to the state-of-the-art methods that use the sequence level advantage and parametrized estimator respectively on the widely used MSCOCO benchmark.",0
"Commonly used techniques for image captioning involve training through cross entropy loss, leading to exposure bias and inconsistencies between optimizing function and evaluation metrics. Recently, reinforcement learning has been introduced as a solution to these issues, with the advantage actor-critic algorithm being a popular choice. However, this method introduces estimation bias. In this study, we propose a method that estimates state value without using a parametrized value estimator. With the deterministic state transition function and sparse reward of image captioning, state value is equivalent to its preceding state-action value, allowing us to reformulate the advantage function. Additionally, we extend the reformulated advantage to n-step, which increases the mean of reformulated advantage while lowering variance. Our self-critical n-step training method outperforms state-of-the-art methods using sequence level advantage and parametrized estimator on the MSCOCO benchmark.",1
"Contextual policy search (CPS) is a class of multi-task reinforcement learning algorithms that is particularly useful for robotic applications. A recent state-of-the-art method is Contextual Covariance Matrix Adaptation Evolution Strategies (C-CMA-ES). It is based on the standard black-box optimization algorithm CMA-ES. There are two useful extensions of CMA-ES that we will transfer to C-CMA-ES and evaluate empirically: ACM-ES, which uses a comparison-based surrogate model, and aCMA-ES, which uses an active update of the covariance matrix. We will show that improvements with these methods can be impressive in terms of sample-efficiency, although this is not relevant any more for the robotic domain.",0
"Contextual policy search (CPS) is a type of multi-task reinforcement learning algorithms that finds practical application in robotics. A recently developed state-of-the-art approach is the Contextual Covariance Matrix Adaptation Evolution Strategies (C-CMA-ES), which builds on the CMA-ES optimization algorithm. We will investigate two beneficial modifications of CMA-ES, namely ACM-ES, which relies on a comparison-based surrogate model, and aCMA-ES, which employs an active update of the covariance matrix. Our empirical evaluation reveals that these methods can significantly enhance sample-efficiency, though this is no longer a relevant concern in the field of robotics.",1
"End-to-end deep reinforcement learning has enabled agents to learn with little preprocessing by humans. However, it is still difficult to learn stably and efficiently because the learning method usually uses a nonlinear function approximation. Neural Episodic Control (NEC), which has been proposed in order to improve sample efficiency, is able to learn stably by estimating action values using a non-parametric method. In this paper, we propose an architecture that incorporates random projection into NEC to train with more stability. In addition, we verify the effectiveness of our architecture by Atari's five games. The main idea is to reduce the number of parameters that have to learn by replacing neural networks with random projection in order to reduce dimensions while keeping the learning end-to-end.",0
"Agents can now learn with minimal human preprocessing thanks to end-to-end deep reinforcement learning. However, learning in a stable and efficient manner remains challenging due to the use of nonlinear function approximation. To improve sample efficiency, Neural Episodic Control (NEC) was introduced, utilizing non-parametric methods to estimate action values and ensure stable learning. In this study, we propose an architecture that integrates random projection into NEC for increased stability during training. Our approach aims to reduce the number of parameters to learn by replacing neural networks with random projection, which reduces dimensions while maintaining end-to-end learning. We validate our architecture on five Atari games to demonstrate its effectiveness.",1
"Reproducibility in reinforcement learning is challenging: uncontrolled stochasticity from many sources, such as the learning algorithm, the learned policy, and the environment itself have led researchers to report the performance of learned agents using aggregate metrics of performance over multiple random seeds for a single environment. Unfortunately, there are still pernicious sources of variability in reinforcement learning agents that make reporting common summary statistics an unsound metric for performance. Our experiments demonstrate the variability of common agents used in the popular OpenAI Baselines repository. We make the case for reporting post-training agent performance as a distribution, rather than a point estimate.",0
"It is difficult to achieve reproducibility in reinforcement learning due to various unpredictable sources of stochasticity, including the learning algorithm, the learned policy, and the environment. To address this issue, researchers have resorted to using aggregate performance metrics across multiple random seeds for a single environment. However, there are still underlying factors that cause variability in reinforcement learning agents, rendering common summary statistics unreliable for evaluating performance. Our research has shown the variability of commonly used agents in the OpenAI Baselines repository, emphasizing the need to report post-training agent performance as a distribution rather than a single value.",1
"Designing RNA molecules has garnered recent interest in medicine, synthetic biology, biotechnology and bioinformatics since many functional RNA molecules were shown to be involved in regulatory processes for transcription, epigenetics and translation. Since an RNA's function depends on its structural properties, the RNA Design problem is to find an RNA sequence which satisfies given structural constraints. Here, we propose a new algorithm for the RNA Design problem, dubbed LEARNA. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Methodologically, for what we believe to be the first time, we jointly optimize over a rich space of architectures for the policy network, the hyperparameters of the training procedure and the formulation of the decision process. Comprehensive empirical results on two widely-used RNA Design benchmarks, as well as a third one that we introduce, show that our approach achieves new state-of-the-art performance on the former while also being orders of magnitudes faster in reaching the previous state-of-the-art performance. In an ablation study, we analyze the importance of our method's different components.",0
"Recently, there has been growing interest in the field of medicine, synthetic biology, biotechnology, and bioinformatics regarding the design of RNA molecules. This is due to the discovery that many functional RNA molecules play a crucial role in regulatory processes such as transcription, epigenetics, and translation. As an RNA's function is dependent on its structural properties, the RNA Design problem aims to find an RNA sequence that fulfills specific structural constraints. Our newly proposed algorithm, LEARNA, tackles this problem by utilizing deep reinforcement learning to train a policy network that can design an entire RNA sequence in a sequential manner based on a specified target structure. Through meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Our method is unique in that it optimizes over a range of architectures for the policy network, hyperparameters of the training procedure, and decision-making formulation. We conducted comprehensive empirical studies on two widely-used RNA Design benchmarks, as well as a third one that we introduced, and found that our approach achieves new state-of-the-art performance on the former while being orders of magnitude faster than the previous state-of-the-art performance. Furthermore, in an ablation study, we analyzed the significance of our method's various components.",1
"Recent research on Software-Defined Networking (SDN) strongly promotes the adoption of distributed controller architectures. To achieve high network performance, designing a scheduling function (SF) to properly dispatch requests from each switch to suitable controllers becomes critical. However, existing literature tends to design the SF targeted at specific network settings. In this paper, a reinforcement-learning-based (RL) approach is proposed with the aim to automatically learn a general, effective, and efficient SF. In particular, a new dispatching system is introduced in which the SF is represented as a neural network that determines the priority of each controller. Based on the priorities, a controller is selected using our proposed probability selection scheme to balance the trade-off between exploration and exploitation during learning. In order to train a general SF, we first formulate the scheduling function design problem as an RL problem. Then a new training approach is developed based on a state-of-the-art deep RL algorithm. Our simulation results show that our RL approach can rapidly design (or learn) SFs with optimal performance. Apart from that, the trained SF can generalize well and outperforms commonly used scheduling heuristics under various network settings.",0
"Current research on Software-Defined Networking (SDN) strongly advocates for the implementation of distributed controller architectures. The scheduling function (SF) plays a crucial role in achieving optimal network performance by effectively dispatching requests from switches to appropriate controllers. However, the existing literature tends to design the SF for specific network configurations. This study proposes a reinforcement-learning-based (RL) approach to automatically acquire a general, effective, and efficient SF. A new dispatching system is introduced in which a neural network represents the SF and determines the priority of each controller. Our proposed probability selection scheme balances exploration and exploitation during learning to select a controller based on its priority. To train a general SF, we formulate the scheduling function design problem as an RL problem and develop a new training approach using a state-of-the-art deep RL algorithm. The simulation results indicate that our RL approach can swiftly design (or learn) SFs with optimal performance. Furthermore, the trained SF can generalize well and surpasses commonly used scheduling heuristics across various network settings.",1
"Larger networks generally have greater representational power at the cost of increased computational complexity. Sparsifying such networks has been an active area of research but has been generally limited to static regularization or dynamic approaches using reinforcement learning. We explore a mixture of experts (MoE) approach to deep dynamic routing, which activates certain experts in the network on a per-example basis. Our novel DeepMoE architecture increases the representational power of standard convolutional networks by adaptively sparsifying and recalibrating channel-wise features in each convolutional layer. We employ a multi-headed sparse gating network to determine the selection and scaling of channels for each input, leveraging exponential combinations of experts within a single convolutional network. Our proposed architecture is evaluated on four benchmark datasets and tasks, and we show that Deep-MoEs are able to achieve higher accuracy with lower computation than standard convolutional networks.",0
"Increased computational complexity is a tradeoff for greater representational power in larger networks. Though sparsifying networks has been researched, it has been limited to static regularization or dynamic reinforcement learning. Our approach involves using a mixture of experts (MoE) to dynamically route through the network, activating specific experts per-example. Our novel DeepMoE architecture adapts by sparsifying and recalibrating channel-wise features for each convolutional layer, increasing representational power. We use a multi-headed sparse gating network to select and scale channels for each input, utilizing exponential combinations of experts within a single convolutional network. Our architecture is evaluated on four benchmark datasets, and we demonstrate that Deep-MoEs achieve higher accuracy with lower computation than standard convolutional networks.",1
"A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves 'knowledge' from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other 'knowledge exchange' methods.",0
"Nowadays, there are numerous deep nets available for different tasks, making it harder to determine which one to use for a new task or to fine-tune a new model. To tackle this issue, we introduce knowledge flow in this paper. It involves transferring knowledge from multiple deep nets, known as teachers, to a new model called the student. The teachers and student can have different structures and be trained on distinct tasks and output spaces. After training using knowledge flow, the student becomes independent of the teachers. Our approach outperforms other knowledge exchange methods and fine-tuning on different supervised and reinforcement learning tasks.",1
"Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization. Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories. This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.",0
"High variance often causes instability in several Reinforcement Learning applications, particularly in domains with high dimensions. Machine learning commonly uses regularization to decrease variance, but it introduces some bias. While existing regularization techniques usually focus on perceptual regularization, reinforcement learning presents an opportunity to utilize temporal regularization by exploiting smoothness in value estimates over trajectories due to the Bellman equation's nature. This paper examines a category of methods for temporal regularization and formally characterizes the bias this technique induces using Markov chain concepts. We demonstrate the characteristics of temporal regularization using a series of simple discrete and continuous MDPs and show that even in high-dimensional Atari games, the technique provides improvement.",1
"Machine learning pipeline potentially consists of several stages of operations like data preprocessing, feature engineering and machine learning model training. Each operation has a set of hyper-parameters, which can become irrelevant for the pipeline when the operation is not selected. This gives rise to a hierarchical conditional hyper-parameter space. To optimize this mixed continuous and discrete conditional hierarchical hyper-parameter space, we propose an efficient pipeline search and configuration algorithm which combines the power of Reinforcement Learning and Bayesian Optimization. Empirical results show that our method performs favorably compared to state of the art methods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.",0
"The machine learning pipeline is composed of several stages, including data preprocessing, feature engineering, and machine learning model training. Each stage has its own set of hyper-parameters, which may not be relevant if the stage is not selected. This creates a hierarchical conditional hyper-parameter space. To efficiently optimize this mixed continuous and discrete conditional hierarchical hyper-parameter space, we suggest a pipeline search and configuration algorithm that combines Reinforcement Learning and Bayesian Optimization. Our method has been proven to perform well in comparison to other state-of-the-art methods such as Auto-sklearn, TPOT, Tree Parzen Window, and Random Search.",1
"Safe reinforcement learning has many variants and it is still an open research problem. Here, we focus on how to use action guidance by means of a non-expert demonstrator to avoid catastrophic events in a domain with sparse, delayed, and deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. This domain is very challenging for reinforcement learning (RL) --- past work has shown that model-free RL algorithms fail to achieve significant learning. In this paper, we shed light into the reasons behind this failure by exemplifying and analyzing the high rate of catastrophic events (i.e., suicides) that happen under random exploration in this domain. While model-free random exploration is typically futile, we propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with small number of rollouts, can be integrated to asynchronous distributed deep reinforcement learning methods. Compared to vanilla deep RL algorithms, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.",0
"The research problem of safe reinforcement learning has numerous variations and has yet to be fully resolved. This study focuses on utilizing non-expert guidance to prevent disastrous occurrences in a domain with minimal, delayed, and misleading rewards, specifically the Pommerman multi-agent benchmark. This domain poses a significant challenge for reinforcement learning, as earlier efforts in model-free RL have been unsuccessful in attaining noteworthy results. The study sheds light on the causes of these failures by examining the high frequency of catastrophic events that arise during random exploration. Rather than relying on futile model-free random exploration, the study proposes a novel approach that incorporates a non-expert simulated demonstrator, such as Monte Carlo tree search with a small number of rollouts, into asynchronous distributed deep reinforcement learning methods. Compared to plain deep RL algorithms, this approach leads to faster learning and improved convergence to better policies in a two-player mini version of the Pommerman game.",1
"Most approaches to visual scene analysis have emphasised parallel processing of the image elements. However, one area in which the sequential nature of vision is apparent, is that of segmenting multiple, potentially similar and partially occluded objects in a scene. In this work, we revisit the recurrent formulation of this challenging problem in the context of reinforcement learning. Motivated by the limitations of the global max-matching assignment of the ground-truth segments to the recurrent states, we develop an actor-critic approach in which the actor recurrently predicts one instance mask at a time and utilises the gradient from a concurrently trained critic network. We formulate the state, action, and the reward such as to let the critic model long-term effects of the current prediction and incorporate this information into the gradient signal. Furthermore, to enable effective exploration in the inherently high-dimensional action space of instance masks, we learn a compact representation using a conditional variational auto-encoder. We show that our actor-critic model consistently provides accuracy benefits over the recurrent baseline on standard instance segmentation benchmarks.",0
"The majority of visual scene analysis methods focus on processing image elements in parallel. However, when it comes to segmenting multiple objects that are partially hidden or similar, the sequential aspect of vision becomes apparent. This study re-examines the challenging task of segmenting such objects using reinforcement learning and a recurrent approach. The authors noticed limitations with assigning ground-truth segments to recurrent states and therefore developed an actor-critic method. The actor predicts one instance mask at a time and receives feedback from a concurrently trained critic network. To explore the high-dimensional action space of instance masks, the authors learned a compact representation using a conditional variational auto-encoder. The state, action, and reward were formulated to incorporate the critic's long-term effects on the prediction. The results show that the actor-critic model consistently produces higher accuracy than the recurrent baseline on standard instance segmentation benchmarks.",1
"We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural ""oracle"". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.",0
"Our research introduces Programmatically Interpretable Reinforcement Learning (PIRL), a reinforcement learning approach that produces understandable and verifiable agent policies. Unlike Deep Reinforcement Learning (DRL), which employs neural networks to represent policies, PIRL employs a high-level programming language specific to the domain. Programmatic policies are easier to interpret than neural networks and can be verified using symbolic methods. We propose a new technique called Neurally Directed Program Search (NDPS) to address the challenging issue of discovering a programmatic policy with maximum reward. NDPS uses DRL to learn a neural policy network, followed by a local search for programmatic policies that minimize the distance from the neural ""oracle."" We assess NDPS's performance by using it to teach a simulated car to drive in the TORCS car-racing environment. Our results demonstrate that NDPS can identify human-readable policies that meet significant performance criteria, and that PIRL policies have smoother trajectories and are more transferable to new environments than DRL policies.",1
"To optimize clinical outcomes, fertility clinics must strategically select which embryos to transfer. Common selection heuristics are formulas expressed in terms of the durations required to reach various developmental milestones, quantities historically annotated manually by experienced embryologists based on time-lapse EmbryoScope videos. We propose a new method for automatic embryo staging that exploits several sources of structure in this time-lapse data. First, noting that in each image the embryo occupies a small subregion, we jointly train a region proposal network with the downstream classifier to isolate the embryo. Notably, because we lack ground-truth bounding boxes, our we weakly supervise the region proposal network optimizing its parameters via reinforcement learning to improve the downstream classifier's loss. Moreover, noting that embryos reaching the blastocyst stage progress monotonically through earlier stages, we develop a dynamic-programming-based decoder that post-processes our predictions to select the most likely monotonic sequence of developmental stages. Our methods outperform vanilla residual networks and rival the best numbers in contemporary papers, as measured by both per-frame accuracy and transition prediction error, despite operating on smaller data than many.",0
"In order to improve the success of fertility treatments, it is crucial for clinics to carefully choose which embryos to transfer. Traditionally, experts have manually annotated embryos by observing their developmental milestones through time-lapse EmbryoScope videos. However, we propose a new approach that utilizes the structural patterns present in this data to automatically stage the embryos. Our method involves training a region proposal network and downstream classifier to isolate the embryo in each image. As we lack ground-truth bounding boxes, we weakly supervise the network using reinforcement learning to enhance the classifier's accuracy. Additionally, we have developed a dynamic-programming-based decoder that selects the most likely monotonic sequence of developmental stages, as embryos progress uniformly through these stages. Our approach outperforms vanilla residual networks and achieves comparable results to state-of-the-art methods, despite working with less data.",1
"Dense video captioning is an extremely challenging task since accurate and coherent description of events in a video requires holistic understanding of video contents as well as contextual reasoning of individual events. Most existing approaches handle this problem by first detecting event proposals from a video and then captioning on a subset of the proposals. As a result, the generated sentences are prone to be redundant or inconsistent since they fail to consider temporal dependency between events. To tackle this challenge, we propose a novel dense video captioning framework, which models temporal dependency across events in a video explicitly and leverages visual and linguistic context from prior events for coherent storytelling. This objective is achieved by 1) integrating an event sequence generation network to select a sequence of event proposals adaptively, and 2) feeding the sequence of event proposals to our sequential video captioning network, which is trained by reinforcement learning with two-level rewards at both event and episode levels for better context modeling. The proposed technique achieves outstanding performances on ActivityNet Captions dataset in most metrics.",0
"Writing accurate and coherent descriptions of events in a video is a difficult task that requires a comprehensive understanding of the video's contents and contextual reasoning of individual events. Most current methods for handling this problem involve detecting event proposals and captioning a subset of them. Unfortunately, this approach often leads to redundant or inconsistent sentences because it fails to consider the temporal dependency between events. To address this challenge, we have developed a novel dense video captioning framework that explicitly models temporal dependency across events and utilizes contextual information from prior events to create a coherent narrative. Our approach achieves this objective by integrating an event sequence generation network to select a sequence of event proposals adaptively and by feeding this sequence to our sequential video captioning network. Our network is trained using reinforcement learning with two-level rewards at both event and episode levels to improve context modeling. We have tested our technique on the ActivityNet Captions dataset and it has achieved outstanding performance in most metrics.",1
"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using a deep neural network as its function approximator and by learning directly from raw images. A drawback of using raw images is that deep RL must learn the state feature representation from the raw images in addition to learning a policy. As a result, deep RL can require a prohibitively large amount of training time and data to reach reasonable performance, making it difficult to use deep RL in real-world applications, especially when data is expensive. In this work, we speed up training by addressing half of what deep RL is trying to solve --- learning features. Our approach is to learn some of the important features by pre-training deep RL network's hidden layers via supervised learning using a small set of human demonstrations. We empirically evaluate our approach using deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600 games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training with human demonstrations in a supervised learning manner is better at discovering features relative to pre-training naively in DQN, and 2) initializing a deep RL network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of human demonstrations.",0
"By using a deep neural network as a function approximator and learning directly from raw images, deep reinforcement learning (deep RL) has excelled in accomplishing intricate sequential tasks. However, a disadvantage of using raw images is that deep RL must not only learn a policy but also learn the state feature representation from the raw images, which can necessitate a considerable amount of training time and data to achieve satisfactory performance. This can make it challenging to use deep RL in real-world applications, particularly when data is costly. Thus, in this study, our aim is to expedite training by concentrating on half of what deep RL seeks to resolve, i.e., learning features. To achieve this, we pre-train the hidden layers of deep RL networks through supervised learning, using a small set of human demonstrations to learn essential features. We evaluate this approach using the deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on Pong, Freeway, and Beamrider Atari 2600 games. Our results demonstrate that our approach is superior to pre-training naively in DQN, and initializing a deep RL network with a pre-trained model leads to a significant reduction in training time, even when pre-training from a few human demonstrations.",1
"We introduce Bayesian least-squares policy iteration (BLSPI), an off-policy, model-free, policy iteration algorithm that uses the Bayesian least-squares temporal-difference (BLSTD) learning algorithm to evaluate policies. An online variant of BLSPI has been also proposed, called randomised BLSPI (RBLSPI), that improves its policy based on an incomplete policy evaluation step. In online setting, the exploration-exploitation dilemma should be addressed as we try to discover the optimal policy by using samples collected by ourselves. RBLSPI exploits the advantage of BLSTD to quantify our uncertainty about the value function. Inspired by Thompson sampling, RBLSPI first samples a value function from a posterior distribution over value functions, and then selects actions based on the sampled value function. The effectiveness and the exploration abilities of RBLSPI are demonstrated experimentally in several environments.",0
"Bayesian least-squares policy iteration (BLSPI) is a policy iteration algorithm that evaluates policies using the Bayesian least-squares temporal-difference (BLSTD) learning algorithm. It is off-policy and model-free. Additionally, a variant called randomised BLSPI (RBLSPI) has been proposed for online use, which improves policy based on incomplete policy evaluation. In the online setting, the exploration-exploitation dilemma must be addressed to discover the optimal policy using self-collected samples. RBLSPI addresses this by using BLSTD to quantify uncertainty about the value function. It first samples a value function from a posterior distribution and then selects actions based on the sampled value function, inspired by Thompson sampling. Experimental results demonstrate the effectiveness and exploration abilities of RBLSPI in various environments.",1
"Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.",0
"The technique of model quantization is commonly used to speed up and compress deep neural network (DNN) inference. However, with the emergence of DNN hardware accelerators that support mixed precision (1-8 bits), finding the optimal bitwidth for each layer poses a significant challenge. This requires domain experts to explore a vast design space, taking into account various factors such as accuracy, latency, energy, and model size. Conventional quantization algorithms overlook the distinct hardware architectures and quantize all layers uniformly. To address this, we present the Hardware-Aware Automated Quantization (HAQ) framework that employs reinforcement learning to determine the quantization policy automatically while taking the hardware accelerator's feedback into account. Rather than relying on proxy signals, we use a hardware simulator to generate direct feedback signals such as latency and energy to the RL agent. Our framework is fully automated, and we can tailor the quantization policy for different neural network and hardware architectures. Compared to conventional methods, our framework significantly reduces latency and energy consumption with negligible accuracy loss. We also identify that the optimal quantization policies for different hardware architectures and resource constraints are dramatically different, providing insights for both neural network and hardware architecture design.",1
"Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).",0
"The aim of Vision-language navigation (VLN) is to direct a physical agent to execute natural language commands in real 3D environments. This study explores three major obstacles in VLN: cross-modal grounding, ill-defined feedback, and generalization difficulties. The authors propose an innovative Reinforced Cross-Modal Matching (RCM) method that uses reinforcement learning (RL) to facilitate both local and global cross-modal grounding. The RCM model significantly outperforms previous approaches by 10% on SPL and achieves a new state-of-the-art performance. Furthermore, the authors present a Self-Supervised Imitation Learning (SIL) technique to enhance policy generalizability by emulating past successful decisions. SIL approximates an improved, more effective policy that reduces the performance gap between familiar and unfamiliar environments from 30.7% to 11.7%.",1
"Attention models have had a significant positive impact on deep learning across a range of tasks. However previous attempts at integrating attention with reinforcement learning have failed to produce significant improvements. We propose the first combination of self attention and reinforcement learning that is capable of producing significant improvements, including new state of the art results in the Arcade Learning Environment. Unlike the selective attention models used in previous attempts, which constrain the attention via preconceived notions of importance, our implementation utilises the Markovian properties inherent in the state input. Our method produces a faithful visualisation of the policy, focusing on the behaviour of the agent. Our experiments demonstrate that the trained policies use multiple simultaneous foci of attention, and are able to modulate attention over time to deal with situations of partial observability.",0
"Although attention models have positively impacted deep learning in various tasks, previous efforts to integrate attention with reinforcement learning have been unsuccessful in generating significant enhancements. Our proposal introduces a novel combination of self-attention and reinforcement learning, which has resulted in substantial improvements, including new state-of-the-art outcomes in the Arcade Learning Environment. Unlike selective attention models that limit attention based on preconceived ideas of importance, our approach employs the intrinsic Markovian characteristics of the state input. The visualization of our method is a reliable representation of the policy, highlighting the agent's behavior. Our experiments demonstrate that the trained policies utilize multiple attentional foci simultaneously and can regulate attention over time to cope with partial observability situations.",1
"Policy gradient algorithms typically combine discounted future rewards with an estimated value function, to compute the direction and magnitude of parameter updates. However, for most Reinforcement Learning tasks, humans can provide additional insight to constrain the policy learning. We introduce a general method to incorporate multiple different feedback channels into a single policy gradient loss. In our formulation, the Multi-Preference Actor Critic (M-PAC), these different types of feedback are implemented as constraints on the policy. We use a Lagrangian relaxation to satisfy these constraints using gradient descent while learning a policy that maximizes rewards. Experiments in Atari and Pendulum verify that constraints are being respected and can accelerate the learning process.",0
"Usually, policy gradient algorithms use a combination of discounted future rewards and estimated value function to determine the direction and magnitude of parameter updates. However, when it comes to most Reinforcement Learning tasks, human input can provide further guidance to restrict policy learning. Our study proposes a universal technique to integrate various feedback channels into a single policy gradient loss. With our Multi-Preference Actor Critic (M-PAC) formulation, we employ these feedback types as constraints on the policy. We use a Lagrangian relaxation method to comply with these constraints while maximizing rewards through gradient descent. Our experiments on Atari and Pendulum demonstrate that our approach respects the constraints and accelerates the learning process.",1
"Wasserstein distances are increasingly used in a wide variety of applications in machine learning. Sliced Wasserstein distances form an important subclass which may be estimated efficiently through one-dimensional sorting operations. In this paper, we propose a new variant of sliced Wasserstein distance, study the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances and draw connections with stratified sampling, and evaluate our approaches experimentally in a range of large-scale experiments in generative modelling and reinforcement learning.",0
"The use of Wasserstein distances has been growing in machine learning applications. Among them, sliced Wasserstein distances are significant as they can be efficiently estimated through one-dimensional sorting operations. This paper introduces a new variation of sliced Wasserstein distance, explores the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances, and establishes links with stratified sampling. The proposed approaches are empirically evaluated in various large-scale experiments in generative modelling and reinforcement learning.",1
"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",0
"Reinforcement learning faces a significant obstacle of developing effective policies for tasks that have few rewards. To tackle this issue, we suggest that a successful exploration strategy should aim to locate decision states in the absence of useful reward signals. These states are positioned at crucial points in the state space, allowing the agent to move to new, uncharted territories. To discover decision states, we propose using previous experience to train a goal-conditioned policy with an information bottleneck. By analyzing where the model employs the goal state, we can identify decision states. This approach is effective even in settings where observations are incomplete, since the model learns sensory cues that correspond with potential subgoals. In new environments, the model can then identify fresh subgoals for further exploration, directing the agent through a succession of possible decision states and into new areas of the state space.",1
"Deep Reinforcement Learning (DRL) algorithms are known to be data inefficient. One reason is that a DRL agent learns both the feature and the policy tabula rasa. Integrating prior knowledge into DRL algorithms is one way to improve learning efficiency since it helps to build helpful representations. In this work, we consider incorporating human knowledge to accelerate the asynchronous advantage actor-critic (A3C) algorithm by pre-training a small amount of non-expert human demonstrations. We leverage the supervised autoencoder framework and propose a novel pre-training strategy that jointly trains a weighted supervised classification loss, an unsupervised reconstruction loss, and an expected return loss. The resulting pre-trained model learns more useful features compared to independently training in supervised or unsupervised fashion. Our pre-training method drastically improved the learning performance of the A3C agent in Atari games of Pong and MsPacman, exceeding the performance of the state-of-the-art algorithms at a much smaller number of game interactions. Our method is light-weight and easy to implement in a single machine. For reproducibility, our code is available at github.com/gabrieledcjr/DeepRL/tree/A3C-ALA2019",0
"DRL algorithms are known to lack efficiency due to their data inefficiency, which is caused by the agent learning both the feature and policy tabula rasa. To enhance the learning efficiency, prior knowledge can be integrated into DRL algorithms as it assists in building useful representations. This research aims to accelerate the asynchronous advantage actor-critic (A3C) algorithm by incorporating human knowledge through pre-training a small amount of non-expert human demonstrations. The researchers propose a pre-training strategy that jointly trains a weighted supervised classification loss, an unsupervised reconstruction loss, and an expected return loss using the supervised autoencoder framework. The resulting model learns more useful features compared to independent supervised or unsupervised training. The pre-training method significantly improves the learning performance of the A3C agent in Atari games of Pong and MsPacman, surpassing the state-of-the-art algorithms with fewer game interactions. The method is simple to implement on a single machine and is available for reproducibility on github.com/gabrieledcjr/DeepRL/tree/A3C-ALA2019.",1
"We propose a new automated digital painting framework, based on a painting agent trained through reinforcement learning. To synthesize an image, the agent selects a sequence of continuous-valued actions representing primitive painting strokes, which are accumulated on a digital canvas. Action selection is guided by a given reference image, which the agent attempts to replicate subject to the limitations of the action space and the agent's learned policy. The painting agent policy is determined using a variant of proximal policy optimization reinforcement learning. During training, our agent is presented with patches sampled from an ensemble of reference images. To accelerate training convergence, we adopt a curriculum learning strategy, whereby reference patches are sampled according to how challenging they are using the current policy. We experiment with differing loss functions, including pixel-wise and perceptual loss, which have consequent differing effects on the learned policy. We demonstrate that our painting agent can learn an effective policy with a high dimensional continuous action space comprising pen pressure, width, tilt, and color, for a variety of painting styles. Through a coarse-to-fine refinement process our agent can paint arbitrarily complex images in the desired style.",0
"Our proposed framework involves an automated digital painting approach that utilizes a painting agent that has been trained through reinforcement learning. The agent performs its task by selecting a series of continuous-valued actions that represent primitive painting strokes, and these actions are then accumulated on a digital canvas to create an image. To ensure that the image replicates the given reference image, the agent's action selection is guided by it, while accounting for the limitations of the action space and the learned policy. We employ a variant of proximal policy optimization reinforcement learning to determine the painting agent's policy. During the training phase, we present the agent with patches taken from a group of reference images. To quicken the training process, we adopt a curriculum learning strategy that samples reference patches based on their level of difficulty relative to the current policy. We experiment with different loss functions, including pixel-wise and perceptual loss, which affect the learned policy differently. Our painting agent can create images in various painting styles, using a high-dimensional continuous action space that comprises pen pressure, width, tilt, and color. Through a coarse-to-fine refinement process, the agent can generate complex images in the desired style.",1
"An important goal of research in Deep Reinforcement Learning in mobile robotics is to train agents capable of solving complex tasks, which require a high level of scene understanding and reasoning from an egocentric perspective. When trained from simulations, optimal environments should satisfy a currently unobtainable combination of high-fidelity photographic observations, massive amounts of different environment configurations and fast simulation speeds. In this paper we argue that research on training agents capable of complex reasoning can be simplified by decoupling from the requirement of high fidelity photographic observations. We present a suite of tasks requiring complex reasoning and exploration in continuous, partially observable 3D environments. The objective is to provide challenging scenarios and a robust baseline agent architecture that can be trained on mid-range consumer hardware in under 24h. Our scenarios combine two key advantages: (i) they are based on a simple but highly efficient 3D environment (ViZDoom) which allows high speed simulation (12000fps); (ii) the scenarios provide the user with a range of difficulty settings, in order to identify the limitations of current state of the art algorithms and network architectures. We aim to increase accessibility to the field of Deep-RL by providing baselines for challenging scenarios where new ideas can be iterated on quickly. We argue that the community should be able to address challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.",0
"The main objective of Deep Reinforcement Learning research in mobile robotics is to train agents that can solve complex tasks requiring a high level of scene understanding and reasoning from an egocentric perspective. However, optimal environments for training these agents from simulations require a combination of high-fidelity photographic observations, various environment configurations, and fast simulation speeds that are currently unobtainable. In this article, we propose a simplified approach to training agents capable of complex reasoning by eliminating the need for high fidelity photographic observations. We present a range of challenging tasks that require complex reasoning and exploration in continuous, partially observable 3D environments, using a simple but efficient 3D environment (ViZDoom) that allows for high-speed simulations (12000fps). Our objective is to provide a robust baseline agent architecture and challenging scenarios that can be trained on mid-range consumer hardware in under 24 hours, making Deep-RL research more accessible to the community. We believe that the community can overcome challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.",1
"We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study \citep{bellemare2017distributional}. In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cram{\'e}r distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using the similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexities of above three algorithms are linear w.r.t.\ the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.",0
"Our approach involves introducing a distributional version of gradient temporal-difference (TD) learning. Recent research has demonstrated the superiority of distributional reinforcement learning over the regular approach (Bellemare et al., 2017). In the context of policy evaluation, we have developed two fresh algorithms, namely distributional GTD2 and distributional TDC. These algorithms use the Cram{\'e}r distance on the distributional variation of the Bellman error objective function, combining the benefits of nonlinear gradient TD algorithms and distributional RL techniques. For control settings, we have proposed the distributional Greedy-GQ, utilizing a similar derivation. We have also shown that distributional GTD2 and TDC converge almost surely to a local optimal solution for general smooth function approximators, including neural networks that have been widely used to tackle real-world RL challenges. These algorithms have a linear computational complexity with respect to the number of parameters in the function approximator, making them efficient for implementation in neural networks.",1
"In this paper, we study a multi-step interactive recommendation problem, where the item recommended at current step may affect the quality of future recommendations. To address the problem, we develop a novel and effective approach, named CFRL, which seamlessly integrates the ideas of both collaborative filtering (CF) and reinforcement learning (RL). More specifically, we first model the recommender-user interactive recommendation problem as an agent-environment RL task, which is mathematically described by a Markov decision process (MDP). Further, to achieve collaborative recommendations for the entire user community, we propose a novel CF-based MDP by encoding the states of all users into a shared latent vector space. Finally, we propose an effective Q-network learning method to learn the agent's optimal policy based on the CF-based MDP. The capability of CFRL is demonstrated by comparing its performance against a variety of existing methods on real-world datasets.",0
"The focus of this paper is on a multi-step interactive recommendation problem that involves the potential impact of current recommendations on future ones. To tackle this issue, we introduce a new and efficient approach called CFRL, which amalgamates concepts from both collaborative filtering (CF) and reinforcement learning (RL). Firstly, we model the problem of recommender-user interaction as an agent-environment RL task, which is represented mathematically by a Markov decision process (MDP). Secondly, we propose a CF-based MDP that involves a shared latent vector space for all users to achieve collaborative recommendations. Finally, we suggest a Q-network learning method to acquire the optimal policy of the agent based on the CF-based MDP. We demonstrate the effectiveness of CFRL by comparing its performance with other existing methods on actual datasets.",1
"Recognition of defects in concrete infrastructure, especially in bridges, is a costly and time consuming crucial first step in the assessment of the structural integrity. Large variation in appearance of the concrete material, changing illumination and weather conditions, a variety of possible surface markings as well as the possibility for different types of defects to overlap, make it a challenging real-world task. In this work we introduce the novel COncrete DEfect BRidge IMage dataset (CODEBRIM) for multi-target classification of five commonly appearing concrete defects. We investigate and compare two reinforcement learning based meta-learning approaches, MetaQNN and efficient neural architecture search, to find suitable convolutional neural network architectures for this challenging multi-class multi-target task. We show that learned architectures have fewer overall parameters in addition to yielding better multi-target accuracy in comparison to popular neural architectures from the literature evaluated in the context of our application.",0
"Detecting defects in concrete infrastructure, particularly in bridges, is a crucial initial step in assessing their structural integrity, but it can be expensive and time-consuming due to various challenges. These challenges include the concrete's diverse appearance, varying lighting and weather conditions, different surface markings, and the possibility of overlapping defects. To address this issue, we created the COncrete DEfect BRidge IMage dataset (CODEBRIM) to classify five common concrete defects. We explored two reinforcement learning-based meta-learning techniques, MetaQNN and efficient neural architecture search, to identify suitable convolutional neural network models for this complex multi-target classification task. Our results demonstrate that the learned models have fewer parameters and achieve better multi-target accuracy than existing neural architectures evaluated in our specific application.",1
"We propose a general-purpose approach to discovering active learning (AL) strategies from data. These strategies are transferable from one domain to another and can be used in conjunction with many machine learning models. To this end, we formalize the annotation process as a Markov decision process, design universal state and action spaces and introduce a new reward function that precisely model the AL objective of minimizing the annotation cost. We seek to find an optimal (non-myopic) AL strategy using reinforcement learning. We evaluate the learned strategies on multiple unrelated domains and show that they consistently outperform state-of-the-art baselines.",0
"Our proposal presents a versatile method for identifying active learning (AL) tactics through data that can be applied across domains and combined with various machine learning models. To achieve this, we establish the annotation process as a Markov decision process, create broad state and action spaces, and introduce a novel reward function that accurately reflects the AL goal of reducing annotation costs. Our aim is to utilize reinforcement learning to identify the optimal (non-myopic) AL strategy. We test the learned strategies in several distinct domains and demonstrate that they consistently outperform existing state-of-the-art methods.",1
"As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.",0
"The increased use of deep reinforcement learning via visual perception has created a growing need to better comprehend and investigate the learned agents. By understanding the decision-making process and its connection to visual inputs, it becomes possible to identify problems within the learned behavior. However, this topic has been relatively neglected in the research community. In this study, we present a method for producing visual inputs that are pertinent to a trained agent. These inputs or states could depict situations that necessitate specific actions. Additionally, critical states that offer high or low rewards are valuable in gauging the system's situational awareness since they may correspond to risky states. To achieve this, we trained a generative model over the environment's state space and utilized its latent space to optimize a target function for the state of interest. Our experiments demonstrate that this method can provide insights for various environments and reinforcement learning methods. We tested this approach on both the standard Atari benchmark games and an autonomous driving simulator, and we were able to identify behavioral weaknesses in an efficient manner. Therefore, we believe that this general approach could be a vital tool for AI safety applications.",1
"In this paper we present our scientific discovery that good representation can be learned via continuous attention during the interaction between Unsupervised Learning(UL) and Reinforcement Learning(RL) modules driven by intrinsic motivation. Specifically, we designed intrinsic rewards generated from UL modules for driving the RL agent to focus on objects for a period of time and to learn good representations of objects for later object recognition task. We evaluate our proposed algorithm in both with and without extrinsic reward settings. Experiments with end-to-end training in simulated environments with applications to few-shot object recognition demonstrated the effectiveness of the proposed algorithm.",0
"Our scientific breakthrough is revealed in this paper, which shows that continuous attention during the interaction between Unsupervised Learning (UL) and Reinforcement Learning (RL) modules driven by intrinsic motivation can lead to the acquisition of good representation. The intrinsic rewards generated from the UL modules are designed to prompt the RL agent to concentrate on objects for a specified duration and attain a strong understanding of them for future object recognition tasks. Our algorithm is tested with and without extrinsic rewards, and the effectiveness of the proposed algorithm is demonstrated through end-to-end training in simulated environments with applications to few-shot object recognition.",1
"Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary.",0
"Significant advancements have been made in deep reinforcement learning for solving computer games and robotic control tasks. However, policies developed through this method tend to overfit to the training environment and may fail to prevent rare, catastrophic events such as automotive accidents. To address this issue, researchers have traditionally trained reinforcement learning algorithms on a set of randomized environments, although this approach only offers protection against common situations. Recently, robust adversarial reinforcement learning (RARL) has emerged as a more efficient method for applying both random and systematic perturbations through a trained adversary. However, RARL only optimizes the expected control objective without explicitly modeling or optimizing risk, meaning that agents do not consider the probability of catastrophic events except through their impact on the expected objective. To address this limitation, this paper introduces risk-averse robust adversarial reinforcement learning (RARARL) by using a risk-averse protagonist and a risk-seeking adversary. The approach is tested on a self-driving vehicle controller using an ensemble of policy networks to model risk as the variance of value functions. Results show that a risk-averse agent is better equipped to handle a risk-seeking adversary and experiences fewer crashes than agents trained without an adversary.",1
"Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.",0
"Unsupervised representation learning through mutual information maximization has been highly effective in achieving state-of-the-art performance in object recognition, speech recognition, and reinforcement learning. However, the use of mutual information as a learning objective is limited due to the exponential sample size required to achieve a tight lower bound of mutual information. This presents a challenge for high-mutual-information prediction tasks like video understanding or reinforcement learning, where overfitting can occur, and only a few relevant factors of variation are captured, resulting in incomplete representations that are suboptimal for downstream tasks. Empirical evidence shows that mutual information-based representation learning approaches fail to produce complete representations in designed and real-world tasks. To address this limitation, the Wasserstein dependency measure uses the Wasserstein distance instead of KL divergence in mutual information estimation, resulting in more complete representations. A practical approximation to this solution, which uses Lipschitz constraint techniques from the GAN literature, demonstrated significant improvement in tasks where incomplete representations are a major challenge.",1
"Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world",0
"Reinforcement learning (RL) algorithms have achieved impressive success in learning complex behaviors from minimal input, primarily in simulations. Despite some progress in using RL algorithms on real systems, their performance on more intricate systems is limited due to RL algorithm's relative data inefficiency. Domain randomization is a promising research direction that has shown impressive results in controlling real robots using RL algorithms. This method involves training a policy on a distribution of environmental conditions in simulations, which, if diverse enough, can generalize to the real world. However, the form and parameters of the simulated environment distribution have a significant impact on the trained policies' real-world performance. Previous research relied on hand-tuned distributions, but this extended abstract demonstrates that optimizing the parameter of the distribution can maximize the trained policies' real-world performance.",1
"Reinforcement learning algorithms rely on exploration to discover new behaviors, which is typically achieved by following a stochastic policy. In continuous control tasks, policies with a Gaussian distribution have been widely adopted. Gaussian exploration however does not result in smooth trajectories that generally correspond to safe and rewarding behaviors in practical tasks. In addition, Gaussian policies do not result in an effective exploration of an environment and become increasingly inefficient as the action rate increases. This contributes to a low sample efficiency often observed in learning continuous control tasks. We introduce a family of stationary autoregressive (AR) stochastic processes to facilitate exploration in continuous control domains. We show that proposed processes possess two desirable features: subsequent process observations are temporally coherent with continuously adjustable degree of coherence, and the process stationary distribution is standard normal. We derive an autoregressive policy (ARP) that implements such processes maintaining the standard agent-environment interface. We show how ARPs can be easily used with the existing off-the-shelf learning algorithms. Empirically we demonstrate that using ARPs results in improved exploration and sample efficiency in both simulated and real world domains, and, furthermore, provides smooth exploration trajectories that enable safe operation of robotic hardware.",0
"Exploration is a crucial component of reinforcement learning algorithms that involves discovering new behaviors by following a stochastic policy. Gaussian distribution policies are commonly used in continuous control tasks but do not necessarily produce safe or rewarding behaviors. Furthermore, Gaussian exploration becomes less efficient as the action rate increases, leading to low sample efficiency in learning continuous control tasks. To address these issues, we propose a family of stationary autoregressive stochastic processes that facilitate exploration in continuous control domains. These processes exhibit two desirable properties: temporally coherent observations and a standard normal stationary distribution. We introduce an autoregressive policy that implements these processes and maintains the standard agent-environment interface. We demonstrate the effectiveness of this approach through empirical experiments in simulated and real-world domains, where we observe improved exploration, sample efficiency, and safe operation of robotic hardware.",1
"Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.",0
"The typical method for learning optimal behavior from an expert's demonstrations assumes that the learner understands the key features of the reward function. However, in this study, we examine the scenario where this is not the case - where the learner and expert have different perspectives. We introduce the concept of teaching risk, which measures the risk of suboptimal policies that may appear optimal to the learner. By establishing bounds on teaching risk, we demonstrate that standard algorithms based on inverse reinforcement learning can produce near-optimal policies. Additionally, we propose a teaching scheme in which the expert can reduce teaching risk by adjusting the learner's perspective, ultimately leading to a near-optimal policy.",1
"Learning is an inherently continuous phenomenon. When humans learn a new task there is no explicit distinction between training and inference. As we learn a task, we keep learning about it while performing the task. What we learn and how we learn it varies during different stages of learning. Learning how to learn and adapt is a key property that enables us to generalize effortlessly to new settings. This is in contrast with conventional settings in machine learning where a trained model is frozen during inference. In this paper we study the problem of learning to learn at both training and test time in the context of visual navigation. A fundamental challenge in navigation is generalization to unseen scenes. In this paper we propose a self-adaptive visual navigation method (SAVN) which learns to adapt to new environments without any explicit supervision. Our solution is a meta-reinforcement learning approach where an agent learns a self-supervised interaction loss that encourages effective navigation. Our experiments, performed in the AI2-THOR framework, show major improvements in both success rate and SPL for visual navigation in novel scenes. Our code and data are available at: https://github.com/allenai/savn .",0
"The process of learning is ongoing and uninterrupted. When humans acquire a new skill, there is no clear distinction between the training phase and the inference phase. We continue to gather knowledge about the task as we perform it, and the way we learn and what we learn changes during different stages of the learning process. The ability to learn how to learn and adapt is crucial in order to effortlessly apply our knowledge to new situations. This is different from conventional machine learning approaches where the model is fixed once it has been trained. In this research, we explore the problem of learning to learn during both training and testing phases in the context of visual navigation. An important challenge in navigation is being able to generalize to new environments. To address this, we propose a self-adaptive visual navigation method (SAVN) that can adapt to new surroundings without explicit guidance. Our method is based on meta-reinforcement learning, where an agent learns through a self-supervised interaction loss that encourages effective navigation. Our experiments, conducted using the AI2-THOR framework, demonstrate significant improvements in both success rate and SPL for visual navigation in unfamiliar scenes. The code and data are available at https://github.com/allenai/savn.",1
"Estimating over-amplification of human epidermal growth factor receptor 2 (HER2) on invasive breast cancer (BC) is regarded as a significant predictive and prognostic marker. We propose a novel deep reinforcement learning (DRL) based model that treats immunohistochemical (IHC) scoring of HER2 as a sequential learning task. For a given image tile sampled from multi-resolution giga-pixel whole slide image (WSI), the model learns to sequentially identify some of the diagnostically relevant regions of interest (ROIs) by following a parameterized policy. The selected ROIs are processed by recurrent and residual convolution networks to learn the discriminative features for different HER2 scores and predict the next location, without requiring to process all the sub-image patches of a given tile for predicting the HER2 score, mimicking the histopathologist who would not usually analyze every part of the slide at the highest magnification. The proposed model incorporates a task-specific regularization term and inhibition of return mechanism to prevent the model from revisiting the previously attended locations. We evaluated our model on two IHC datasets: a publicly available dataset from the HER2 scoring challenge contest and another dataset consisting of WSIs of gastroenteropancreatic neuroendocrine tumor sections stained with Glo1 marker. We demonstrate that the proposed model outperforms other methods based on state-of-the-art deep convolutional networks. To the best of our knowledge, this is the first study using DRL for IHC scoring and could potentially lead to wider use of DRL in the domain of computational pathology reducing the computational burden of the analysis of large multigigapixel histology images.",0
"A significant predictive and prognostic marker for invasive breast cancer (BC) is the estimation of over-amplification of human epidermal growth factor receptor 2 (HER2). Our proposed novel model utilizes deep reinforcement learning (DRL) to treat the immunohistochemical (IHC) scoring of HER2 as a sequential learning task. By following a parameterized policy, the model learns to identify diagnostically relevant regions of interest (ROIs) in a given image tile sampled from a multi-resolution giga-pixel whole slide image (WSI). Recurrent and residual convolution networks process the selected ROIs to learn discriminative features for different HER2 scores and predict the next location, mimicking the process of a histopathologist who would not analyze every part of the slide at the highest magnification. The proposed model includes a task-specific regularization term and inhibition of return mechanism to prevent revisiting previously attended locations. Our evaluation on two IHC datasets demonstrates that the proposed model outperforms other methods based on state-of-the-art deep convolutional networks. This is the first study using DRL for IHC scoring and could potentially lead to wider use of DRL in the domain of computational pathology, reducing the computational burden of analyzing large multigigapixel histology images.",1
"In autonomous embedded systems, it is often vital to reduce the amount of actions taken in the real world and energy required to learn a policy. Training reinforcement learning agents from high dimensional image representations can be very expensive and time consuming. Autoencoders are deep neural network used to compress high dimensional data such as pixelated images into small latent representations. This compression model is vital to efficiently learn policies, especially when learning on embedded systems. We have implemented this model on the NVIDIA Jetson TX2 embedded GPU, and evaluated the power consumption, throughput, and energy consumption of the autoencoders for various CPU/GPU core combinations, frequencies, and model parameters. Additionally, we have shown the reconstructions generated by the autoencoder to analyze the quality of the generated compressed representation and also the performance of the reinforcement learning agent. Finally, we have presented an assessment of the viability of training these models on embedded systems and their usefulness in developing autonomous policies. Using autoencoders, we were able to achieve 4-5 $\times$ improved performance compared to a baseline RL agent with a convolutional feature extractor, while using less than 2W of power.",0
"Reducing the number of actions taken in the real world and the energy required for policy learning is crucial in autonomous embedded systems. However, training reinforcement learning agents from high dimensional image representations can be both expensive and time-consuming. To address this issue, autoencoders, which are deep neural networks used to compress high dimensional data into small latent representations, are employed. This compression model is vital for efficient policy learning, particularly in embedded systems. Our study involved implementing this model on the NVIDIA Jetson TX2 embedded GPU and analyzing the power consumption, throughput, and energy consumption of the autoencoders for various CPU/GPU core combinations, frequencies, and model parameters. Furthermore, we evaluated the quality of the generated compressed representation and the performance of the reinforcement learning agent by analyzing the reconstructions generated by the autoencoder. Ultimately, we assessed the feasibility of training these models on embedded systems and their usefulness in developing autonomous policies. Our results show that using autoencoders, we were able to achieve 4-5 times better performance than a baseline RL agent with a convolutional feature extractor while using less than 2W of power.",1
"In this paper we study a new reinforcement learning setting where the environment is non-rewarding, contains several possibly related objects of various controllability, and where an apt agent Bob acts independently, with non-observable intentions. We argue that this setting defines a realistic scenario and we present a generic discrete-state discrete-action model of such environments. To learn in this environment, we propose an unsupervised reinforcement learning agent called CLIC for Curriculum Learning and Imitation for Control. CLIC learns to control individual objects in its environment, and imitates Bob's interactions with these objects. It selects objects to focus on when training and imitating by maximizing its learning progress. We show that CLIC is an effective baseline in our new setting. It can effectively observe Bob to gain control of objects faster, even if Bob is not explicitly teaching. It can also follow Bob when he acts as a mentor and provides ordered demonstrations. Finally, when Bob controls objects that the agent cannot, or in presence of a hierarchy between objects in the environment, we show that CLIC ignores non-reproducible and already mastered interactions with objects, resulting in a greater benefit from imitation.",0
"The objective of this paper is to examine a novel reinforcement learning scenario in which the environment lacks rewards, comprises multiple possibly correlated objects with varying degrees of controllability, and involves an independent agent Bob with non-observable intentions. We contend that this situation mirrors real-life settings and therefore present a comprehensive model of such environments with discrete-state and discrete-action components. To enable learning in such a context, we introduce an unsupervised reinforcement learning agent, CLIC (Curriculum Learning and Imitation for Control), which is capable of managing individual objects in the surroundings and emulating Bob's interactions with them. CLIC selects specific objects for training and imitation to optimize its learning progression. Our findings indicate that CLIC is a reliable benchmark in this new environment; it can efficiently monitor Bob's actions to gain control of objects more quickly, even in the absence of explicit instruction. It can also track Bob's guidance when he serves as a mentor and provides ordered demonstrations. Additionally, when Bob controls objects that the agent cannot, or when a hierarchy exists among objects in the environment, CLIC refrains from duplicating already mastered interactions with objects, resulting in a greater benefit from imitation.",1
"With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the ""gist"" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.",0
"The human visual system can quickly comprehend a complex scene with just a brief glance, thanks to its ability to recognize the ""gist"" of the environment based on prior knowledge. This paper explores whether incorporating memory into computer vision systems can enhance object detection accuracy and reduce computation time. By integrating lightweight feature extractors that only identify the scene's gist with traditional ones, we demonstrate that minimal computation is necessary for precise detections when temporal memory is present. Furthermore, our model's memory holds sufficient data for implementing reinforcement learning algorithms to learn an adaptive inference policy. Our approach achieves top-notch performance on the Imagenet VID 2015 dataset using mobile devices such as a Pixel 3 phone, operating at speeds of over 70 FPS.",1
"Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",0
"Reinforcement learning faces a major challenge in efficient exploration. One reason for this is that the variability of the rewards depends on the current state and action, making it heteroscedastic. Traditional exploration methods such as upper confidence bound algorithms and Thompson sampling fail to account for heteroscedasticity, even in the bandit setting. To address this issue, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning, building on recent findings in the bandit setting. Our approach utilizes advances in distributional reinforcement learning to create a novel, tractable approximation of IDS for deep Q-learning. This exploration strategy accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate significant improvement over other approaches.",1
"Using reinforcement learning to learn control policies is a challenge when the task is complex with potentially long horizons. Ensuring adequate but safe exploration is also crucial for controlling physical systems. In this paper, we use temporal logic to facilitate specification and learning of complex tasks. We combine temporal logic with control Lyapunov functions to improve exploration. We incorporate control barrier functions to safeguard the exploration and deployment process. We develop a flexible and learnable system that allows users to specify task objectives and constraints in different forms and at various levels. The framework is also able to take advantage of known system dynamics and handle unknown environmental dynamics by integrating model-free learning with model-based planning.",0
"Learning control policies through reinforcement learning can be difficult when dealing with complex tasks that have long horizons. Additionally, ensuring safe exploration is crucial when controlling physical systems. This study utilizes temporal logic to enhance the specification and learning of complex tasks. Control Lyapunov functions are combined with temporal logic to improve exploration, and control barrier functions are implemented to ensure safe exploration and deployment. The system is flexible and learnable, allowing users to specify task objectives and constraints in different forms and at various levels. It can also integrate model-free learning with model-based planning to handle unknown environmental dynamics while taking advantage of known system dynamics.",1
"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model 2) they are an ""end-to-end"" approach, directly optimizing the performance metric of interest 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties. This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities.",0
"Reinforcement learning and continuous control problems often use direct policy gradient methods, which have gained popularity due to their ease of implementation without knowledge of the underlying model, their direct optimization of performance metrics, and their ability to handle complex policies. However, a major drawback is that these methods require solving non-convex optimization problems, which can be computationally and statistically inefficient. In contrast, optimal control theory's system identification and model based planning have a stronger theoretical foundation and are more efficient. Nevertheless, recent work has shown that policy gradient methods can globally converge to optimal solutions and be efficient in terms of sample and computational complexities.",1
"Reinforcement learning algorithms can be used to optimally solve dynamic decision-making and control problems. With continuous-valued state and input variables, reinforcement learning algorithms must rely on function approximators to represent the value function and policy mappings. Commonly used numerical approximators, such as neural networks or basis function expansions, have two main drawbacks: they are black-box models offering no insight in the mappings learned, and they require significant trial and error tuning of their meta-parameters. In this paper, we propose a new approach to constructing smooth value functions by means of symbolic regression. We introduce three off-line methods for finding value functions based on a state transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. The methods are illustrated on four nonlinear control problems: velocity control under friction, one-link and two-link pendulum swing-up, and magnetic manipulation. The results show that the value functions not only yield well-performing policies, but also are compact, human-readable and mathematically tractable. This makes them potentially suitable for further analysis of the closed-loop system. A comparison with alternative approaches using neural networks shows that our method constructs well-performing value functions with substantially fewer parameters.",0
"Dynamic decision-making and control problems can be optimally solved using reinforcement learning algorithms. However, when dealing with continuous-valued state and input variables, these algorithms require function approximators to represent the value function and policy mappings. Unfortunately, commonly used numerical approximators like neural networks or basis function expansions have two main disadvantages: they offer no insight into the learned mappings and need extensive trial and error tuning of their meta-parameters. To address these issues, this paper proposes a new approach that uses symbolic regression to construct smooth value functions. The authors introduce three off-line methods to find value functions based on a state transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. These methods are demonstrated on four nonlinear control problems, and the results show that the value functions are not only well-performing but also compact, human-readable, and mathematically tractable. This makes them ideal for further analysis of the closed-loop system. Additionally, the authors compared their method with alternative approaches using neural networks and found that their method constructs well-performing value functions with substantially fewer parameters.",1
"In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.",0
"In more intricate transfer learning situations, it is possible that new tasks may not have a strong connection to previous tasks. This means that methods that only transfer information from the final parameters of a source model may not be effective. To address this, a higher level of abstraction is necessary. The Leap framework, which transfers knowledge across learning processes, is proposed to address this issue. Each task is linked to a manifold, and a meta-learning objective is constructed to minimize the expected length of the path travelled in the training process. This method only utilizes information obtained during training and is inexpensive to compute. It is demonstrated that Leap outperforms other methods in both meta-learning and transfer learning, particularly in computer vision tasks. Additionally, it is shown that Leap can successfully transfer knowledge across learning processes in complex reinforcement learning environments such as Atari, which involves millions of gradient steps.",1
"The recommender system is an important form of intelligent application, which assists users to alleviate from information redundancy. Among the metrics used to evaluate a recommender system, the metric of conversion has become more and more important. The majority of existing recommender systems perform poorly on the metric of conversion due to its extremely sparse feedback signal. To tackle this challenge, we propose a deep hierarchical reinforcement learning based recommendation framework, which consists of two components, i.e., high-level agent and low-level agent. The high-level agent catches long-term sparse conversion signals, and automatically sets abstract goals for low-level agent, while the low-level agent follows the abstract goals and interacts with real-time environment. To solve the inherent problem in hierarchical reinforcement learning, we propose a novel deep hierarchical reinforcement learning algorithm via multi-goals abstraction (HRL-MG). Our proposed algorithm contains three characteristics: 1) the high-level agent generates multiple goals to guide the low-level agent in different stages, which reduces the difficulty of approaching high-level goals; 2) different goals share the same state encoder parameters, which increases the update frequency of the high-level agent and thus accelerates the convergence of our proposed algorithm; 3) an appreciate benefit assignment function is designed to allocate rewards in each goal so as to coordinate different goals in a consistent direction. We evaluate our proposed algorithm based on a real-world e-commerce dataset and validate its effectiveness.",0
"The recommender system is a useful intelligent application that helps users avoid receiving repetitive information. One of the crucial metrics used to assess recommender systems is conversion, which has become increasingly vital. Unfortunately, most existing recommender systems are not efficient in achieving the conversion metric due to the scarce feedback signal. To tackle this issue, we introduce a deep hierarchical reinforcement learning-based recommendation framework with two agents; a high-level and a low-level agent. The high-level agent captures long-term sparse conversion signals and establishes abstract goals for the low-level agent, which then interacts with the real-time environment. To overcome inherent hierarchical reinforcement learning problems, we propose a new algorithm via multi-goals abstraction (HRL-MG), which has three distinct features. Firstly, the high-level agent generates multiple goals to guide the low-level agent during different stages, reducing the high-level goals' difficulty. Secondly, different objectives use the same state encoder parameters, accelerating the convergence of our proposed algorithm. Finally, we designed an appropriate benefit assignment function to coordinate different goals in a consistent direction. We tested our proposed algorithm using a real-world e-commerce dataset and validated its effectiveness.",1
"We propose Deep Q-Networks (DQN) with model-based exploration, an algorithm combining both model-free and model-based approaches that explores better and learns environments with sparse rewards more efficiently. DQN is a general-purpose, model-free algorithm and has been proven to perform well in a variety of tasks including Atari 2600 games since it's first proposed by Minh et el. However, like many other reinforcement learning (RL) algorithms, DQN suffers from poor sample efficiency when rewards are sparse in an environment. As a result, most of the transitions stored in the replay memory have no informative reward signal, and provide limited value to the convergence and training of the Q-Network. However, one insight is that these transitions can be used to learn the dynamics of the environment as a supervised learning problem. The transitions also provide information of the distribution of visited states. Our algorithm utilizes these two observations to perform a one-step planning during exploration to pick an action that leads to states least likely to be seen, thus improving the performance of exploration. We demonstrate our agent's performance in two classic environments with sparse rewards in OpenAI gym: Mountain Car and Lunar Lander.",0
"An algorithm called Deep Q-Networks (DQN) with model-based exploration is proposed to efficiently learn environments with sparse rewards by combining model-free and model-based approaches. DQN is a widely used model-free algorithm that has performed well in various tasks, including Atari 2600 games. However, like other reinforcement learning algorithms, DQN has poor sample efficiency when rewards are scarce, resulting in transitions stored in the replay memory with no informative reward signal. Nevertheless, these transitions can be used for supervised learning to learn the environment's dynamics and the distribution of visited states. Our algorithm leverages these observations to perform one-step planning during exploration, selecting actions that lead to the least likely states to improve exploration performance. We showcase our agent's performance on two classic sparse reward environments in OpenAI gym: Mountain Car and Lunar Lander.",1
"In this paper, we propose a distributed off-policy actor critic method to solve multi-agent reinforcement learning problems. Specifically, we assume that all agents keep local estimates of the global optimal policy parameter and update their local value function estimates independently. Then, we introduce an additional consensus step to let all the agents asymptotically achieve agreement on the global optimal policy function. The convergence analysis of the proposed algorithm is provided and the effectiveness of the proposed algorithm is validated using a distributed resource allocation example. Compared to relevant distributed actor critic methods, here the agents do not share information about their local tasks, but instead they coordinate to estimate the global policy function.",0
"This paper presents a method for solving multi-agent reinforcement learning problems by proposing a distributed off-policy actor critic approach. The method involves each agent maintaining a local estimate of the global optimal policy parameter and independently updating their local value function estimates. Additionally, a consensus step is introduced to allow all agents to achieve agreement on the global optimal policy function. The algorithm's convergence is analyzed, and a distributed resource allocation example is used to validate its effectiveness. Unlike other distributed actor critic methods, the proposed method does not involve sharing information about local tasks but instead focuses on coordinating the estimation of the global policy function.",1
"Multi-task learning, as it is understood nowadays, consists of using one single model to carry out several similar tasks. From classifying hand-written characters of different alphabets to figuring out how to play several Atari games using reinforcement learning, multi-task models have been able to widen their performance range across different tasks, although these tasks are usually of a similar nature. In this work, we attempt to widen this range even further, by including heterogeneous tasks in a single learning procedure. To do so, we firstly formally define a multi-network model, identifying the necessary components and characteristics to allow different adaptations of said model depending on the tasks it is required to fulfill. Secondly, employing the formal definition as a starting point, we develop an illustrative model example consisting of three different tasks (classification, regression and data sampling). The performance of this model implementation is then analyzed, showing its capabilities. Motivated by the results of the analysis, we enumerate a set of open challenges and future research lines over which the full potential of the proposed model definition can be exploited.",0
"Nowadays, multi-task learning involves using a single model to perform multiple similar tasks, such as recognizing hand-written characters or playing Atari games using reinforcement learning. However, these tasks usually have a comparable nature. In this study, we aim to expand the scope of multi-task learning by incorporating diverse tasks into a single learning process. Firstly, we define a multi-network model that outlines the necessary components and characteristics to enable various adaptations based on the required tasks. Secondly, we create an example model that includes three distinct tasks: classification, regression, and data sampling. We evaluate the performance of this model and analyze its capabilities. Based on our findings, we identify several open challenges and future research opportunities for fully exploiting the potential of our proposed model definition.",1
"Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",0
"The use of three techniques in reinforcement learning, known collectively as the `deadly triad', can lead to divergence in Q-learning algorithms. These techniques are bootstrapping, off-policy learning, and function approximation, and their impact on divergence is not yet fully understood. This paper presents a simple analysis based on a linear approximation to the Q-value updates, which sheds light on the conditions under which divergence occurs under the deadly triad. The analysis focuses on whether the leading order approximation to the deep-Q update is a contraction in the sup norm. Based on this analysis, the authors develop an algorithm that enables stable deep Q-learning for continuous control without the use of conventional tricks such as target networks, adaptive gradient optimizers, or multiple Q functions. The algorithm is shown to perform above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",1
"Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) on-line learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.   Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous car-following with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.",0
"RL algorithms have had limited success in real-world scenarios due to the lack of safety guarantees during the learning process. The absence of such guarantees could result in system failure or breakage before an optimal controller can be created. To address this issue, we propose a controller architecture that combines a model-free RL-based controller with model-based controllers that utilize control barrier functions (CBFs) and on-line learning of unknown system dynamics. This approach ensures safety during learning. Our framework uses RL algorithms to learn high-performance controllers and CBF-based controllers to guarantee safety and guide the learning process by limiting the set of explorable policies. We use Gaussian Processes (GPs) to model system dynamics and uncertainties. Our novel RL-CBF algorithm guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We tested our algorithm on an inverted pendulum and autonomous car-following with wireless vehicle-to-vehicle communication. Our algorithm achieved much greater sample efficiency in learning than other state-of-the-art algorithms and maintained safety throughout the learning process.",1
"When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.",0
"When attempting to learn policies for real-world situations, there are two significant issues to consider: (i) how to effectively use previously collected off-policy, non-optimal behavior data, and (ii) how to balance competing objectives and constraints. As a result, we have investigated the problem of batch policy learning under multiple constraints and have provided a systematic solution. Our approach includes a versatile meta-algorithm that can accommodate any batch reinforcement learning and online learning procedure as subroutines. We have also developed a specific algorithm and have provided performance guarantees for both the primary objective and all constraints. To ensure constraint satisfaction, we have introduced a straightforward method for off-policy policy evaluation (OPE) and have derived PAC-style bounds. Our algorithm has demonstrated impressive results in various domains, including a challenging simulated car driving problem with multiple constraints such as lane keeping and smooth driving. Furthermore, our OPE method has outperformed other popular OPE techniques on a standalone basis, particularly in a high-dimensional environment.",1
"Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.",0
"Learning a single task using deep reinforcement learning algorithms necessitates a vast amount of experience. Meta-reinforcement learning (meta-RL) algorithms could theoretically enable agents to acquire new skills using only a small amount of experience, but their practicality is limited by significant challenges. Current methods often rely on on-policy experience, which reduces their sample efficiency. Furthermore, they lack the ability to reason about task uncertainty when adapting to new tasks, which limits their effectiveness in solving problems with sparse rewards. To overcome these challenges, we present an off-policy meta-RL algorithm that separates task inference from control. Our method uses online probabilistic filtering of latent task variables to deduce how to tackle a new task with limited experience. This probabilistic interpretation makes structured and efficient exploration feasible through posterior sampling. We demonstrate how to combine these task variables with off-policy RL algorithms to achieve greater efficiency in both meta-training and adaptation. Our method outperforms prior algorithms in sample efficiency by 20-100X and asymptotic performance on numerous meta-RL benchmarks.",1
