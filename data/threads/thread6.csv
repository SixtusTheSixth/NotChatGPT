"Face parsing is an important problem in computer vision that finds numerous applications including recognition and editing. Recently, deep convolutional neural networks (CNNs) have been applied to image parsing and segmentation with the state-of-the-art performance. In this paper, we propose a face parsing algorithm that combines hierarchical representations learned by a CNN, and accurate label propagations achieved by a spatially variant recurrent neural network (RNN). The RNN-based propagation approach enables efficient inference over a global space with the guidance of semantic edges generated by a local convolutional model. Since the convolutional architecture can be shallow and the spatial RNN can have few parameters, the framework is much faster and more light-weighted than the state-of-the-art CNNs for the same task. We apply the proposed model to coarse-grained and fine-grained face parsing. For fine-grained face parsing, we develop a two-stage approach by first identifying the main regions and then segmenting the detail components, which achieves better performance in terms of accuracy and efficiency. With a single GPU, the proposed algorithm parses face images accurately at 300 frames per second, which facilitates real-time applications.",0
"The task of face parsing is significant in computer vision and has various uses, such as identification and modification. Recently, deep CNNs have been utilized for image parsing and segmentation, resulting in state-of-the-art performance. This paper introduces a face parsing algorithm that combines hierarchical representations obtained from a CNN and precise label propagations achieved through a spatially variant RNN. The RNN-based propagation approach allows for efficient inference across a global space with the guidance of semantic edges produced by a local convolutional model. The proposed framework is faster and more lightweight than current CNNs for the same task since the convolutional architecture can be shallow, and the spatial RNN can have minimal parameters. The model is applied to both coarse-grained and fine-grained face parsing, and a two-stage approach is developed for the latter, which leads to better accuracy and efficiency. The proposed algorithm can parse face images accurately at 300 frames per second using a single GPU, making it suitable for real-time applications.",1
"In this paper, we suggest a framework to make use of mutual information as a regularization criterion to train Auto-Encoders (AEs). In the proposed framework, AEs are regularized by minimization of the mutual information between input and encoding variables of AEs during the training phase. In order to estimate the entropy of the encoding variables and the mutual information, we propose a non-parametric method. We also give an information theoretic view of Variational AEs (VAEs), which suggests that VAEs can be considered as parametric methods that estimate entropy. Experimental results show that the proposed non-parametric models have more degree of freedom in terms of representation learning of features drawn from complex distributions such as Mixture of Gaussians, compared to methods which estimate entropy using parametric approaches, such as Variational AEs.",0
"The paper proposes a framework for using mutual information as a regularization criterion in training Auto-Encoders (AEs). The suggested framework involves minimizing the mutual information between input and encoding variables of AEs during the training phase to regularize AEs. To estimate the entropy of encoding variables and mutual information, a non-parametric method is proposed. Furthermore, the paper presents an information theoretic view of Variational AEs (VAEs), considering them as parametric methods that estimate entropy. The experimental results indicate that non-parametric models have more freedom in representation learning of features from complex distributions like Mixture of Gaussians than methods that estimate entropy using parametric approaches like VAEs.",1
"This paper proposes the SVDNet for retrieval problems, with focus on the application of person re-identification (re-ID). We view each weight vector within a fully connected (FC) layer in a convolutional neuron network (CNN) as a projection basis. It is observed that the weight vectors are usually highly correlated. This problem leads to correlations among entries of the FC descriptor, and compromises the retrieval performance based on the Euclidean distance. To address the problem, this paper proposes to optimize the deep representation learning process with Singular Vector Decomposition (SVD). Specifically, with the restraint and relaxation iteration (RRI) training scheme, we are able to iteratively integrate the orthogonality constraint in CNN training, yielding the so-called SVDNet. We conduct experiments on the Market-1501, CUHK03, and Duke datasets, and show that RRI effectively reduces the correlation among the projection vectors, produces more discriminative FC descriptors, and significantly improves the re-ID accuracy. On the Market-1501 dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for CaffeNet, and from 73.8% to 82.3% for ResNet-50.",0
"The SVDNet is proposed in this paper for retrieval issues, with a specific focus on person re-identification. The authors suggest that the weight vectors in a fully connected layer of a convolutional neuron network can serve as a projection basis, but they are often highly correlated, which negatively impacts retrieval performance based on the Euclidean distance. To overcome this, the paper recommends using Singular Vector Decomposition (SVD) to optimize the deep learning process. By employing the restraint and relaxation iteration (RRI) training scheme, the orthogonality constraint can be iteratively integrated in CNN training, producing the SVDNet. The authors conducted experiments on the Market-1501, CUHK03, and Duke datasets, and found that RRI significantly reduced the correlation among the projection vectors, improved the FC descriptors, and increased re-ID accuracy. For example, CaffeNet's rank-1 accuracy improved from 55.3% to 80.5%, while ResNet-50's increased from 73.8% to 82.3% on the Market-1501 dataset.",1
"Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology, we model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert, and to learn task-appropriate partitionings of the system state space.",0
"The process of Learning from demonstration (LfD) involves creating behavioral models of a task by observing an expert's demonstrations. These models are useful for system control by applying the expert's actions to similar situations. However, many LfD methods have strict assumptions about the expert's behavior, such as assuming a perfect policy or directly observing their actions, making them less practical for general system identification. Instead, our work takes a more flexible approach by allowing for stochastic expert policies and using a Bayesian methodology to model the full range of possible expert controllers. Additionally, we demonstrate how this approach can be applied in a nonparametric setting to infer the expert's state representation complexity and identify task-relevant system state partitions.",1
"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",0
"The success of deep learning in vision can be attributed to various factors such as models with high capacity, increased computational power, and availability of large-scale labeled data. Despite significant advances in representation and computational capabilities, the size of the biggest dataset has remained constant since 2012. To explore the relationship between ""enormous data"" and visual deep learning, this paper examines the JFT-300M dataset with 375M noisy labels for 300M images. The findings reveal that the performance on vision tasks increases logarithmically based on the volume of training data size. Additionally, representation learning or pre-training shows promise in improving performance on many vision tasks by just training a better base model. Moreover, the study presents new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation, and human pose estimation. The paper concludes by encouraging the vision community to value datasets and collaborate in building larger ones.",1
"We present an unsupervised representation learning approach using videos without semantic labels. We leverage the temporal coherence as a supervisory signal by formulating representation learning as a sequence sorting task. We take temporally shuffled frames (i.e., in non-chronological order) as inputs and train a convolutional neural network to sort the shuffled sequences. Similar to comparison-based sorting algorithms, we propose to extract features from all frame pairs and aggregate them to predict the correct order. As sorting shuffled image sequence requires an understanding of the statistical temporal structure of images, training with such a proxy task allows us to learn rich and generalizable visual representation. We validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification and object detection tasks.",0
"Our approach to unsupervised representation learning using videos without semantic labels involves utilizing temporal coherence as a supervisory signal. This is achieved by framing representation learning as a sequence sorting task, where we take temporally shuffled frames and train a convolutional neural network to sort them. Our proposed method involves extracting features from all frame pairs and aggregating them to predict the correct order, similar to comparison-based sorting algorithms. By training with this proxy task, we can learn a rich and generalizable visual representation, as sorting shuffled image sequences requires an understanding of the statistical temporal structure of images. We have validated the effectiveness of our learned representation by pre-training it on high-level recognition problems. The experimental results demonstrate that our method outperforms state-of-the-art methods on action recognition, image classification, and object detection tasks.",1
"Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.",0
"The prediction of future frames in videos is a promising approach for unsupervised video representation learning. The generation of video frames is a natural process based on the inherent pixel flows from previous frames, which are influenced by the appearance and motion dynamics in the video. However, current methods concentrate on directly generating pixel values, leading to low-quality predictions. This study introduces a dual motion Generative Adversarial Net (GAN) architecture that implements a dual-learning mechanism to enforce consistency between future-frame predictions and pixel-wise flows in the video. The primal future-frame prediction and dual future-flow prediction form a closed loop that generates informative feedback signals to improve video prediction. To ensure that both synthesized future frames and flows are realistic, a dual adversarial training method is proposed. The new probabilistic motion encoder, based on variational autoencoders, handles natural motion uncertainty in different pixel locations. Extensive experiments demonstrate that the dual motion GAN performs significantly better than existing methods in generating new video frames and predicting future flows. The model generalizes well across diverse visual scenes and outperforms other approaches in unsupervised video representation learning.",1
"Unsupervised learning techniques in computer vision often require learning latent representations, such as low-dimensional linear and non-linear subspaces. Noise and outliers in the data can frustrate these approaches by obscuring the latent spaces.   Our main goal is deeper understanding and new development of robust approaches for representation learning. We provide a new interpretation for existing robust approaches and present two specific contributions: a new robust PCA approach, which can separate foreground features from dynamic background, and a novel robust spectral clustering method, that can cluster facial images with high accuracy. Both contributions show superior performance to standard methods on real-world test sets.",0
"To successfully implement unsupervised learning techniques in computer vision, it is often necessary to learn latent representations, including low-dimensional linear and non-linear subspaces. However, challenges such as noise and outliers in the data can impede these approaches by masking the latent spaces. Our primary objective is to gain a deeper understanding of representation learning and develop more resilient approaches. We offer a fresh interpretation of existing robust methods and introduce two specific innovations: a new robust PCA approach that can differentiate foreground features from dynamic backgrounds, and a unique robust spectral clustering method that can accurately cluster facial images. Both contributions demonstrate superior performance compared to standard methods when tested in real-world environments.",1
"Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill-posed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.",0
"The use of hashing for approximate nearest neighbor search in large-scale multimedia retrieval has become widespread due to its efficiency and quality of retrieval. Recently, deep learning to hash has gained popularity, as it enhances retrieval quality through end-to-end representation learning and hash encoding. However, the existing methods encounter difficulty in optimizing with sign activations, requiring them to first learn continuous representations and then generate binary hash codes in a separate step, leading to significant loss of retrieval quality. This paper proposes HashNet, a novel deep architecture that addresses the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by using a continuation method with convergence guarantees. The key idea is to start with a simpler network with smoothed activation function and gradually evolve it during training until it becomes the original deep network with the sign activation function. HashNet can generate binary hash codes with exact precision and achieve state-of-the-art multimedia retrieval performance on standard benchmarks, as demonstrated by comprehensive empirical evidence.",1
"Nonnegative matrix factorization is a powerful technique to realize dimension reduction and pattern recognition through single-layer data representation learning. Deep learning, however, with its carefully designed hierarchical structure, is able to combine hidden features to form more representative features for pattern recognition. In this paper, we proposed sparse deep nonnegative matrix factorization models to analyze complex data for more accurate classification and better feature interpretation. Such models are designed to learn localized features or generate more discriminative representations for samples in distinct classes by imposing $L_1$-norm penalty on the columns of certain factors. By extending one-layer model into multi-layer one with sparsity, we provided a hierarchical way to analyze big data and extract hidden features intuitively due to nonnegativity. We adopted the Nesterov's accelerated gradient algorithm to accelerate the computing process with the convergence rate of $O(1/k^2)$ after $k$ steps iteration. We also analyzed the computing complexity of our framework to demonstrate their efficiency. To improve the performance of dealing with linearly inseparable data, we also considered to incorporate popular nonlinear functions into this framework and explored their performance. We applied our models onto two benchmarking image datasets, demonstrating our models can achieve competitive or better classification performance and produce intuitive interpretations compared with the typical NMF and competing multi-layer models.",0
"Nonnegative matrix factorization is a useful method for reducing dimensions and recognizing patterns through single-layer data representation learning. However, deep learning can combine hidden features to create more representative features for pattern recognition because of its well-designed hierarchical structure. To improve classification accuracy and feature interpretation for complex data, we proposed sparse deep nonnegative matrix factorization models that learn localized features and generate more discriminative representations for distinct classes. We achieved this by imposing an L1-norm penalty on certain factor columns. By extending the one-layer model into a multi-layer one with sparsity, we provided a hierarchical approach to analyzing big data and extracting hidden features, thanks to nonnegativity. We accelerated the computing process using Nesterov's accelerated gradient algorithm, with a convergence rate of O(1/k^2) after k iterations. We also demonstrated the efficiency of our framework by analyzing its computing complexity. To handle linearly inseparable data, we considered adding popular nonlinear functions to the framework and explored their performance. We tested our models on two benchmarking image datasets and found that they achieved competitive or better classification performance and produced intuitive interpretations compared to typical NMF and competing multi-layer models.",1
"Convolutional Neural Network (CNN) models have become the state-of-the-art for most computer vision tasks with natural images. However, these are not best suited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology slides due to large size of these images. Current approaches construct smaller patches from WSIs which results in the loss of contextual information. We propose to capture the spatial context using novel Representation-Aggregation Network (RAN) for segmentation purposes, wherein the first network learns patch-level representation and the second network aggregates context from a grid of neighbouring patches. We can use any CNN for representation learning, and can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for context-aggregation. Our method significantly outperformed conventional patch-based CNN approaches on segmentation of tumour in WSIs of breast cancer tissue sections.",0
"For most computer vision tasks involving natural images, Convolutional Neural Network (CNN) models are considered the best option. However, they are not optimal for Whole Slide Images (WSIs) of histology slides with multi-gigapixel resolution due to their size. Current approaches involve breaking down WSIs into smaller patches, leading to loss of contextual information. To address this issue, we propose a novel Representation-Aggregation Network (RAN) for segmentation, where the first network learns patch-level representation, and the second network aggregates context from neighbouring patches' grid. Our method employs any CNN for representation learning and CNN or 2D-Long Short Term Memory (2D-LSTM) for context-aggregation. Our approach performed significantly better than conventional patch-based CNN methods in segmenting breast cancer tissue section tumours in WSIs.",1
"We report on CMU Informedia Lab's system used in Google's YouTube 8 Million Video Understanding Challenge. In this multi-label video classification task, our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the official test set. We attribute the good performance to three components: 1) Refined video representation learning with residual links and hypercolumns 2) Latent concept mining which captures interactions among concepts. 3) Learning with temporal segments and weighted multi-model ensemble. We conduct experiments to validate and analyze the contribution of our models. We also share some unsuccessful trials leveraging conventional approaches such as recurrent neural networks for video representation learning for this large-scale video dataset. All the codes to reproduce our results are publicly available at https://github.com/Martini09/informedia-yt8m-release.",0
"The system developed by CMU Informedia Lab for Google's YouTube 8 Million Video Understanding Challenge is presented in this report. Our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and official test set, respectively, in this multi-label video classification task. Our success can be attributed to three factors: 1) Refined video representation learning using residual links and hypercolumns, 2) Latent concept mining capturing concept interactions, and 3) Learning with temporal segments and weighted multi-model ensemble. We conducted experiments to validate and analyze our models' contribution and shared some unsuccessful attempts using conventional approaches like recurrent neural networks for video representation learning for this large-scale video dataset. We provide publicly available codes to reproduce our results at https://github.com/Martini09/informedia-yt8m-release.",1
"In this work, we address the problem of improvement of robustness of feature representations learned using convolutional neural networks (CNNs) to image deformation. We argue that higher moment statistics of feature distributions could be shifted due to image deformations, and the shift leads to degrade of performance and cannot be reduced by ordinary normalization methods as observed in experimental analyses. In order to attenuate this effect, we apply additional non-linearity in CNNs by combining power functions with learnable parameters into convolution operation. In the experiments, we observe that CNNs which employ the proposed method obtain remarkable boost in both the generalization performance and the robustness under various types of deformations using large scale benchmark datasets. For instance, a model equipped with the proposed method obtains 3.3\% performance boost in mAP on Pascal Voc object detection task using deformed images, compared to the reference model, while both models provide the same performance using original images. To the best of our knowledge, this is the first work that studies robustness of deep features learned using CNNs to a wide range of deformations for object recognition and detection.",0
"This study aims to enhance the resilience of feature representations generated by convolutional neural networks (CNNs) to image distortions. The authors contend that higher moment statistics of feature distributions may shift due to image deformations, resulting in a decrease in performance, which cannot be mitigated by conventional normalization methods, as demonstrated in experimental analyses. To address this issue, the authors incorporate additional non-linearity into CNNs by integrating power functions with learnable parameters into the convolution operation. Their experiments reveal that CNNs utilizing this technique exhibit significant improvements in both generalization performance and robustness under various deformation types, as observed in large-scale benchmark datasets. For example, the proposed model achieves a 3.3% increase in mAP in the Pascal Voc object detection task when working with deformed images, compared to the reference model, while both models achieve the same performance with original images. This is the first study to examine the resilience of deep features generated by CNNs to a wide range of distortions for object detection and recognition, to the best of our knowledge.",1
"In this paper, we propose an end-to-end group-wise deep co-saliency detection approach to address the co-salient object discovery problem based on the fully convolutional network (FCN) with group input and group output. The proposed approach captures the group-wise interaction information for group images by learning a semantics-aware image representation based on a convolutional neural network, which adaptively learns the group-wise features for co-saliency detection. Furthermore, the proposed approach discovers the collaborative and interactive relationships between group-wise feature representation and single-image individual feature representation, and model this in a collaborative learning framework. Finally, we set up a unified end-to-end deep learning scheme to jointly optimize the process of group-wise feature representation learning and the collaborative learning, leading to more reliable and robust co-saliency detection results. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.",0
"Our paper introduces a novel method for co-saliency detection that utilizes an end-to-end group-wise deep learning approach. Our method employs a fully convolutional network (FCN) with group input and output, enabling the capture of group-wise interaction information for group images. By leveraging a convolutional neural network, we learn a semantics-aware image representation that adapts to group-wise features for co-saliency detection. Additionally, we explore the collaborative and interactive relationships between group-wise feature representation and single-image individual feature representation, which we model in a collaborative learning framework. We propose a unified end-to-end deep learning scheme that optimizes the process of group-wise feature representation learning and collaborative learning, resulting in more reliable and robust co-saliency detection results. Our experimental results demonstrate the superiority of our approach over state-of-the-art methods.",1
"Deep learning owes its success to three key factors: scale of data, enhanced models to learn representations from data, and scale of computation. This book chapter presented the importance of the data-driven approach to learn good representations from both big data and small data. In terms of big data, it has been widely accepted in the research community that the more data the better for both representation and classification improvement. The question is then how to learn representations from big data, and how to perform representation learning when data is scarce. We addressed the first question by presenting CNN model enhancements in the aspects of representation, optimization, and generalization. To address the small data challenge, we showed transfer representation learning to be effective. Transfer representation learning transfers the learned representation from a source domain where abundant training data is available to a target domain where training data is scarce. Transfer representation learning gave the OM and melanoma diagnosis modules of our XPRIZE Tricorder device (which finished $2^{nd}$ out of $310$ competing teams) a significant boost in diagnosis accuracy.",0
"The success of deep learning can be attributed to three important factors: the amount of data available, models that can effectively learn data representations, and the computational power required to process it all. This chapter emphasizes the significance of a data-driven approach for acquiring good representations from both small and large data sets. The research community acknowledges that more data improves representation and classification, especially in the case of large data sets. However, the challenge is to determine how to learn representations from such large data sets and how to perform representation learning when data is scarce. To tackle these issues, we have introduced CNN model enhancements that optimize representation, generalization, and optimization. Additionally, we have demonstrated the effectiveness of transfer representation learning in addressing the small data challenge. This technique transfers the learned representation from a source domain where abundant training data is available, to a target domain where the training data is sparse. Our XPRIZE Tricorder device, which ranked $2^{nd}$ out of $310$ competing teams, benefited from transfer representation learning in improving the accuracy of its OM and melanoma diagnosis modules.",1
"Delineation of line patterns in images is a basic step required in various applications such as blood vessel detection in medical images, segmentation of rivers or roads in aerial images, detection of cracks in walls or pavements, etc. In this paper we present trainable B-COSFIRE filters, which are a model of some neurons in area V1 of the primary visual cortex, and apply it to the delineation of line patterns in different kinds of images. B-COSFIRE filters are trainable as their selectivity is determined in an automatic configuration process given a prototype pattern of interest. They are configurable to detect any preferred line structure (e.g. segments, corners, cross-overs, etc.), so usable for automatic data representation learning. We carried out experiments on two data sets, namely a line-network data set from INRIA and a data set of retinal fundus images named IOSTAR. The results that we achieved confirm the robustness of the proposed approach and its effectiveness in the delineation of line structures in different kinds of images.",0
"Various applications, including medical image analysis for detecting blood vessels, aerial image segmentation for identifying rivers or roads, and detecting cracks in walls or pavements, require the initial step of delineating line patterns in images. This study introduces trainable B-COSFIRE filters, which mimic neurons in the primary visual cortex's area V1 and can be configured to detect any preferred line structure, such as corners or cross-overs. These filters are trainable through an automatic configuration process using a prototype pattern of interest, making them useful for automatic data representation learning. The proposed approach was tested on two datasets, including a line-network dataset from INRIA and a retinal fundus image dataset named IOSTAR, and achieved robust and effective results in delineating line structures in various types of images.",1
"Traffic scene recognition is an important and challenging issue in Intelligent Transportation Systems (ITS). Recently, Convolutional Neural Network (CNN) models have achieved great success in many applications, including scene classification. The remarkable representational learning capability of CNN remains to be further explored for solving real-world problems. Vector of Locally Aggregated Descriptors (VLAD) encoding has also proved to be a powerful method in catching global contextual information. In this paper, we attempted to solve the traffic scene recognition problem by combining the features representational capabilities of CNN with the VLAD encoding scheme. More specifically, the CNN features of image patches generated by a region proposal algorithm are encoded by applying VLAD, which subsequently represent an image in a compact representation. To catch the spatial information, spatial pyramids are exploited to encode CNN features. We experimented with a dataset of 10 categories of traffic scenes, with satisfactory categorization performances.",0
"Recognizing traffic scenes is a challenging and significant matter in Intelligent Transportation Systems. In recent times, Convolutional Neural Network (CNN) models have demonstrated exceptional accomplishment in several applications, such as scene classification. However, the CNN's remarkable ability to learn representation can be further explored to tackle real-world problems. Vector of Locally Aggregated Descriptors (VLAD) encoding is another effective method for capturing global contextual information. This study aims to address the traffic scene recognition issue by merging the CNN feature representation capabilities with the VLAD encoding scheme. Specifically, we encode the CNN features of image patches produced by a region proposal algorithm with VLAD, which results in a compact representation of an image. We utilize spatial pyramids to encode CNN features to capture spatial information. We conducted experiments on a dataset containing ten categories of traffic scenes, and the classification performance was satisfactory.",1
"Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.",0
"The task of Video Question Answering presents a complex issue in retrieving visual information, as it aims to provide answers to questions about referenced video content. However, presently existing approaches to visual question answering mainly address the issue of static image questions, which may be insufficient for effectively answering video questions due to the lack of consideration for temporal dynamics. This paper investigates the problem of video question answering by incorporating a frame-level attention mechanism to model its temporal dynamics. We introduce the attribute-augmented attention network learning framework that enables joint frame-level attribute detection and unified video representation learning for video question answering. Furthermore, we implement a multi-step reasoning process to enhance the performance of our attention network. We also create a large-scale video question answering dataset and conduct experiments on both multiple-choice and open-ended video question answering tasks to demonstrate the efficacy of our proposed method.",1
"Deep learning for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate the generation of synthetic training data for action recognition, as it has recently shown promising results for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines. We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". It contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix synthetic and real videos, even if the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos.",0
"The development of deep learning technology for recognizing human actions in videos has been making great strides, but is hindered by the need for labor-intensive manual labeling of extensive video collections. The present study explores the use of synthetic training data for action recognition, which has recently shown promising outcomes for various computer vision tasks. We propose an interpretable parametric generative model of human action videos that employs procedural generation and other computer graphics techniques used in modern game engines. This approach generates a diverse, realistic, and physically accurate dataset of human action videos, named PHAV (short for ""Procedural Human Action Videos""), consisting of 39,982 videos with over 1,000 examples for each of the 35 action categories. Our model can define 14 synthetic actions that do not rely on existing motion capture sequences. We introduce a deep multi-task representation learning architecture that allows us to mix synthetic and real videos, even when the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks indicate that combining our synthetic videos with small real-world datasets can enhance recognition performance, significantly surpassing the performance of state-of-the-art unsupervised generative models of videos that rely on fine-tuning.",1
"Auto-Encoders are unsupervised models that aim to learn patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this \textit{signal recovery perspective} enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the \textit{true} hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.",0
"Auto-Encoders are models that operate without supervision and aim to acquire knowledge about patterns in observed data by minimizing a cost related to reconstruction. The acquired representations are often both sparse and distributed. In contrast, compressed sensing and sparse coding assume a data generation process where the observed data results from some latent signal source and aim to recover the corresponding signal from measurements. Viewing auto-encoders from this perspective of signal recovery allows for a more consistent understanding of these techniques. In this study, we present evidence that the true hidden representation can be recovered with high accuracy if the weight matrices have high incoherence with unit $ \ell^{2} $ row length and the bias vectors are approximately equal to the negative of the data mean. Moreover, the recovery becomes increasingly precise as the sparsity of hidden signals grows. We also demonstrate that auto-encoders are capable of recovering the data generation dictionary using only data samples.",1
"Class imbalance is a challenging issue in practical classification problems for deep learning models as well as traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this paper, we propose Deep Over-sampling (DOS), a framework for extending the synthetic over-sampling method to exploit the deep feature space acquired by a convolutional neural network (CNN). Its key feature is an explicit, supervised representation learning, for which the training data presents each raw input sample with a synthetic embedding target in the deep feature space, which is sampled from the linear subspace of in-class neighbors. We implement an iterative process of training the CNN and updating the targets, which induces smaller in-class variance among the embeddings, to increase the discriminative power of the deep representation. We present an empirical study using public benchmarks, which shows that the DOS framework not only counteracts class imbalance better than the existing method, but also improves the performance of the CNN in the standard, balanced settings.",0
"The problem of class imbalance poses a significant challenge for both traditional and deep learning models in practical classification problems. While synthetic over-sampling has been a successful countermeasure in the past, it has not been as effective in dealing with complex, structured data handled by deep learning models. In this study, we introduce a new approach called Deep Over-sampling (DOS) that builds on synthetic over-sampling by leveraging the deep feature space obtained from a convolutional neural network (CNN). DOS employs explicit, supervised representation learning by training the data with a synthetic embedding target in the deep feature space, which is sampled from the linear subspace of in-class neighbors. The process involves iterative training of the CNN and target updates to minimize in-class variance and enhance the discriminative power of the deep representation. We conducted an empirical study using public benchmarks, which demonstrated that DOS not only addresses class imbalance more effectively than existing methods but also improves the performance of the CNN in a standard, balanced setting.",1
"Existing block-diagonal representation researches mainly focuses on casting block-diagonal regularization on training data, while only little attention is dedicated to concurrently learning both block-diagonal representations of training and test data. In this paper, we propose a discriminative block-diagonal low-rank representation (BDLRR) method for recognition. In particular, the elaborate BDLRR is formulated as a joint optimization problem of shrinking the unfavorable representation from off-block-diagonal elements and strengthening the compact block-diagonal representation under the semi-supervised framework of low-rank representation. To this end, we first impose penalty constraints on the negative representation to eliminate the correlation between different classes such that the incoherence criterion of the extra-class representation is boosted. Moreover, a constructed subspace model is developed to enhance the self-expressive power of training samples and further build the representation bridge between the training and test samples, such that the coherence of the learned intra-class representation is consistently heightened. Finally, the resulting optimization problem is solved elegantly by employing an alternative optimization strategy, and a simple recognition algorithm on the learned representation is utilized for final prediction. Extensive experimental results demonstrate that the proposed method achieves superb recognition results on four face image datasets, three character datasets, and the fifteen scene multi-categories dataset. It not only shows superior potential on image recognition but also outperforms state-of-the-art methods.",0
"Previous studies on block-diagonal representation have mostly focused on applying block-diagonal regularization to training data, with little attention given to simultaneously learning block-diagonal representations of both training and test data. In this paper, we introduce a discriminative block-diagonal low-rank representation (BDLRR) method for recognition. Our approach involves a joint optimization problem that aims to eliminate unfavorable representations from off-block-diagonal elements and reinforce compact block-diagonal representation using a semi-supervised low-rank representation framework. We accomplish this by imposing penalty constraints on negative representations to remove correlation between different classes, improving the incoherence criterion of the extra-class representation. Additionally, we develop a subspace model to enhance the self-expressive power of training samples and establish a representation bridge between training and test samples. This consistently heightens the coherence of the learned intra-class representation. We solve the resulting optimization problem through an alternative optimization strategy and utilize a simple recognition algorithm for final prediction. Our method achieves exceptional recognition results on four face image datasets, three character datasets, and the fifteen scene multi-categories dataset, surpassing state-of-the-art methods and demonstrating its potential for image recognition.",1
"How can we learn a classifier that is ""fair"" for a protected or sensitive group, when we do not know if the input to the classifier belongs to the protected group? How can we train such a classifier when data on the protected group is difficult to attain? In many settings, finding out the sensitive input attribute can be prohibitively expensive even during model training, and sometimes impossible during model serving. For example, in recommender systems, if we want to predict if a user will click on a given recommendation, we often do not know many attributes of the user, e.g., race or age, and many attributes of the content are hard to determine, e.g., the language or topic. Thus, it is not feasible to use a different classifier calibrated based on knowledge of the sensitive attribute.   Here, we use an adversarial training procedure to remove information about the sensitive attribute from the latent representation learned by a neural network. In particular, we study how the choice of data for the adversarial training effects the resulting fairness properties. We find two interesting results: a small amount of data is needed to train these adversarial models, and the data distribution empirically drives the adversary's notion of fairness.",0
"In situations where we are uncertain about whether the input to a classifier belongs to a protected or sensitive group, and obtaining data on the group is challenging, how can we develop a ""fair"" classifier? This can be particularly difficult during model serving, as determining the sensitive attribute may be prohibitively expensive or impossible. For instance, in recommender systems, it is often hard to ascertain user attributes such as age or race, as well as content attributes like language or topic. Thus, using a different classifier that is calibrated based on knowledge of the sensitive attribute may not be practical. To address this issue, we utilize an adversarial training method that eliminates information about the sensitive attribute from the neural network's latent representation. Our research investigates how the fairness properties of these adversarial models are influenced by the data used for training, and we discover two key findings: a small amount of data is sufficient to train these models, and the data distribution determines the adversary's conception of fairness.",1
"Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object edge detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 400K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.",0
"Understanding indoor scenes is crucial for applications like assisting humans and navigating robots. Deep neural networks that leverage data have surpassed traditional methods due to their ability to learn representations. However, a significant obstacle in improving representations is the dearth of available per-pixel ground truth data. Core scene understanding tasks like semantic segmentation, normal prediction, and object edge detection require such data. Synthetic data has been proposed as a solution, but a systematic study on how it's generated is lacking. We introduce a large-scale synthetic dataset that includes 400K physically-based rendered images from 45K realistic 3D indoor scenes. We evaluate the impact of rendering methods and scene lighting on training for three computer vision tasks. Our study highlights the importance of realistic rendering for training with synthetic data and demonstrates that pretraining with our dataset can surpass the current state of the art on all three tasks.",1
"Objective: To transform heterogeneous clinical data from electronic health records into clinically meaningful constructed features using data driven method that rely, in part, on temporal relations among data. Materials and Methods: The clinically meaningful representations of medical concepts and patients are the key for health analytic applications. Most of existing approaches directly construct features mapped to raw data (e.g., ICD or CPT codes), or utilize some ontology mapping such as SNOMED codes. However, none of the existing approaches leverage EHR data directly for learning such concept representation. We propose a new way to represent heterogeneous medical concepts (e.g., diagnoses, medications and procedures) based on co-occurrence patterns in longitudinal electronic health records. The intuition behind the method is to map medical concepts that are co-occuring closely in time to similar concept vectors so that their distance will be small. We also derive a simple method to construct patient vectors from the related medical concept vectors. Results: For qualitative evaluation, we study similar medical concepts across diagnosis, medication and procedure. In quantitative evaluation, our proposed representation significantly improves the predictive modeling performance for onset of heart failure (HF), where classification methods (e.g. logistic regression, neural network, support vector machine and K-nearest neighbors) achieve up to 23% improvement in area under the ROC curve (AUC) using this proposed representation. Conclusion: We proposed an effective method for patient and medical concept representation learning. The resulting representation can map relevant concepts together and also improves predictive modeling performance.",0
"The aim of this study was to create clinically meaningful features from electronic health records using a data-driven approach that utilizes temporal relationships among data. Previous methods for constructing features either directly mapped raw data or used ontology mapping, but did not leverage EHR data for learning concept representation. Therefore, we developed a new approach that identifies co-occurring medical concepts in longitudinal EHRs and maps them to similar concept vectors, resulting in a better representation of heterogeneous medical concepts. We also derived a simple method for constructing patient vectors from the related medical concept vectors. Our proposed representation demonstrated significant improvement in predictive modeling performance for heart failure onset, achieving up to a 23% increase in AUC using classification methods. Overall, our method effectively learns patient and medical concept representation, allowing for better mapping of relevant concepts and improved predictive modeling performance.",1
"Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.",0
"Reinforcement learning (RL) poses significant challenges in representation learning and option discovery. Proto-value functions (PVFs) have been widely employed for representation learning in MDPs. In this study, we tackle the option discovery problem by demonstrating how PVFs implicitly define options. We achieve this by introducing eigenpurposes, which are intrinsic reward functions generated from the learned representations. Eigenpurposes facilitate the discovery of options that traverse the principal directions of the state space and are useful for multiple tasks since they are discovered without considering the environment's rewards. Additionally, the different options operate at varying time scales, making them ideal for exploration. We present the features of eigenpurposes in both traditional tabular domains and Atari 2600 games.",1
"We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.",0
"We introduce a novel technique called Deep Generalized Canonical Correlation Analysis (DGCCA) which enables learning nonlinear transformations of multiple data views to maximize information exchange between them. While previous methods exist for nonlinear two-view representation learning and linear many-view representation learning, DGCCA is the first multiview representation learning technique that combines the statistical power of incorporating information from many independent sources with the flexibility of deep representation learning. We describe the DGCCA formulation and present an efficient stochastic optimization algorithm for solving it. We apply DGCCA to two datasets for the tasks of phonetic transcription and recommending hashtags and friends on Twitter. Our results demonstrate DGCCA's superiority over existing methods for phonetic transcription and hashtag recommendation, while performing equally well as standard linear many-view techniques.",1
"There is general consensus that learning representations is useful for a variety of reasons, e.g. efficient use of labeled data (semi-supervised learning), transfer learning and understanding hidden structure of data. Popular techniques for representation learning include clustering, manifold learning, kernel-learning, autoencoders, Boltzmann machines, etc.   To study the relative merits of these techniques, it's essential to formalize the definition and goals of representation learning, so that they are all become instances of the same definition. This paper introduces such a formal framework that also formalizes the utility of learning the representation. It is related to previous Bayesian notions, but with some new twists. We show the usefulness of our framework by exhibiting simple and natural settings -- linear mixture models and loglinear models, where the power of representation learning can be formally shown. In these examples, representation learning can be performed provably and efficiently under plausible assumptions (despite being NP-hard), and furthermore: (i) it greatly reduces the need for labeled data (semi-supervised learning) and (ii) it allows solving classification tasks when simpler approaches like nearest neighbors require too much data (iii) it is more powerful than manifold learning methods.",0
"Representation learning is widely accepted as valuable for various reasons, including the efficient utilization of labeled data in semi-supervised learning, transfer learning, and discerning hidden data structures. Techniques like clustering, kernel-learning, manifold learning, autoencoders, and Boltzmann machines are commonly used for representation learning. However, it is crucial to establish a formal definition and objectives for representation learning to compare the efficacy of these techniques. This paper presents a formal framework that defines the usefulness of learning the representation and is based on previous Bayesian concepts but with innovative aspects. Linear mixture and loglinear models serve as simple and natural examples to demonstrate the utility of the framework. Despite being NP-hard, representation learning can be performed efficiently and provably under reasonable assumptions in these examples. In addition, it minimizes the need for labeled data in semi-supervised learning, solves classification problems when simpler techniques like nearest neighbors require excessive data, and surpasses manifold learning methods in power.",1
"Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at https://sermanet.github.io/rewards",0
"The biggest challenges to implementing reinforcement learning (RL) agents in real-world scenarios are the time spent designing the reward function and exploring the environment. Developing a reward function for many tasks requires significant manual effort and may necessitate the installation of additional sensors to measure task success. Moreover, numerous tasks comprise implicit intermediate steps that must be completed sequentially, making it difficult to assess progress. To address these issues, we suggest utilizing the intermediate visual representations acquired by deep models to deduce perceptual reward functions from a limited number of demonstrations. We have devised a technique that can identify the key intermediate steps of a task from a few demonstration sequences and automatically determine the most discerning features for identifying them. While our approach employs the features of a pre-trained deep model, it does not necessitate explicit sub-goal specification. The resultant reward functions can then be used by an RL agent to execute the task in real-world settings. We have demonstrated the effectiveness of this strategy by presenting qualitative and quantitative results for two real-world tasks and a comparison with a human-created reward function. Additionally, we have demonstrated that our method can be used to teach a robot to open doors in the real world, even when the demonstration is provided by a human using their own hand. To our knowledge, this is the first time that complex robotic manipulation abilities have been learned directly from video demonstrations without supervised labels. Supplementary materials and data are available at https://sermanet.github.io/rewards.",1
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",0
"Learning how to combine abstract, symbolic reasoning with continuous neural reasoning is a major challenge in representation learning. To move towards this goal, we introduce a novel architecture called neural equivalence networks, which is designed to learn continuous semantic representations of algebraic and logical expressions. These networks are trained to recognize semantic equivalence, even when expressions have vastly different syntax. The difficulty lies in the fact that semantic representations must be computed in a syntax-directed manner due to the compositional nature of semantics, but small changes in syntax can result in significant changes in semantics, which can pose a challenge for continuous neural architectures. We conduct a comprehensive evaluation on the task of verifying equivalence across a wide range of symbolic algebraic and boolean expression types, and demonstrate that our model outperforms existing architectures with significant improvements.",1
"Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.",0
"The interrelatedness of paralinguistic tasks means that knowledge acquired in one area can be applied to others. This study explores the transfer of knowledge between speaker, emotion, and gender recognition tasks, as well as the cross-dataset transfer of emotion recognition data. Our focus is on progressive neural networks, which offer a solution to the forgetting effect that occurs when pre-training networks for various tasks. We compare these networks to the traditional method of pre-training and fine-tuning, and our experiments reveal that using representations from different paralinguistic tasks can enhance emotion recognition, and that transfer learning can improve the performance of emotion recognition systems by incorporating additional datasets.",1
"With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.",0
"Word2vec remains a popular neural language model due to its simple architecture and efficient ability to learn word embeddings from large text datasets. However, the model's limitation is the inability to optimally represent words with multiple meanings, as only a single embedding is learned for each word in the vocabulary. Additionally, creating embeddings for new words not in the vocabulary is not possible. To address these issues, we propose an extension of the continuous bag-of-words (CBOW) word2vec model called context encoders (ConEc). This model uses the negative sampling training objective to predict context-based similarities and creates embeddings for out-of-vocabulary words and words with multiple meanings by multiplying the matrix of trained word2vec embeddings with a word's average context vector. The benefits of this approach are demonstrated by using these embeddings as features in the CoNLL 2003 named entity recognition (NER) task.",1
"Learning binary representation is essential to large-scale computer vision tasks. Most existing algorithms require a separate quantization constraint to learn effective hashing functions. In this work, we present Direct Binary Embedding (DBE), a simple yet very effective algorithm to learn binary representation in an end-to-end fashion. By appending an ingeniously designed DBE layer to the deep convolutional neural network (DCNN), DBE learns binary code directly from the continuous DBE layer activation without quantization error. By employing the deep residual network (ResNet) as DCNN component, DBE captures rich semantics from images. Furthermore, in the effort of handling multilabel images, we design a joint cross entropy loss that includes both softmax cross entropy and weighted binary cross entropy in consideration of the correlation and independence of labels, respectively. Extensive experiments demonstrate the significant superiority of DBE over state-of-the-art methods on tasks of natural object recognition, image retrieval and image annotation.",0
"Gaining knowledge of binary representation is crucial for computer vision tasks on a large scale. Most current algorithms necessitate a distinct quantization limit to master effective hashing functions. This paper introduces Direct Binary Embedding (DBE), which is a straightforward but highly efficient algorithm that learns binary representation in an end-to-end approach. DBE adds a cleverly designed DBE layer to the deep convolutional neural network (DCNN), enabling it to learn binary code directly from the continuous DBE layer activation without quantization error. By using the deep residual network (ResNet) as the DCNN component, DBE captures rich semantics from images. Additionally, to handle multilabel images, a joint cross entropy loss that includes both softmax cross entropy and weighted binary cross entropy is designed, taking into account the correlation and independence of labels, respectively. Comprehensive experiments prove the substantial superiority of DBE over state-of-the-art methods in natural object recognition, image retrieval, and image annotation tasks.",1
"We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.",0
"We make use of vast amounts of readily-available, synchronized data to acquire an in-depth, discerning representation that is shared across three principal natural modalities: vision, sound, and language. Our method involves a joint training of a deep convolutional network using over a year's worth of sound from videos and millions of sentences matched with images to achieve aligned representation learning. Our research indicates that this representation is valuable for numerous tasks, including cross-modal retrieval and transferring classifiers between modalities. Additionally, although our network was solely trained with image+text and image+sound pairs, it can also transfer between sound and text, which was never observed during training. The visualizations of our representation reveal numerous hidden units that automatically emerge to detect concepts, regardless of the modality.",1
"End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.",0
"To train automated speech recognition (ASR) systems from start to finish, significant data and computational resources are necessary. However, we have explored an alternative approach to training ASR models with limited GPU memory, throughput, and training data by using transfer learning through model adaptation. In a series of systematic experiments, we have adapted a Wav2Letter convolutional neural network, which was originally trained for English ASR, to the German language. Our results indicate that this technique allows for faster training on consumer-grade resources, requiring less training data to achieve the same level of accuracy. This approach can significantly reduce the cost of training ASR models in other languages. Furthermore, our model introspection uncovered that minor modifications to the network's weights, particularly for inner layers, produced satisfactory results.",1
"Nowadays, distributed smart cameras are deployed for a wide set of tasks in several application scenarios, ranging from object recognition, image retrieval, and forensic applications. Due to limited bandwidth in distributed systems, efficient coding of local visual features has in fact been an active topic of research. In this paper, we propose a novel approach to obtain a compact representation of high-dimensional visual data using sensor fusion techniques. We convert the problem of visual analysis in resource-limited scenarios to a multi-view representation learning, and we show that the key to finding properly compressed representation is to exploit the position of cameras with respect to each other as a norm-based regularization in the particular signal representation of sparse coding. Learning the representation of each camera is viewed as an individual task and a multi-task learning with joint sparsity for all nodes is employed. The proposed representation learning scheme is referred to as the multi-view task-driven learning for visual sensor network (MT-VSN). We demonstrate that MT-VSN outperforms state-of-the-art in various surveillance recognition tasks.",0
"In modern times, distributed smart cameras are utilized for various tasks in different application scenarios, such as object recognition, image retrieval, and forensic applications. As a result of limited bandwidth in distributed systems, efficient coding of local visual features has been a popular area of research. This paper presents a new approach to obtain a condensed representation of high-dimensional visual data using sensor fusion techniques. The issue of visual analysis in resource-limited scenarios is transformed into a multi-view representation learning problem, and the position of cameras in relation to one another is used as a norm-based regularization in the sparse coding signal representation to obtain a properly compressed representation. Learning the representation of each camera is considered an individual task, and a joint sparsity multi-task learning approach is utilized for all nodes. The proposed representation learning technique is named MT-VSN (multi-view task-driven learning for visual sensor network), and it is shown to outperform current state-of-the-art methods in various surveillance recognition tasks.",1
"Softmax loss is widely used in deep neural networks for multi-class classification, where each class is represented by a weight vector, a sample is represented as a feature vector, and the feature vector has the largest projection on the weight vector of the correct category when the model correctly classifies a sample. To ensure generalization, weight decay that shrinks the weight norm is often used as regularizer. Different from traditional learning algorithms where features are fixed and only weights are tunable, features are also tunable as representation learning in deep learning. Thus, we propose feature incay to also regularize representation learning, which favors feature vectors with large norm when the samples can be correctly classified. With the feature incay, feature vectors are further pushed away from the origin along the direction of their corresponding weight vectors, which achieves better inter-class separability. In addition, the proposed feature incay encourages intra-class compactness along the directions of weight vectors by increasing the small feature norm faster than the large ones. Empirical results on MNIST, CIFAR10 and CIFAR100 demonstrate feature incay can improve the generalization ability.",0
"Deep neural networks often use softmax loss for multi-class classification, with each class represented by a weight vector and each sample represented as a feature vector. Correct classification occurs when the feature vector has the largest projection on the weight vector of the correct category. To ensure generalization, weight decay is used as a regularizer to shrink the weight norm. Unlike traditional learning algorithms, deep learning allows for tunable features as representation learning. Therefore, we propose feature incay as a regularizer to further improve representation learning by favoring feature vectors with a large norm when samples are correctly classified. Feature incay pushes feature vectors away from the origin along the direction of their corresponding weight vectors, improving inter-class separability, and encourages intra-class compactness along the directions of weight vectors by increasing the small feature norm faster than the large ones. Empirical results on MNIST, CIFAR10 and CIFAR100 show that feature incay improves generalization ability.",1
"In this paper, we present a novel method of no-reference image quality assessment (NR-IQA), which is to predict the perceptual quality score of a given image without using any reference image. The proposed method harnesses three functions (i) the visual attention mechanism, which affects many aspects of visual perception including image quality assessment, however, is overlooked in the NR-IQA literature. The method assumes that the fixation areas on an image contain key information to the process of IQA. (ii) the robust averaging strategy, which is a means \--- supported by psychology studies \--- to integrating multiple/step-wise evidence to make a final perceptual judgment. (iii) the multi-task learning, which is believed to be an effectual means to shape representation learning and could result in a more generalized model.   To exploit the synergy of the three, we consider the NR-IQA as a dynamic perception process, in which the model samples a sequence of ""informative"" areas and aggregates the information to learn a representation for the tasks of jointly predicting the image quality score and the distortion type.   The model learning is implemented by a reinforcement strategy, in which the rewards of both tasks guide the learning of the optimal sampling policy to acquire the ""task-informative"" image regions so that the predictions can be made accurately and efficiently (in terms of the sampling steps). The reinforcement learning is realized by a deep network with the policy gradient method and trained through back-propagation.   In experiments, the model is tested on the TID2008 dataset and it outperforms several state-of-the-art methods. Furthermore, the model is very efficient in the sense that a small number of fixations are used in NR-IQA.",0
"This paper introduces a new approach to assess the quality of an image without relying on a reference image. The proposed method combines three functions: the visual attention mechanism, the robust averaging strategy, and the multi-task learning. The method considers no-reference image quality assessment as a dynamic perception process, where the model samples informative areas to predict the image quality score and the distortion type. The model learning is achieved through a reinforcement strategy, where rewards guide the learning of the optimal sampling policy. The deep network with the policy gradient method is used to realize the reinforcement learning and trained through back-propagation. The experiments conducted on the TID2008 dataset show that the proposed method outperforms several state-of-the-art methods and is highly efficient as it uses a small number of fixations for no-reference image quality assessment.",1
"Real-world robotics problems often occur in domains that differ significantly from the robot's prior training environment. For many robotic control tasks, real world experience is expensive to obtain, but data is easy to collect in either an instrumented environment or in simulation. We propose a novel domain adaptation approach for robot perception that adapts visual representations learned on a large easy-to-obtain source dataset (e.g. synthetic images) to a target real-world domain, without requiring expensive manual data annotation of real world data before policy search. Supervised domain adaptation methods minimize cross-domain differences using pairs of aligned images that contain the same object or scene in both the source and target domains, thus learning a domain-invariant representation. However, they require manual alignment of such image pairs. Fully unsupervised adaptation methods rely on minimizing the discrepancy between the feature distributions across domains. We propose a novel, more powerful combination of both distribution and pairwise image alignment, and remove the requirement for expensive annotation by using weakly aligned pairs of images in the source and target domains. Focusing on adapting from simulation to real world data using a PR2 robot, we evaluate our approach on a manipulation task and show that by using weakly paired images, our method compensates for domain shift more effectively than previous techniques, enabling better robot performance in the real world.",0
"Problems in real-world robotics often differ greatly from the robot's previous training environment. Obtaining real-world experience for many robotic control tasks can be costly, but collecting data in an instrumented environment or simulation is easy. Our proposed solution is a new approach for robot perception domain adaptation that adapts visual representations learned from a large, easy-to-obtain source dataset (such as synthetic images) to a target real-world domain. This approach does not require expensive manual data annotation of real-world data prior to policy search. In supervised domain adaptation methods, cross-domain differences are minimized using pairs of aligned images containing the same object or scene in both domains, learning a domain-invariant representation. However, this requires manual alignment of image pairs. Fully unsupervised adaptation methods rely on minimizing the feature distribution discrepancy across domains. Our approach combines both distribution and pairwise image alignment, removing the need for costly annotation by using weakly aligned pairs of images in the source and target domains. We evaluate our approach on a manipulation task, adapting from simulation to real-world data using a PR2 robot. Our method compensates for domain shift more effectively than previous techniques by using weakly paired images, resulting in better robot performance in the real world.",1
"Mammography screening for early detection of breast lesions currently suffers from high amounts of false positive findings, which result in unnecessary invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many of these false-positive findings prior to biopsy. Current approaches estimate tissue properties by means of quantitative parameters taken from generative, biophysical models fit to the q-space encoded signal under certain assumptions regarding noise and spatial homogeneity. This process is prone to fitting instability and partial information loss due to model simplicity. We reveal unexplored potentials of the signal by integrating all data processing components into a convolutional neural network (CNN) architecture that is designed to propagate clinical target information down to the raw input images. This approach enables simultaneous and target-specific optimization of image normalization, signal exploitation, global representation learning and classification. Using a multicentric data set of 222 patients, we demonstrate that our approach significantly improves clinical decision making with respect to the current state of the art.",0
"The current system of mammography screening for breast lesions has a high rate of false positives, leading to unnecessary biopsies. However, using diffusion-weighted MR images (DWI) can help to reduce these false positives before biopsy. Currently, tissue properties are estimated using quantitative parameters from biophysical models, which can be unreliable due to fitting instability and partial information loss. Instead, we propose integrating all data processing components into a convolutional neural network (CNN) architecture, allowing for simultaneous and target-specific optimization of image normalization, signal exploitation, global representation learning, and classification. Through testing on a dataset of 222 patients, we demonstrate that our approach improves clinical decision-making compared to current practices.",1
"Entity images could provide significant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More specifically, we first construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.",0
"The use of entity images can convey valuable visual information for learning knowledge representation. Traditional methods for knowledge representation learning only consider structured triples and neglect the abundant visual information that can be extracted from entity images. This paper introduces a new approach to knowledge representation learning called the Image-embodied Knowledge Representation Learning model (IKRL), which incorporates both triple facts and images. Initially, an entity's images are represented using a neural image encoder. These image representations are then combined into an overall image-based representation using an attention-based method. The effectiveness of our IKRL models is assessed through knowledge graph completion and triple classification tasks. Our experimental results demonstrate that our models outperform all baselines in both tasks, indicating the importance of visual information for knowledge representation and the effectiveness of our models in incorporating visual information into knowledge representation learning.",1
"Multiresolution analysis and matrix factorization are foundational tools in computer vision. In this work, we study the interface between these two distinct topics and obtain techniques to uncover hierarchical block structure in symmetric matrices -- an important aspect in the success of many vision problems. Our new algorithm, the incremental multiresolution matrix factorization, uncovers such structure one feature at a time, and hence scales well to large matrices. We describe how this multiscale analysis goes much farther than what a direct global factorization of the data can identify. We evaluate the efficacy of the resulting factorizations for relative leveraging within regression tasks using medical imaging data. We also use the factorization on representations learned by popular deep networks, providing evidence of their ability to infer semantic relationships even when they are not explicitly trained to do so. We show that this algorithm can be used as an exploratory tool to improve the network architecture, and within numerous other settings in vision.",0
"The fundamental tools in computer vision are multiresolution analysis and matrix factorization. This study examines the intersection of these two distinct topics and develops methods to expose hierarchical block structures in symmetric matrices, which is an important aspect for many vision problems. Our novel algorithm, the incremental multiresolution matrix factorization, reveals such structures gradually and is therefore suitable for large matrices. This multiscale analysis goes beyond what a direct global factorization can detect. We assess the effectiveness of the resulting factorizations for relative leveraging in regression tasks using medical imaging data. Additionally, we apply the factorization to representations acquired by popular deep networks, demonstrating their ability to deduce semantic relationships even when they are not explicitly trained to do so. We demonstrate that this algorithm can serve as an exploratory tool for enhancing network architecture and in various other vision contexts.",1
"The empirical fact that classifiers, trained on given data collections, perform poorly when tested on data acquired in different settings is theoretically explained in domain adaptation through a shift among distributions of the source and target domains. Alleviating the domain shift problem, especially in the challenging setting where no labeled data are available for the target domain, is paramount for having visual recognition systems working in the wild. As the problem stems from a shift among distributions, intuitively one should try to align them. In the literature, this has resulted in a stream of works attempting to align the feature representations learned from the source and target domains. Here we take a different route. Rather than introducing regularization terms aiming to promote the alignment of the two representations, we act at the distribution level through the introduction of \emph{DomaIn Alignment Layers} (\DIAL), able to match the observed source and target data distributions to a reference one. Thorough experiments on three different public benchmarks we confirm the power of our approach.",0
"The poor performance of classifiers on new data sets that are acquired in different environments than the ones they were trained on is explained by the shift in distributions between the source and target domains in domain adaptation. It is crucial to solve this problem, especially when there is no labeled data available for the target domain, in order to have visual recognition systems that work effectively in the real world. To address this issue, researchers have attempted to align the feature representations of the source and target domains through the introduction of regularization terms. However, we propose a different approach by introducing DomIn Alignment Layers (DIAL) that match the observed source and target data distributions to a reference distribution. Our experiments on three public benchmarks demonstrate the effectiveness of our approach.",1
"End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning system is specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all ""peripheral"" modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex.   In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.",0
"End-to-end learning involves training a potentially intricate learning system using gradient-based learning to the entire system. The end-to-end learning system is designed to have all modules be differentiable, which allows for a holistic learning process to cover not only a central learning machine but also ""peripheral"" modules like representation learning and memory formation. Its effectiveness has been demonstrated in various tasks, such as playing multiple Atari video games with a single architecture. However, as network architectures become increasingly complex while striving for solutions to tougher challenges, we question the future viability of end-to-end learning in scaling to diverse and complex data processing architectures. We identify potential inefficiencies and argue that end-to-end learning fails to optimally utilize the modular design of current neural networks. Our simple experiments surprisingly reveal these inefficiencies, which can lead to the complete breakdown of learning.",1
"We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.",0
"Our proposal is to use split-brain autoencoders, a modified version of the conventional autoencoder structure, to perform unsupervised representation learning. This approach involves adding a division to the network, creating two separate sub-networks. Each sub-network is trained to tackle a challenging task of predicting one subset of the data channels from the other. By working together, these sub-networks can extract features from the entire input signal. The cross-channel prediction tasks imposed on the network induce a representation that can be applied to other previously unseen tasks. Our method has surpassed the current standard of performance on various extensive transfer learning benchmarks.",1
"The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",0
"The algorithm known as the Correlation Filter is designed to differentiate between images and their translations using a linear template. This algorithm is ideal for tracking objects due to its formulation in the Fourier domain, which allows for quick solutions and the ability to retrain the detector once per frame. Previous studies utilizing the Correlation Filter have used either manually designed or pre-trained features for different tasks. However, this study has successfully addressed this limitation by interpreting the Correlation Filter learner as a differentiable layer within a deep neural network, thus enabling the learning of deep features that are closely linked to the Correlation Filter. Our experiments have shown that this approach allows for the creation of lightweight architectures that achieve state-of-the-art performance at high framerates, which is a crucial practical benefit.",1
"Although stochastic approximation learning methods have been widely used in the machine learning literature for over 50 years, formal theoretical analyses of specific machine learning algorithms are less common because stochastic approximation theorems typically possess assumptions which are difficult to communicate and verify. This paper presents a new stochastic approximation theorem for state-dependent noise with easily verifiable assumptions applicable to the analysis and design of important deep learning algorithms including: adaptive learning, contrastive divergence learning, stochastic descent expectation maximization, and active learning.",0
"Even though stochastic approximation learning methods have been extensively utilized in machine learning research for more than five decades, there are not many formal theoretical analyses of particular machine learning algorithms. This is because stochastic approximation theorems normally have challenging assumptions to convey and confirm. This study introduces a fresh stochastic approximation theorem for state-dependent noise, which comprises verifiable assumptions that can be applied to analyze and establish significant deep learning algorithms such as adaptive learning, contrastive divergence learning, stochastic descent expectation maximization, and active learning.",1
"A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.",0
"The development of incrementally learning systems that can learn about a growing number of concepts from a stream of data is a major challenge in the pursuit of artificial intelligence. This study proposes a new training approach, iCaRL, which enables class-incremental learning by only requiring a small number of classes to be present during training while allowing for the gradual addition of new classes. iCaRL simultaneously learns strong classifiers and data representations, setting it apart from previous methods that were limited to fixed data representations and incompatible with deep learning architectures. Our experiments on CIFAR-100 and ImageNet ILSVRC 2012 data demonstrate that iCaRL can successfully learn many classes incrementally over an extended period, where other strategies quickly fail.",1
"Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.",0
"Many generative models of 3D human motion have limited capabilities and cannot be applied to new movements or activities. This study proposes a deep learning approach for human motion capture data that can learn a comprehensive representation from a vast collection of motion capture data and effectively generalize to new motions. The approach employs an encoding-decoding network that predicts future 3D poses from recent past data, extracting a feature representation of human motion. The study evaluates several network architectures that make different assumptions about time dependencies and limb correlations. The results demonstrate that the approach outperforms recent state-of-the-art methods in skeletal motion prediction, even without action-specific training data. The study concludes that deep feedforward networks trained on a generic mocap database can successfully extract features from human motion data and serve as a foundation for classification and prediction.",1
"Designed as extremely deep architectures, deep residual networks which provide a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems. Being directly applied to a scene labeling problem, however, they were limited to capture long-range contextual dependence, which is a critical aspect. To address this issue, we propose a novel approach, Contextual Recurrent Residual Networks (CRRN) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network. Furthermore, our proposed end-to-end CRRN is completely trained from scratch, without using any pre-trained models in contrast to most existing methods usually fine-tuned from the state-of-the-art pre-trained models, e.g. VGG-16, ResNet, etc. The experiments are conducted on four challenging scene labeling datasets, i.e. SiftFlow, CamVid, Stanford background and SUN datasets, and compared against various state-of-the-art scene labeling methods.",0
"Deep residual networks have achieved impressive results in computer vision tasks due to their ability to provide a rich visual representation and robust convergence behaviors. However, when applied to scene labeling problems, they struggle to capture long-range contextual dependence, which is crucial. To overcome this limitation, we propose Contextual Recurrent Residual Networks (CRRN), a novel approach that can handle both visual representation learning and long-range context modeling in an end-to-end deep network. Unlike existing methods that rely on pre-trained models such as VGG-16 or ResNet, our CRRN is trained from scratch. We evaluate our approach on four challenging scene labeling datasets (SiftFlow, CamVid, Stanford background, and SUN) and compare its performance against various state-of-the-art scene labeling methods.",1
"We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, Depth, and RGB-D videos.",0
"Our approach to representation learning is unsupervised and focuses on encoding motion dependencies in videos in a compact manner. Our framework achieves this by predicting long-term 3D motions using a pair of images from a video clip. To simplify the learning process, we propose that motion be described as a sequence of atomic 3D flows computed with RGB-D modality. Our prediction model uses a Recurrent Neural Network based Encoder-Decoder framework. We assert that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We validate the effectiveness of our learned temporal representations by using them for activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is versatile and can be applied to any input modality, i.e., RGB, Depth, and RGB-D videos.",1
"In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.",0
"Multimodal learning problems have been successfully tackled in recent years using Deep Learning, with the goal of creating useful joint representations for data fusion applications. When the available modalities are time series data, such as audio, video, and sensor signals, it becomes necessary to account for their temporal structure during the fusion process. This study presents the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model that fuses multiple input modalities that are inherently temporal in nature. The model has several key features, including the simultaneous learning of the joint representation and temporal dependencies between modalities, the use of multiple loss terms in the objective function, a maximum correlation loss term to enhance the learning of cross-modal information, and the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. The proposed model was validated through experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. The study empirically analyzed the contributions of different components of the proposed CorrRNN model and demonstrated its robustness, effectiveness, and state-of-the-art performance on multiple datasets.",1
"This workshop explores the interface between cognitive neuroscience and recent advances in AI fields that aim to reproduce human performance such as natural language processing and computer vision, and specifically deep learning approaches to such problems.   When studying the cognitive capabilities of the brain, scientists follow a system identification approach in which they present different stimuli to the subjects and try to model the response that different brain areas have of that stimulus. The goal is to understand the brain by trying to find the function that expresses the activity of brain areas in terms of different properties of the stimulus. Experimental stimuli are becoming increasingly complex with more and more people being interested in studying real life phenomena such as the perception of natural images or natural sentences. There is therefore a need for a rich and adequate vector representation of the properties of the stimulus, that we can obtain using advances in machine learning.   In parallel, new ML approaches, many of which in deep learning, are inspired to a certain extent by human behavior or biological principles. Neural networks for example were originally inspired by biological neurons. More recently, processes such as attention are being used which have are inspired by human behavior. However, the large bulk of these methods are independent of findings about brain function, and it is unclear whether it is at all beneficial for machine learning to try to emulate brain function in order to achieve the same tasks that the brain achieves.",0
"The workshop delves into the intersection of cognitive neuroscience and modern AI developments that aim to replicate human performance, including natural language processing and computer vision using deep learning techniques. To comprehend the brain's cognitive abilities, scientists adopt a system identification methodology where they present various stimuli to subjects and model the response of different brain regions to those stimuli. The objective is to find the function that expresses brain activity in terms of stimulus properties. As experimental stimuli become more complex, a sophisticated vector representation of the stimulus properties is necessary, which modern machine learning techniques can provide. While many machine learning approaches, particularly deep learning, are inspired by human behavior or biological principles, most of these methods are not informed by brain function research. It remains uncertain whether emulating brain function is beneficial for machine learning to achieve tasks similar to the brain's accomplishments.",1
"We present a principled approach to uncover the structure of visual data by solving a novel deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Unfortunately, permutation matrices are discrete, thereby posing difficulties for gradient-based methods. To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations. Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).",0
"A systematic approach is presented to reveal the composition of visual data through a new deep learning task called visual permutation learning. The objective of this task is to identify the arrangement that restores the structure of data from its jumbled versions. In the case of natural images, this task involves restoring the initial image from patches that have been shuffled by an obscure permutation matrix. However, gradient-based techniques face obstacles as permutation matrices are discrete. Thus, we propose the use of doubly-stochastic matrices, which are a continuous approximation of these matrices, generated from standard CNN predictions using Sinkhorn iterations. We present an end-to-end CNN model called DeepPermNet by unrolling these iterations in a Sinkhorn network layer. DeepPermNet is effective in solving two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. The results demonstrate outstanding performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).",1
Triplet networks are widely used models that are characterized by good performance in classification and retrieval tasks. In this work we propose to train a triplet network by putting it as the discriminator in Generative Adversarial Nets (GANs). We make use of the good capability of representation learning of the discriminator to increase the predictive quality of the model. We evaluated our approach on Cifar10 and MNIST datasets and observed significant improvement on the classification performance using the simple k-nn method.,0
"The use of triplet networks is prevalent due to their impressive performance in retrieval and classification tasks. Our study introduces a novel approach to train a triplet network by utilizing it as the discriminator in Generative Adversarial Nets (GANs). By leveraging the representation learning ability of the discriminator, we enhance the model's predictive quality. We tested our method on Cifar10 and MNIST datasets and witnessed a substantial enhancement in classification performance when utilizing the straightforward k-nn method.",1
"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.",0
"Our focus is on examining the characteristics of byte-level recurrent language models. These models can generate disentangled features that correspond to high-level concepts if provided with enough capacity, training data, and compute time. We have identified a solitary unit that performs sentiment analysis. These models learn these representations through unsupervised methods and have achieved state-of-the-art results on the binary subset of the Stanford Sentiment Treebank. Furthermore, they are highly efficient in terms of data usage. Even when working with only a few labeled examples, our approach rivals the performance of robust baselines trained on complete datasets. In addition, we demonstrate that the sentiment unit has a direct impact on the model's generative process. By setting its value to positive or negative, we can generate samples with the corresponding sentiment.",1
"We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called ""odd-one-out learning"". In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition.   On action classification, our method obtains 60.3\% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.",0
"A novel auxiliary task called ""odd-one-out learning"" is utilized in our proposed self-supervised CNN pre-training technique. Through this task, the machine is trained to identify the odd or unrelated element from a group of related elements, without the need for manual annotation. We apply this technique to self-supervised video representation learning by having the network predict the odd video subsequence from a set of sub-sequences sampled from videos. The odd subsequence has incorrect temporal order of frames, while the even ones have the correct order. Our multi-stream convolutional neural network is learned end-to-end and generates temporal representations for videos that can be generalized to other related tasks like action recognition. Our method achieves 60.3\% on the UCF101 dataset and outperforms state-of-the-art self-supervised learning methods by approximately 10%. Similarly, on the HMDB51 dataset, we outperform self-supervised state-of-the-art methods by 12.7% on the action classification task.",1
"Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: -Data insufficiency:Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. -Interpretation:The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.",0
"While deep learning methods have shown promise for predictive modeling in healthcare, there are still significant challenges that need to be addressed. These include data insufficiency, where the sample size is often too small to produce satisfactory results, and interpretation, where the representations learned by deep learning methods must align with medical knowledge. To overcome these challenges, we propose the GRaph-based Attention Model (GRAM), which supplements electronic health records (EHR) with hierarchical information from medical ontologies. Using an attention mechanism, GRAM represents medical concepts as a combination of their ancestors in the ontology, based on data volume and ontology structure. In comparison to other methods, such as recurrent neural networks (RNN), GRAM achieved higher accuracy in predicting rare diseases and improved performance in predicting heart failure, while using less training data. Furthermore, the medical concept representations learned by GRAM align well with the medical ontology, and its attention behaviors can adapt to higher-level concepts in the face of data insufficiency.",1
GPU activity prediction is an important and complex problem. This is due to the high level of contention among thousands of parallel threads. This problem was mostly addressed using heuristics. We propose a representation learning approach to address this problem. We model any performance metric as a temporal function of the executed instructions with the intuition that the flow of instructions can be identified as distinct activities of the code. Our experiments show high accuracy and non-trivial predictive power of representation learning on a benchmark.,0
"Predicting GPU activity is a crucial and intricate issue because of the significant competition among numerous parallel threads. Traditionally, heuristics have been employed to tackle this issue. However, we suggest utilizing a representation learning strategy to address this problem. Our approach involves modeling any performance metric as a temporal function of the executed instructions. We believe that by identifying the flow of instructions as unique code activities, we can improve our predictions. Our research indicates that representation learning has a high level of accuracy and significant predictive power, as demonstrated by our benchmark experiments.",1
"Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.",0
"Several methods for learning joint image and text embedding rely solely on supervised information from paired images and their corresponding textual attributes. However, with the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that can generate more robust multi-modal representations across domains. Our approach combines representation learning models, like auto-encoders, with cross-domain learning criteria such as Maximum Mean Discrepancy loss to create joint embeddings for semantic and visual features. We introduce an innovative technique called unsupervised-data adaptation inference, which constructs more comprehensive embeddings for both labeled and unlabeled data. Our method is evaluated on two datasets, Animals with Attributes and Caltech-UCSD Birds 200-2011, and has a wide range of applications such as zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, our framework surpasses the current state of the art on many of the tasks considered.",1
"A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",0
"The article introduces a framework for unsupervised learning of representations using the infomax principle for large neural populations. An asymptotic approximation of Shannon's mutual information is used to obtain a good initial approximation to the global information-theoretic optimum through a hierarchical infomax method. The article proposes an efficient algorithm based on gradient descent to learn representations from input datasets, which works for complete, overcomplete, and undercomplete bases. Numerical experiments confirm that the method is robust and efficient in extracting salient features from input datasets. Compared to existing methods, the proposed algorithm is faster and more robust in unsupervised representation learning. Additionally, the method can be extended to train supervised or unsupervised deep structure networks.",1
"Reinforcement learning optimizes policies for expected cumulative reward. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, making it a difficult and impoverished signal for end-to-end optimization. To augment reward, we consider a range of self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. While current results show that learning from reward alone is feasible, pure reinforcement learning methods are constrained by computational and data efficiency issues that can be remedied by auxiliary losses. Self-supervised pre-training and joint optimization improve the data efficiency and policy returns of end-to-end reinforcement learning.",0
"The process of reinforcement learning involves optimizing policies to achieve a cumulative reward. However, relying solely on this reward can be problematic, as it is often delayed and infrequent, making it challenging to optimize effectively. To address this issue, we propose incorporating self-supervised tasks that provide additional sources of supervision for representation learning. By incorporating states, actions, and successors, these tasks offer instantaneous and ubiquitous supervision, even in the absence of reward. While reinforcement learning alone is feasible, it is limited by computational and data efficiency issues. The inclusion of auxiliary losses through self-supervised pre-training and joint optimization can improve data efficiency and policy returns, resulting in more effective reinforcement learning.",1
"The goal of unsupervised representation learning is to extract a new representation of data, such that solving many different tasks becomes easier. Existing methods typically focus on vectorized data and offer little support for relational data, which additionally describe relationships among instances. In this work we introduce an approach for relational unsupervised representation learning. Viewing a relational dataset as a hypergraph, new features are obtained by clustering vertices and hyperedges. To find a representation suited for many relational learning tasks, a wide range of similarities between relational objects is considered, e.g. feature and structural similarities. We experimentally evaluate the proposed approach and show that models learned on such latent representations perform better, have lower complexity, and outperform the existing approaches on classification tasks.",0
"Unsupervised representation learning aims to derive a novel data representation that facilitates the resolution of various tasks. However, current techniques are primarily designed for vectorized data and do not adequately address relational data that encompasses inter-instance relationships. Our study introduces a relational unsupervised representation learning approach that treats relational datasets as hypergraphs and generates new features by clustering vertices and hyperedges. To arrive at a representation that suits several relational learning tasks, we consider a broad range of similarities between relational objects, such as feature and structural similarities. We perform experiments to assess the efficacy of the proposed approach and demonstrate that models trained on such latent representations exhibit superior performance, lower complexity, and surpass existing methods in classification tasks.",1
"Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning.",0
"The development of neural network based classifiers has revolutionized the concept of automatic feature learning from an unattainable goal of stronger AI to a customary and expected feature of practical systems. Since the advent of AlexNet, all successful submissions of the ImageNet challenge have incorporated end-to-end representation learning. Representation learning has now become an important and separate task from supervised learning due to the usefulness of good representations for transfer learning. Although supervised methods are currently the top performers in learning transferable representations, recent research has shown that generative models can also be effective in representation learning. The question arises: can the quality of representations learned from these generative methods ever match those from their supervised counterparts? In this study, we argue that generative models have greater potential for representation learning from an information theoretic perspective. We present several experimentally validated assumptions that demonstrate the upper bounds of supervised learning in its capacity for representation learning, whereas certain generative models, specifically Generative Adversarial Networks (GANs), are not limited in the same way. This analysis aims to motivate further exploration of generative representation learning with a rigorous perspective.",1
"Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only ""autoencodes"" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution $p(z)$ and decoding distribution $p(x|z)$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.",0
"Representation learning aims to uncover specific features in observed data through a learned representation that is useful for downstream tasks such as classification. For example, an effective representation for 2D images could concentrate solely on the overall structure while disregarding detailed texture. In this study, we introduce a straightforward yet systematic approach to develop such global representations by integrating Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE, and PixelRNN/CNN. Our proposed VAE model provides us with the ability to manage what the global latent code can learn, and through the appropriate architecture design, we can compel the global latent code to discard irrelevant information such as texture in 2D images, hence the VAE only ""autoencodes"" data in a lossy manner. Furthermore, by utilizing autoregressive models as both prior distribution $p(z)$ and decoding distribution $p(x|z)$, we can significantly enhance the generative modeling performance of VAEs, achieving new state-of-the-art outcomes for density estimation tasks such as MNIST, OMNIGLOT, and Caltech-101 Silhouettes.",1
"We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchi- cal predictor. Our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions. We theoret- ically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the al- gorithm on text classification and language mod- eling, respectively, and show that they compare favorably to common baselines in terms of accu- racy and running time.",0
"The focus of our study is on multi-class classification, where the predictor has a hierarchical structure that permits a large number of labels during both the training and testing phases. The efficacy of such models is heavily reliant on the tree structure, and while past research has addressed how to learn the tree structure, it assumed that the feature vectors remained unchanged. We present a new algorithm that concurrently performs representation learning for input data and hierarchical predictor learning. Our technique optimizes an objective function that favors balanced and easily separable multi-way node partitions. We conduct a theoretical analysis of this objective, demonstrating that it produces a boosting style property and an error classification bound. We then demonstrate how to extend the algorithm to conditional density estimation. We empirically validate both algorithm variations on text classification and language modeling, respectively, and demonstrate that they outperform common baselines in terms of accuracy and runtime.",1
"Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.",0
"The task of acquiring transformation invariant representations of visual data is a significant issue in the field of computer vision. While deep convolutional networks have produced impressive results for image and video classification, they have struggled with classifying images that have undergone geometric transformations. This paper introduces a new approach, the Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are naturally invariant to isometric transformations, like rotation and translation, of input images. Instead of using traditional convolution and pooling layers, TIGraNet employs graph spectral convolution and dynamic graph pooling layers, allowing for increased resistance to data variability and sustained performance with limited training sets. Our tests demonstrate that TIGraNet performs exceptionally well on rotated and translated images from the test set, outperforming classical architectures that are sensitive to transformations in the data.",1
"While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (""PredNet"") architecture that is inspired by the concept of ""predictive coding"" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",0
"Despite significant progress in utilizing deep learning algorithms to address supervised learning tasks, the challenge of unsupervised learning, which involves using unlabeled examples to gain insights into the structure of a domain, remains an unresolved issue. This study examines the prediction of future frames in a video sequence as an unsupervised learning technique to understand the structure of the visual world. We propose a predictive neural network called ""PredNet,"" which is inspired by the concept of ""predictive coding"" from neuroscience literature. These networks learn to predict future frames by each layer making local predictions and only sending deviations from these predictions to subsequent layers. Our findings indicate that these networks can effectively learn to predict the movement of synthetic objects, and that they can learn internal representations that facilitate object recognition with fewer training views. Furthermore, these networks can handle complex natural image streams, such as car-mounted camera videos, and capture both egocentric and object movement in the visual scene. The representation captured by these networks is also useful for estimating the steering angle, suggesting that prediction is a powerful framework for unsupervised learning that enables implicit learning of object and scene structure.",1
"Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",0
"Adversarial examples pose a threat to most machine learning classifiers, including deep neural networks, as they can lead to incorrect outputs while being imperceptible to human eyes. Rather than presenting a single method, this paper aims to advance our understanding of adversarial examples through theoretical analysis using topology. By incorporating an oracle ($f_2$) similar to human eyes, we identify the key reasons why a classifier ($f_1$) can be fooled. Investigating the topological relationship between the two spaces corresponding to $f_1$ and $f_2$, we establish necessary and sufficient conditions to determine if $f_1$ is always strong-robust against adversarial examples. Our theorems reveal that even one unnecessary feature can render $f_1$ not strong-robust, and highlight the importance of feature representation learning in developing a classifier that is accurate and strong-robust.",1
"Visual relations, such as ""person ride bike"" and ""bike next to car"", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate $\approx$ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors.",0
"The comprehension of an image can be enhanced through visual relations, such as ""person ride bike"" and ""bike next to car"", which have already proven useful in connecting computer vision with natural language. However, due to the complexity of modeling subject-predicate-object relation triplets, little has been done to predict and localize visual relations. To address this issue, we introduce the Visual Translation Embedding network (VTransE), which places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation. Our network includes a feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional manner, allowing for training and inference in a single pass. VTransE is the first end-to-end relation detection network and outperforms other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Despite being a purely visual model, VTransE is still competitive with Lu's multi-modal model with language priors.",1
"In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.",0
"For fields like healthcare, it's crucial to have predictive models that are both precise and offer causal interpretation. To achieve this, we suggest using a causal regularizer that directs predictive models towards solutions that can be causally interpreted. We conducted a thorough assessment of Electronic Health Records (EHR) on a larger scale, and our causally-regularized model performed better than its L1-regularized equivalent in terms of causal accuracy, while also displaying competitive predictive performance. Our methodology involved applying nonlinear causality analysis to a neural network architecture that was specifically causally regularized. We also demonstrate that the suggested causal regularizer can be combined with neural representation learning algorithms, resulting in up to a 20% improvement in detecting multivariate causation, a common scenario in healthcare, where several causal factors must exist simultaneously to influence the target variable.",1
"In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the neuronal-birth is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.",0
"This paper discusses the challenges of online representation learning in non-stationary environments and proposes a new framework inspired by adult neurogenesis in the hippocampus. This framework incorporates the addition and deletion of hidden units, allowing for continuous adaptation of model architecture. The birth of new units is determined by the current performance of the dictionary, while the death of units is implemented through l1/l2-regularization. Hidden unit connectivity adaptation is achieved through sparsity in dictionary elements. Empirical evaluation on various datasets shows that the proposed approach outperforms the state-of-the-art fixed-size online sparse coding method by Mairal et al. (2009) in non-stationary data scenarios. The study also identifies certain data and model properties associated with the improvements.",1
"Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.",0
"Dataset augmentation is a commonly used technique in supervised learning where domain-specific transformations are applied to expand training sets artificially. However, this method has limitations as each new domain requires careful consideration and testing of transformations, which limits its reusability and practicality. This paper proposes a simpler, domain-agnostic approach to dataset augmentation, where existing data points are transformed using simple techniques such as adding noise or interpolating. The main innovation is that transformations are carried out in a learned feature space rather than input space, which is timely given the renewed interest in unsupervised representation learning. This proposal has not been tested empirically before, but our approach using context vectors generated by sequence-to-sequence models shows promising results for both static and sequential data.",1
"Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.",0
"Contemporary multi-task learning methods are limited by their assumption of linear models, which is now considered shallow in the age of deep learning. This paper introduces a novel deep multi-task representation learning framework that enables cross-task sharing structure to be learned at every layer in a deep network. Our approach generalizes matrix factorization techniques used by conventional MTL algorithms to tensor factorization, facilitating automatic learning of end-to-end knowledge sharing in deep networks. Unlike existing deep learning approaches that require a user-defined multi-task sharing strategy, our approach is applicable to both homogeneous and heterogeneous MTL. Experimental results demonstrate the effectiveness of our deep multi-task representation learning approach by achieving higher accuracy and fewer design choices.",1
"The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout. We show that our regularized loss function can be efficiently minimized using Information Dropout, a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity. When the task is the reconstruction of the input, we show that our loss function yields a Variational Autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Finally, we prove that we can promote the creation of disentangled representations simply by enforcing a factorized prior, a fact that has been observed empirically in recent work. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.",0
"The cross-entropy loss function used in deep learning is related to optimal representations, but it lacks some key properties. To address this, we propose adding a regularization term that involves injecting multiplicative noise into a Deep Neural Network's activations. This is similar to dropout, a common practice. We demonstrate that our regularized loss function can be efficiently minimized using Information Dropout, which is rooted in information theory and adapts to data while exploiting limited-capacity architectures. Our loss function also produces a Variational Autoencoder for the input reconstruction task, linking representation learning, information theory, and variational inference. By enforcing a factorized prior, we can promote the creation of disentangled representations, as recently observed empirically. Experiments support our theoretical intuitions, showing that information dropout yields comparable or better generalization performance than binary dropout, particularly on smaller models, by adapting the noise to both the network structure and the test sample.",1
"In this paper we introduce the deep kernelized autoencoder, a neural network model that allows an explicit approximation of (i) the mapping from an input space to an arbitrary, user-specified kernel space and (ii) the back-projection from such a kernel space to input space. The proposed method is based on traditional autoencoders and is trained through a new unsupervised loss function. During training, we optimize both the reconstruction accuracy of input samples and the alignment between a kernel matrix given as prior and the inner products of the hidden representations computed by the autoencoder. Kernel alignment provides control over the hidden representation learned by the autoencoder. Experiments have been performed to evaluate both reconstruction and kernel alignment performance. Additionally, we applied our method to emulate kPCA on a denoising task obtaining promising results.",0
"This article presents the deep kernelized autoencoder, a type of neural network that permits the explicit approximation of two mappings: (i) from an input space to a user-specified kernel space, and (ii) from the kernel space back to the input space. The deep kernelized autoencoder is derived from traditional autoencoders and trained with a new unsupervised loss function. During training, both input sample reconstruction accuracy and kernel matrix alignment are optimized. The alignment of the kernel matrix and the inner products of hidden representations computed by the autoencoder allows for control over the learned hidden representation. The article reports on experiments that assess both reconstruction and kernel alignment performance, and demonstrates the method's success in emulating kPCA on a denoising task.",1
"Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.",0
"Despite notable advancements in the last 25 years, unconstrained face verification remains a difficult issue. This paper suggests a solution that combines a deep CNN-based technique with a low-dimensional discriminative embedding acquired through triplet probability restrictions to tackle the problem of unconstrained face verification. Not only does this embedding lead to enhanced performance, it also offers significant advantages in terms of memory and post-processing operations such as subject-specific clustering. The proposed algorithm's experiments on the challenging IJB-A dataset reveal that it performs either similarly or better than state-of-the-art methods in terms of verification and identification metrics while utilizing less training data and time. The proposed approach's superior performance on the CFP dataset demonstrates that the deep CNN's representation is resilient to extreme pose variation. Additionally, through simple clustering experiments on both IJB-A and LFW datasets, we demonstrate the deep features' resilience to challenges like age, pose, blur, and clutter.",1
"Limited annotated data available for the recognition of facial expression and action units embarrasses the training of deep networks, which can learn disentangled invariant features. However, a linear model with just several parameters normally is not demanding in terms of training data. In this paper, we propose an elegant linear model to untangle confounding factors in challenging realistic multichannel signals such as 2D face videos. The simple yet powerful model does not rely on huge training data and is natural for recognizing facial actions without explicitly disentangling the identity. Base on well-understood intuitive linear models such as Sparse Representation based Classification (SRC), previous attempts require a prepossessing of explicit decoupling which is practically inexact. Instead, we exploit the low-rank property across frames to subtract the underlying neutral faces which are modeled jointly with sparse representation on the action components with group sparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot automatic method on raw face videos performs as competitive as SRC applied on manually prepared action components and performs even better than SRC in terms of true positive rate. We apply the model to the even more challenging task of facial action unit recognition, verified on the MPI Face Video Database (MPI-VDB) achieving a decent performance. All the programs and data have been made publicly available.",0
"The insufficient annotated data available for recognizing facial expressions and action units presents a challenge in training deep networks that can learn invariant features. However, a linear model with a few parameters requires less training data. In this study, we propose a simple linear model to untangle confounding factors in complex multichannel signals like 2D facial videos. This model does not rely on extensive training data and is natural for recognizing facial actions without disentangling the identity explicitly. Previous attempts have required explicit decoupling, which is imprecise. We exploit the low-rank property across frames to subtract the underlying neutral faces, modeled jointly with sparse representation on the action components with group sparsity enforced. Our one-shot automatic method performs competitively to manually prepared action components on the extended Cohn-Kanade dataset (CK+), and even better in terms of true positive rate. We also apply the model to the more challenging task of facial action unit recognition, verified on the MPI Face Video Database (MPI-VDB) with decent performance. All data and programs are publicly available.",1
"In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the driver identification problem show that ARNet can learn a good generalized driving style representation: It significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods.",0
"The aim of this paper is to investigate the acquisition of generalized driving style representations using GPS trip data. To achieve this, we introduce a unique approach that combines supervised and unsupervised feature learning through an Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework called trip2vec. This approach allows us to directly learn driving styles from GPS records. Our experiments demonstrate the effectiveness of ARNet in learning a good generalized driving style representation. Specifically, it outperforms existing methods and alternative architectures by achieving the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (with at least 3% improvement) compared to traditional supervised learning methods.",1
"Obtaining common representations from different modalities is important in that they are interchangeable with each other in a classification problem. For example, we can train a classifier on image features in the common representations and apply it to the testing of the text features in the representations. Existing multi-modal representation learning methods mainly aim to extract rich information from paired samples and train a classifier by the corresponding labels; however, collecting paired samples and their labels simultaneously involves high labor costs. Addressing paired modal samples without their labels and single modal data with their labels independently is much easier than addressing labeled multi-modal data. To obtain the common representations under such a situation, we propose to make the distributions over different modalities similar in the learned representations, namely modality-invariant representations. In particular, we propose a novel algorithm for modality-invariant representation learning, named Deep Modality Invariant Adversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation (DA). Using the modality-invariant representations learned by DeMIAN, we achieved better classification accuracy than with the state-of-the-art methods, especially for some benchmark datasets of zero-shot learning.",0
"In classification problems, the ability to interchange common representations derived from different modalities is crucial. For instance, a classifier trained on image features in a common representation can be used to test text features in the same representation. However, existing multi-modal representation learning methods focus on extracting rich information from paired samples and training a classifier with corresponding labels, which can be very expensive. In contrast, addressing labeled single modal data and unpaired modal samples without their labels is simpler. To obtain common representations in such scenarios, we propose learning modality-invariant representations that make the distributions across modalities similar. Our proposed algorithm, Deep Modality Invariant Adversarial Network (DeMIAN), leverages the concept of Domain Adaptation (DA). With DeMIAN, we achieved better classification accuracy than state-of-the-art methods, particularly in benchmark datasets for zero-shot learning.",1
"Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all the faces are from a pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real datasets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces.",0
"In recent years, face recognition techniques have undergone significant development. Despite this progress, recognizing partially obscured faces remains a challenge for existing face recognition systems. This capability is critical for real-world applications such as surveillance and security. Although much research has focused on developing face de-occlusion methods, most are only effective under limited conditions, such as a predefined, closed set of faces. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially obscured faces in diverse environments. The RLA model consists of two LSTM components: the multi-scale spatial LSTM encoder, which generates a latent representation by sequentially reading facial patches of various sizes, and the LSTM decoder with a dual-channel architecture, which reconstructs the overall face and detects occlusion. The decoder breaks down the task of face de-occlusion into restoring the occluded part step by step, while minimizing the loss of identity information and ensuring face recognition accuracy over recovered faces. We also introduce an identity-preserving adversarial training scheme to improve the RLA. Extensive experiments on synthetic and real datasets of faces with occlusion demonstrate the effectiveness of our proposed RLA in removing various types of facial occlusion at different locations. Our method outperforms other de-occlusion methods in promoting recognition performance over partially-occluded faces.",1
"Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music.",0
"Deep neural network success stories often involve supervised learning, which relies on available labels to power gradient-based learning methods. However, creating such labels can be costly, leading to growing interest in weak labels that offer coarse information and have uncertainty regarding time, location, or value. Utilizing weak labels poses significant challenges for the learning process, and current methods often use standard supervised approaches that reassign or prune labels during learning. However, this limits information gain as only the importance of labels where the network yields reasonable results is enhanced. Our proposal is to treat weak-label training as an unsupervised problem and use the labels to guide representation learning to induce structure. We introduce two autoencoder extensions: class activity penalties and structured dropout. We showcase the effectiveness of our approach in the context of score-informed source separation of music.",1
"We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work.",0
"Our paper introduces a new technique for learning multi-modal representations of images and text. To our knowledge, this is the first method that employs adversarial learning principles to multi-modal learning without the use of image-text pairs. Instead, we only utilize category information, in contrast to previous approaches that rely on image-text pairs. Our results demonstrate that our method achieves multi-modal features that are more similar to the image and text distributions in the feature space, as compared to other methods that use image-text pairs. Furthermore, we show that our multi-modal feature contains universal semantic information, even when trained solely for category prediction. Our model is end-to-end and easy to extend to other multi-modal learning tasks.",1
"Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks.   Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.",0
"The goal of hashing is to produce compact code words that maintain similarity, making them ideal for image retrieval on a large scale. Many current hashing methods involve encoding images as a feature vector, followed by a separate step to generate hash codes through binarization. However, this two-stage process may not be optimal. In this study, we introduce Deep Residual Hashing (DRH), a deep architecture for supervised hashing that incorporates residual learning for simultaneous representation learning and hash coding. The DRH model includes a sub-network with stacked residual blocks, a hashing layer for binarization, a supervised retrieval loss function based on neighbourhood component analysis, and hashing-related losses and regularisation to improve the quality of hash coding and control quantization error. Our extensive experiments on a large chest x-ray image database with co-morbidities demonstrate significant improvements over the latest state-of-the-art methods.",1
"With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.   This article surveys various constraints that encourage a learning algorithm to discover factorial representations. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to.   This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.",0
"Representation learning has regained importance in the field of artificial intelligence with the renewed interest in neural networks. It involves discovering valuable encodings of data that make relevant information in a domain explicit. Factorial representations are particularly useful as they identify independent causal factors of variation in data and are compact, accurate, and easily interpreted by humans. These representations support various applications, including generating new examples, search and indexing, novelty detection, and transfer learning. This article examines the constraints that encourage learning algorithms to discover factorial representations, categorizing them as either unsupervised or supervised inductive biases. Unsupervised biases use assumptions about the environment, such as the statistical distribution of factor coefficients, while supervised biases are constraints based on additional information connected to observations. This survey reviews a range of models that address the challenge of learning factorial representations, providing a framework for comparing them based on the strengths of their underlying supervised and unsupervised inductive biases.",1
"In this work, we address the challenging video scene parsing problem by developing effective representation learning methods given limited parsing annotations. In particular, we contribute two novel methods that constitute a unified parsing framework. (1) \textbf{Predictive feature learning}} from nearly unlimited unlabeled video data. Different from existing methods learning features from single frame parsing, we learn spatiotemporal discriminative features by enforcing a parsing network to predict future frames and their parsing maps (if available) given only historical frames. In this way, the network can effectively learn to capture video dynamics and temporal context, which are critical clues for video scene parsing, without requiring extra manual annotations. (2) \textbf{Prediction steering parsing}} architecture that effectively adapts the learned spatiotemporal features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance. Extensive experiments over two challenging datasets, Cityscapes and Camvid, have demonstrated the effectiveness of our methods by showing significant improvement over well-established baselines.",0
"Our work tackles the difficult task of video scene parsing by developing novel methods for representation learning despite limited parsing annotations. We present two innovative approaches that form a cohesive parsing framework. The first method, called Predictive Feature Learning, involves learning spatiotemporal discriminative features by having a parsing network predict future frames and their parsing maps based solely on historical frames. This approach allows the network to capture critical clues for video scene parsing, such as video dynamics and temporal context, without requiring extra manual annotations. The second method, Prediction Steering Parsing, adapts the learned features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance. Our experiments on two challenging datasets, Cityscapes and Camvid, demonstrate the effectiveness of our methods, which outperform well-established baselines.",1
"In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.",0
"This paper examines the issue of categorizing events using multi-variate time series data that contains both continuous and categorical variables. Due to the intricate temporal connections between variables and the sparsity of the data, the problem of event classification is particularly challenging. Most current methods tackle this problem by developing manual features or dividing the issue into homogeneous variates. However, we propose three representation learning algorithms that operate on symbolized sequences, enabling the use of a deep architecture for classifying heterogeneous time-series data. These representations are trained along with the rest of the network architecture in an end-to-end manner, ensuring that the learned features are effective for the given task. Through experiments on three real-world datasets, we demonstrate the effectiveness of the proposed approaches.",1
"Supervised (pre-)training currently yields state-of-the-art performance for representation learning for visual recognition, yet it comes at the cost of (1) intensive manual annotations and (2) an inherent restriction in the scope of data relevant for learning. In this work, we explore unsupervised feature learning from unlabeled video. We introduce a novel object-centric approach to temporal coherence that encourages similar representations to be learned for object-like regions segmented from nearby frames. Our framework relies on a Siamese-triplet network to train a deep convolutional neural network (CNN) representation. Compared to existing temporal coherence methods, our idea has the advantage of lightweight preprocessing of the unlabeled video (no tracking required) while still being able to extract object-level regions from which to learn invariances. Furthermore, as we show in results on several standard datasets, our method typically achieves substantial accuracy gains over competing unsupervised methods for image classification and retrieval tasks.",0
"Representation learning for visual recognition currently achieves state-of-the-art performance through supervised (pre-)training. However, this approach requires extensive manual annotations and is limited in the scope of relevant data for learning. This study investigates unsupervised feature learning from unlabeled video and introduces a unique object-centric method for temporal coherence that encourages similar representations to be learned for object-like regions segmented from nearby frames. The framework employs a Siamese-triplet network to train a deep convolutional neural network (CNN) representation. Compared to existing methods, our approach offers the advantage of lightweight preprocessing of unlabeled video without requiring tracking, while still being able to extract object-level regions for learning invariances. Additionally, our results on various standard datasets demonstrate substantial accuracy gains over competing unsupervised methods for image classification and retrieval tasks.",1
"We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels.",0
"Our proposal involves utilizing deconvolutional networks and SAX discretization to acquire a new model for learning the representation of multivariate time series. The deconvolutional networks efficiently utilize the potent expressiveness of deep neural networks through unsupervised learning. We have specifically designed a network structure that captures the cross-channel correlation by means of deconvolution, thereby enabling the pooling operation to perform dimension reduction along each position in the individual channel. The feature vectors undergo discretization using Symbolic Aggregate Approximation to extract the bag of features that aids in classification. We provide a detailed comparison with the sequence distance based approach to showcase the effectiveness of our model on standard datasets. Additionally, we generate a Markov matrix from the discretized representation obtained from the deconvolution to depict the time series as complex networks, which demonstrate more class-specific statistical properties and clear structures for different labels.",1
"Since about 100 years ago, to learn the intrinsic structure of data, many representation learning approaches have been proposed, including both linear ones and nonlinear ones, supervised ones and unsupervised ones. Particularly, deep architectures are widely applied for representation learning in recent years, and have delivered top results in many tasks, such as image classification, object detection and speech recognition. In this paper, we review the development of data representation learning methods. Specifically, we investigate both traditional feature learning algorithms and state-of-the-art deep learning models. The history of data representation learning is introduced, while available resources (e.g. online course, tutorial and book information) and toolboxes are provided. Finally, we conclude this paper with remarks and some interesting research directions on data representation learning.",0
"For the past century, various approaches to representation learning have emerged to understand the inner structure of data. These include both linear and nonlinear methods, as well as supervised and unsupervised techniques. Recently, deep architectures have gained popularity for representation learning and have achieved excellent results in tasks such as speech recognition, object detection, and image classification. This paper aims to examine the history of data representation learning methods, including both traditional feature learning algorithms and state-of-the-art deep learning models. We provide available resources like online courses, tutorials, and book information, as well as toolboxes. In conclusion, some interesting research directions on data representation learning are discussed.",1
"Person re-identification (Re-ID) poses a unique challenge to deep learning: how to learn a deep model with millions of parameters on a small training set of few or no labels. In this paper, a number of deep transfer learning models are proposed to address the data sparsity problem. First, a deep network architecture is designed which differs from existing deep Re-ID models in that (a) it is more suitable for transferring representations learned from large image classification datasets, and (b) classification loss and verification loss are combined, each of which adopts a different dropout strategy. Second, a two-stepped fine-tuning strategy is developed to transfer knowledge from auxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel unsupervised deep transfer learning model is developed based on co-training. The proposed models outperform the state-of-the-art deep Re-ID models by large margins: we achieve Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03, Market1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model (45.1\%) beats most supervised models.",0
"The challenge of person re-identification (Re-ID) for deep learning lies in learning a deep model with millions of parameters using a small training set with few or no labels. This paper proposes several deep transfer learning models to address the issue of data sparsity. Firstly, a deep network architecture is designed that is more appropriate for transferring representations learned from large image classification datasets, and combines classification loss and verification loss with different dropout strategies. Secondly, a two-step fine-tuning strategy is developed to transfer knowledge from auxiliary datasets. Thirdly, a novel unsupervised deep transfer learning model is developed based on co-training for an unlabelled Re-ID dataset. The proposed models significantly outperform state-of-the-art deep Re-ID models, achieving a Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03, Market1501, and VIPeR, respectively. Notably, on VIPeR, our unsupervised model (45.1\%) outperforms most supervised models.",1
"The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.",0
"The CNN-based encoding of video features to represent human actions has been largely overlooked, with previous CNN work focusing on merging spatial and temporal networks for shorter sequences. To address this gap, we introduce a new video representation called temporal linear encoding (TLE), which functions as a new layer embedded in CNNs. TLE captures appearance and motion information across entire videos, encoding this into a robust feature representation through end-to-end learning. Advantages of TLE include the ability to encode entire videos into a compact feature representation, applicable to various network types, and modeling feature interactions in a more expressive manner without information loss. Our experiments on the HMDB51 and UCF101 human action datasets demonstrate that TLE outperforms current state-of-the-art methods on both.",1
"Learning representations of data, and in particular learning features for a subsequent prediction task, has been a fruitful area of research delivering impressive empirical results in recent years. However, relatively little is understood about what makes a representation `good'. We propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in the risk of some learner using the learned features as compared to the original inputs. We describe a set of sufficient conditions for unsupervised representation learning to provide a benefit, as measured by this risk gap. These conditions decompose the problem of when representation learning works into its constituent parts, which can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about the joint distribution, and analysis of the feature learner and subsequent supervised learner. We provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low-dimensional manifold and when it forms clusters. We compare our approach to a recently proposed analysis of semi-supervised learning.",0
"Over the years, learning data representations has yielded impressive empirical results, particularly in learning features that aid in subsequent prediction tasks. However, the concept of a ""good"" representation remains largely unexplored. Our proposal introduces the risk gap, which measures the difference in risk between a learner using learned features and one using the original inputs for a given prediction context. We present a set of sufficient conditions for unsupervised representation learning to be advantageous, based on the risk gap. These conditions decompose the problem into separate parts that can be evaluated using an unlabeled sample, domain-specific assumptions about the joint distribution, and analysis of the feature and supervised learners. We provide two examples of such conditions, both based on specific properties of the unlabeled distribution: proximity to a low-dimensional manifold and formation of clusters. Finally, we compare our approach to a recently proposed semi-supervised learning analysis.",1
"Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a task-specific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation can be attained by maximizing the total correlation between the input, latent, and output variables. From the base model, we introduce a semantic noise modeling method which enables class-conditional perturbation on latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise while maintaining its original semantic feature. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed class-conditional perturbation process including t-SNE visualization.",0
"Recent successes in deep learning are attributed to the latent representation learned from multi-layered neural networks through hierarchical feature abstraction. However, the performance of generalization greatly depends on the task-specific objective of the designed network model and the learned latent representation obtained from an appropriate training scenario. In this study, we introduce a novel method to improve the latent representation through a neural network model. Our approach is based on the assumption that maximizing the total correlation between the input, latent, and output variables can lead to a good base representation. Additionally, we propose a semantic noise modeling method to enhance the representational power of the learned latent feature. During training, the latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise, which maintains its original semantic feature and implicitly brings the effect of semantic augmentation on the latent space. Our proposed model can be easily learned using common gradient-based optimization algorithms. Experimental results show that our method outperforms various previous approaches, and we provide empirical analyses for the proposed class-conditional perturbation process, including t-SNE visualization.",1
"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.",0
"Our approach leverages extensive quantities of unlabelled video to develop a scene dynamics model that can be applied to video recognition tasks (such as action classification) and video generation tasks (such as future prediction). Our solution utilizes a generative adversarial network with a spatio-temporal convolutional architecture to differentiate the scene's foreground from the background. Our tests indicate that this model can produce short videos up to one second long at full frame rate with greater accuracy than simple baselines and can forecast plausible futures of static images. Additionally, our experiments and visualizations demonstrate that the model acquires valuable features for action recognition with minimal supervision, implying that scene dynamics provide a promising indication for representation learning. We believe that generative video models can have a significant impact on video comprehension and simulation applications.",1
"Task engagement is defined as loadings on energetic arousal (affect), task motivation, and concentration (cognition). It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overfitting. In this paper, we proposed two deep models (i.e., a deep classifier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were fine-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training.",0
"The concept of task engagement involves factors such as energetic arousal, task motivation, and concentration. However, obtaining accurate cognitive state data can be difficult and costly, leading to poor performance from traditional computational models that rely on limited label information for engagement assessment. To address this issue, our study proposes two deep models (a deep classifier and a deep autoencoder) for engagement assessment with minimal label information. We recruited 15 pilots to participate in a 4-hour flight simulation and recorded their electroencephalograph (EEG) signals during the exercise. We then preprocessed and extracted power spectral features from the EEG signals before training our deep models on the unlabeled data. We fine-tuned our models using different proportions of the labeled data and tested them on the remaining labeled data. Our experimental results show that the deep models' new data representations outperformed the original EEG features for engagement assessment, achieving better accuracies in six different scenarios, with the accuracy increasing as more labeled data was used for training. Our study concludes that deep models are particularly useful for engagement assessment when limited label information is available.",1
"Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.",0
"Currently, representation learning methods are leading the way in various natural language processing and knowledge base inference tasks. However, a significant obstacle is how to effectively include commonsense knowledge into such models. A recent solution involves regularizing relation and entity representations by propositionalization of first-order logic rules. Nonetheless, this method is limited to domains with few entities and rules. In this study, we introduce a highly efficient way to incorporate implication rules into distributed representations for automated knowledge base construction. We convert entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules extracted from WordNet. Surprisingly, we discover that restricting the entity-tuple embedding space does not compromise the model's expressiveness and, in fact, improves generalization as a regularizer. By incorporating a small number of commonsense rules, we achieve a 2 percentage points mean average precision increase over a matrix factorization baseline while experiencing an insignificant increase in runtime.",1
"This work advocates Eulerian motion representation learning over the current standard Lagrangian optical flow model. Eulerian motion is well captured by using phase, as obtained by decomposing the image through a complex-steerable pyramid. We discuss the gain of Eulerian motion in a set of practical use cases: (i) action recognition, (ii) motion prediction in static images, (iii) motion transfer in static images and, (iv) motion transfer in video. For each task we motivate the phase-based direction and provide a possible approach.",0
"The focus of this study is on promoting the learning of Eulerian motion representation as a superior alternative to the Lagrangian optical flow model currently in use. The use of phase, acquired by decomposing the image via a complex-steerable pyramid, effectively captures Eulerian motion. The advantages of Eulerian motion are explored through several practical applications: (i) recognizing actions, (ii) predicting motion in still images, (iii) transferring motion in still images, and (iv) transferring motion in videos. The direction of phase-based approaches is justified for each task and a potential method is suggested.",1
"Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",0
"The learning ability of Generative Adversarial Networks (GAN) to create outstanding representations for unlabelled data has been applied to image generation and scene classification. However, their use in retrieval has not been explored yet. This research demonstrates that GAN's representations can indeed be used for retrieval. The study focuses on heritage documents that feature unlabelled Merchant Marks, sketch-like symbols akin to hieroglyphs. A new GAN architecture is introduced, which is specifically designed for sketch retrieval. Its performance is compared to a modified version of the original GAN architecture concerning simple invariance properties. The experiments suggest that sketch-GANs can learn suitable representations for retrieval, with increased stability to rotation, scale, and translation when compared to the standard GAN architecture.",1
"With increasing demand for efficient image and video analysis, test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications. We propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction. In particular, our approach incorporates the cost of feature computation and model inference, and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models. We formulate this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space. A high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism. We demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets, which achieves $90\%$ of the state-of-the-art performances by using $15\%$ of their overall costs.",0
"As the need for efficient image and video analysis continues to grow, the cost of scene parsing during testing becomes increasingly important for vision applications that are either large-scale or time-sensitive. To address this issue, we have developed a dynamic hierarchical model for anytime scene labeling that allows for flexible trade-offs between accuracy and efficiency in pixel-level prediction. Our approach takes into account the cost of feature computation and model inference and optimizes model performance based on a given test-time budget by learning a sequence of image-adaptive hierarchical models. We have formulated this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space. By using an approximate policy iteration method with an action proposal mechanism, we have developed a high-quality policy for feature and model selection. Through testing on three semantic segmentation datasets, we have demonstrated the advantages of our dynamic non-myopic anytime scene parsing, achieving 90% of the state-of-the-art performance with only 15% of the overall cost.",1
"Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.",0
"For a while now, deep learning algorithms have faced criticism from neuroscientists due to their incompatibility with current knowledge of neurobiology. In this paper, we aim to explore more biologically plausible versions of deep representation learning, with a primary focus on unsupervised learning. However, we also aim to develop a learning mechanism that can account for supervised, unsupervised, and reinforcement learning. We start by considering the fundamental learning rule that governs synaptic weight updates - Spike-Timing-Dependent Plasticity - which arises from a simple update rule that aligns with gradient descent on some objective function. As long as the neuronal dynamics push firing rates towards better values of the objective function, we can interpret this as gradient descent on supervised, unsupervised, or reward-driven tasks. Our second main idea is that this aligns with a form of the variational EM algorithm that implements approximate, rather than exact, posteriors through neural dynamics. We also introduce a novel approach for estimating the gradients required to update hidden states, using an approximation that propagates activations forward and backward. Pairs of layers learn to form a denoising auto-encoder, which contributes to the theory about the probabilistic interpretation of auto-encoders and justifies improved sampling schemes based on the generative interpretation of denoising auto-encoders. We validate all these ideas on generative learning tasks.",1
"Probabilistic models learned as density estimators can be exploited in representation learning beside being toolboxes used to answer inference queries only. However, how to extract useful representations highly depends on the particular model involved. We argue that tractable inference, i.e. inference that can be computed in polynomial time, can enable general schemes to extract features from black box models. We plan to investigate how Tractable Probabilistic Models (TPMs) can be exploited to generate embeddings by random query evaluations. We devise two experimental designs to assess and compare different TPMs as feature extractors in an unsupervised representation learning framework. We show some experimental results on standard image datasets by applying such a method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings.",0
"In addition to serving as tools for answering inference questions, probabilistic models learned as density estimators have the potential to be utilized in representation learning. However, the effectiveness of extracting useful representations is heavily reliant on the specific model being used. We propose that tractable inference, which can be computed in polynomial time, can facilitate the extraction of features from black box models in a general manner. Our research aims to explore how Tractable Probabilistic Models (TPMs) can be leveraged to produce embeddings via random query evaluations. We have developed two experimental designs to evaluate and compare various TPMs as feature extractors in an unsupervised representation learning framework. By applying this method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings, we have obtained some experimental results on common image datasets.",1
"Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments.",0
"The ability of deep neural networks to solve perception problems has increased significantly and they often perform better than humans in tasks such as object recognition, semantic segmentation, and scene understanding. This success can be attributed to their capacity to learn valuable representations of complicated inputs, which is also a challenge for humans. Our study investigates the correlation between the representations learned by these networks and psychological representations obtained from similarity judgments in humans. Our findings reveal that deep features learned to classify objects account for a considerable portion of the variance in human similarity judgments for a collection of animal images. However, these features fail to capture certain qualitative distinctions that are crucial to human representations. To address this, we introduce a technique for adapting deep features to align with human similarity judgments, leading to image representations that could potentially broaden the range of psychological experiments.",1
"In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uni- and multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations.",0
"The Inter-Battery Topic Model (IBTM) is presented in this paper, which enhances traditional topic models by incorporating a factorized latent variable representation. By doing so, the model combines the advantages of a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and the ability to handle missing data. This factorization is achieved by representing data as aligned pairs of observations from different views, allowing for a representation that models topics that exist in both views separately from those that are unique to a single view. This structured consolidation leads to efficient and robust inference and a compact representation. Learning is approached in a Bayesian manner, maximizing a rigorous bound on the log-likelihood. The model's benefits are demonstrated on a synthetic dataset, and it is evaluated on two classification tasks in both uni- and multi-modality settings, achieving state-of-the-art results with extremely compact representations using off-the-shelf convolutional neural network (CNN) features.",1
"This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.",0
"This article discusses the challenges of classification tasks in a specific target domain where labeled training data are only available from related source domains. Two frameworks, domain adaptation and domain generalization, are concerned with such tasks, with the main difference being the availability of unlabeled target data. Scatter Component Analysis (SCA) is a fast representation learning algorithm that can be used for both domain adaptation and domain generalization. SCA uses a geometrical measure, scatter, which operates on a reproducing kernel Hilbert space to find a representation that balances maximizing class separability, minimizing domain mismatch, and achieving data separability. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which leads to a quick and accurate solution. Comprehensive experiments on cross-domain object recognition datasets demonstrate that SCA performs faster than several state-of-the-art algorithms and achieves state-of-the-art classification accuracy in both domain adaptation and domain generalization. Additionally, scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.",1
"What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require millions of semantic labels. We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively, we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data. Finally, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3%",0
"The suitable supervisory signal for training visual representations is under scrutiny. Currently, ConvNets in computer vision rely on category labels from datasets like ImageNet to learn. However, unlike machines, living organisms do not require vast amounts of semantic labels to learn visual representations. This is because biological agents learn through physical interactions with their environment, unlike current vision systems that passively observe images and videos downloaded from the internet. For instance, babies learn by pushing, poking, grasping, and observing objects. To imitate this process, we constructed a system on a Baxter platform that interacts physically with objects in a tabletop environment. The system uses four different physical interactions to amass over 130K datapoints, each of which supervises a shared ConvNet architecture, enabling the learning of visual representations. We demonstrate the quality of the learned representations by examining neuron activations and performing nearest neighbor retrieval on the learned representation. We evaluate the learned ConvNet quantitatively on image classification tasks and demonstrate improvements compared to learning without external data. Lastly, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3%.",1
"Recently, machine learning based single image super resolution (SR) approaches focus on jointly learning representations for high-resolution (HR) and low-resolution (LR) image patch pairs to improve the quality of the super-resolved images. However, due to treat all image pixels equally without considering the salient structures, these approaches usually fail to produce visual pleasant images with sharp edges and fine details. To address this issue, in this work we present a new novel SR approach, which replaces the main building blocks of the classical interpolation pipeline by a flexible, content-adaptive deep neural networks. In particular, two well-designed structure-aware components, respectively capturing local- and holistic- image contents, are naturally incorporated into the fully-convolutional representation learning to enhance the image sharpness and naturalness. Extensively evaluations on several standard benchmarks (e.g., Set5, Set14 and BSD200) demonstrate that our approach can achieve superior results, especially on the image with salient structures, over many existing state-of-the-art SR methods under both quantitative and qualitative measures.",0
"Current approaches to single image super resolution (SR) using machine learning have shifted their focus to jointly learning representations for high-resolution (HR) and low-resolution (LR) image patch pairs in order to improve super-resolved image quality. However, these approaches fail to produce visually pleasing images with sharp edges and fine details as they treat all image pixels equally without considering salient structures. To overcome this limitation, we propose a new SR approach that replaces the classical interpolation pipeline with a flexible, content-adaptive deep neural network. Our approach incorporates two well-designed structure-aware components, which capture local and holistic image contents, into the fully-convolutional representation learning process to enhance image sharpness and naturalness. Extensive evaluations on standard benchmarks (Set5, Set14, and BSD200) demonstrate that our approach produces superior results, especially for images with salient structures, compared to existing state-of-the-art SR methods using both quantitative and qualitative measures.",1
"The keep-growing content of Web images may be the next important data source to scale up deep neural networks, which recently obtained a great success in the ImageNet classification challenge and related tasks. This prospect, however, has not been validated on convolutional networks (convnet) -- one of best performing deep models -- because of their supervised regime. While unsupervised alternatives are not so good as convnet in generalizing the learned model to new domains, we use convnet to leverage semi-supervised representation learning. Our approach is to use massive amounts of unlabeled and noisy Web images to train convnets as general feature detectors despite challenges coming from data such as high level of mislabeled data, outliers, and data biases. Extensive experiments are conducted at several data scales, different network architectures, and data reranking techniques. The learned representations are evaluated on nine public datasets of various topics. The best results obtained by our convnets, trained on 3.14 million Web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is closing the gap with VGG-16. These prominent results suggest a budget solution to use deep learning in practice and motivate more research in semi-supervised representation learning.",0
"The growing content of images on the internet has the potential to enhance deep neural networks, which have achieved great success in image classification and similar tasks. However, the effectiveness of convolutional networks (convnet), one of the top-performing deep models, has not been confirmed due to their supervised regime. Though unsupervised alternatives are not as effective as convnet in generalizing to new domains, we use convnet for semi-supervised representation learning. We train convnets on massive amounts of unlabeled and noisy web images to detect general features, despite challenges like mislabeled data, outliers, and data biases. We conduct extensive experiments at various data scales, network architectures, and data reranking techniques and evaluate the learned representations on nine public datasets. Our convnets, trained on 3.14 million web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and are closing the gap with VGG-16. These results suggest a cost-effective solution for using deep learning and motivate further research in semi-supervised representation learning.",1
"We propose a reinforcement learning based approach to tackle the cost-sensitive learning problem where each input feature has a specific cost. The acquisition process is handled through a stochastic policy which allows features to be acquired in an adaptive way. The general architecture of our approach relies on representation learning to enable performing prediction on any partially observed sample, whatever the set of its observed features are. The resulting model is an original mix of representation learning and of reinforcement learning ideas. It is learned with policy gradient techniques to minimize a budgeted inference cost. We demonstrate the effectiveness of our proposed method with several experiments on a variety of datasets for the sparse prediction problem where all features have the same cost, but also for some cost-sensitive settings.",0
"Our proposed solution for the cost-sensitive learning problem involves using reinforcement learning techniques. In this approach, each input feature is assigned a specific cost, and we handle the acquisition process through a stochastic policy that allows for adaptive feature acquisition. The architecture of our approach utilizes representation learning to enable predictions on any partially observed sample, regardless of the observed features. Our model combines ideas from both representation and reinforcement learning, and it is learned using policy gradient techniques to minimize the cost of inference. We demonstrate the effectiveness of our approach through experiments on various datasets, including sparse prediction problems where all features have equal cost and some cost-sensitive settings.",1
"This paper describes an effective and efficient image classification framework nominated distributed deep representation learning model (DDRL). The aim is to strike the balance between the computational intensive deep learning approaches (tuned parameters) which are intended for distributed computing, and the approaches that focused on the designed parameters but often limited by sequential computing and cannot scale up. In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets. The result implies that our approach is more efficient than the conventional deep learning approaches, and can be applied to big data that is too complex for parameter designing focused approaches. More specifically, DDRL contains two main components, i.e., feature extraction and selection. A hierarchical distributed deep representation learning algorithm is designed to extract image statistics and a nonlinear mapping algorithm is used to map the inherent statistics into abstract features. Both algorithms are carefully designed to avoid millions of parameters tuning. This leads to a more compact solution for image classification of big data. We note that the proposed approach is designed to be friendly with parallel computing. It is generic and easy to be deployed to different distributed computing resources. In the experiments, the largescale image datasets are classified with a DDRM implementation on Hadoop MapReduce, which shows high scalability and resilience.",0
"The distributed deep representation learning model (DDRL) is a novel image classification framework that strikes a balance between deep learning approaches designed for distributed computing and those focused on parameter design but limited by sequential computing. The paper presents DDRL's two main components, feature extraction and selection, and a hierarchical distributed deep representation learning algorithm that extracts image statistics and a nonlinear mapping algorithm that maps these statistics into abstract features. The approach has been shown to achieve state-of-the-art classification accuracy efficiently on medium and large datasets, making it more efficient than conventional deep learning approaches for big data that is too complex for parameter designing focused approaches. DDRL is also parallel computing-friendly, generic, and easy to deploy on different distributed computing resources, as demonstrated by experiments on large-scale image datasets using a DDRM implementation on Hadoop MapReduce, which shows high scalability and resilience.",1
"With this positional paper we present a representation learning view on predicate invention. The intention of this proposal is to bridge the relational and deep learning communities on the problem of predicate invention. We propose a theory reconstruction approach, a formalism that extends autoencoder approach to representation learning to the relational settings. Our intention is to start a discussion to define a unifying framework for predicate invention and theory revision.",0
"In this position paper, we offer a perspective on predicate invention from the standpoint of representation learning. Our aim is to connect the relational and deep learning communities by addressing the issue of predicate invention. We suggest a theory reconstruction methodology, which builds upon the autoencoder approach to representation learning in the relational context. Our goal is to initiate a dialogue regarding the development of a cohesive framework for predicate invention and theory revision.",1
"The way people look in terms of facial attributes (ethnicity, hair color, facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat, hoodies, etc.) is highly dependent on geo-location and weather condition, respectively. This work explores, for the first time, the use of this contextual information, as people with wearable cameras walk across different neighborhoods of a city, in order to learn a rich feature representation for facial attribute classification, without the costly manual annotation required by previous methods. By tracking the faces of casual walkers on more than 40 hours of egocentric video, we are able to cover tens of thousands of different identities and automatically extract nearly 5 million pairs of images connected by or from different face tracks, along with their weather and location context, under pose and lighting variations. These image pairs are then fed into a deep network that preserves similarity of images connected by the same track, in order to capture identity-related attribute features, and optimizes for location and weather prediction to capture additional facial attribute features. Finally, the network is fine-tuned with manually annotated samples. We perform an extensive experimental analysis on wearable data and two standard benchmark datasets based on web images (LFWA and CelebA). Our method outperforms by a large margin a network trained from scratch. Moreover, even without using manually annotated identity labels for pre-training as in previous methods, our approach achieves results that are better than the state of the art.",0
"This study investigates how people's appearance, including facial attributes like ethnicity, hair color, and facial hair, as well as clothing and accessories such as sunglasses and hats, are influenced by their location and weather conditions. By analyzing wearable camera footage from various neighborhoods in a city, the study aims to develop a comprehensive facial attribute classification system without the need for costly manual annotation. The researchers tracked casual walkers for over 40 hours, gathering nearly 5 million pairs of images with varying lighting and poses, along with location and weather data. Using a deep network, the images were analyzed for identity-related attribute features and optimized for location and weather prediction. The network was then fine-tuned with manually annotated samples. The study's approach outperformed existing techniques and achieved state-of-the-art results without using manually annotated identity labels for pre-training.",1
"In this paper, we propose a recurrent framework for Joint Unsupervised LEarning (JULE) of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks.",0
"The purpose of this paper is to introduce a new approach called Joint Unsupervised LEarning (JULE) that combines deep representations and image clusters using a recurrent framework. The clustering algorithm is expressed as a series of steps in a recurrent process that is built on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly, with clustering performed in the forward pass and representation learning in the backward pass. The key idea behind this framework is that good representations benefit image clustering, and clustering results guide representation learning. By using a unified weighted triplet loss to optimize the model end-to-end, we achieve more powerful representations and more precise image clusters. Our experiments demonstrate that JULE outperforms existing methods for image clustering across a range of datasets, and the learned representations can be applied effectively to other tasks.",1
"Several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. The resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. In this paper, we present a method for the out-of-sample extension of graph embeddings using deep neural networks (DNN) to parametrically approximate these nonlinear maps. Compared with traditional nonparametric out-of-sample extension methods, we demonstrate that the DNNs can generalize with equal or better fidelity and require orders of magnitude less computation at test time. Moreover, we find that unsupervised pretraining of the DNNs improves optimization for larger network sizes, thus removing sensitivity to model selection.",0
"A number of popular techniques for graph embedding rely on performing expensive eigendecompositions to transform input data. The resulting eigenvectors only encode embedding coordinates for training samples, requiring further computation for novel data. This paper presents a method for out-of-sample extension using deep neural networks (DNNs) to parametrically approximate nonlinear maps. Compared to nonparametric methods, DNNs generalize with equal or better fidelity and require significantly less computation at test time. Furthermore, unsupervised pretraining improves optimization for larger networks, eliminating sensitivity to model selection.",1
"This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.",0
"In this paper, InfoGAN is introduced as an extension to the Generative Adversarial Network that can autonomously learn disentangled representations. This is achieved by maximizing the mutual information between a small subset of latent variables and the observation. We provide a lower bound to the mutual information objective that can be optimized efficiently and demonstrate that our training procedure is a variant of the Wake-Sleep algorithm. InfoGAN is successful in disentangling various factors such as writing styles from digit shapes, pose from lighting, and background digits from the central digit. Moreover, InfoGAN discovers visual concepts including hair styles, presence/absence of eyeglasses, and emotions on face images. Our experiments illustrate that InfoGAN can learn interpretable representations that are comparable to those learned by fully supervised methods.",1
"Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.",0
"While memory units have been commonly used to improve deep networks' ability to capture long-term dependencies in prediction and reasoning tasks, little research has been conducted on deep generative models (DGMs) that can infer high-level invariant representations from unlabeled data. This study introduces a DGM with a large external memory and an attention mechanism to capture local detail information that may be lost during bottom-up abstraction in representation learning. The smooth attention model allows the entire network to be trained end-to-end using variational auto-encoding Bayesian methods to optimize a variational bound of data likelihood. An asymmetric recognition network is jointly learned to infer high-level invariant representations, reducing competition between bottom-up invariant feature extraction and top-down instance detail generation. Multiple experiments conducted on various datasets demonstrate that memory can significantly improve DGM performance and achieve state-of-the-art results in density estimation, image generation, and missing value imputation.",1
"We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",0
"Our new approach to representation learning for domain adaptation deals with training and testing data that have similar but different distributions. Our inspiration comes from the domain adaptation theory, which suggests that effective domain transfer requires predictions based on features that do not discriminate between the training and test domains. Our approach involves using neural network architectures trained on labeled data from the source domain and unlabeled data from the target domain. As training progresses, we encourage the emergence of features that are discriminative for the main learning task on the source domain but indiscriminate with respect to the domain shift. This can be achieved in almost any feed-forward model by adding a few standard layers and a gradient reversal layer. The resulting architecture can be trained using standard backpropagation and stochastic gradient descent, making it easy to implement using deep learning packages. We demonstrate the success of our approach in two classification problems and a descriptor learning task for person re-identification. Our approach achieves state-of-the-art domain adaptation performance on standard benchmarks.",1
"Cross-domain visual data matching is one of the fundamental problems in many real-world vision tasks, e.g., matching persons across ID photos and surveillance videos. Conventional approaches to this problem usually involves two steps: i) projecting samples from different domains into a common space, and ii) computing (dis-)similarity in this space based on a certain distance. In this paper, we present a novel pairwise similarity measure that advances existing models by i) expanding traditional linear projections into affine transformations and ii) fusing affine Mahalanobis distance and Cosine similarity by a data-driven combination. Moreover, we unify our similarity measure with feature representation learning via deep convolutional neural networks. Specifically, we incorporate the similarity measure matrix into the deep architecture, enabling an end-to-end way of model optimization. We extensively evaluate our generalized similarity model in several challenging cross-domain matching tasks: person re-identification under different views and face verification over different modalities (i.e., faces from still images and videos, older and younger faces, and sketch and photo portraits). The experimental results demonstrate superior performance of our model over other state-of-the-art methods.",0
"Matching visual data across different domains is a crucial issue in various real-world vision applications such as the identification of individuals across ID photos and surveillance videos. The typical approach to this problem includes two steps: first, projecting samples from different domains into a common space, and secondly, calculating the (dis-)similarity in this space based on a specific distance. This paper introduces a new pairwise similarity measurement that improves existing models by expanding traditional linear projections into affine transformations and combining affine Mahalanobis distance and Cosine similarity in a data-driven way. Additionally, the similarity measure is integrated with feature representation learning via deep convolutional neural networks. This allows for an end-to-end approach to model optimization. The effectiveness of the proposed similarity model is evaluated in several challenging cross-domain matching tasks, including person re-identification and face verification. The experimental results demonstrate that our model outperforms other state-of-the-art methods.",1
"We consider the statistical problem of learning common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a ""semantic similarity"". We present an interpretation of this ""semantic similarity"" as learning of equivalence classes. We discuss properties of the embedding obtained by Siamese networks and provide empirical results that demonstrate the ability of Siamese networks to learn common variability.",0
"The focus of our study is on the statistical challenge of identifying shared patterns in data that are captured simultaneously by multiple sensors. Our research reveals that Siamese neural networks offer a natural solution to this problem, particularly in situations where exploratory, data-driven approaches are necessary due to the absence of either a model or labeled information. Over the past few years, numerous scientists have successfully employed Siamese neural networks to generate a data embedding that reflects ""semantic similarity."" We propose a novel understanding of this ""semantic similarity"" as an indication of the acquisition of equivalence classes. Our research delves into the properties of the data embedding produced by Siamese networks and provides empirical evidence of their capacity to identify shared patterns.",1
We explore unsupervised representation learning of radio communication signals in raw sampled time series representation. We demonstrate that we can learn modulation basis functions using convolutional autoencoders and visually recognize their relationship to the analytic bases used in digital communications. We also propose and evaluate quantitative met- rics for quality of encoding using domain relevant performance metrics.,0
"Our study involves the unsupervised acquisition of representation for radio communication signals in their original sampled time series form. Through the use of convolutional autoencoders, we are able to learn basis functions for modulation and visually observe their connection to digital communication's analytic bases. Additionally, we suggest and assess objective measures for encoding quality using domain-appropriate performance metrics.",1
"Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.",0
"Recently, variational inference has been used in representation learning alongside deep generative models. A Bayesian nonparametric variational family called the variational Gaussian process (VGP) has been developed, which modifies its shape to match complex posterior distributions. The VGP creates rough posterior samples by producing latent inputs and distorting them through random non-linear mappings. The distribution over the random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. The VGP is capable of learning any model, as demonstrated by a universal approximation theorem. Auto-encoders inspired the variational objective used for inference, which performs black box inference over a wide range of models. The VGP has attained new state-of-the-art unsupervised learning results, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.",1
"The past decade has witnessed the rapid development of feature representation learning and distance metric learning, whereas the two steps are often discussed separately. To explore their interaction, this work proposes an end-to-end learning framework called DARI, i.e. Distance metric And Representation Integration, and validates the effectiveness of DARI in the challenging task of person verification. Given the training images annotated with the labels, we first produce a large number of triplet units, and each one contains three images, i.e. one person and the matched/mismatch references. For each triplet unit, the distance disparity between the matched pair and the mismatched pair tends to be maximized. We solve this objective by building a deep architecture of convolutional neural networks. In particular, the Mahalanobis distance matrix is naturally factorized as one top fully-connected layer that is seamlessly integrated with other bottom layers representing the image feature. The image feature and the distance metric can be thus simultaneously optimized via the one-shot backward propagation. On several public datasets, DARI shows very promising performance on re-identifying individuals cross cameras against various challenges, and outperforms other state-of-the-art approaches.",0
"Over the past decade, feature representation learning and distance metric learning have experienced rapid development, yet they are often discussed separately. This study proposes an integrated learning framework called DARI (Distance metric And Representation Integration) to explore their interaction and evaluates its effectiveness in person verification. Using labeled training images, a large number of triplet units are generated, each consisting of three images: one person and matched/mismatched references. The objective is to maximize the distance disparity between the matched and mismatched pairs, which is achieved by a deep convolutional neural network architecture. The Mahalanobis distance matrix is factorized as one top fully-connected layer that seamlessly integrates with other bottom layers representing the image feature, allowing simultaneous optimization of the image feature and distance metric via one-shot backward propagation. DARI shows promising performance on re-identifying individuals cross cameras against various challenges, outperforming other state-of-the-art approaches on several public datasets.",1
"Methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since neural networks are not convex and standard guarantees do not apply. This paper develops the first rigorous link between online convex optimization and error backpropagation on convolutional networks. The first step is to introduce circadian games, a mild generalization of convex games with similar convergence properties. The main result is that error backpropagation on a convolutional network is equivalent to playing out a circadian game. It follows immediately that the waking-regret of players in the game (the units in the neural network) controls the overall rate of convergence of the network. Finally, we explore some implications of the results: (i) we describe the representations learned by a neural network game-theoretically, (ii) propose a learning setting at the level of individual units that can be plugged into deep architectures, and (iii) propose a new approach to adaptive model selection by applying bandit algorithms to choose which players to wake on each round.",0
"Convex optimization techniques like accelerated gradient descent are commonly used in the development of deep learning algorithms. However, it is not clear why these methods are effective, as neural networks are not convex and standard guarantees do not apply. This study establishes a rigorous connection between error backpropagation on convolutional networks and online convex optimization. To do this, the authors introduce circadian games as a mild generalization of convex games with similar convergence properties. The main finding is that error backpropagation on a convolutional network is the same as playing out a circadian game, and the waking-regret of players in the game controls the overall rate of convergence of the network. The authors also investigate the implications of their results, including describing the learned representations of a neural network game-theoretically, proposing a learning setting for individual units that can be integrated into deep architectures, and suggesting a novel approach to adaptive model selection using bandit algorithms to select which players to wake on each round.",1
"Methods from convex optimization are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since modern convolutional networks (convnets), incorporating rectifier units and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. This paper provides the first convergence rates for gradient descent on rectifier convnets. The proof utilizes the particular structure of rectifier networks which consists in binary active/inactive gates applied on top of an underlying linear network. The approach generalizes to max-pooling, dropout and maxout. In other words, to precisely the neural networks that perform best empirically. The key step is to introduce gated games, an extension of convex games with similar convergence properties that capture the gating function of rectifiers. The main result is that rectifier convnets converge to a critical point at a rate controlled by the gated-regret of the units in the network. Corollaries of the main result include: (i) a game-theoretic description of the representations learned by a neural network; (ii) a logarithmic-regret algorithm for training neural nets; and (iii) a formal setting for analyzing conditional computation in neural nets that can be applied to recently developed models of attention.",0
"Convex optimization techniques are commonly utilized in the development of deep learning algorithms. Despite the success of these methods, it remains unclear why they work effectively with modern convolutional networks (convnets) that incorporate non-smooth and non-convex elements such as rectifier units and max-pooling. As a result, traditional guarantees may not be applicable. This study presents the first convergence rates for gradient descent on rectifier convnets by leveraging the unique structure of rectifier networks, which involve binary active/inactive gates applied on top of a linear network. This technique extends to other neural networks that perform well in practice, including max-pooling, dropout, and maxout. The key step involves introducing gated games, an extension of convex games that has similar convergence properties and captures the gating function of rectifiers. The central finding is that rectifier convnets converge to a critical point at a rate controlled by the gated-regret of the network's units. The study's corollaries include a game-theoretic explanation of the representations learned by a neural network, a logarithmic-regret algorithm for training neural networks, and a formal framework for analyzing conditional computation in neural networks that can be used for recently developed attention models.",1
"Quantitatively assessing relationships between latent variables and observed variables is important for understanding and developing generative models and representation learning. In this paper, we propose latent-observed dissimilarity (LOD) to evaluate the dissimilarity between the probabilistic characteristics of latent and observed variables. We also define four essential types of generative models with different independence/conditional independence configurations. Experiments using tractable real-world data show that LOD can effectively capture the differences between models and reflect the capability for higher layer learning. They also show that the conditional independence of latent variables given observed variables contributes to improving the transmission of information and characteristics from lower layers to higher layers.",0
"The assessment of relationships between latent and observed variables in a quantitative manner is crucial for comprehending and advancing generative models and representation learning. This study introduces the concept of latent-observed dissimilarity (LOD) to measure the differences between the probabilistic features of these variables. Additionally, it identifies four fundamental types of generative models that have distinct configurations of independence/conditional independence. Empirical observations using real-world data that is manageable demonstrate that LOD is effective in capturing inconsistencies between models and highlighting the ability to learn at higher layers. Furthermore, these observations indicate that the conditional independence of latent variables when given observed variables plays a vital role in enhancing the transmission of information and characteristics from lower to higher layers.",1
"We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.",0
"A general approach for acquiring data representations from multiple tasks is discussed in this paper. This approach is justified for both multitask learning and learning-to-learn scenarios. The paper provides a detailed illustration of this method in the context of linear feature learning. The theoretical advantage of multitask representation learning over independent task learning is established based on certain conditions. Specifically, the paper examines the example of half-space learning and identifies the regime in which multitask representation learning is advantageous over independent task learning as a function of the sample size, number of tasks, and intrinsic data dimensionality. The findings have potential applications in multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.",1
"This paper proposes a novel approach to person re-identification, a fundamental task in distributed multi-camera surveillance systems. Although a variety of powerful algorithms have been presented in the past few years, most of them usually focus on designing hand-crafted features and learning metrics either individually or sequentially. Different from previous works, we formulate a unified deep ranking framework that jointly tackles both of these key components to maximize their strengths. We start from the principle that the correct match of the probe image should be positioned in the top rank within the whole gallery set. An effective learning-to-rank algorithm is proposed to minimize the cost corresponding to the ranking disorders of the gallery. The ranking model is solved with a deep convolutional neural network (CNN) that builds the relation between input image pairs and their similarity scores through joint representation learning directly from raw image pixels. The proposed framework allows us to get rid of feature engineering and does not rely on any assumption. An extensive comparative evaluation is given, demonstrating that our approach significantly outperforms all state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better ability to generalize across datasets without fine-tuning.",0
"A fresh method for person re-identification, a crucial task in distributed multi-camera surveillance systems, is presented in this paper. While a variety of powerful algorithms have been introduced in recent years, most of them have focused on developing hand-crafted features and learning metrics either individually or sequentially. In contrast, the authors propose a unified deep ranking framework that simultaneously addresses both of these essential components to maximize their strengths. The framework starts with the principle that the correct match of the probe image should be ranked first among the entire gallery set. To minimize the cost corresponding to the ranking disorders of the gallery, an effective learning-to-rank algorithm is proposed. The ranking model is solved with a deep convolutional neural network (CNN) that establishes the relationship between input image pairs and their similarity scores through joint representation learning straight from raw image pixels. The proposed framework eliminates the need for feature engineering and does not rely on any assumptions. The authors provide an extensive comparative evaluation of the framework, demonstrating that it outperforms all state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01, and CAVIAR4REID datasets. The approach also has better ability to generalize across datasets without fine-tuning.",1
"Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.",0
"Previous research has mainly focused on analyzing affect based on facial expressions rather than body language. This study, however, concentrates on the analysis of body affect. It is important to note that affect and action are intertwined in natural human interactions. For instance, an individual might be speaking while exhibiting a smile. Consequently, recognizing body affect in a sequence necessitates algorithms that can capture both micro movements that differentiate between emotions and macro variations of different actions. In contrast to traditional approaches for time-series data analytics, this paper introduces a multi-task learning model that can learn a shared representation suited for action-affect classification and generation. Conditional Restricted Boltzmann Machines are used to build the model, which is then enhanced with a factored multi-task component to create Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). The study evaluates the proposed approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and demonstrates the model's superior classification performance and generative abilities compared to the state-of-the-art.",1
"This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.",0
"The aim of this paper is to examine the relationship between two advanced classifiers, namely decision forests (DFs) and convolutional neural networks (CNNs). DFs are known for their computational efficiency due to their conditional computation feature, which limits computation to a small section of the tree. On the other hand, CNNs are recognized for their ability to learn representations that lead to excellent accuracy. The authors present a methodical analysis of how to merge conditional computation with representation learning to create a range of hybrid models that provide varying accuracy to efficiency ratios, which they call conditional networks. These are either decision trees with added data transformation operators or CNNs with block-diagonal sparse weight matrices and explicit data routing functions. The researchers evaluated the performance of these models on image classification tasks using CIFAR and Imagenet datasets. The results showed that the hybrid models were comparable in accuracy to the state of the art CNNs while using a fraction of the computational cost and having fewer parameters.",1
"Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).",0
"In a single-modality scenario, unsupervised methods have been effective for discriminative tasks. However, this paper proposes a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. By utilizing joint dictionary learning techniques for sparse coding, the framework can model relationships at a higher level by enforcing shared sparse representation. This allows for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. With the exponential growth of multimodal data on the Web, it is becoming increasingly important to learn good multimodal features. Our framework substantially improves classification performance under both unimodal and multimodal settings and is effective for highly nonlinear correlations between modalities. We demonstrate the effectiveness of our approach experimentally in image denoising, multimedia event detection and retrieval, category classification, and sentiment classification.",1
"Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \emph{generative unsupervised feature learning} with a \emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.",0
"To achieve high accuracy, representation learning systems typically require a vast amount of labeled data. Neural networks and other high-dimensional parametric models have recently been successful in creating rich representations using compressive, reconstructive, or supervised criteria. However, these models often lose the semantic structure of observations. Although human perception excels at understanding semantics, it cannot always be expressed in labels. Therefore, oracles or human-in-the-loop systems, such as crowdsourcing, are employed to generate similarity constraints using implicit similarity functions. This work proposes combining generative unsupervised feature learning with a probabilistic treatment of oracle information, such as triplets, to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models. A fast variational algorithm is used to learn the joint model and demonstrate applicability to a popular image dataset. The proposed approach uses implicit triplet information to provide rich information for learning representations that outperform previous metric learning approaches and generative models without this side-information in various predictive tasks. Additionally, the approach compartmentalizes the latent spaces semantically to allow for interpretation of latent variables.",1
"Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view representation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an $N\times N$ eigenvalue system where $N$ is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to first transform the original inputs to an $M$-dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality $M$ of the random feature space may need to be very large in order to obtain a sufficiently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a stochastic optimization algorithm, recently proposed for linear CCA and its neural-network extension, to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with $1.4$ million training samples and a random feature space of dimensionality $M=100000$ on a typical workstation.",0
"Kernel canonical correlation analysis (KCCA) is a versatile nonlinear multi-view representation learning technique commonly used in statistics and machine learning. Although a closed-form solution exists for the KCCA objective, solving an $N\times N$ eigenvalue system where $N$ represents the training set size is computationally expensive, making it impractical for large-scale problems. To tackle this problem, various approximation techniques have been developed, such as transforming the original inputs to an $M$-dimensional random feature space before applying linear CCA to the transformed inputs. However, this approach may require a very high dimensionality $M$ of the random feature space, making it challenging to perform the linear CCA step on the resulting high-dimensional data matrices. To address this, we propose using a stochastic optimization algorithm recently introduced for linear CCA and its neural-network extension, which reduces the computation requirements of approximate KCCA. This method enables us to run approximate KCCA on a speech dataset with $1.4$ million training samples and a random feature space of dimensionality $M=100000$ on a typical workstation.",1
"Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.",0
"The recent success in training deep neural networks has spurred research into the features learned on their intermediate layers. This research is challenging due to the non-linear computations performed by millions of parameters, but it is valuable in enhancing our understanding of current models and developing improved versions. The paper investigates convergent learning in neural networks, where representations learned by multiple nets converge to similar features. The authors propose a method for probing representations by comparing the learned representations of multiple networks at the neuron or group level. Three techniques are used to align different neural networks on a feature level: bipartite matching, sparse prediction, and spectral clustering. The initial investigation uncovers several unknown properties of neural networks, including reliable and inconsistent feature learning, common low-dimensional subspaces, and a mix of local and distributed codes across multiple units. The authors argue that future research into convergent learning will yield additional insights.",1
"This paper strives for video event detection using a representation learned from deep convolutional neural networks. Different from the leading approaches, who all learn from the 1,000 classes defined in the ImageNet Large Scale Visual Recognition Challenge, we investigate how to leverage the complete ImageNet hierarchy for pre-training deep networks. To deal with the problems of over-specific classes and classes with few images, we introduce a bottom-up and top-down approach for reorganization of the ImageNet hierarchy based on all its 21,814 classes and more than 14 million images. Experiments on the TRECVID Multimedia Event Detection 2013 and 2015 datasets show that video representations derived from the layers of a deep neural network pre-trained with our reorganized hierarchy i) improves over standard pre-training, ii) is complementary among different reorganizations, iii) maintains the benefits of fusion with other modalities, and iv) leads to state-of-the-art event detection results. The reorganized hierarchies and their derived Caffe models are publicly available at http://tinyurl.com/imagenetshuffle.",0
"The purpose of this paper is to explore video event detection by utilizing a representation acquired from deep convolutional neural networks. In contrast to the current leading techniques, which rely on learning from the 1,000 classes defined in the ImageNet Large Scale Visual Recognition Challenge, we aim to investigate the potential benefits of employing the complete ImageNet hierarchy for pre-training deep networks. To address issues such as over-specific classes and classes with limited images, we propose a bottom-up and top-down approach for rearranging the ImageNet hierarchy, which includes all of its 21,814 classes and over 14 million images. Our experiments on the TRECVID Multimedia Event Detection 2013 and 2015 datasets demonstrate that utilizing video representations generated from the layers of a deep neural network pre-trained with our reorganized hierarchy i) yields better results than standard pre-training, ii) is complementary across different reorganizations, iii) retains the advantages of combining with other modalities, and iv) produces state-of-the-art event detection outcomes. The restructured hierarchies and associated Caffe models are publicly accessible at http://tinyurl.com/imagenetshuffle.",1
"Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.",0
"Effective representation learning of concepts has been established as a crucial foundation for various applications, including document classification and machine translation. The accurate representation of medical concepts, such as medication, diagnosis, procedure codes, and patient visits, will have a great impact on healthcare analytics. Nonetheless, Electronic Health Records (EHR) contain multiple concepts, including diagnosis, procedure, and medication codes, in patient visit sequences, which presents two relational information types, namely the sequential order of visits and the co-occurrence of codes within each visit. In this paper, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a vast EHR dataset comprising over 3 million visits but also enables us to confirm the learned representations with clinical experts. In our experiments, Med2Vec demonstrates a significant improvement in essential medical applications compared to commonly used baselines such as Skip-gram, GloVe, and stacked autoencoder, while also providing clinically meaningful interpretation.",1
"We explore the question of whether the representations learned by classifiers can be used to enhance the quality of generative models. Our conjecture is that labels correspond to characteristics of natural data which are most salient to humans: identity in faces, objects in images, and utterances in speech. We propose to take advantage of this by using the representations from discriminative classifiers to augment the objective function corresponding to a generative model. In particular we enhance the objective function of the variational autoencoder, a popular generative model, with a discriminative regularization term. We show that enhancing the objective function in this way leads to samples that are clearer and have higher visual quality than the samples from the standard variational autoencoders.",0
"Our investigation focuses on whether classifiers' learned representations can improve the caliber of generative models. We hypothesize that labels correspond to human-perceivable characteristics present in natural data, such as facial identity, object recognition, and speech utterances. To leverage this, we suggest using discriminative classifier representations to supplement the objective function of a generative model. Specifically, we amplify the objective function of the widely-used variational autoencoder with a discriminative regularization term. Our findings demonstrate that this approach results in samples that are more precise and visually appealing than those produced by conventional variational autoencoders.",1
"Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.",0
"Canonical correlation analysis (CCA) is a traditional technique for discovering correlated variables in multi-view data. Nonlinear variations of CCA, such as kernel and deep neural network methods, have been introduced to identify maximally correlated projections among groups of functions chosen by the user. However, these methods are computationally intensive. In the 1950s, Lancaster studied the theory of nonlinear CCA without functional limitations in the population setting, but practical algorithms were not inspired by these findings. We have revisited Lancaster's theory to create a practical algorithm for nonparametric CCA (NCCA). We have shown that the solution can be represented as the singular value decomposition of a particular operator linked to the joint density of the views. Therefore, by estimating the population density from data, NCCA can be reduced to solving an eigenvalue system that is similar to kernel CCA but does not require the inversion of any kernel matrix. Additionally, we have developed a partially linear CCA (PLCCA) version in which one view undergoes a linear projection while the other is nonparametric. By utilizing a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, faster, and perform better than kernel CCA, and comparable to deep CCA.",1
"Representation learning is the foundation for the recent success of neural network models. However, the distributed representations generated by neural networks are far from ideal. Due to their highly entangled nature, they are di cult to reuse and interpret, and they do a poor job of capturing the sparsity which is present in real- world transformations. In this paper, I describe methods for learning disentangled representations in the two domains of graphics and computation. These methods allow neural methods to learn representations which are easy to interpret and reuse, yet they incur little or no penalty to performance. In the Graphics section, I demonstrate the ability of these methods to infer the generating parameters of images and rerender those images under novel conditions. In the Computation section, I describe a model which is able to factorize a multitask learning problem into subtasks and which experiences no catastrophic forgetting. Together these techniques provide the tools to design a wide range of models that learn disentangled representations and better model the factors of variation in the real world.",0
"Neural network models have achieved recent success thanks to representation learning, but their distributed representations have significant drawbacks. These entangled representations are challenging to interpret and reuse, and they struggle to capture sparsity in real-world transformations. This paper presents techniques for learning disentangled representations in the graphics and computation domains. These methods enable neural networks to generate easy-to-interpret and reusable representations without compromising performance. The Graphics section showcases the ability of these techniques to identify the generating parameters of images and render them in novel scenarios. In the Computation section, a model is described that breaks down a multitask learning problem into subtasks without experiencing catastrophic forgetting. These techniques offer a range of tools for designing models that capture the factors of variation in the real world.",1
"We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.",0
"The focus of our study is on acquiring knowledge representations (features), in situations where we have access to numerous unlabeled perspectives of the data for learning, but only one view is available for downstream tasks. Previously, researchers have proposed different techniques based on deep neural networks to solve this problem. These techniques usually involve either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. Our study examines various methods based on previous work and novel variations, and we conduct empirical comparisons on image, speech, and text tasks. We discovered that correlation-based representation learning has an advantage, while the most exceptional outcomes on most tasks are produced by our new variation, which is deep canonically correlated autoencoders (DCCAE). Additionally, we explored a stochastic optimization method for minibatch correlation-based objectives and assessed the time/performance trade-offs for kernel-based and neural network-based implementations.",1
"Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem. We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an unsupervised algorithm that uses denoising autoencoders to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process. The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training.",0
"Machine learning benefits from disentangled distributed representations of data as they are more expressive and require fewer examples for generalization. However, in the case of complex data, the distributed representations of multiple objects in a single input can cause interference and lead to ambiguities, known as the binding problem. We emphasize the significance of the binding problem in representation learning and propose a probabilistic framework that explicitly models inputs as a composition of multiple objects. Our proposed unsupervised algorithm uses denoising autoencoders for dynamically binding features together in multi-object inputs through an Expectation-Maximization-like clustering process. We demonstrate the effectiveness of this method on artificially generated datasets of binary images and show that it can even bind together new objects that were not seen by the autoencoder during training.",1
"This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.",0
"In this study, we investigate the potential of spatial context as a valuable source of supervisory signal to develop a robust visual representation. Utilizing a large unlabeled image collection, we select random pairs of patches from each image and employ a convolutional neural network to predict the relative position of the second patch with respect to the first. The success of this task depends on the model's ability to identify objects and their components. Our findings reveal that the learned feature representation using this within-image context effectively captures visual similarities across images. For instance, it enables unsupervised detection of objects such as cats, people, and birds from the Pascal VOC 2011 detection dataset. Moreover, we demonstrate that incorporating the learned ConvNet into the R-CNN framework results in a significant improvement over randomly-initialized ConvNets, leading to state-of-the-art performance among algorithms that solely rely on Pascal-provided training set annotations.",1
"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",0
"Recently, convolutional networks (CNNs) have become widely used in computer vision applications for supervised learning. However, unsupervised learning with CNNs has not received as much attention. The purpose of this study is to bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a category of CNNs known as deep convolutional generative adversarial networks (DCGANs) that have specific structural limitations and prove that they are a promising option for unsupervised learning. We present compelling evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator by training on different image datasets. Furthermore, we use the learned features for new tasks, demonstrating their usefulness as general image representations.",1
"Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Knowledge graphs are typically treated as static: A knowledge graph grows more links when more facts become available but the ground truth values associated with links is considered time invariant. In this paper we address the issue of knowledge graphs where triple states depend on time. We assume that changes in the knowledge graph always arrive in form of events, in the sense that the events are the gateway to the knowledge graph. We train an event prediction model which uses both knowledge graph background information and information on recent events. By predicting future events, we also predict likely changes in the knowledge graph and thus obtain a model for the evolution of the knowledge graph as well. Our experiments demonstrate that our approach performs well in a clinical application, a recommendation engine and a sensor network application.",0
"Representation learning, also known as embedding learning, has been proven effective in modeling large-scale semantic knowledge graphs. The tensor representation of the knowledge graph is crucial, and models use latent representations of generalized entities to predict its entries. Typically, knowledge graphs are considered static, but this paper tackles the issue of knowledge graphs where triple states depend on time. The authors assume that changes in the knowledge graph occur as events and train an event prediction model that uses both background knowledge graph information and recent events. By predicting future events, they can anticipate changes in the knowledge graph and create a model for its evolution. The authors demonstrate the effectiveness of their approach in a clinical application, a recommendation engine, and a sensor network application.",1
"Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel Deep Image Saliency Computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse- and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. Specifically, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects-of-interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across datasets without additional training. The executable version of DISC is available online: http://vision.sysu.edu.cn/projects/DISC.",0
"Salient object detection is becoming increasingly important in various pattern recognition and image processing tasks. However, current saliency models often rely on prior assumptions about object and background properties, requiring heavy feature or model engineering. To address this, we propose a new Deep Image Saliency Computing (DISC) framework that uses a deep convolutional neural network to learn fine-grained image saliency. Our model is built on two stacked CNNs that produce a coarse-level saliency map and a refined fine-grained saliency map, respectively. The two CNNs collaborate to highlight objects-of-interest while preserving object details in complex backgrounds. DISC outperforms other state-of-the-art methods and generalizes well across datasets without additional training. An executable version of DISC is available online at http://vision.sysu.edu.cn/projects/DISC.",1
"In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP.",0
"The current methods for learning object detection representation simplify the relationship between a candidate window and the object's ground truth bounding box by using overlap thresholding. However, this paper highlights the loss of information that occurs due to this simplification and proposes a new representation learning pipeline that captures the relative location and size information discarded by thresholding. This relationship is not just limited to objects of the target category but also includes surrounding objects of other categories. The proposed pipeline effectively captures this relationship by utilizing image regions with multiple contexts and rotations, which handles the visual and semantic variation caused by different window-object configurations. The experimental results show that our approach improves object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014 and achieves the best results among published results on the challenging ILSVRC2014 test dataset with 48.6% mAP. In addition, our approach outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP on PASCAL VOC.",1
"We introduce a framework for analyzing transductive combination of Gaussian process (GP) experts, where independently trained GP experts are combined in a way that depends on test point location, in order to scale GPs to big data. The framework provides some theoretical justification for the generalized product of GP experts (gPoE-GP) which was previously shown to work well in practice but lacks theoretical basis. Based on the proposed framework, an improvement over gPoE-GP is introduced and empirically validated.",0
"A framework is presented to analyze the transductive combination of Gaussian process (GP) experts. This involves combining independently trained GP experts in a manner that relies on the location of the test point, enabling GPs to handle big data. The framework also offers theoretical support for the generalized product of GP experts (gPoE-GP), which has been successful in practice but lacked a theoretical basis. Using the proposed framework, a refinement to gPoE-GP is introduced and experimentally verified.",1
"Complex-valued neural networks (CVNNs) are an emerging field of research in neural networks due to their potential representational properties for audio, image, and physiological signals. It is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions, such as the Fourier transform. We show how CVNNs can be used to learn complex representations of real valued time-series data. We present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the Wirtinger derivative. We test our methods on a representation learning task for real-valued signals, recurrent complex-valued networks and their real-valued counterparts. Our results show that recurrent complex-valued networks can perform as well as their real-valued counterparts while learning filters that are representative of the domain of the data.",0
"The potential representational properties of Complex-valued neural networks (CVNNs) for audio, image, and physiological signals are making them an emerging field of research in the neural network. It is customary to convert sequences of real values to the complex domain by applying a set of complex basis functions such as the Fourier transform in signal processing. Our study demonstrates how CVNNs can be utilized to acquire complex representations of real valued time-series data. We have introduced techniques and outcomes that employ a framework which can combine holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result entitled the Wirtinger derivative. Our experimentation is based on a representation learning task for real-valued signals, recurrent complex-valued networks, and their real-valued counterparts. Our findings indicate that recurrent complex-valued networks can perform equally well as their real-valued counterparts while learning filters that are representative of the domain of the data.",1
"This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, the activity of users organizing content, are highly effective when used in conjunction with content-based ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from convnets, in improving the user engagement rate of our item-to-item recommendation system.",0
"In this paper, a recommendation system called Pinterest Related Pins is introduced. This system utilizes a combination of collaborative filtering and content-based ranking. The study highlights the effectiveness of user curation signals when combined with content-based ranking. Additionally, the paper showcases the positive impact of visual features, such as image or object representations learned from convnets, on the user engagement rate of the recommendation system.",1
"Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.",0
"A novel approach is presented in this paper for generating natural textures using convolutional neural networks that have been optimized for object recognition. The samples produced by the model are of exceptional quality, highlighting the impressive generative capabilities of neural networks that are trained in a purely discriminative manner. The texture representations in the model are based on the correlations between feature maps in multiple network layers, with texture representations becoming increasingly effective at capturing the statistical properties of natural images as they progress through the layers, while also revealing more about the objects present in the images. This model offers a useful tool for neuroscience research by enabling the generation of stimuli, and it could also offer valuable insights into the deep representations that are learned by convolutional neural networks.",1
"Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.",0
"The Deep Canonical Correlation Analysis (Deep CCA) is an extension of the traditional CCA using deep neural networks. It has been successful in various domains for multi-view representation learning. However, optimizing the Deep CCA objective stochastically is not easy as it does not decouple over training examples. The previous optimizers for Deep CCA are either batch-based algorithms or stochastic optimization with large minibatches, leading to high memory consumption. This paper proposes a solution to the problem of optimizing Deep CCA stochastically with small minibatches. It utilizes an iterative approach to the CCA objective and achieves similar performance to previous optimizers, thereby reducing the memory requirement.",1
"We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.",0
"The Encoder-Recurrent-Decoder (ERD) model is suggested as a solution for recognizing and predicting human body pose in videos and motion capture. It is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. The ERD architecture is tested in motion capture generation, body pose labeling, and body pose forecasting in videos. The model is capable of handling multiple subjects and activity domains, and can generate new motions while avoiding drifting for extended periods. ERD outperforms per frame body part detectors by resolving left-right body part confusions in human pose labeling. In video pose forecasting, ERD predicts body joint displacements across a 400ms temporal horizon and outperforms a first-order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. The experiments demonstrate that such representation learning is crucial for labeling and prediction in space-time, which distinguishes the spatio-temporal visual domain from 1D text, speech, or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.",1
"Several recent approaches showed how the representations learned by Convolutional Neural Networks can be repurposed for novel tasks. Most commonly it has been shown that the activation features of the last fully connected layers (fc7 or fc6) of the network, followed by a linear classifier outperform the state-of-the-art on several recognition challenge datasets. Instead of recognition, this paper focuses on the image retrieval problem and proposes a examines alternative pooling strategies derived for CNN features. The presented scheme uses the features maps from an earlier layer 5 of the CNN architecture, which has been shown to preserve coarse spatial information and is semantically meaningful. We examine several pooling strategies and demonstrate superior performance on the image retrieval task (INRIA Holidays) at the fraction of the computational cost, while using a relatively small memory requirements. In addition to retrieval, we see similar efficiency gains on the SUN397 scene categorization dataset, demonstrating wide applicability of this simple strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from different geographical locations in the world for image retrieval that stresses more dramatic changes in appearance and viewpoint.",0
"Recent approaches have demonstrated the potential to repurpose the learned representations of Convolutional Neural Networks for various tasks. Specifically, these approaches have shown that the activation features of the last fully connected layers (fc7 or fc6) of the network, along with a linear classifier, outperform the state-of-the-art on recognition challenges. However, this paper focuses on the image retrieval problem and proposes alternative pooling strategies for CNN features. The proposed scheme uses feature maps from an earlier layer 5 of the CNN architecture, which preserves coarse spatial information and is semantically meaningful. The paper examines various pooling strategies and demonstrates superior performance on the image retrieval task (INRIA Holidays) while using a relatively small memory and computational cost. The approach also shows efficiency gains on the SUN397 scene categorization dataset, indicating its wide applicability. The paper introduces and evaluates a novel GeoPlaces5K dataset, which features more dramatic changes in appearance and viewpoint for image retrieval in different geographical locations worldwide.",1
"In this paper we propose a strategy for semi-supervised image classification that leverages unsupervised representation learning and co-training. The strategy, that is called CURL from Co-trained Unsupervised Representation Learning, iteratively builds two classifiers on two different views of the data. The two views correspond to different representations learned from both labeled and unlabeled data and differ in the fusion scheme used to combine the image features. To assess the performance of our proposal, we conducted several experiments on widely used data sets for scene and object recognition. We considered three scenarios (inductive, transductive and self-taught learning) that differ in the strategy followed to exploit the unlabeled data. As image features we considered a combination of GIST, PHOG, and LBP as well as features extracted from a Convolutional Neural Network. Moreover, two embodiments of CURL are investigated: one using Ensemble Projection as unsupervised representation learning coupled with Logistic Regression, and one based on LapSVM. The results show that CURL clearly outperforms other supervised and semi-supervised learning methods in the state of the art.",0
"This paper presents a novel approach to semi-supervised image classification, which utilizes unsupervised representation learning and co-training. Our proposed strategy, called CURL (Co-trained Unsupervised Representation Learning), involves the iterative construction of two classifiers on two distinct data views. These views consist of different representations learned from both labeled and unlabeled data, and they vary in their fusion scheme for combining image features. To evaluate our approach, we conducted experiments on well-known data sets for scene and object recognition, exploring three different learning scenarios (inductive, transductive, and self-taught). We utilized various image features, including GIST, PHOG, and LBP, as well as features obtained from a Convolutional Neural Network. We also investigated two versions of CURL: one that uses Ensemble Projection for unsupervised representation learning coupled with Logistic Regression, and another based on LapSVM. Our results demonstrate that CURL significantly outperforms other state-of-the-art supervised and semi-supervised learning methods.",1
"Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for both linear SVMs and the nonlinear extension with latent representation learning. For linear SVMs, to deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights are analytically updated. For nonlinear latent SVMs, we consider learning one layer of latent representations in SVMs and extend the data augmentation technique in conjunction with first-order Taylor-expansion to deal with the intractable expected non-smooth hinge loss and the nonlinearity of latent representations. Finally, we apply the similar data augmentation ideas to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions, and we further develop a non-linear extension of logistic regression by incorporating one layer of latent representations. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMs further improve the prediction performance on several image datasets.",0
"The use of Dropout and other feature noising techniques has shown potential in preventing overfitting by artificially contaminating the training data. Despite extensive research on generalized linear models, little attention has been given to support vector machines (SVMs), which have been highly successful in supervised learning. This paper introduces a Dropout training method for both linear and nonlinear SVMs with latent representation learning. For linear SVMs, an iteratively re-weighted least square (IRLS) algorithm is developed to handle the intractable expectation of the non-smooth hinge loss under corrupting distributions by using data augmentation techniques. The algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights are updated analytically. For nonlinear latent SVMs, a first-order Taylor-expansion is used in conjunction with data augmentation techniques to address the intractable expected non-smooth hinge loss and the nonlinearity of latent representations when learning one layer of latent representations in SVMs. Furthermore, a new IRLS algorithm is proposed for the expected logistic loss under corrupting distributions, and a non-linear extension of logistic regression is developed by integrating one layer of latent representations. These algorithms provide insights into the connection and difference between the hinge loss and logistic loss in Dropout training. The empirical results on various real datasets show that Dropout training significantly improves the classification accuracy of both linear and nonlinear SVMs, and the nonlinear SVMs also enhance the prediction performance on image datasets.",1
"Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their distance from the source task such that a correlation between the performance of tasks and their distance from the source task w.r.t. the proposed factors is observed.",0
"There is growing evidence indicating that Convolutional Networks (ConvNets) are the most effective way to learn representations for visual recognition tasks. Typically, a ConvNet is trained on a large dataset that has been labeled (source), and the feed-forward units activation of the trained network at a specific layer is then used as a general representation of an input image for a task that has a smaller training set (target). Recent studies have demonstrated that this type of representation transfer is useful for a broad range of visual recognition tasks. This study examines several factors that impact the transferability of these representations, including the architecture of the source ConvNet, the distribution of the training data, and the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. The study shows that by optimizing these factors, significant improvements can be made for various visual recognition tasks. Additionally, the study demonstrates that these tasks can be ordered based on their distance from the source task, and there is a correlation between the performance of tasks and their distance from the source task concerning the proposed factors.",1
"We address the problem of communicating domain knowledge from a user to the designer of a clustering algorithm. We propose a protocol in which the user provides a clustering of a relatively small random sample of a data set. The algorithm designer then uses that sample to come up with a data representation under which $k$-means clustering results in a clustering (of the full data set) that is aligned with the user's clustering. We provide a formal statistical model for analyzing the sample complexity of learning a clustering representation with this paradigm. We then introduce a notion of capacity of a class of possible representations, in the spirit of the VC-dimension, showing that classes of representations that have finite such dimension can be successfully learned with sample size error bounds, and end our discussion with an analysis of that dimension for classes of representations induced by linear embeddings.",0
"Our focus is on solving the challenge of conveying domain expertise from a user to a clustering algorithm designer. Our proposed solution involves the user clustering a small random subset of the data set, which the designer then uses to create a data representation that produces $k$-means clustering results aligned with the user's clustering. We present a statistical model to analyze the sample complexity of this approach and introduce a capacity concept for representation classes, similar to VC-dimension. We demonstrate that representation classes with finite dimension can be learned with bounded errors using sample size and analyze the dimension for linear embedding-induced representation classes.",1
"Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.",0
"Many recommender systems use Collaborative filtering (CF) as a successful approach. However, conventional CF-based methods solely depend on the ratings given to items by users as the source of information for making recommendations, resulting in degraded performance due to sparsity. To address this issue, auxiliary information such as item content information can be utilized. Collaborative topic regression (CTR) is an appealing method that tightly couples the two components that learn from two different sources of information, but it may not be effective with very sparse auxiliary information. In this paper, we propose a hierarchical Bayesian model, called collaborative deep learning (CDL), which jointly performs deep representation learning for content information and collaborative filtering for the ratings matrix. The proposed method advances the state of the art, as demonstrated through extensive experiments performed on three real-world datasets from different domains.",1
"Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modelling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.",0
"Representation learning through non-probabilistic methods is widely associated with auto-encoders. Their ease of use and simplicity in training make them a popular choice. Recent research has highlighted their ability to capture manifold structure, leading to a surge of interest in auto-encoder scoring and classification. While gated auto-encoders (GAEs) offer flexibility in learning transformations among images and pixel covariances, they remain understudied. In this study, we adopt a dynamical systems approach to GAEs, developing a scoring function and exploring their connection with Restricted Boltzmann Machines. Our experiments on deep learning benchmarks demonstrate the effectiveness of GAEs for both single and multi-label classification.",1
"Sketch-based face recognition is an interesting task in vision and multimedia research, yet it is quite challenging due to the great difference between face photos and sketches. In this paper, we propose a novel approach for photo-sketch generation, aiming to automatically transform face photos into detail-preserving personal sketches. Unlike the traditional models synthesizing sketches based on a dictionary of exemplars, we develop a fully convolutional network to learn the end-to-end photo-sketch mapping. Our approach takes whole face photos as inputs and directly generates the corresponding sketch images with efficient inference and learning, in which the architecture are stacked by only convolutional kernels of very small sizes. To well capture the person identity during the photo-sketch transformation, we define our optimization objective in the form of joint generative-discriminative minimization. In particular, a discriminative regularization term is incorporated into the photo-sketch generation, enhancing the discriminability of the generated person sketches against other individuals. Extensive experiments on several standard benchmarks suggest that our approach outperforms other state-of-the-art methods in both photo-sketch generation and face sketch verification.",0
"The task of sketch-based face recognition is a complex one in the realm of vision and multimedia research due to the substantial difference between sketches and face photos. This paper introduces a new method for generating sketches from photos automatically, which aims to maintain the details of personal sketches. Unlike traditional models that rely on exemplars from a dictionary, we use a fully convolutional network that learns the photo-sketch mapping end-to-end. Our approach takes whole face photos as input and generates corresponding sketch images using efficient inference and learning, with an architecture composed of only convolutional kernels of very small sizes. To ensure that our approach captures the individual's identity during photo-sketch transformation, we use a joint generative-discriminative minimization optimization objective. This incorporates a discriminative regularization term in the photo-sketch generation process, improving the discriminability of generated person sketches against other individuals. Our approach outperforms other state-of-the-art methods in both photo-sketch generation and face sketch verification, as demonstrated by extensive experiments on several standard benchmarks.",1
"We present a representation learning method that learns features at multiple different levels of scale. Working within the unsupervised framework of denoising autoencoders, we observe that when the input is heavily corrupted during training, the network tends to learn coarse-grained features, whereas when the input is only slightly corrupted, the network tends to learn fine-grained features. This motivates the scheduled denoising autoencoder, which starts with a high level of noise that lowers as training progresses. We find that the resulting representation yields a significant boost on a later supervised task compared to the original input, or to a standard denoising autoencoder trained at a single noise level. After supervised fine-tuning our best model achieves the lowest ever reported error on the CIFAR-10 data set among permutation-invariant methods.",0
"Our method for representation learning involves the acquisition of features at various scales. We utilize denoising autoencoders in an unsupervised framework and note that when the input has high levels of corruption, the network learns general features. Conversely, lower levels of corruption lead to the learning of more specific features. This observation serves as the basis for the scheduled denoising autoencoder, which commences training with high noise and gradually reduces it over time. We have discovered that this approach results in a representation that significantly enhances supervised tasks when compared to the original input or a standard denoising autoencoder trained at a single noise level. Our top-performing model achieved the lowest error rate ever reported for a permutation-invariant method on the CIFAR-10 data set after supervised fine-tuning.",1
"Representation learning is currently a very hot topic in modern machine learning, mostly due to the great success of the deep learning methods. In particular low-dimensional representation which discriminates classes can not only enhance the classification procedure, but also make it faster, while contrary to the high-dimensional embeddings can be efficiently used for visual based exploratory data analysis.   In this paper we propose Maximum Entropy Linear Manifold (MELM), a multidimensional generalization of Multithreshold Entropy Linear Classifier model which is able to find a low-dimensional linear data projection maximizing discriminativeness of projected classes. As a result we obtain a linear embedding which can be used for classification, class aware dimensionality reduction and data visualization. MELM provides highly discriminative 2D projections of the data which can be used as a method for constructing robust classifiers.   We provide both empirical evaluation as well as some interesting theoretical properties of our objective function such us scale and affine transformation invariance, connections with PCA and bounding of the expected balanced accuracy error.",0
"Representation learning is a popular topic in modern machine learning due to the success of deep learning methods. Discriminatory low-dimensional representation can enhance and expedite classification procedures, as opposed to high-dimensional embeddings, which can be efficiently used for visual-based exploratory data analysis. The Maximum Entropy Linear Manifold (MELM) proposed in this paper is a multidimensional generalization of the Multithreshold Entropy Linear Classifier model. It finds a low-dimensional linear data projection that maximizes the discriminativeness of projected classes. This results in a linear embedding that can be used for classification, class-aware dimensionality reduction, and data visualization. MELM provides highly discriminative 2D projections of data that can be used to construct robust classifiers. The objective function of MELM has interesting theoretical properties, such as scale and affine transformation invariance, connections with PCA, and bounding of the expected balanced accuracy error. Empirical evaluations are provided in addition to the theoretical properties.",1
"Efficient and accurate joint representation of a collection of images, that belong to the same class, is a major research challenge for practical image set classification. Existing methods either make prior assumptions about the data structure, or perform heavy computations to learn structure from the data itself. In this paper, we propose an efficient image set representation that does not make any prior assumptions about the structure of the underlying data. We learn the non-linear structure of image sets with Deep Extreme Learning Machines (DELM) that are very efficient and generalize well even on a limited number of training samples. Extensive experiments on a broad range of public datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm consistently outperforms state-of-the-art image set classification methods both in terms of speed and accuracy.",0
"A significant research challenge in classifying a set of images that belong to the same category is to efficiently and accurately represent them. Current methods either assume a data structure or require extensive computations to learn the structure from the data. This study presents a novel image set representation technique that does not rely on prior assumptions about the data structure. The proposed approach uses Deep Extreme Learning Machines (DELM) to learn the non-linear structure of image sets. DELMs are highly efficient and can generalize well even with limited training samples. The proposed algorithm outperforms state-of-the-art image set classification methods in terms of speed and accuracy across various public datasets for image set classification, including Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, and ETH-80.",1
"While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",0
"Although depth can enhance network performances, it can also pose difficulties for gradient-based training, given that deeper networks typically exhibit greater non-linearity. To address this issue, a knowledge distillation approach has been introduced to develop smaller and faster-to-execute models. Recent studies reveal that a student network can imitate the soft output of a larger teacher network by using the same approach. In this paper, we extend this concept to enable the training of a deeper and thinner student network than the teacher network, leveraging not only the outputs but also the intermediate representations learned by the teacher as hints to enhance the training process and performance of the student. To map the student hidden layer to the prediction of the teacher hidden layer, additional parameters are added since the student intermediate hidden layer is usually smaller than that of the teacher. This allows us to train deeper students with better generalization or faster execution, depending on the selected student capacity. For instance, on CIFAR-10, a deep student network with almost 10.4 times fewer parameters outperforms a larger, state-of-the-art teacher network.",1
"We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.",0
"The focus of our discussion is on data representation that is capable of automatic learning from data, maintains invariance to transformations, and is selective, meaning that only those points sharing the same transformation are represented similarly. The mathematical findings presented in this study enhance some of the fundamental assertions of i-theory, a recent hypothesis on feedforward processing in the sensory cortex.",1
"Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these ""deep learning"" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.",0
"Recently, neural networks such as fully-connected and convolutional ones have been successfully used to accomplish various tasks like speech recognition, bioinformatics, natural language processing, and image classification. The majority of deep learning models leverage the softmax activation function for prediction and cross-entropy loss minimization in classification tasks. In this article, we present evidence of a slight but consistent benefit of substituting the softmax layer with a linear support vector machine, which minimizes a margin-based loss rather than cross-entropy loss. While previous research has experimented with combining neural nets and SVMs, our outcomes using L2-SVMs reveal that substituting softmax with linear SVMs delivers significant improvements on well-known deep learning datasets, such as MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.",1
"We introduce a new representation learning algorithm suited to the context of domain adaptation, in which data at training and test time come from similar but different distributions. Our algorithm is directly inspired by theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on a data representation that cannot discriminate between the training (source) and test (target) domains. We propose a training objective that implements this idea in the context of a neural network, whose hidden layer is trained to be predictive of the classification task, but uninformative as to the domain of the input. Our experiments on a sentiment analysis classification benchmark, where the target domain data available at training time is unlabeled, show that our neural network for domain adaption algorithm has better performance than either a standard neural network or an SVM, even if trained on input features extracted with the state-of-the-art marginalized stacked denoising autoencoders of Chen et al. (2012).",0
"Our novel representation learning algorithm is specifically designed for domain adaptation scenarios where the training and test data originate from similar yet distinct distributions. Drawing inspiration from existing domain adaptation theory, we assert that effective domain transfer necessitates predictions based on a representation that does not differentiate between the training (source) and test (target) domains. To implement this concept in a neural network, we propose a training objective that trains the hidden layer to predict the classification task while remaining uninformed about the input's domain. In our sentiment analysis classification benchmark experiments, where the training data is unlabeled, our neural network for domain adaptation algorithm outperformed both a standard neural network and an SVM, even when utilizing input features extracted using Chen et al.'s (2012) state-of-the-art marginalized stacked denoising autoencoders.",1
"Recently proposed budding tree is a decision tree algorithm in which every node is part internal node and part leaf. This allows representing every decision tree in a continuous parameter space, and therefore a budding tree can be jointly trained with backpropagation, like a neural network. Even though this continuity allows it to be used in hierarchical representation learning, the learned representations are local: Activation makes a soft selection among all root-to-leaf paths in a tree. In this work we extend the budding tree and propose the distributed tree where the children use different and independent splits and hence multiple paths in a tree can be traversed at the same time. This ability to combine multiple paths gives the power of a distributed representation, as in a traditional perceptron layer. We show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks.",0
"A novel decision tree algorithm called budding tree has been introduced recently. Unlike traditional decision trees, the nodes in budding trees are a combination of internal nodes and leaf nodes, which enables the representation of decision trees in a continuous parameter space. As a result, budding trees can be trained using backpropagation, similar to neural networks. Although this continuous representation allows for hierarchical learning, the learned representations are confined to local activations that softly select among all paths from the root to the leaf in a tree. To address this limitation, we propose an extension of the budding tree called distributed tree. In distributed trees, children use different and independent splits, which allows multiple paths to be traversed simultaneously, resulting in a distributed representation similar to a traditional perceptron layer. Our experiments show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks.",1
"A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irrelevant underlying data dimensions. One very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy (MMD) [5], a measure of distance between probability distributions. We are not the first to suggest that MMD can be a useful criterion in developing representations that apply across multiple domains or tasks [1]. However, in this paper we describe a number of novel applications of this criterion that we have devised, all based on the idea of developing unbiased representations. These formulations include: a standard domain adaptation framework; a method of learning invariant representations; an approach based on noise-insensitive autoencoders; and a novel form of generative model.",0
"Transfer learning relies heavily on representation learning, which involves creating representations that reveal the important factors underlying data. By mapping these salient factors, new domains and tasks can be easily learned. A crucial goal for these representations is to be unbiased, and different types of representation learning can be derived from various definitions of unwanted bias. To determine the level of bias in a representation, maximum mean discrepancy (MMD) is a useful approach as it measures the distance between probability distributions. While previous research has also suggested MMD as a criterion for developing representations applicable to multiple tasks or domains, our paper describes several innovative applications of this criterion, all centered around creating unbiased representations. These applications include a standard domain adaptation framework, a method for learning invariant representations, a noise-insensitive autoencoder approach, and a new type of generative model.",1
"Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.",0
"Convolutional Neural Networks (ConvNets) have demonstrated remarkable performance in various visual classification tasks, except for ImageNet wherein object alignment and scale are not controlled. As the degree of variation in data increases, the difficulty of feature learning also increases since models must learn to be invariant to specific changes in appearance. Despite this, ConvNets can still learn invariances and create highly distinguishing features with sufficient data. However, the question remains: can we achieve more efficient learning with fewer parameters, less data, and more discriminative features if we incorporate certain invariances into the learning process? This article introduces a simple model that enables ConvNets to learn scale-invariant features locally without increasing the number of model parameters. Results on a modified MNIST dataset demonstrate that incorporating scale-invariance allows ConvNets to learn more discriminative features with reduced risk of over-fitting when confronted with scale variation.",1
"We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.",0
"Our approach to detector learning utilizes joint training over both weak and strong labels, and transfers learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning have relied on independent learning of detector models through latent variable optimization, which fails to share deep representation knowledge across classes and necessitates strong initialization. Other methods have transferred deep representations from domains with strong labels to those with only weak labels, but have not optimized over individual latent boxes and may therefore miss specific salient structures for a particular category. Our proposed model combines these approaches, concurrently training a representation and detectors for categories with either weak or strong labels. We have developed a novel formulation of a joint multiple instance learning method that includes classification-style data examples when available, and performs domain transfer learning to improve the underlying detector representation. Our model has been shown to outperform known methods on ImageNet-200 detection with weak labels.",1
"After intensive research, heterogenous face recognition is still a challenging problem. The main difficulties are owing to the complex relationship between heterogenous face image spaces. The heterogeneity is always tightly coupled with other variations, which makes the relationship of heterogenous face images highly nonlinear. Many excellent methods have been proposed to model the nonlinear relationship, but they apt to overfit to the training set, due to limited samples. Inspired by the unsupervised algorithms in deep learning, this paper proposes an novel framework for heterogeneous face recognition. We first extract Gabor features at some localized facial points, and then use Restricted Boltzmann Machines (RBMs) to learn a shared representation locally to remove the heterogeneity around each facial point. Finally, the shared representations of local RBMs are connected together and processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases are selected to evaluate the proposed method. For Sketch-Photo problem, we obtain perfect results on the CUFS database. For NIR-VIS problem, we produce new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.",0
"Despite extensive research, heterogenous face recognition remains a difficult problem due to the intricate relationship between different image spaces. Heterogeneity is closely linked to other variations, which complicates the relationship between heterogenous face images and makes it highly nonlinear. Although many methods have been proposed to model this nonlinear relationship, they tend to overfit to the training set due to limited samples. This paper proposes a novel framework for heterogeneous face recognition inspired by unsupervised algorithms in deep learning. The approach involves extracting Gabor features at localized facial points and using Restricted Boltzmann Machines (RBMs) to learn a shared representation locally to remove heterogeneity around each facial point. Finally, the shared representations of local RBMs are connected and processed by PCA. The proposed method is evaluated on two problems (Sketch-Photo and NIR-VIS) and three databases, with perfect results obtained on the CUFS database for the Sketch-Photo problem and new state-of-the-art performance achieved on the CASIA HFB and NIR-VIS 2.0 databases for the NIR-VIS problem.",1
"We revisit a pioneer unsupervised learning technique called archetypal analysis, which is related to successful data analysis methods such as sparse coding and non-negative matrix factorization. Since it was proposed, archetypal analysis did not gain a lot of popularity even though it produces more interpretable models than other alternatives. Because no efficient implementation has ever been made publicly available, its application to important scientific problems may have been severely limited. Our goal is to bring back into favour archetypal analysis. We propose a fast optimization scheme using an active-set strategy, and provide an efficient open-source implementation interfaced with Matlab, R, and Python. Then, we demonstrate the usefulness of archetypal analysis for computer vision tasks, such as codebook learning, signal classification, and large image collection visualization.",0
"The article discusses the underutilized unsupervised learning method, archetypal analysis, which bears similarity to sparse coding and non-negative matrix factorization. Despite its potential for producing more easily understood models than other alternatives, archetypal analysis has not received much attention since its inception due to a lack of efficient implementation. This has limited its application in scientific research. The authors aim to revive interest in archetypal analysis by developing a speedy optimization technique using an active-set strategy and offering an efficient open-source implementation that can be used with Matlab, R, and Python. They also demonstrate the usefulness of archetypal analysis in computer vision tasks such as codebook learning, signal classification, and visualizing massive image collections.",1
"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.",0
"The effectiveness of machine learning algorithms hinges on how data is represented, and we propose that this is because different representations can obscure or reveal varying explanatory factors of variation in the data. While domain-specific knowledge can aid in designing representations, generic priors can also be utilized, fueling the development of more potent representation-learning algorithms in pursuit of AI. This paper examines recent advancements in unsupervised feature learning and deep learning, encompassing probabilistic models, auto-encoders, manifold learning, and deep networks. This prompts inquiries about the long-term objectives for obtaining high-quality representations, computing representations (i.e., inference), and the geometrical associations between representation learning, density estimation, and manifold learning that remain unanswered.",1
"We consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning. In those learning tasks, the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space. To overcome this problem, the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering.   Inspired by the recent research works on deep neural network and representation learning, in this paper, we introduce the multiple-layer auto-encoder into image representation, we also apply the locally invariant ideal to our image representation with auto-encoders and propose a novel method, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact representation which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure.   Extensive experiments on image clustering show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-word cases.",0
"The problem of image representation is being explored for unsupervised and semi-supervised learning tasks. The raw image vectors are often inadequate for capturing the intrinsic structures due to their dense feature space. Therefore, a suitable representation space that can explicitly represent the data for clustering and other learning tasks needs to be mapped. Drawing inspiration from recent research on representation learning and deep neural networks, this paper proposes the use of multi-layer auto-encoders for image representation. The locally invariant ideal is also applied to the image representation, resulting in a novel method called Graph regularized Auto-Encoder (GAE). GAE provides a compact representation that uncovers hidden semantics and respects the intrinsic geometric structure simultaneously. The proposed algorithm has been extensively tested on real-world cases, and the results demonstrate its superiority over existing state-of-the-art algorithms for image clustering.",1
"Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.",0
"Representation learning algorithms for language and image processing are typically local, as they rely on surrounding points to identify features for a data point. However, language processing often requires considering the global context to understand the correct meaning of a word. We aim to address this limitation by creating a representation learning algorithm that incorporates joint prediction in its feature production technique. Our approach involves using efficient variational methods to learn Factorial Hidden Markov Models from large texts and using variational distributions to produce features for each word that consider the entire input sequence, not just local context. Our experiments on part-of-speech tagging and chunking demonstrate that these features are competitive with, or superior to, existing state-of-the-art representation learning methods.",1
"When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial 'seed' patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).",0
"In the realm of visual object recognition utilizing deep learning, data augmentation is frequently employed to produce additional training data without incurring extra labeling expenses. Its implementation curbs overfitting and enhances the efficiency of the algorithm. The present study delves into the feasibility of data augmentation as the primary component of an unsupervised feature learning structure. For this purpose, we randomly sample image patches and designate each as an individual surrogate class. We expand these basic one-element classes by subjecting the initial seed patches to various transformations. Subsequently, we train a convolutional neural network to differentiate among these surrogate classes. The feature representation acquired by the network can then be utilized in a range of vision tasks. Our results demonstrate that this straightforward feature learning algorithm is remarkably effective, generating competitive classification outcomes on numerous prominent vision datasets such as STL-10, CIFAR-10, and Caltech-101.",1
"Representation learning and unsupervised learning are two central topics of machine learning and signal processing. Deep learning is one of the most effective unsupervised representation learning approach. The main contributions of this paper to the topics are as follows. (i) We propose to view the representative deep learning approaches as special cases of the knowledge reuse framework of clustering ensemble. (ii) We propose to view sparse coding when used as a feature encoder as the consensus function of clustering ensemble, and view dictionary learning as the training process of the base clusterings of clustering ensemble. (ii) Based on the above two views, we propose a very simple deep learning algorithm, named deep random model ensemble (DRME). It is a stack of random model ensembles. Each random model ensemble is a special k-means ensemble that discards the expectation-maximization optimization of each base k-means but only preserves the default initialization method of the base k-means. (iv) We propose to select the most powerful representation among the layers by applying DRME to clustering where the single-linkage is used as the clustering algorithm. Moreover, the DRME based clustering can also detect the number of the natural clusters accurately. Extensive experimental comparisons with 5 representation learning methods on 19 benchmark data sets demonstrate the effectiveness of DRME.",0
"Two important areas of machine learning and signal processing are representation learning and unsupervised learning. Deep learning is a highly effective approach to unsupervised representation learning. This paper makes significant contributions to these topics. Firstly, we suggest that representative deep learning approaches can be seen as special cases of clustering ensemble's knowledge reuse framework. Secondly, we propose that sparse coding can be viewed as the consensus function of clustering ensemble when used as a feature encoder, and dictionary learning can be seen as the training process of clustering ensemble's base clusterings. Thirdly, we introduce a simple deep learning algorithm, called deep random model ensemble (DRME), which is a stack of random model ensembles. Each random model ensemble is a unique k-means ensemble that discards the expectation-maximization optimization of each base k-means and only keeps the default initialization method. Finally, we suggest using DRME to cluster data using single-linkage as the clustering algorithm to select the most powerful representation among the layers. DRME-based clustering can also accurately detect the number of natural clusters. We demonstrate the effectiveness of DRME through extensive experimental comparisons with five representation learning methods on 19 benchmark data sets.",1
"Unsupervised deep learning is one of the most powerful representation learning techniques. Restricted Boltzman machine, sparse coding, regularized auto-encoders, and convolutional neural networks are pioneering building blocks of deep learning. In this paper, we propose a new building block -- distributed random models. The proposed method is a special full implementation of the product of experts: (i) each expert owns multiple hidden units and different experts have different numbers of hidden units; (ii) the model of each expert is a k-center clustering, whose k-centers are only uniformly sampled examples, and whose output (i.e. the hidden units) is a sparse code that only the similarity values from a few nearest neighbors are reserved. The relationship between the pioneering building blocks, several notable research branches and the proposed method is analyzed. Experimental results show that the proposed deep model can learn better representations than deep belief networks and meanwhile can train a much larger network with much less time than deep belief networks.",0
"One of the most effective methods for representation learning is unsupervised deep learning. The building blocks of this technique include pioneering methods like Restricted Boltzman Machine, Sparse Coding, Regularized Auto-Encoders, and Convolutional Neural Networks. In this particular study, we introduce a new building block called Distributed Random Models. This innovative approach is a complete implementation of the Product of Experts and involves numerous hidden units owned by each expert, with varying numbers across different experts. Each expert's model is based on k-center clustering, where the k-centers are uniformly sampled examples. The output of this model is a sparse code, which only preserves similarity values from a few nearest neighbors. We explore the relationship between our proposed method and other notable research branches. Our experimental results demonstrate that our deep model performs better than Deep Belief Networks and can be trained with much less time, even with a much larger network.",1
"We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.",0
"Our research investigates the impact of incorporating prior knowledge into the intermediate level of neural networks to learn a task that state-of-the-art machine learning algorithms have failed to master. We base our investigation on the notion that humans learn these intermediate concepts through guidance and supervision provided by others using a curriculum. Our experiments provide affirmative evidence to support this hypothesis. We employ a two-tiered MLP architecture to train on a dataset of 64x64 binary input images, each containing three pentomino tetris-shaped sprites placed in different positions using scaling and rotation transformations. The final objective is to determine if all sprites are the same or if one of them is different. In the first phase of training, the intermediate-level targets involve recognizing the presence of sprites at each location, while the second phase takes the output of the first phase as input and predicts the final task's target binary event. With only a few tens of thousand examples, the two-tiered MLP architecture learned the task flawlessly, while all other algorithms, including unsupervised pre-training, SVMs, decision trees, and boosting, performed no better than chance. We believe that the difficulty in optimization without intermediate pre-training results from the combination of two highly non-linear tasks. Our findings align with cultural learning hypotheses that draw inspiration from observations of optimization challenges in deep learning, presumably due to effective local minima.",1
"This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing three-level representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge.",0
"In this paper, we present our approach to addressing the challenge of multi-modal learning in ICML. Our solution involves creating three-level representations using a specific methodology for selecting appropriate tag words. Firstly, we utilize traditional techniques to generate level-1 representations that involve using MPEG-7 and gist descriptors along with additional features provided by the contest organizers. Furthermore, we utilize a bag-of-words model with a vocabulary of 4000 words to represent the corresponding tags. Secondly, we incorporate two stacked RBMs for each modality to learn level-2 representations. Lastly, we introduce a bimodal auto-encoder to learn the level-3 representations by assessing the similarities and differences between the image-tags pairs. During the testing phase, we implement a data-specific approach to select the correct tag words based on the dataset observations, resulting in a significant improvement in overall performance. Our approach achieved a 100% average accuracy on the private test set, which secured our position in the first place in this competition.",1
"The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.",0
"The central theme of the ICML 2013 Workshop was the Challenges in Representation Learning, which were categorized into three, namely the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. An overview of the datasets prepared for these challenges, as well as a summary of the competition's outcomes, are presented. Furthermore, we offer recommendations for future challenge organizers and insights on the knowledge that can be acquired from machine learning competitions.",1
"Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.",0
"The use of deep learning for representation learning has become prevalent in classification. However, it is unclear how to attain good classification results with deep neural networks when limited labeled data is available and how to enhance classification through the learned features. This article introduces the Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve deep neural network classification performance. At the ICML 2013 Black Box Challenge, Bing Xu and Jingjing Xie employed these methods independently and achieved high rankings in both public and private leaderboards.",1
"We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.",0
"A novel algorithm is introduced in this study, which aims to acquire representations that account for domain mismatch and can be implemented as linear classifiers with efficiency. The proposed method involves creating a linear transformation that translates traits from the target domain to the source domain while training the classifier. By optimizing both the transformation and classifier parameters simultaneously, the algorithm offers a cost function based on misclassification loss that is time-effective. Our approach combines multiple features that were not previously available in a single algorithm, including multi-class adaptation through representation learning, the ability to map across heterogeneous feature spaces, and scalability to large datasets. We conducted experiments on various image datasets, which showed improved accuracy and computational advantages compared to previous methods.",1
"Copulas are a powerful tool for modeling multivariate distributions as they allow to separately estimate the univariate marginal distributions and the joint dependency structure. However, known parametric copulas offer limited flexibility especially in high dimensions, while commonly used non-parametric methods suffer from the curse of dimensionality. A popular remedy is to construct a tree-based hierarchy of conditional bivariate copulas. In this paper, we propose a flexible, yet conceptually simple alternative based on implicit generative neural networks. The key challenge is to ensure marginal uniformity of the estimated copula distribution. We achieve this by learning a multivariate latent distribution with unspecified marginals but the desired dependency structure. By applying the probability integral transform, we can then obtain samples from the high-dimensional copula distribution without relying on parametric assumptions or the need to find a suitable tree structure. Experiments on synthetic and real data from finance, physics, and image generation demonstrate the performance of this approach.",0
"Copulas are a useful technique for modeling multivariate distributions because they allow for the separate estimation of univariate marginal distributions and joint dependency structures. However, parametric copulas have limited flexibility in high dimensions, and non-parametric methods are affected by the curse of dimensionality. To address this, some researchers construct a hierarchy of conditional bivariate copulas using a tree-based approach. In this paper, we propose an alternative method that uses implicit generative neural networks. Our approach generates a multivariate latent distribution with the desired dependency structure but unspecified marginals, ensuring marginal uniformity of the estimated copula distribution. This method does not rely on parametric assumptions or the need for a suitable tree structure. We demonstrate the effectiveness of this approach through experiments on synthetic and real data from various domains.",1
"We propose Generative Probabilistic Image Colorization, a diffusion-based generative process that trains a sequence of probabilistic models to reverse each step of noise corruption. Given a line-drawing image as input, our method suggests multiple candidate colorized images. Therefore, our method accounts for the ill-posed nature of the colorization problem. We conducted comprehensive experiments investigating the colorization of line-drawing images, report the influence of a score-based MCMC approach that corrects the marginal distribution of estimated samples, and further compare different combinations of models and the similarity of their generated images. Despite using only a relatively small training dataset, we experimentally develop a method to generate multiple diverse colorization candidates which avoids mode collapse and does not require any additional constraints, losses, or re-training with alternative training conditions. Our proposed approach performed well not only on color-conditional image generation tasks using biased initial values, but also on some practical image completion and inpainting tasks.",0
"Our proposed method, Generative Probabilistic Image Colorization, utilizes diffusion-based techniques to generate multiple potential colorized images from a line-drawing input. This is achieved through a sequence of probabilistic models that reverse each step of noise corruption, accounting for the ill-posed nature of the colorization problem. We conducted comprehensive experiments to investigate the colorization of line-drawing images, including the use of a score-based MCMC approach to correct the marginal distribution of estimated samples. We also compared different model combinations and the similarity of their generated images. Despite using a relatively small training dataset, our method can generate diverse colorization candidates without mode collapse or additional constraints, losses, or re-training. Our approach performed well on color-conditional image generation tasks and practical image completion and inpainting tasks, demonstrating its versatility.",1
"In this paper, we propose a method for keypoint discovery from a 2D image using image-level supervision. Recent works on unsupervised keypoint discovery reliably discover keypoints of aligned instances. However, when the target instances have high viewpoint or appearance variation, the discovered keypoints do not match the semantic correspondences over different images. Our work aims to discover keypoints even when the target instances have high viewpoint and appearance variation by using image-level supervision. Motivated by the weakly-supervised learning approach, our method exploits image-level supervision to identify discriminative parts and infer the viewpoint of the target instance. To discover diverse parts, we adopt a conditional image generation approach using a pair of images with structural deformation. Finally, we enforce a viewpoint-based equivariance constraint using the keypoints from the image-level supervision to resolve the spatial correlation problem that consistently appears in the images taken from various viewpoints. Our approach achieves state-of-the-art performance for the task of keypoint estimation on the limited supervision scenarios. Furthermore, the discovered keypoints are directly applicable to downstream tasks without requiring any keypoint labels.",0
"This paper presents a technique for detecting keypoints in a 2D image with the aid of image-level supervision. While some existing methods can identify keypoints for aligned instances, they may fail to match semantic correspondences across different images when target instances display significant viewpoint or appearance variation. Our method addresses this issue by using image-level supervision to identify discriminative parts and infer the viewpoint of the target instance. To discover diverse parts, we employ a conditional image generation approach that uses a pair of images with structural deformation. Additionally, we enforce a viewpoint-based equivariance constraint using the keypoints identified through image-level supervision to resolve the spatial correlation problem that arises with images taken from various viewpoints. Our approach outperforms other methods for keypoint estimation in limited supervision scenarios, and the keypoints it detects can be used directly for downstream tasks without requiring keypoint labels.",1
"In this work we propose a new task: artistic visualization of classical Chinese poems, where the goal is to generatepaintings of a certain artistic style for classical Chinese poems. For this purpose, we construct a new dataset called Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality poem-painting pairs collected manually from an influential modern Chinese artistFeng Zikai. As its small scale poses challenges for effectively training poem-to-painting generation models, we introduce the secondpart of Paint4Poem, which consists of 3,648 caption-painting pairs collected manually from Feng Zikai's paintings and 89,204 poem-painting pairs collected automatically from the web. We expect the former to help learning the artist painting style as it containshis most paintings, and the latter to help learning the semantic relevance between poems and paintings. Further, we analyze Paint4Poem regarding poem diversity, painting style, and the semantic relevance between poems and paintings. We create abenchmark for Paint4Poem: we train two representative text-to-image generation models: AttnGAN and MirrorGAN, and evaluate theirperformance regarding painting pictorial quality, painting stylistic relevance, and semantic relevance between poems and paintings.The results indicate that the models are able to generate paintings that have good pictorial quality and mimic Feng Zikai's style, but thereflection of poem semantics is limited. The dataset also poses many interesting research directions on this task, including transferlearning, few-shot learning, text-to-image generation for low-resource data etc. The dataset is publicly available.(https://github.com/paint4poem/paint4poem)",0
"Our work introduces a new task, which involves creating artistic visualizations of classical Chinese poems. The objective is to generate paintings that reflect a particular artistic style for these poems. To achieve this goal, we have developed a new dataset called Paint4Poem. This dataset comprises two parts: the first includes 301 high-quality pairs of poems and paintings manually collected from the renowned Chinese artist Feng Zikai. However, the limited size of this part poses a challenge for training poem-to-painting generation models effectively. Hence, we have introduced the second part of Paint4Poem, which includes 3,648 caption-painting pairs manually collected from Feng Zikai's paintings and 89,204 poem-painting pairs automatically collected from the web. We expect the first part to help models learn the artist's style, while the second part to aid in learning the semantic relevance between poems and paintings. We have analyzed the dataset for poem diversity, painting style, and semantic relevance between poems and paintings. To benchmark Paint4Poem, we have trained two text-to-image generation models, AttnGAN and MirrorGAN, and evaluated their performance based on painting pictorial quality, painting stylistic relevance, and semantic relevance between poems and paintings. The results demonstrate that the models can generate high-quality paintings that mimic Feng Zikai's style, but the reflection of poem semantics is limited. The dataset provides an exciting avenue for future research on this task, including transfer learning, few-shot learning, and text-to-image generation for low-resource data. The dataset is publicly available on GitHub.",1
"Recently, there has been an increasing interest in image editing methods that employ pre-trained unconditional image generators (e.g., StyleGAN). However, applying these methods to translate images to multiple visual domains remains challenging. Existing works do not often preserve the domain-invariant part of the image (e.g., the identity in human face translations), they do not usually handle multiple domains, or do not allow for multi-modal translations. This work proposes an implicit style function (ISF) to straightforwardly achieve multi-modal and multi-domain image-to-image translation from pre-trained unconditional generators. The ISF manipulates the semantics of an input latent code to make the image generated from it lying in the desired visual domain. Our results in human face and animal manipulations show significantly improved results over the baselines. Our model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data are available at: \url{https://github.com/yhlleo/stylegan-mmuit}.",0
"Lately, there has been a growing fascination with utilizing pre-trained unconditional image generators, such as StyleGAN, for image editing techniques. However, it remains challenging to apply these methods to translate images into various visual domains. This is because current practices often fail to maintain the domain-invariant aspect of the image, such as the identity in human face translations. Additionally, they typically don't handle multiple domains or allow for multi-modal translations. As a solution, this study proposes an implicit style function (ISF) that can effortlessly attain multi-modal and multi-domain image-to-image translations using pre-trained unconditional generators. The ISF modifies the semantics of an input latent code to generate an image that fits the desired visual domain. The results of human face and animal manipulations demonstrate a noticeable improvement over the baselines. The model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data can be accessed at: \url{https://github.com/yhlleo/stylegan-mmuit}.",1
"We propose a novel loss for generative models, dubbed as GRaWD (Generative Random Walk Deviation), to improve learning representations of unexplored visual spaces. Quality learning representation of unseen classes (or styles) is critical to facilitate novel image generation and better generative understanding of unseen visual classes, i.e., zero-shot learning (ZSL). By generating representations of unseen classes based on their semantic descriptions, e.g., attributes or text, generative ZSL attempts to differentiate unseen from seen categories. The proposed GRaWD loss is defined by constructing a dynamic graph that includes the seen class/style centers and generated samples in the current minibatch. Our loss initiates a random walk probability from each center through visual generations produced from hallucinated unseen classes. As a deviation signal, we encourage the random walk to eventually land after t steps in a feature representation that is difficult to classify as any of the seen classes. We demonstrate that the proposed loss can improve unseen class representation quality inductively on text-based ZSL benchmarks on CUB and NABirds datasets and attribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. In addition, we investigate the ability of the proposed loss to generate meaningful novel visual art on the WikiArt dataset. The results of experiments and human evaluations demonstrate that the proposed GRaWD loss can improve StyleGAN1 and StyleGAN2 generation quality and create novel art that is significantly more preferable. Our code is made publicly available at https://github.com/Vision-CAIR/GRaWD.",0
"To enhance the representation of unexplored visual spaces, we introduce a new loss for generative models called GRaWD (Generative Random Walk Deviation). The proper learning representation of unseen styles or classes is crucial to enable better generative understanding of those classes, particularly in zero-shot learning (ZSL). Generative ZSL creates representations of unseen classes based on their semantic descriptions, trying to distinguish between unseen and seen categories. Our proposed GRaWD loss is constructed by forming a dynamic graph that includes the centers of the seen classes/styles and the generated samples in the current minibatch. Our loss begins a random walk probability from each center through visual generations produced from hallucinated unseen classes. As a deviation signal, we urge the random walk to eventually reach a feature representation that is difficult to classify as any of the seen classes. We demonstrate that this approach can improve the quality of representation for unseen classes in text-based ZSL evaluations on CUB and NABirds datasets, and attribute-based ZSL evaluations on AWA2, SUN, and aPY datasets. Furthermore, we examine the potential of this approach to generate meaningful novel visual art on the WikiArt dataset. The results of our experiments and human evaluations show that our proposed GRaWD loss can enhance the quality of StyleGAN1 and StyleGAN2 generation and generate art that is significantly more appealing. Our code is freely available at https://github.com/Vision-CAIR/GRaWD.",1
"Generating fine-grained, realistic images from text has many applications in the visual and semantic realm. Considering that, we propose Bangla Attentional Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage processing for high-resolution Bangla text-to-image generation. Our model can integrate the most specific details at different sub-regions of the image. We distinctively concentrate on the relevant words in the natural language description. This framework has achieved a better inception score on the CUB dataset. For the first time, a fine-grained image is generated from Bangla text using attentional GAN. Bangla has achieved 7th position among 100 most spoken languages. This inspires us to explicitly focus on this language, which will ensure the inevitable need of many people. Moreover, Bangla has a more complex syntactic structure and less natural language processing resource that validates our work more.",0
"The generation of realistic images from text has numerous applications in the semantic and visual domains. To facilitate this, we have developed the Bangla Attentional Generative Adversarial Network (AttnGAN), which employs multi-stage processing to produce high-resolution images from Bangla text. Our model is capable of incorporating specific details from different sub-regions of the image, while focusing on the most relevant words in the natural language description. The framework has achieved a superior inception score on the CUB dataset, and is the first to generate fine-grained images from Bangla text via attentional GAN. Bangla, which is the 7th most spoken language in the world, has motivated us to concentrate on its explicit usage, thereby serving the needs of a larger audience. Additionally, the complex syntactic structure and scarcity of natural language processing resources in Bangla validate the significance of our work.",1
"Variational autoencoders (VAEs), as an important aspect of generative models, have received a lot of research interests and reached many successful applications. However, it is always a challenge to achieve the consistency between the learned latent distribution and the prior latent distribution when optimizing the evidence lower bound (ELBO), and finally leads to an unsatisfactory performance in data generation. In this paper, we propose a latent distribution consistency approach to avoid such substantial inconsistency between the posterior and prior latent distributions in ELBO optimizing. We name our method as latent distribution consistency VAE (LDC-VAE). We achieve this purpose by assuming the real posterior distribution in latent space as a Gibbs form, and approximating it by using our encoder. However, there is no analytical solution for such Gibbs posterior in approximation, and traditional approximation ways are time consuming, such as using the iterative sampling-based MCMC. To address this problem, we use the Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior. Meanwhile, we use the SVGD to train a sampler net which can obtain efficient samples from the Gibbs posterior. Comparative studies on the popular image generation datasets show that our method has achieved comparable or even better performance than several powerful improvements of VAEs.",0
"Generative models have garnered significant interest with the development of variational autoencoders (VAEs). However, optimizing the evidence lower bound (ELBO) often presents the challenge of maintaining consistency between the learned and prior latent distributions, resulting in unsatisfactory data generation. In response, we propose a latent distribution consistency approach to address inconsistency between posterior and prior latent distributions during ELBO optimization. Our method, named LDC-VAE, approximates the real posterior latent distribution as a Gibbs form using an encoder, since no analytical solution exists for such a distribution. We utilize Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior and train a sampler net using SVGD to obtain efficient samples. Our comparative studies on popular image generation datasets demonstrate that our method outperforms several powerful improvements of VAEs.",1
"Recent advances in generative adversarial networks (GANs) have shown remarkable progress in generating high-quality images. However, this gain in performance depends on the availability of a large amount of training data. In limited data regimes, training typically diverges, and therefore the generated samples are of low quality and lack diversity. Previous works have addressed training in low data setting by leveraging transfer learning and data augmentation techniques. We propose a novel transfer learning method for GANs in the limited data domain by leveraging informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We perform experiments on several standard vision datasets using various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the proposed method effectively transfers knowledge to domains with few target images, outperforming existing state-of-the-art techniques in terms of image quality and diversity. We also show the utility of data instance prior in large-scale unconditional image generation.",0
"Recent advancements in generative adversarial networks (GANs) have made significant strides in producing high-quality images. However, this progress is dependent on having access to a large pool of training data. When data is scarce, training often fails, resulting in subpar quality and limited diversity in generated samples. In previous research, transfer learning and data augmentation techniques have been utilized to address training in low data scenarios. In this study, we propose a new transfer learning method for GANs in limited data settings that uses informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We conducted experiments on multiple standard vision datasets using different GAN architectures (BigGAN, SNGAN, StyleGAN2) to prove that our approach effectively transfers knowledge to domains with few target images and outperforms existing state-of-the-art techniques in terms of image quality and diversity. Additionally, we demonstrate that the data instance prior is useful in large-scale unconditional image generation.",1
"With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.",0
"Media and visual forensics need to develop detectors that can identify and attribute images to their generators due to the recent advancements in Generative Adversarial Networks (GANs). Although existing works have shown high accuracy in attributing images to their corresponding GAN sources, they are limited to a closed set scenario and are not scalable with the influx of new GANs. To address this issue, we propose an iterative algorithm that can discover images generated from previously unseen GANs by utilizing the unique fingerprints that all GANs leave on their generated images. Our algorithm comprises multiple components, such as network training, out-of-distribution detection, clustering, merge, and refine steps. Our experiments demonstrate that our algorithm can accurately identify unseen GANs and can also generalize to GANs trained on unseen real datasets. Furthermore, our approach is useful for attribution and discovery of GANs in an online fashion and for the standard task of real/fake detection. Our approach shows promise in discovering new GANs and can be used in an open-world setup.",1
"A novel explainable AI method called CLEAR Image is introduced in this paper. CLEAR Image is based on the view that a satisfactory explanation should be contrastive, counterfactual and measurable. CLEAR Image explains an image's classification probability by contrasting the image with a corresponding image generated automatically via adversarial learning. This enables both salient segmentation and perturbations that faithfully determine each segment's importance. CLEAR Image was successfully applied to a medical imaging case study where it outperformed methods such as Grad-CAM and LIME by an average of 27% using a novel pointing game metric. CLEAR Image excels in identifying cases of ""causal overdetermination"" where there are multiple patches in an image, any one of which is sufficient by itself to cause the classification probability to be close to one.",0
"The paper presents a new approach to explainable AI called CLEAR Image, which emphasizes the importance of contrastive, counterfactual, and measurable explanations. This method uses adversarial learning to generate a corresponding image for the given image, which enables salient segmentation and perturbations to determine each segment's significance accurately. In a medical imaging case study, CLEAR Image outperformed existing methods like Grad-CAM and LIME with a 27% higher score on a novel pointing game metric. Additionally, CLEAR Image excels in identifying ""causal overdetermination"" cases where multiple patches in an image can cause the classification probability to be close to one.",1
"We propose a novel approach, MUSE, to illustrate textual attributes visually via portrait generation. MUSE takes a set of attributes written in text, in addition to facial features extracted from a photo of the subject as input. We propose 11 attribute types to represent inspirations from a subject's profile, emotion, story, and environment. We propose a novel stacked neural network architecture by extending an image-to-image generative model to accept textual attributes. Experiments show that our approach significantly outperforms several state-of-the-art methods without using textual attributes, with Inception Score score increased by 6% and Fr\'echet Inception Distance (FID) score decreased by 11%, respectively. We also propose a new attribute reconstruction metric to evaluate whether the generated portraits preserve the subject's attributes. Experiments show that our approach can accurately illustrate 78% textual attributes, which also help MUSE capture the subject in a more creative and expressive way.",0
"Our proposed method, MUSE, offers a new way to visually represent textual attributes through portrait generation. To achieve this, MUSE takes in a set of attributes written in text, along with facial features extracted from a photo of the subject. We have introduced 11 attribute types to represent different aspects of a subject's profile, emotion, story, and environment. To extend an image-to-image generative model to accept textual attributes, we have devised a novel stacked neural network architecture. Our experiments have demonstrated that MUSE significantly outperforms several state-of-the-art methods that do not use textual attributes, with a 6% increase in Inception Score score and an 11% decrease in Fr\'echet Inception Distance (FID) score. We have also introduced a new attribute reconstruction metric to evaluate the preservation of subject attributes in the generated portraits, which show that MUSE accurately represents 78% of the textual attributes. Overall, our approach captures the subject in a more creative and expressive manner.",1
"Drawing and annotating comic illustrations is a complex and difficult process. No existing machine learning algorithms have been developed to create comic illustrations based on descriptions of illustrations, or the dialogue in comics. Moreover, it is not known if a generative adversarial network (GAN) can generate original comics that correspond to the dialogue and/or descriptions. GANs are successful in producing photo-realistic images, but this technology does not necessarily translate to generation of flawless comics. What is more, comic evaluation is a prominent challenge as common metrics such as Inception Score will not perform comparably, as they are designed to work on photos. In this paper: 1. We implement ComicGAN, a novel text-to-comic pipeline based on a text-to-image GAN that synthesizes comics according to text descriptions. 2. We describe an in-depth empirical study of the technical difficulties of comic generation using GAN's. ComicGAN has two novel features: (i) text description creation from labels via permutation and augmentation, and (ii) custom image encoding with Convolutional Neural Networks. We extensively evaluate the proposed ComicGAN in two scenarios, namely image generation from descriptions, and image generation from dialogue. Our results on 1000 Dilbert comic panels and 6000 descriptions show synthetic comic panels from text inputs resemble original Dilbert panels. Novel methods for text description creation and custom image encoding brought improvements to Frechet Inception Distance, detail, and overall image quality over baseline algorithms. Generating illustrations from descriptions provided clear comics including characters and colours that were specified in the descriptions.",0
"Creating comic illustrations and adding annotations is a challenging and intricate process. Despite the absence of machine learning algorithms that can produce comics based on descriptions or dialogue, it is uncertain if generative adversarial networks (GANs) can generate comics that correspond to the dialogue and descriptions. Although GANs are successful in creating photo-realistic images, this does not necessarily guarantee the creation of flawless comics. Furthermore, evaluating comics presents a significant challenge since standard metrics like Inception Score are not applicable to comics. This paper introduces ComicGAN, a novel text-to-comic pipeline that uses a text-to-image GAN to generate comics based on text descriptions. ComicGAN features two innovative aspects, namely, text description creation through permutation and augmentation and custom image encoding using Convolutional Neural Networks. The proposed ComicGAN is extensively evaluated in two scenarios, namely, image generation from descriptions and image generation from dialogue. Results show that the synthetic comic panels from text inputs closely resemble the original Dilbert panels. The novel methods for text description creation and custom image encoding significantly improved the Frechet Inception Distance, detail, and overall image quality compared to baseline algorithms. The generated illustrations from descriptions produce clear comics with specified characters and colors.",1
"Applications based on Deep Neural Networks (DNNs) have grown exponentially in the past decade. To match their increasing computational needs, several Non-Volatile Memory (NVM) crossbar-based accelerators have been proposed. Apart from improved energy efficiency and performance, these approximate hardware also possess intrinsic robustness for defense against Adversarial Attacks, which is an important security concern for DNNs. Prior works have focused on quantifying this intrinsic robustness for vanilla networks, that is DNNs trained on unperturbed inputs. However, adversarial training of DNNs is the benchmark technique for robustness, and sole reliance on intrinsic robustness of the hardware may not be sufficient. In this work, we explore the design of robust DNNs through the amalgamation of adversarial training and the intrinsic robustness offered by NVM crossbar-based analog hardware. First, we study the noise stability of such networks on unperturbed inputs and observe that internal activations of adversarially trained networks have lower Signal-to-Noise Ratio (SNR), and are sensitive to noise than vanilla networks. As a result, they suffer significantly higher performance degradation due to the non-ideal computations; on an average 2x accuracy drop. On the other hand, for adversarial images generated using Projected-Gradient-Descent (PGD) White-Box attacks, ResNet-10/20 adversarially trained on CIFAR-10/100 display a 5-10% gain in robust accuracy due to the underlying NVM crossbar when the attack epsilon ($\epsilon_{attack}$, the degree of input perturbations) is greater than the epsilon of the adversarial training ($\epsilon_{train}$). Our results indicate that implementing adversarially trained networks on analog hardware requires careful calibration between hardware non-idealities and $\epsilon_{train}$ to achieve optimum robustness and performance.",0
"In the past decade, there has been a significant increase in applications using Deep Neural Networks (DNNs), resulting in a need for Non-Volatile Memory (NVM) crossbar-based accelerators to match their computational requirements. These accelerators offer improved energy efficiency and performance, as well as intrinsic robustness against Adversarial Attacks, which is a vital security concern for DNNs. However, previous studies have only quantified the intrinsic robustness of vanilla networks, without considering the adversarial training of DNNs, which is the benchmark technique for robustness. Therefore, this study explores the design of robust DNNs by combining adversarial training and the intrinsic robustness provided by NVM crossbar-based analog hardware. The noise stability of adversarially trained networks was found to be lower than that of vanilla networks, resulting in a significant decrease in performance due to non-ideal computations. However, when facing adversarial images generated using Projected-Gradient-Descent (PGD) White-Box attacks, ResNet-10/20 adversarially trained on CIFAR-10/100 displayed a 5-10% gain in robust accuracy due to the underlying NVM crossbar, provided the attack epsilon ($\epsilon_{attack}$) was greater than the epsilon of the adversarial training ($\epsilon_{train}$). Thus, careful calibration between hardware non-idealities and $\epsilon_{train}$ is necessary to achieve optimum performance and robustness when implementing adversarially trained networks on analog hardware.",1
"Recently, the Vision Transformer (ViT) has shown impressive performance on high-level and low-level vision tasks. In this paper, we propose a new ViT architecture, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), for single image dehazing. The HyLoG-ViT block consists of two paths, the local ViT path and the global ViT path, which are used to capture local and global dependencies. The hybrid features are fused via convolution layers. As a result, the HyLoG-ViT reduces the computational complexity and introduces locality in the networks. Then, the HyLoG-ViT blocks are incorporated within our dehazing networks, which jointly learn the intrinsic image decomposition and image dehazing. Specifically, the network consists of one shared encoder and three decoders for reflectance prediction, shading prediction, and haze-free image generation. The tasks of reflectance and shading prediction can produce meaningful intermediate features that can serve as complementary features for haze-free image generation. To effectively aggregate the complementary features, we propose a complementary features selection module (CFSM) to select the useful ones for image dehazing. Extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our proposed Transformer-based dehazing network can achieve comparable or even better performance than CNNs-based dehazing models.",0
"The Vision Transformer (ViT) has demonstrated exceptional performance across high-level and low-level vision tasks. In this paper, we introduce a fresh ViT architecture called the Hybrid Local-Global Vision Transformer (HyLoG-ViT) to address single image dehazing. The HyLoG-ViT block includes two paths - the local ViT path and the global ViT path - to capture both local and global dependencies, and these hybrid features are combined using convolution layers to decrease computational complexity and introduce locality in the networks. We then integrate the HyLoG-ViT blocks into our dehazing networks, which jointly learn intrinsic image decomposition and image dehazing. Specifically, the network comprises one shared encoder and three decoders for reflectance prediction, shading prediction, and haze-free image generation. The reflectance and shading prediction tasks produce meaningful intermediate features that serve as complementary features for haze-free image generation. To effectively aggregate these complementary features, we propose a complementary features selection module (CFSM) that selects useful ones for image dehazing. Our experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks demonstrate that our Transformer-based dehazing network outperforms or is comparable to CNN-based dehazing models.",1
"Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive control, semantically meaningful and fully disentangled parameters should be used as modifications. However, many existing techniques do not provide such fine-grained controls or use indirect editing methods i.e. mimic motions of other individuals. In this paper, a Portrait Image Neural Renderer (PIRenderer) is proposed to control the face motions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superiority of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by extracting sequential motions from audio inputs. We show that our model can generate coherent videos with convincing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.",0
"The production of portrait images through the manipulation of pre-existing faces is a crucial undertaking for social media industries. In order to facilitate ease of use and intuitive control, modifications should be made using semantically meaningful and fully disentangled parameters. However, many current techniques fail to provide such detailed controls, instead relying on indirect editing methods that mimic the motions of other individuals. To address this issue, this paper presents the Portrait Image Neural Renderer (PIRenderer), which utilizes the parameters of three-dimensional morphable face models (3DMMs) to precisely control facial movements and generate photorealistic portrait images. Experiments demonstrate the effectiveness of the proposed model in both direct and indirect editing tasks. Furthermore, the model has been extended to handle the audio-driven facial reenactment task, extracting sequential motions from audio inputs to produce coherent videos with convincing movements from a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.",1
"Recent advance in diffusion models incorporates the Stochastic Differential Equation (SDE), which brings the state-of-the art performance on image generation tasks. This paper improves such diffusion models by analyzing the model at the zero diffusion time. In real datasets, the score function diverges as the diffusion time ($t$) decreases to zero, and this observation leads an argument that the score estimation fails at $t=0$ with any neural network structure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that resolves the score diverging problem with an easily applicable modification to any diffusion models. Additionally, we introduce a new SDE that overcomes the theoretic and practical limitations of Variance Exploding SDE. On top of that, the introduced Soft Truncation method improves the sample quality by mitigating the loss scale issue that happens at $t=0$. We further provide a theoretic result of the proposed method to uncover the behind mechanism of the diffusion models.",0
"Diffusion models have recently been enhanced through the incorporation of Stochastic Differential Equation (SDE), which has resulted in state-of-the-art performance in image generation tasks. This study seeks to improve on these diffusion models by examining the model at zero diffusion time. In real datasets, the score function diverges as the diffusion time ($t$) approaches zero, which suggests that score estimation will fail with any neural network structure. As a solution, we introduce the Unbounded Diffusion Model (UDM), which addresses the score diverging problem with a modification that can be easily applied to any diffusion models. Additionally, a new SDE is presented, which overcomes the theoretical and practical limitations of Variance Exploding SDE. Furthermore, a Soft Truncation method is introduced to improve sample quality by addressing the loss scale issue that occurs at $t=0$. We also provide a theoretical explanation of the proposed method to reveal the underlying mechanism of diffusion models.",1
"We develop a rigorous and general framework for constructing information-theoretic divergences that subsume both $f$-divergences and integral probability metrics (IPMs), such as the $1$-Wasserstein distance. We prove under which assumptions these divergences, hereafter referred to as $(f,\Gamma)$-divergences, provide a notion of `distance' between probability measures and show that they can be expressed as a two-stage mass-redistribution/mass-transport process. The $(f,\Gamma)$-divergences inherit features from IPMs, such as the ability to compare distributions which are not absolutely continuous, as well as from $f$-divergences, namely the strict concavity of their variational representations and the ability to control heavy-tailed distributions for particular choices of $f$. When combined, these features establish a divergence with improved properties for estimation, statistical learning, and uncertainty quantification applications. Using statistical learning as an example, we demonstrate their advantage in training generative adversarial networks (GANs) for heavy-tailed, not-absolutely continuous sample distributions. We also show improved performance and stability over gradient-penalized Wasserstein GAN in image generation.",0
"A comprehensive and versatile framework is presented for constructing information-theoretic divergences that encompass both $f$-divergences and integral probability metrics (IPMs), including the $1$-Wasserstein distance. These divergences, called $(f,\Gamma)$-divergences, are proven to serve as a measure of similarity between probability measures, and their construction involves a two-stage process of mass-redistribution and mass-transport. The $(f,\Gamma)$-divergences inherit desirable features from both IPMs and $f$-divergences, such as the ability to compare distributions that are not absolutely continuous, strict concavity of variational representations, and control over heavy-tailed distributions for certain choices of $f$. The combination of these features results in a divergence that has improved properties for various applications, including estimation, statistical learning, and uncertainty quantification. The effectiveness of $(f,\Gamma)$-divergences is demonstrated by training generative adversarial networks (GANs) for heavy-tailed, non-absolutely continuous sample distributions, and by showing improved performance and stability over gradient-penalized Wasserstein GAN in image generation.",1
"Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.",0
"Although Denoising Diffusion Probabilistic Models (DDPM) have demonstrated exceptional performance in generating images without conditions, the randomness inherent in the generative process makes it difficult to produce images with specific semantics. To address this issue, we introduce Iterative Latent Variable Refinement (ILVR), a technique that guides the generative process in DDPM to generate high-quality images based on a reference image. By refining the generative process in DDPM, our approach allows for the generation of images from diverse sets that are directed by the reference image. Our proposed approach, ILVR, facilitates the generation of high-quality images while maintaining control over the generation process. The controllability of our method also enables a single DDPM to adapt to various image generation tasks, such as generating images with different downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles, without requiring additional learning.",1
"Recently, some works found an interesting phenomenon that adversarially robust classifiers can generate good images comparable to generative models. We investigate this phenomenon from an energy perspective and provide a novel explanation. We reformulate adversarial example generation, adversarial training, and image generation in terms of an energy function. We find that adversarial training contributes to obtaining an energy function that is flat and has low energy around the real data, which is the key for generative capability. Based on our new understanding, we further propose a better adversarial training method, Joint Energy Adversarial Training (JEAT), which can generate high-quality images and achieve new state-of-the-art robustness under a wide range of attacks. The Inception Score of the images (CIFAR-10) generated by JEAT is 8.80, much better than original robust classifiers (7.50).",0
"A recent discovery has revealed that adversarially robust classifiers possess the ability to generate high-quality images comparable to those produced by generative models. In order to investigate this phenomenon, we have adopted an energy-based approach and have provided a novel explanation. By reformulating adversarial example generation, adversarial training, and image generation in terms of an energy function, we have found that adversarial training plays a crucial role in achieving a flat energy function with low energy levels around real data, which is essential for generative capability. Building upon this new understanding, we have proposed a superior adversarial training method, Joint Energy Adversarial Training (JEAT), which has demonstrated exceptional image quality and unprecedented robustness against a broad range of attacks. With an Inception Score of 8.80 for the generated images (CIFAR-10), JEAT outperforms the original robust classifiers (7.50).",1
"The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply, which usually manifests as that the images generated by generators tend to have a high similarity amongst them, even though their corresponding latent vectors have been very different. In this paper, we introduce a pluggable diversity penalty module (DPM) to alleviate mode collapse of GANs. It reduces the similarity of image pairs in feature space, i.e., if two latent vectors are different, then we enforce the generator to generate two images with different features. The normalized Gram matrix is used to measure the similarity. We compare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN (Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et al. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results show that the diversity penalty module can help GAN capture much more modes of the data distribution. Further, in classification tasks, we apply this method as image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the classification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared with WGAN GP (Gulrajani et al. 2017), respectively. In domain translation, diversity penalty module can help StarGAN (Choi et al. 2018) generate more accurate attention masks and accelarate the convergence process. Finally, we quantitatively evaluate the proposed method with IS and FID on CelebA, CIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity penalty module gets much higher IS and lower FID compared with some SOTA GAN architectures.",0
"The vanilla GAN devised by Goodfellow et al. in 2014 is prone to mode collapse, which can result in generated images that are too similar to one another despite having different underlying latent vectors. To address this issue, we propose a diversity penalty module (DPM) that reduces image pair similarity in feature space, forcing the generator to generate images with more varied features even when the latent vectors are different. We use the normalized Gram matrix to measure similarity and compare our approach with several other GAN architectures on 2D synthetic datasets, demonstrating that the DPM can help GANs capture more modes of the data distribution. We also apply this method to image data augmentation on MNIST, Fashion-MNIST, and CIFAR-10, resulting in improved classification accuracy compared to WGAN GP. In domain translation, the DPM can help StarGAN generate more accurate attention masks and accelerate convergence. We evaluate our approach on CelebA, CIFAR-10, MNIST, and Fashion-MNIST using IS and FID metrics, showing that our approach outperforms some state-of-the-art GAN architectures.",1
"We propose a conditional generative adversarial network (GAN) model for zero-shot video generation. In this study, we have explored zero-shot conditional generation setting. In other words, we generate unseen videos from training samples with missing classes. The task is an extension of conditional data generation. The key idea is to learn disentangled representations in the latent space of a GAN. To realize this objective, we base our model on the motion and content decomposed GAN and conditional GAN for image generation. We build the model to find better-disentangled representations and to generate good-quality videos. We demonstrate the effectiveness of our proposed model through experiments on the Weizmann action database and the MUG facial expression database.",0
"Our study introduces a novel approach to zero-shot video generation through a conditional generative adversarial network (GAN) model. Specifically, we investigate the zero-shot conditional generation scenario, where we generate videos that have missing classes from the training samples. This task expands on the concept of conditional data generation, with our key objective being to acquire disentangled representations in the latent space of the GAN. To achieve this, we adopt the motion and content decomposed GAN and conditional GAN for image generation. Our model aims to produce high-quality videos with better-disentangled representations. We validate the effectiveness of our proposed method through experiments conducted on the Weizmann action database and the MUG facial expression database.",1
"Deep learning is a technique for machine learning using multi-layer neural networks. It has been used for image synthesis and image recognition, but in recent years, it has also been used for various social detection and social labeling. In this analysis, we compared (1) the number of Iterations per minute between the GPU and CPU when using the VGG model and the NIN model, and (2) the number of Iterations per minute by the number of pixels when using the VGG model, using an image with 128 pixels. When the number of pixels was 64 or 128, the processing time was almost the same when using the GPU, but when the number of pixels was changed to 256, the number of iterations per minute decreased and the processing time increased by about three times. In this case study, since the number of pixels becomes core dumping when the number of pixels is 512 or more, we can consider that we should consider improvement in the vector calculation part. If we aim to achieve 8K highly saturated computer graphics using neural networks, we will need to consider an environment that allows computation even when the size of the image becomes even more highly saturated and massive, and parallel computation when performing image recognition and tuning.",0
"Deep learning involves using multi-layer neural networks for machine learning. It has proven effective in image recognition and synthesis, as well as social detection and labeling. Our analysis compared the number of iterations per minute for the VGG and NIN models using GPU and CPU, as well as the number of iterations per minute by the number of pixels using the VGG model with a 128-pixel image. For images with 64 or 128 pixels, processing time was similar on the GPU. However, with 256 pixels, processing time increased by about three times. When the number of pixels exceeded 512, the system experienced core dumping, indicating a need for improvement in vector calculation. To achieve highly saturated computer graphics using neural networks, a parallel computation environment that can handle even larger images must be created.",1
"This paper investigates an open research task of text-to-image synthesis for automatically generating or manipulating images from text descriptions. Prevailing methods mainly use the text as conditions for GAN generation, and train different models for the text-guided image generation and manipulation tasks. In this paper, we propose a novel unified framework of Cycle-consistent Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image manipulation tasks. Specifically, we first train a GAN model without text input, aiming to generate images with high diversity and quality. Then we learn a GAN inversion model to convert the images back to the GAN latent space and obtain the inverted latent codes for each image, where we introduce the cycle-consistency training to learn more robust and consistent inverted latent codes. We further uncover the latent space semantics of the trained GAN model, by learning a similarity model between text representations and the latent codes. In the text-guided optimization module, we generate images with the desired semantic attributes by optimizing the inverted latent codes. Extensive experiments on the Recipe1M and CUB datasets validate the efficacy of our proposed framework.",0
"The purpose of this study is to explore the text-to-image synthesis research area, which involves automatically generating or modifying images based on text descriptions. Current methods rely on using text as conditions for GAN generation, and separate models are trained for text-guided image generation and manipulation tasks. This paper proposes a new framework called Cycle-consistent Inverse GAN (CI-GAN) that can handle both tasks. The approach involves training a GAN model without text input to generate diverse high-quality images, followed by a GAN inversion model to convert the images back to the GAN latent space. Cycle-consistency training is used to learn more robust and consistent inverted latent codes. A similarity model is then introduced to uncover the latent space semantics of the trained GAN model. In the text-guided optimization module, the framework optimizes inverted latent codes to generate images with desired semantic attributes. The study validates the effectiveness of this approach through experiments conducted on the Recipe1M and CUB datasets.",1
"Generative diffusion models have emerged as leading models in speech and image generation. However, in order to perform well with a small number of denoising steps, a costly tuning of the set of noise parameters is needed. In this work, we present a simple and versatile learning scheme that can step-by-step adjust those noise parameters, for any given number of steps, while the previous work needs to retune for each number separately. Furthermore, without modifying the weights of the diffusion model, we are able to significantly improve the synthesis results, for a small number of steps. Our approach comes at a negligible computation cost.",0
"Leading models for speech and image generation are generative diffusion models. However, to operate effectively with only a few denoising steps, it is necessary to perform an expensive tuning of the noise parameter set. This study introduces a flexible and straightforward learning approach that can smoothly adjust these noise parameters for any desired number of steps. In contrast, previous methods require retuning for each step count separately. Additionally, we can substantially enhance the synthesis results for a small number of steps without changing the weights of the diffusion model. Our approach is computationally inexpensive.",1
"As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition (MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants.",0
"The attention mechanism, specifically self-attention, is a crucial component of modern deep learning that aids in the discovery of global correlations. However, the question remains if hand-crafted attention is indispensable when modeling global context. Our research has found that self-attention is not superior to the matrix decomposition (MD) model that was developed two decades ago regarding computational cost and performance for encoding long-distance dependencies. We address the global context problem by modeling it as a low-rank recovery issue and demonstrate that optimization algorithms can aid in the design of global information blocks. Our paper presents a series of Hamburgers that use optimization algorithms to factorize input representations into sub-matrices and reconstruct a low-rank embedding. By carefully handling gradients back-propagated through MDs, Hamburgers with varying MDs perform exceptionally well against the commonly used global context module, self-attention. We conducted extensive experiments in vision tasks that demand learning global context, such as semantic segmentation and image generation, and observed significant improvements over self-attention and its variations.",1
"In this study, we introduce a measure for machine perception, inspired by the concept of Just Noticeable Difference (JND) of human perception. Based on this measure, we suggest an adversarial image generation algorithm, which iteratively distorts an image by an additive noise until the model detects the change in the image by outputting a false label. The noise added to the original image is defined as the gradient of the cost function of the model. A novel cost function is defined to explicitly minimize the amount of perturbation applied to the input image while enforcing the perceptual similarity between the adversarial and input images. For this purpose, the cost function is regularized by the well-known total variation and bounded range terms to meet the natural appearance of the adversarial image. We evaluate the adversarial images generated by our algorithm both qualitatively and quantitatively on CIFAR10, ImageNet, and MS COCO datasets. Our experiments on image classification and object detection tasks show that adversarial images generated by our JND method are both more successful in deceiving the recognition/detection models and less perturbed compared to the images generated by the state-of-the-art methods, namely, FGV, FSGM, and DeepFool methods.",0
"This study presents a new measure for machine perception, inspired by the Just Noticeable Difference (JND) concept in human perception. An adversarial image generation algorithm is proposed, which distorts an image through additive noise until the model outputs a false label. The noise is determined by the cost function gradient, and a novel cost function is defined to minimize perturbation while maintaining perceptual similarity between the adversarial and input images. The cost function is regularized by total variation and bounded range terms to preserve natural image appearance. The generated adversarial images are evaluated on CIFAR10, ImageNet, and MS COCO datasets, and the results show that our JND method is more successful in deceiving recognition/detection models and less perturbed than state-of-the-art methods such as FGV, FSGM, and DeepFool.",1
"Generating images according to natural language descriptions is a challenging task. Prior research has mainly focused to enhance the quality of generation by investigating the use of spatial attention and/or textual attention thereby neglecting the relationship between channels. In this work, we propose the Combined Attention Generative Adversarial Network (CAGAN) to generate photo-realistic images according to textual descriptions. The proposed CAGAN utilises two attention models: word attention to draw different sub-regions conditioned on related words; and squeeze-and-excitation attention to capture non-linear interaction among channels. With spectral normalisation to stabilise training, our proposed CAGAN improves the state of the art on the IS and FID on the CUB dataset and the FID on the more challenging COCO dataset. Furthermore, we demonstrate that judging a model by a single evaluation metric can be misleading by developing an additional model adding local self-attention which scores a higher IS, outperforming the state of the art on the CUB dataset, but generates unrealistic images through feature repetition.",0
"Generating realistic images based on natural language descriptions presents a difficult challenge. Previous research has focused on improving the quality of image generation through the use of spatial and/or textual attention, without considering the relationship between channels. Our work introduces the Combined Attention Generative Adversarial Network (CAGAN), which employs two attention models: word attention to select sub-regions based on related words, and squeeze-and-excitation attention to capture non-linear interactions among channels. We use spectral normalisation to stabilise training, and our CAGAN outperforms previous approaches on the IS and FID metrics on both the CUB and COCO datasets. We also show that relying on a single evaluation metric can be misleading, as an additional model with local self-attention achieves a higher IS score but generates unrealistic images due to feature repetition.",1
"Empirically multidimensional discriminator (critic) output can be advantageous, while a solid explanation for it has not been discussed. In this paper, (i) we rigorously prove that high-dimensional critic output has advantage on distinguishing real and fake distributions; (ii) we also introduce an square-root velocity transformation (SRVT) block which further magnifies this advantage. The proof is based on our proposed maximal p-centrality discrepancy which is bounded above by p-Wasserstein distance and perfectly fits the Wasserstein GAN framework with high-dimensional critic output n. We have also showed when n = 1, the proposed discrepancy is equivalent to 1-Wasserstein distance. The SRVT block is applied to break the symmetric structure of high-dimensional critic output and improve the generalization capability of the discriminator network. In terms of implementation, the proposed framework does not require additional hyper-parameter tuning, which largely facilitates its usage. Experiments on image generation tasks show performance improvement on benchmark datasets.",0
"Although the advantages of using a multidimensional discriminator output have been observed, there is a lack of explanation for why this is the case. This paper aims to (i) provide a rigorous proof that high-dimensional critic output is advantageous in differentiating between real and fake distributions, and (ii) introduce a square-root velocity transformation (SRVT) block that enhances this advantage even further. Our proof is based on the proposed maximal p-centrality discrepancy, which is bounded above by the p-Wasserstein distance and is a perfect fit for the Wasserstein GAN framework with high-dimensional critic output n. We also demonstrate that when n = 1, the proposed discrepancy is equivalent to the 1-Wasserstein distance. The SRVT block breaks down the symmetrical structure of high-dimensional critic output and improves the generalization capability of the discriminator network. The proposed framework does not require additional hyper-parameter tuning, making it easy to implement. Our experimental results on image generation tasks show significant performance improvements on benchmark datasets.",1
"Deep neural networks have become the default choice for many applications like image and video recognition, segmentation and other image and video related tasks.However, a critical challenge with these models is the lack of explainability.This requirement of generating explainable predictions has motivated the research community to perform various analysis on trained models.In this study, we analyze the learned feature maps of trained models using MNIST images for achieving more explainable predictions.Our study is focused on deriving a set of primitive elements, here called visual concepts, that can be used to generate any arbitrary sample from the data generating distribution.We derive the primitive elements from the feature maps learned by the model.We illustrate the idea by generating visual concepts from a Variational Autoencoder trained using MNIST images.We augment the training data of MNIST dataset by adding about 60,000 new images generated with visual concepts chosen at random.With this we were able to reduce the reconstruction loss (mean square error) from an initial value of 120 without augmentation to 60 with augmentation.Our approach is a first step towards the final goal of achieving trained deep neural network models whose predictions, features in hidden layers and the learned filters can be well explained.Such a model when deployed in production can easily be modified to adapt to new data, whereas existing deep learning models need a re training or fine tuning. This process again needs a huge number of data samples that are not easy to generate unless the model has good explainability.",0
"Although deep neural networks are commonly used for tasks like image and video recognition, segmentation, and other related tasks, their lack of explainability poses a major challenge. In order to address this issue, researchers have performed various analyses on trained models to generate more explainable predictions. Our study focuses on analyzing the learned feature maps of trained models using MNIST images to derive a set of visual concepts that can be used to generate any arbitrary sample from the data generating distribution. We demonstrate this by generating visual concepts with a Variational Autoencoder trained with MNIST images and adding about 60,000 new images generated with random visual concepts to augment the training data. This resulted in a reduction of the reconstruction loss from 120 to 60. Our approach is a first step towards achieving trained deep neural network models that can be easily modified to adapt to new data, without requiring retraining or fine-tuning, which is currently difficult to achieve without good explainability.",1
"Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.",0
"The most commonly used networks for image generation are Generative Adversarial Networks (GANs). To prevent the discriminator from forgetting and to improve image generation quality without requiring class labels, self-supervised GANs were introduced. However, there has been no prior study on the generalizability of self-supervision tasks across various GAN architectures. Therefore, this study examines the generalizability of a previously proposed self-supervision task, deshuffling, on different GAN architectures. The deshuffling task is assigned to two GAN discriminators, and its effects on both architectures are studied. The evaluations are extended to various datasets, and it is shown that DeshuffleGAN performs the best compared to other self-supervised GANs in terms of FID results for several datasets. The contribution of deshuffling is also compared with rotation prediction, and it is demonstrated that deshuffling is more effective. A conditional DeshuffleGAN is designed to evaluate the quality of the learnt representations. The results indicate that the effects of these tasks on adversarial training may not always be cooperative. The code for this study is available at https://github.com/gulcinbaykal/DeshuffleGAN.",1
"We proposes a flexible person generation framework called Dressing in Order (DiOr), which supports 2D pose transfer, virtual try-on, and several fashion editing tasks. The key to DiOr is a novel recurrent generation pipeline to sequentially put garments on a person, so that trying on the same garments in different orders will result in different looks. Our system can produce dressing effects not achievable by existing work, including different interactions of garments (e.g., wearing a top tucked into the bottom or over it), as well as layering of multiple garments of the same type (e.g., jacket over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each garment, enabling these elements to be edited separately. Joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. Extensive evaluations show that DiOr outperforms other recent methods like ADGAN in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision.",0
"We introduce DiOr, a person generation framework that is flexible and supports various fashion editing tasks such as virtual try-on and 2D pose transfer. The uniqueness of DiOr lies in its recurrent generation pipeline, which allows for the sequential placement of garments on a person resulting in different looks when the order of clothing is varied. Our system stands out from existing work by enabling different garment interactions like layering and tucking, which were previously unattainable. Additionally, DiOr allows for the separate editing of garment shape and texture. The joint training of pose transfer and inpainting helps to preserve details and coherence of the generated garments. Evaluations demonstrate that DiOr produces higher quality outputs compared to other recent methods like ADGAN, and it can handle a broad range of editing functions with no direct supervision.",1
"This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (eg, class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems:(P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (aka empirical cGAN losses) often fails in practice;(P2) Since regression labels are scalar and infinitely many, conventional label input methods are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) method and an improved label input (ILI) method to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under mild assumptions in this work. Two new benchmark datasets (RC-49 and Cell-200) and a novel evaluation metric (Sliding Fr\'echet Inception Distance) are also proposed for this continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49, UTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN substantially outperforms cGAN both visually and quantitatively.",0
"In this paper, the authors introduce the continuous conditional generative adversarial network (CcGAN), which is the first generative model that can create images based on continuous, scalar conditions (referred to as regression labels). While existing conditional GANs (cGANs) are designed for categorical conditions, conditioning on regression labels presents two significant challenges. Firstly, there may be very few or no real images for certain regression labels, which makes minimizing existing empirical cGAN losses difficult. Secondly, regression labels are scalar and infinite, meaning traditional label input methods are unsuitable. The CcGAN addresses these challenges by proposing a reformulation of existing empirical cGAN losses to suit the continuous scenario and introducing two label input methods, namely the naive label input (NLI) and the improved label input (ILI) methods. The authors also propose two novel empirical discriminator losses and a novel empirical generator loss. Furthermore, they introduce two new benchmark datasets and an evaluation metric for this continuous scenario. Experimental results show that the CcGAN generates diverse, high-quality images and outperforms cGAN both visually and quantitatively.",1
"Training real-world neural network models to achieve high performance and generalizability typically requires a substantial amount of labeled data, spanning a broad range of variation. This data-labeling process can be both labor and cost intensive. To achieve desirable predictive performance, a trained model is typically applied into a domain where the data distribution is similar to the training dataset. However, for many agricultural machine learning problems, training datasets are collected at a specific location, during a specific period in time of the growing season. Since agricultural systems exhibit substantial variability in terms of crop type, cultivar, management, seasonal growth dynamics, lighting condition, sensor type, etc, a model trained from one dataset often does not generalize well across domains. To enable more data efficient and generalizable neural network models in agriculture, we propose a method that generates photorealistic agricultural images from a synthetic 3D crop model domain into real world crop domains. The method uses a semantically constrained GAN (generative adversarial network) to preserve the fruit position and geometry. We observe that a baseline CycleGAN method generates visually realistic target domain images but does not preserve fruit position information while our method maintains fruit positions well. Image generation results in vineyard grape day and night images show the visual outputs of our network are much better compared to a baseline network. Incremental training experiments in vineyard grape detection tasks show that the images generated from our method can significantly speed the domain adaption process, increase performance for a given number of labeled images (i.e. data efficiency), and decrease labeling requirements.",0
"To achieve high performance and generalizability in neural network models for agriculture, a significant amount of labeled data covering a broad range of variation is required, which can be labor and cost intensive. However, training datasets for agricultural machine learning problems are often collected at a specific location and time, which limits their generalizability due to the substantial variability in agricultural systems. To address this issue, we propose a method that generates photorealistic agricultural images from a synthetic 3D crop model domain into real-world crop domains using a semantically constrained GAN. Our approach preserves fruit position and geometry better than a baseline CycleGAN method, as shown in vineyard grape day and night images. Incremental training experiments demonstrate that our method significantly speeds up the domain adaption process, increases performance for a given number of labeled images, and decreases labeling requirements, making it more data efficient and generalizable.",1
"Current state-of-the-art photorealistic generators are computationally expensive, involve unstable training processes, and have real and synthetic distributions that are dissimilar in higher-dimensional spaces. To solve these issues, we propose a variant of the StackGAN architecture. The new architecture incorporates conditional generators to construct an image in many stages. In our model, we generate grayscale facial images in two different stages: noise to edges (stage one) and edges to grayscale (stage two). Our model is trained with the CelebA facial image dataset and achieved a Fr\'echet Inception Distance (FID) score of 73 for edge images and a score of 59 for grayscale images generated using the synthetic edge images. Although our model achieved subpar results in relation to state-of-the-art models, dropout layers could reduce the overfitting in our conditional mapping. Additionally, since most images can be broken down into important features, improvements to our model can generalize to other datasets. Therefore, our model can potentially serve as a superior alternative to traditional means of generating photorealistic images.",0
"Cutting-edge photorealistic generators are expensive in terms of computation, entail unstable training processes, and exhibit differences between their true and artificial distributions in higher-dimensional spaces. We propose a modified StackGAN architecture to tackle these challenges. Our novel design employs conditional generators to manufacture an image in multiple phases. Specifically, we produce grayscale facial images in two stages: noise to edges (first stage) and edges to grayscale (second stage). We train our model on the CelebA facial image dataset, yielding a Fr\'echet Inception Distance (FID) score of 73 for edge images and 59 for grayscale images generated from synthetic edge images. Although our model's results are less impressive than those of cutting-edge models, we could mitigate overfitting in our conditional mapping by using dropout layers. Furthermore, since most images can be decomposed into key features, upgrades to our model may be applicable to other datasets. As a result, our model could potentially serve as a superior alternative to conventional methods of generating photorealistic images.",1
"Over recent years, deep learning-based computer vision systems have been applied to images at an ever-increasing pace, oftentimes representing the only type of consumption for those images. Given the dramatic explosion in the number of images generated per day, a question arises: how much better would an image codec targeting machine-consumption perform against state-of-the-art codecs targeting human-consumption? In this paper, we propose an image codec for machines which is neural network (NN) based and end-to-end learned. In particular, we propose a set of training strategies that address the delicate problem of balancing competing loss functions, such as computer vision task losses, image distortion losses, and rate loss. Our experimental results show that our NN-based codec outperforms the state-of-the-art Versa-tile Video Coding (VVC) standard on the object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast thanks to its compact size. To the best of our knowledge, this is the first end-to-end learned machine-targeted image codec.",0
"In recent years, there has been a growing use of deep learning-based computer vision systems to analyze images. As the number of images produced each day continues to rise, it raises the question of how well an image codec designed for machine consumption would perform compared to one designed for human consumption. This paper proposes a new image codec for machines that uses neural networks and is end-to-end learned. The training process addresses the challenge of balancing competing loss functions, including computer vision task losses, image distortion losses, and rate loss. Our experimental results demonstrate that our NN-based codec outperforms the state-of-the-art VVC standard in object detection and instance segmentation tasks, achieving a BD-rate gain of -37.87% and -32.90%, respectively, while remaining compact and efficient. This is the first machine-targeted image codec to be end-to-end learned.",1
"While the researches on single image super-resolution (SISR), especially equipped with deep neural networks (DNNs), have achieved tremendous successes recently, they still suffer from two major limitations. Firstly, the real image degradation is usually unknown and highly variant from one to another, making it extremely hard to train a single model to handle the general SISR task. Secondly, most of current methods mainly focus on the downsampling process of the degradation, but ignore or underestimate the inevitable noise contamination. For example, the commonly-used independent and identically distributed (i.i.d.) Gaussian noise distribution always largely deviates from the real image noise (e.g., camera sensor noise), which limits their performance in real scenarios. To address these issues, this paper proposes a model-based unsupervised SISR method to deal with the general SISR task with unknown degradations. Instead of the traditional i.i.d. Gaussian noise assumption, a novel patch-based non-i.i.d. noise modeling method is proposed to fit the complex real noise. Besides, a deep generator parameterized by a DNN is used to map the latent variable to the high-resolution image, and the conventional hyper-Laplacian prior is also elaborately embedded into such generator to further constrain the image gradients. Finally, a Monte Carlo EM algorithm is designed to solve our model, which provides a general inference framework to update the image generator both w.r.t. the latent variable and the network parameters. Comprehensive experiments demonstrate that the proposed method can evidently surpass the current state of the art (SotA) method (about 1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster speed.",0
"Although single image super-resolution (SISR) research, particularly those utilizing deep neural networks (DNNs), has achieved significant success, it still has two significant limitations. First, it is challenging to train a single model to handle the general SISR task since real image degradation is typically unknown and highly variable. Second, current methods often overlook or underestimate noise contamination, such as camera sensor noise, when focusing on the downsampling process of degradation. To address these limitations, this paper proposes an unsupervised SISR method that models complex real noise using a patch-based non-i.i.d. noise modeling method instead of the traditional i.i.d. Gaussian noise assumption. A deep generator, parameterized by a DNN, maps the latent variable to the high-resolution image, and the conventional hyper-Laplacian prior is embedded into the generator to further constrain image gradients. A Monte Carlo EM algorithm is designed to solve the model, providing a general inference framework to update the image generator with respect to both the latent variable and network parameters. Comprehensive experiments show that the proposed method surpasses the current state of the art method (by about 1dB PSNR) with a smaller model (0.34M vs. 2.40M) and faster speed.",1
"We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and $p$-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework.",0
"Our proposed approach to generative models is based on manifold matching and involves a distribution generator and a metric generator. The real data set is considered as a manifold embedded in a high-dimensional Euclidean space, and the distribution generator generates samples that follow a distribution condensed around the real data manifold. This is achieved by matching two sets of points using their geometric shape descriptors with a learned distance metric. The metric generator learns a distance metric close to some intrinsic geodesic distance on the real data manifold using both real data and generated samples. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply our approach to unsupervised and supervised learning tasks which include unconditional image generation and super-resolution. Our method produces competitive results compared to existing generative models and improves visual qualities by generating samples with more natural textures. Our experiments and analysis demonstrate the feasibility and effectiveness of our proposed framework.",1
"Conditional image synthesis from layout has recently attracted much interest. Previous approaches condition the generator on object locations as well as class labels but lack fine-grained control over the diverse appearance aspects of individual objects. Gaining control over the image generation process is fundamental to build practical applications with a user-friendly interface. In this paper, we propose a method for attribute controlled image synthesis from layout which allows to specify the appearance of individual objects without affecting the rest of the image. We extend a state-of-the-art approach for layout-to-image generation to additionally condition individual objects on attributes. We create and experiment on a synthetic, as well as the challenging Visual Genome dataset. Our qualitative and quantitative results show that our method can successfully control the fine-grained details of individual objects when modelling complex scenes with multiple objects. Source code, dataset and pre-trained models are publicly available (https://github.com/stanifrolov/AttrLostGAN).",0
"The concept of generating images based on conditional layouts has recently gained popularity. However, previous approaches lacked the ability to precisely control the appearance of individual objects within the generated image, despite conditioning the generator on object locations and class labels. In order to develop practical applications with an easy-to-use interface, it is crucial to gain control over the image generation process. In this study, we introduce a technique for generating attribute-controlled images from layouts, which enables the specification of individual object appearance without affecting the rest of the image. Our method extends a cutting-edge approach for layout-to-image generation by incorporating object attributes. We evaluate our approach on both a synthetic dataset and the complex Visual Genome dataset, and our results demonstrate that our method successfully controls the fine-grained details of individual objects in complex scenes with multiple objects. Our source code, dataset, and pre-trained models are publicly available at https://github.com/stanifrolov/AttrLostGAN.",1
"In this paper, we treat the image generation task using an autoencoder, a representative latent model. Unlike many studies regularizing the latent variable's distribution by assuming a manually specified prior, we approach the image generation task using an autoencoder by directly estimating the latent distribution. To this end, we introduce 'latent density estimator' which captures latent distribution explicitly and propose its structure. Through experiments, we show that our generative model generates images with the improved visual quality compared to previous autoencoder-based generative models.",0
"In this article, we discuss the use of an autoencoder as a typical latent model for the image generation task. Our approach differs from previous studies that regulate the distribution of the latent variable by assuming a manually specified prior. Instead, we estimate the latent distribution directly using the autoencoder and introduce a 'latent density estimator' to capture the distribution explicitly. We present the structure of this estimator and demonstrate through experiments that our generative model produces images with higher visual quality than previous autoencoder-based generative models.",1
"Radio echo sounding (RES) is a common technique used in subsurface glacial imaging, which provides insight into the underlying rock and ice. However, systematic noise is introduced into the data during collection, complicating interpretation of the results. Researchers most often use a combination of manual interpretation and filtering techniques to denoise data; however, these processes are time intensive and inconsistent. Fully Convolutional Networks have been proposed as an automated alternative to identify layer boundaries in radargrams. However, they require high-quality manually processed training data and struggle to interpolate data in noisy samples (Varshney et al. 2020).   Herein, the authors propose a GAN based model to interpolate layer boundaries through noise and highlight layers in two-dimensional glacial RES data. In real-world noisy images, filtering often results in loss of data such that interpolating layer boundaries is nearly impossible. Furthermore, traditional machine learning approaches are not suited to this task because of the lack of paired data, so we employ an unpaired image-to-image translation model. For this model, we create a synthetic dataset to represent the domain of images with clear, highlighted layers and use an existing real-world RES dataset as our noisy domain.   We implement a CycleGAN trained on these two domains to highlight layers in noisy images that can interpolate effectively without significant loss of structure or fidelity. Though the current implementation is not a perfect solution, the model clearly highlights layers in noisy data and allows researchers to determine layer size and position without mathematical filtering, manual processing, or ground-truth images for training. This is significant because clean images generated by our model enable subsurface researchers to determine glacial layer thickness more efficiently.",0
"The technique of radio echo sounding (RES) is commonly utilized in subsurface glacial imaging to gain knowledge about the underlying rock and ice. However, data collection introduces systematic noise, which makes it challenging to interpret the outcomes. Researchers typically use manual interpretation and filtering techniques to denoise data, but these processes are inconsistent and time-consuming. Although Fully Convolutional Networks have been suggested as an automated alternative to identify layer boundaries in radargrams, they struggle to interpolate data in noisy samples and necessitate high-quality manually processed training data. In this study, the authors propose a GAN based model to highlight layers in two-dimensional glacial RES data and interpolate layer boundaries through noise. Filtering often leads to loss of data in real-world noisy images, making it almost impossible to interpolate layer boundaries. Traditional machine learning approaches are also unsuitable for this task due to the absence of paired data. Therefore, the authors employ an unpaired image-to-image translation model and create a synthetic dataset to represent images with clear, highlighted layers. They use an existing real-world RES dataset as their noisy domain and train a CycleGAN on these two domains to effectively interpolate layers in noisy images without significant loss of structure or fidelity. This model enables subsurface researchers to determine glacial layer thickness more efficiently without the need for mathematical filtering, manual processing, or ground-truth images for training. Although the current implementation is not perfect, it is a significant step forward in highlighting layers in noisy data.",1
"Generative adversarial networks have been widely used in image synthesis in recent years and the quality of the generated image has been greatly improved. However, the flexibility to control and decouple facial attributes (e.g., eyes, nose, mouth) is still limited. In this paper, we propose a novel approach, called ChildGAN, to generate a child's image according to the images of parents with heredity prior. The main idea is to disentangle the latent space of a pre-trained generation model and precisely control the face attributes of child images with clear semantics. We use distances between face landmarks as pseudo labels to figure out the most influential semantic vectors of the corresponding face attributes by calculating the gradient of latent vectors to pseudo labels. Furthermore, we disentangle the semantic vectors by weighting irrelevant features and orthogonalizing them with Schmidt Orthogonalization. Finally, we fuse the latent vector of the parents by leveraging the disentangled semantic vectors under the guidance of biological genetic laws. Extensive experiments demonstrate that our approach outperforms the existing methods with encouraging results.",0
"The use of generative adversarial networks to create images has become popular in recent years, leading to significant improvements in the quality of the generated images. Despite this progress, controlling and separating facial attributes like eyes, nose, and mouth remains challenging. This paper proposes a new method, called ChildGAN, to generate images of children using the images of their parents as a starting point. The approach involves breaking down the latent space of a pre-trained generation model and precisely controlling the facial attributes of the child's image. The distances between facial landmarks are used as pseudo labels to identify the most influential semantic vectors, and irrelevant features are weighted and orthogonalized using Schmidt Orthogonalization. The latent vector of the parents is then fused using the disentangled semantic vectors according to biological genetic laws. Extensive experiments demonstrate that this approach outperforms existing methods.",1
"In this paper, we propose a novel loss function for training Generative Adversarial Networks (GANs) aiming towards deeper theoretical understanding as well as improved stability and performance for the underlying optimization problem. The new loss function is based on cumulant generating functions giving rise to \emph{Cumulant GAN}. Relying on a recently-derived variational formula, we show that the corresponding optimization problem is equivalent to R{\'e}nyi divergence minimization, thus offering a (partially) unified perspective of GAN losses: the R{\'e}nyi family encompasses Kullback-Leibler divergence (KLD), reverse KLD, Hellinger distance and $\chi^2$-divergence. Wasserstein GAN is also a member of cumulant GAN. In terms of stability, we rigorously prove the linear convergence of cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions and the standard gradient descent ascent algorithm. Finally, we experimentally demonstrate that image generation is more robust relative to Wasserstein GAN and it is substantially improved in terms of both inception score and Fr\'echet inception distance when both weaker and stronger discriminators are considered.",0
"The purpose of this paper is to introduce a new loss function for Generative Adversarial Networks (GANs) that aims to enhance the theoretical understanding and stability of the optimization process, ultimately resulting in improved performance. The proposed loss function, referred to as Cumulant GAN, is based on cumulant generating functions. By using a variational formula, we demonstrate that optimizing this function is equivalent to minimizing R{\'e}nyi divergence, which unifies several other GAN losses such as Kullback-Leibler divergence, reverse KLD, Hellinger distance, and $\chi^2$-divergence. Additionally, Cumulant GAN includes Wasserstein GAN as a member. We prove the linear convergence of Cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions, and the standard gradient descent ascent algorithm. Finally, our experimental results demonstrate that Cumulant GAN generates more robust images than Wasserstein GAN, with significant improvement in both inception score and Fr\'echet inception distance when using weaker or stronger discriminators.",1
"Single image generative models perform synthesis and manipulation tasks by capturing the distribution of patches within a single image. The classical (pre Deep Learning) prevailing approaches for these tasks are based on an optimization process that maximizes patch similarity between the input and generated output. Recently, however, Single Image GANs were introduced both as a superior solution for such manipulation tasks, but also for remarkable novel generative tasks. Despite their impressiveness, single image GANs require long training time (usually hours) for each image and each task. They often suffer from artifacts and are prone to optimization issues such as mode collapse. In this paper, we show that all of these tasks can be performed without any training, within several seconds, in a unified, surprisingly simple framework. We revisit and cast the ""good-old"" patch-based methods into a novel optimization-free framework. We start with an initial coarse guess, and then simply refine the details coarse-to-fine using patch-nearest-neighbor search. This allows generating random novel images better and much faster than GANs. We further demonstrate a wide range of applications, such as image editing and reshuffling, retargeting to different sizes, structural analogies, image collage and a newly introduced task of conditional inpainting. Not only is our method faster ($\times 10^3$-$\times 10^4$ than a GAN), it produces superior results (confirmed by quantitative and qualitative evaluation), less artifacts and more realistic global structure than any of the previous approaches (whether GAN-based or classical patch-based).",0
"Generative models that operate on a single image typically manipulate and synthesize by capturing the distribution of patches within the image. Traditional methods for these tasks before the advent of deep learning relied on optimizing patch similarity between inputs and outputs. However, Single Image GANs have now emerged as a superior solution for manipulation and generative tasks, although they require lengthy training times and suffer from issues such as mode collapse and artifacts. In this study, we propose a novel method that performs all these tasks without any training and within seconds. Our approach involves casting the old patch-based techniques into an optimization-free framework that starts with a coarse guess before refining details using patch-nearest-neighbor search. Our method generates better random images faster than GANs and has a wide range of applications, including image editing and reshuffling, retargeting to different sizes, and image collage. Furthermore, our approach produces superior results with less artifacts and more realistic global structures than previous approaches.",1
"Self-supervised monocular depth estimation has become an appealing solution to the lack of ground truth labels, but its reconstruction loss often produces over-smoothed results across object boundaries and is incapable of handling occlusion explicitly. In this paper, we propose a new approach to leverage pseudo ground truth depth maps of stereo images generated from self-supervised stereo matching methods. The confidence map of the pseudo ground truth depth map is estimated to mitigate performance degeneration by inaccurate pseudo depth maps. To cope with the prediction error of the confidence map itself, we also leverage the threshold network that learns the threshold dynamically conditioned on the pseudo depth maps. The pseudo depth labels filtered out by the thresholded confidence map are used to supervise the monocular depth network. Furthermore, we propose the probabilistic framework that refines the monocular depth map with the help of its uncertainty map through the pixel-adaptive convolution (PAC) layer. Experimental results demonstrate superior performance to state-of-the-art monocular depth estimation methods. Lastly, we exhibit that the proposed threshold learning can also be used to improve the performance of existing confidence estimation approaches.",0
"The lack of ground truth labels has made self-supervised monocular depth estimation a popular choice, but the reconstruction loss often leads to smoothed-out results and difficulty in handling occlusion. To address this, we suggest a new method that uses pseudo ground truth depth maps generated from self-supervised stereo matching methods. We estimate the confidence map of the pseudo ground truth depth map to combat inaccuracies, and use a threshold network to filter out erroneous pseudo depth labels. This filtered data supervises the monocular depth network, and we also introduce a probabilistic framework that refines the depth map with the help of its uncertainty map through the pixel-adaptive convolution layer. Our experimental results show that this approach outperforms existing methods, and we demonstrate that the threshold learning can improve the performance of other confidence estimation approaches.",1
"In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder (http://artbreeder.com), where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5% of the time. Our source code and dataset are publicly available at https://github.com/catlab-team/graph2pix.",0
"This paper presents a framework for generating images through graph-based image-to-image translation. The framework uses data from Artbreeder, a popular platform for creating artworks through interpolation of multiple GAN-generated images. This approach produces a tree-like structure that traces the historical data of image creation. Based on this structure, the paper proposes a new graph-to-image translation model, Graph2Pix, which takes a graph and corresponding images as input and generates a single output image. The experiments demonstrate that Graph2Pix outperforms several image-to-image translation frameworks on benchmark metrics such as LPIPS, with a 25% improvement, and human perception studies with 81.5% preference rate. The source code and dataset are publicly available at https://github.com/catlab-team/graph2pix.",1
"Deep Neural Networks have been very successfully used for many computer vision and pattern recognition applications. While Convolutional Neural Networks(CNNs) have shown the path to state of art image classifications, Generative Adversarial Networks or GANs have provided state of art capabilities in image generation. In this paper we extend the applications of CNNs and GANs to experiment with up-sampling techniques in the domains of security and surveillance. Through this work we evaluate, compare and contrast the state of art techniques in both CNN and GAN based image and video up-sampling in the surveillance domain. As a result of this study we also provide experimental evidence to establish DISTS as a stronger Image Quality Assessment(IQA) metric for comparing GAN Based Image Up-sampling in the surveillance domain.",0
"Many computer vision and pattern recognition applications have seen great success through the use of Deep Neural Networks. Specifically, Convolutional Neural Networks (CNNs) have established a new standard for image classification, while Generative Adversarial Networks (GANs) have pushed image generation to new heights. This study aims to expand the applications of CNNs and GANs by experimenting with up-sampling techniques in the fields of security and surveillance. By comparing and contrasting the latest techniques in both CNN and GAN-based image and video up-sampling, we provide experimental evidence that supports DISTS as a stronger metric for Image Quality Assessment (IQA) when evaluating GAN-based image up-sampling in the surveillance domain.",1
"Conditional Generative Adversarial Networks (cGANs) extend the standard unconditional GAN framework to learning joint data-label distributions from samples, and have been established as powerful generative models capable of generating high-fidelity imagery. A challenge of training such a model lies in properly infusing class information into its generator and discriminator. For the discriminator, class conditioning can be achieved by either (1) directly incorporating labels as input or (2) involving labels in an auxiliary classification loss. In this paper, we show that the former directly aligns the class-conditioned fake-and-real data distributions $P(\text{image}|\text{class})$ ({\em data matching}), while the latter aligns data-conditioned class distributions $P(\text{class}|\text{image})$ ({\em label matching}). Although class separability does not directly translate to sample quality and becomes a burden if classification itself is intrinsically difficult, the discriminator cannot provide useful guidance for the generator if features of distinct classes are mapped to the same point and thus become inseparable. Motivated by this intuition, we propose a Dual Projection GAN (P2GAN) model that learns to balance between {\em data matching} and {\em label matching}. We then propose an improved cGAN model with Auxiliary Classification that directly aligns the fake and real conditionals $P(\text{class}|\text{image})$ by minimizing their $f$-divergence. Experiments on a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world datasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of our proposed models.",0
"Conditional Generative Adversarial Networks (cGANs) are an extension of the standard GAN framework, which learns joint data-label distributions from samples. They are powerful generative models that can produce high-quality images. The main challenge in training cGANs is incorporating class information into the generator and discriminator. There are two methods for achieving class conditioning in the discriminator: (1) by directly incorporating labels as input or (2) by involving labels in an auxiliary classification loss. The former aligns the class-conditioned fake-and-real data distributions (data matching), while the latter aligns data-conditioned class distributions (label matching). Our proposed Dual Projection GAN (P2GAN) model balances both data matching and label matching, as mapping features of distinct classes to the same point makes them inseparable, hindering the discriminator's ability to guide the generator. We also propose an improved cGAN model with Auxiliary Classification that directly aligns the fake and real conditionals by minimizing their $f$-divergence. Experiments on synthetic and real-world datasets demonstrate the effectiveness of our proposed models.",1
"Multi-modal generation has been widely explored in recent years. Current research directions involve generating text based on an image or vice versa. In this paper, we propose a new task called CIGLI: Conditional Image Generation from Language and Image. Instead of generating an image based on text as in text-image generation, this task requires the generation of an image from a textual description and an image prompt. We designed a new dataset to ensure that the text description describes information from both images, and that solely analyzing the description is insufficient to generate an image. We then propose a novel language-image fusion model which improves the performance over two established baseline methods, as evaluated by quantitative (automatic) and qualitative (human) evaluations. The code and dataset is available at https://github.com/vincentlux/CIGLI.",0
"In recent years, there has been extensive exploration of multi-modal generation. The current trend in research is to generate text or images based on each other. The purpose of this paper is to introduce a new task called CIGLI, which stands for Conditional Image Generation from Language and Image. Unlike text-image generation, this task involves generating an image from a textual description and an image prompt. To ensure that the text description is comprehensive and can generate an image, we developed a new dataset. Our proposed language-image fusion model enhances the performance of two established baseline methods, as assessed by quantitative (automatic) and qualitative (human) evaluations. The code and dataset can be found at https://github.com/vincentlux/CIGLI.",1
"Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",0
"Recently, autoregressive models and their sequential factorization of data likelihood have shown promise in image representation and synthesis. However, their unidirectional, sequential attention to previously synthesized image patches above or to the left results in a linear 1D order of image context. This approach ignores large portions of the scene until synthesis is almost complete and processes the entire image on a single scale, lacking global contextual information. To address this, we combine the autoregressive formulation with a multinomial diffusion process, creating a coarse-to-fine hierarchy of context. Our approach progressively incorporates context from previous stages in a coarse-to-fine manner, resulting in greatly improved image modification capabilities and high-fidelity image generation. We can incorporate unrestricted, user-provided masks to perform local image editing, solving free-form image inpainting and local, text-guided image modification without requiring mask-specific training. Efficient training in a compressed latent space enables these capabilities.",1
"This paper considers the problem of generating an HDR image of a scene from its LDR images. Recent studies employ deep learning and solve the problem in an end-to-end fashion, leading to significant performance improvements. However, it is still hard to generate a good quality image from LDR images of a dynamic scene captured by a hand-held camera, e.g., occlusion due to the large motion of foreground objects, causing ghosting artifacts. The key to success relies on how well we can fuse the input images in their feature space, where we wish to remove the factors leading to low-quality image generation while performing the fundamental computations for HDR image generation, e.g., selecting the best-exposed image/region. We propose a novel method that can better fuse the features based on two ideas. One is multi-step feature fusion; our network gradually fuses the features in a stack of blocks having the same structure. The other is the design of the component block that effectively performs two operations essential to the problem, i.e., comparing and selecting appropriate images/regions. Experimental results show that the proposed method outperforms the previous state-of-the-art methods on the standard benchmark tests.",0
"The subject of this essay is the creation of an HDR image from LDR images of a scene. While deep learning has been used to make significant improvements in generating high-quality images, there are still challenges when working with dynamic scenes captured by a handheld camera. These challenges include occlusion and ghosting artifacts caused by the motion of foreground objects. The key to success is to fuse the input images in their feature space, which involves removing factors that lead to low-quality image generation while performing the fundamental computations for HDR image generation. We propose a new method that improves feature fusion by using the multi-step feature fusion approach and designing a component block that can effectively compare and select appropriate images/regions. Our experimental results demonstrate that our approach outperforms previous state-of-the-art methods on standard benchmark tests.",1
"Image generation has been heavily investigated in computer vision, where one core research challenge is to generate images from arbitrarily complex distributions with little supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. However, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to compute likelihoods, etc. In this paper, we propose a new unsupervised non-parametric method named mixture of infinite conditional GANs or MIC-GANs, to tackle several GAN issues together, aiming for image generation with parsimonious prior knowledge. Through comprehensive evaluations across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MICGANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available: github.com/yinghdb/MICGANs.",0
"The generation of images has been a major focus in the field of computer vision, with the primary challenge being the ability to create images from complex distributions with minimal supervision. Generative Adversarial Networks (GANs) have been widely used due to their success in achieving this goal. However, GANs are also known to have certain drawbacks, including issues with mode collapse, non-structured latent space, and the inability to compute likelihoods. To address these challenges, our paper introduces a new non-parametric, unsupervised method called mixture of infinite conditional GANs (MIC-GANs), which aims to generate images with minimal prior knowledge. Our evaluations demonstrate that MIC-GANs are effective in organizing the latent space, avoiding mode collapse, and outperforming other state-of-the-art methods. Additionally, MIC-GANs are adaptable, versatile, and resilient, making them a promising solution to common GAN issues. Our code is accessible at github.com/yinghdb/MICGANs.",1
"Unsupervised disentanglement learning is a crucial issue for understanding and exploiting deep generative models. Recently, SeFa tries to find latent disentangled directions by performing SVD on the first projection of a pre-trained GAN. However, it is only applied to the first layer and works in a post-processing way. Hessian Penalty minimizes the off-diagonal entries of the output's Hessian matrix to facilitate disentanglement, and can be applied to multi-layers.However, it constrains each entry of output independently, making it not sufficient in disentangling the latent directions (e.g., shape, size, rotation, etc.) of spatially correlated variations. In this paper, we propose a simple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative model to learn disentangled representations. It simply encourages the variation of output caused by perturbations on different latent dimensions to be orthogonal, and the Jacobian with respect to the input is calculated to represent this variation. We show that our OroJaR also encourages the output's Hessian matrix to be diagonal in an indirect manner. In contrast to the Hessian Penalty, our OroJaR constrains the output in a holistic way, making it very effective in disentangling latent dimensions corresponding to spatially correlated variations. Quantitative and qualitative experimental results show that our method is effective in disentangled and controllable image generation, and performs favorably against the state-of-the-art methods. Our code is available at https://github.com/csyxwei/OroJaR",0
"The significance of unsupervised disentanglement learning in comprehending and utilizing deep generative models is crucial. SeFa has recently attempted to identify latent disentangled directions by conducting SVD on the first projection of a pre-trained GAN. However, this approach is only implemented on the initial layer and functions in a post-processing manner. On the other hand, Hessian Penalty is utilized to minimize the off-diagonal entries of the output's Hessian matrix to facilitate disentanglement, and can be applied to multi-layers. Nevertheless, this method independently constrains each output entry, rendering it inadequate in disentangling latent directions, such as shape, size, and rotation, that are linked to spatially correlated variations. In this study, we introduce a straightforward approach called Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative models to learn disentangled representations. Our method merely encourages the output's variation caused by perturbations on distinct latent dimensions to be orthogonal, and the Jacobian with respect to the input is computed to denote this variation. We demonstrate that our OroJaR indirectly encourages the output's Hessian matrix to be diagonal. Unlike the Hessian Penalty, our OroJaR comprehensively restricts the output, making it highly effective in disentangling latent dimensions related to spatially correlated variations. Our quantitative and qualitative experimental outcomes reveal that our approach is effective in generating disentangled and controllable images and outperforms the state-of-the-art methods. Interested individuals can access our code at https://github.com/csyxwei/OroJaR.",1
"Remarkable results have been achieved by DCNN based self-supervised depth estimation approaches. However, most of these approaches can only handle either day-time or night-time images, while their performance degrades for all-day images due to large domain shift and the variation of illumination between day and night images. To relieve these limitations, we propose a domain-separated network for self-supervised depth estimation of all-day images. Specifically, to relieve the negative influence of disturbing terms (illumination, etc.), we partition the information of day and night image pairs into two complementary sub-spaces: private and invariant domains, where the former contains the unique information (illumination, etc.) of day and night images and the latter contains essential shared information (texture, etc.). Meanwhile, to guarantee that the day and night images contain the same information, the domain-separated network takes the day-time images and corresponding night-time images (generated by GAN) as input, and the private and invariant feature extractors are learned by orthogonality and similarity loss, where the domain gap can be alleviated, thus better depth maps can be expected. Meanwhile, the reconstruction and photometric losses are utilized to estimate complementary information and depth maps effectively. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the challenging Oxford RobotCar dataset, proving the superiority of our proposed approach.",0
"DCNN based self-supervised depth estimation methods have yielded impressive outcomes. However, these methods are limited to processing either daytime or nighttime images, with their performance deteriorating for all-day images due to significant domain shifts and variation in illumination. To address these limitations, we introduce a domain-separated network for self-supervised depth estimation of all-day images. We partition the information of day and night image pairs into two distinct sub-spaces - private and invariant domains - to mitigate the negative impact of illumination and other interfering factors. The private domain contains unique information about day and night images, while the invariant domain encompasses essential shared information such as texture. To ensure that day and night images contain the same information, the domain-separated network takes day-time and corresponding night-time images (generated by GAN) as input. We learn the private and invariant feature extractors using orthogonality and similarity loss to alleviate the domain gap, resulting in better depth maps. Additionally, we utilize reconstruction and photometric losses to estimate complementary information and depth maps effectively. Our approach yields state-of-the-art depth estimation results for all-day images on the challenging Oxford RobotCar dataset, demonstrating its superiority.",1
"Despite recent advancements in single-domain or single-object image generation, it is still challenging to generate complex scenes containing diverse, multiple objects and their interactions. Scene graphs, composed of nodes as objects and directed-edges as relationships among objects, offer an alternative representation of a scene that is more semantically grounded than images. We hypothesize that a generative model for scene graphs might be able to learn the underlying semantic structure of real-world scenes more effectively than images, and hence, generate realistic novel scenes in the form of scene graphs. In this work, we explore a new task for the unconditional generation of semantic scene graphs. We develop a deep auto-regressive model called SceneGraphGen which can directly learn the probability distribution over labelled and directed graphs using a hierarchical recurrent architecture. The model takes a seed object as input and generates a scene graph in a sequence of steps, each step generating an object node, followed by a sequence of relationship edges connecting to the previous nodes. We show that the scene graphs generated by SceneGraphGen are diverse and follow the semantic patterns of real-world scenes. Additionally, we demonstrate the application of the generated graphs in image synthesis, anomaly detection and scene graph completion.",0
"Although there have been advancements in creating single-object images, it remains difficult to generate complex scenes with multiple objects and their interactions. Scene graphs offer a more semantically grounded alternative representation of a scene, composed of nodes as objects and directed-edges as relationships among objects. It is hypothesized that a generative model for scene graphs could learn the underlying semantic structure of real-world scenes more effectively than images and generate realistic novel scenes in the form of scene graphs. This work explores a new task for the unconditional generation of semantic scene graphs using a deep auto-regressive model called SceneGraphGen. The model directly learns the probability distribution over labelled and directed graphs using a hierarchical recurrent architecture. It generates a scene graph in a sequence of steps, each step generating an object node, followed by a sequence of relationship edges connecting to the previous nodes. The scene graphs generated by SceneGraphGen are diverse and follow the semantic patterns of real-world scenes. Additionally, the generated graphs are applied in image synthesis, anomaly detection, and scene graph completion.",1
"Great progress has been made by the advances in Generative Adversarial Networks (GANs) for image generation. However, there lacks enough understanding on how a realistic image can be generated by the deep representations of GANs from a random vector. This chapter will give a summary of recent works on interpreting deep generative models. We will see how the human-understandable concepts that emerge in the learned representation can be identified and used for interactive image generation and editing.",0
"Significant advancements have been achieved in image generation through the development of Generative Adversarial Networks (GANs). Despite this, there remains a lack of comprehension regarding how GANs' deep representations can generate lifelike images from a random vector. The upcoming chapter will provide an overview of current research on interpreting deep generative models. It will demonstrate how understandable human concepts can be identified within the learned representation and employed for interactive image editing and generation.",1
"The interest of the machine learning community in image synthesis has grown significantly in recent years, with the introduction of a wide range of deep generative models and means for training them. Such machines' ultimate goal is to match the distributions of the given training images and the synthesized ones. In this work, we propose a general model-agnostic technique for improving the image quality and the distribution fidelity of generated images, obtained by any generative model. Our method, termed BIGRoC (boosting image generation via a robust classifier), is based on a post-processing procedure via the guidance of a given robust classifier and without a need for additional training of the generative model. Given a synthesized image, we propose to update it through projected gradient steps over the robust classifier, in an attempt to refine its recognition. We demonstrate this post-processing algorithm on various image synthesis methods and show a significant improvement of the generated images, both quantitatively and qualitatively.",0
"Recently, there has been a surge of interest in image synthesis within the machine learning community due to the emergence of deep generative models and various methods for training them. These machines aim to match the distribution of training images with those that are synthesized. In this study, we introduce a universal technique called BIGRoC (boosting image generation via a robust classifier) that can enhance the quality and distribution accuracy of images generated by any generative model. Our approach involves using a robust classifier as a guide for post-processing synthesized images without requiring additional training for the generative model. By implementing projected gradient steps over the robust classifier, we refine the recognition of the synthesized image. We demonstrate the effectiveness of our post-processing algorithm on different image synthesis techniques and observe a significant enhancement in both quantitative and qualitative aspects of the generated images.",1
"Recently, Generative Adversarial Networks (GANs)} have been widely used for portrait image generation. However, in the latent space learned by GANs, different attributes, such as pose, shape, and texture style, are generally entangled, making the explicit control of specific attributes difficult. To address this issue, we propose a SofGAN image generator to decouple the latent space of portraits into two subspaces: a geometry space and a texture space. The latent codes sampled from the two subspaces are fed to two network branches separately, one to generate the 3D geometry of portraits with canonical pose, and the other to generate textures. The aligned 3D geometries also come with semantic part segmentation, encoded as a semantic occupancy field (SOF). The SOF allows the rendering of consistent 2D semantic segmentation maps at arbitrary views, which are then fused with the generated texture maps and stylized to a portrait photo using our semantic instance-wise (SIW) module. Through extensive experiments, we show that our system can generate high quality portrait images with independently controllable geometry and texture attributes. The method also generalizes well in various applications such as appearance-consistent facial animation and dynamic styling.",0
"Generative Adversarial Networks (GANs) have become a popular tool for generating portrait images. However, GANs tend to entangle different attributes such as pose, shape, and texture style in the latent space, making it challenging to control specific attributes. To overcome this problem, we propose a new image generator called SofGAN that separates the latent space of portraits into two subspaces: a geometry space and a texture space. We feed the latent codes from these subspaces separately to two network branches, one to create 3D geometry of portraits with a canonical pose and the other to generate textures. The 3D geometries are segmented into semantic parts and encoded as a semantic occupancy field (SOF). This allows us to render consistent 2D semantic segmentation maps at any angle, which we then fuse with the generated texture maps and stylize to create a portrait photo using our semantic instance-wise (SIW) module. Our experiments show that our system can produce high-quality portrait images with independently controllable geometry and texture attributes. Our method also performs well in a wide range of applications, including appearance-consistent facial animation and dynamic styling.",1
"Electronic Health Records often suffer from missing data, which poses a major problem in clinical practice and clinical studies. A novel approach for dealing with missing data are Generative Adversarial Nets (GANs), which have been generating huge research interest in image generation and transformation. Recently, researchers have attempted to apply GANs to missing data generation and imputation for EHR data: a major challenge here is the categorical nature of the data. State-of-the-art solutions to the GAN-based generation of categorical data involve either reinforcement learning, or learning a bidirectional mapping between the categorical and the real latent feature space, so that the GANs only need to generate real-valued features. However, these methods are designed to generate complete feature vectors instead of imputing only the subsets of missing features. In this paper we propose a simple and yet effective approach that is based on previous work on GANs for data imputation. We first motivate our solution by discussing the reason why adversarial training often fails in case of categorical features. Then we derive a novel way to re-code the categorical features to stabilize the adversarial training. Based on experiments on two real-world EHR data with multiple settings, we show that our imputation approach largely improves the prediction accuracy, compared to more traditional data imputation approaches.",0
"Clinical practice and studies are often hindered by missing data in Electronic Health Records. One potential solution for this issue is the use of Generative Adversarial Nets (GANs), which have gained significant attention in image manipulation research. However, applying GANs to EHR data imputation poses a unique challenge due to the categorical nature of the data. Current solutions involve either reinforcement learning or a bidirectional mapping between the categorical and real latent feature space. However, these methods generate complete feature vectors rather than imputing only subsets of missing features. Our proposed approach builds upon previous GAN-based data imputation work and addresses the issue of adversarial training instability for categorical data. We demonstrate the effectiveness of our approach through experiments on two real-world EHR datasets, showing significant improvements in prediction accuracy compared to traditional data imputation methods.",1
"Automated inspection and detection of foreign objects on railways is important for rail transportation safety as it helps prevent potential accidents and trains derailment. Most existing vision-based approaches focus on the detection of frontal intrusion objects with prior labels, such as categories and locations of the objects. In reality, foreign objects with unknown categories can appear anytime on railway tracks. In this paper, we develop a semi-supervised convolutional autoencoder based framework that only requires railway track images without prior knowledge on the foreign objects in the training process. It consists of three different modules, a bottleneck feature generator as encoder, a photographic image generator as decoder, and a reconstruction discriminator developed via adversarial learning. In the proposed framework, the problem of detecting the presence, location, and shape of foreign objects is addressed by comparing the input and reconstructed images as well as setting thresholds based on reconstruction errors. The proposed method is evaluated through comprehensive studies under different performance criteria. The results show that the proposed method outperforms some well-known benchmarking methods. The proposed framework is useful for data analytics via the train Internet-of-Things (IoT) systems",0
"Automated inspection and detection of foreign objects on railways is crucial for ensuring the safety of rail transportation by preventing potential accidents and train derailments. Although many existing vision-based approaches focus on detecting frontal intrusion objects with predefined labels, foreign objects with unknown categories can appear unexpectedly on railway tracks. To address this issue, we propose a semi-supervised convolutional autoencoder-based framework that can detect the presence, location, and shape of foreign objects using only railway track images without prior knowledge of the objects. The framework consists of three modules, namely a bottleneck feature generator as encoder, a photographic image generator as decoder, and a reconstruction discriminator developed via adversarial learning. By comparing the input and reconstructed images and setting thresholds based on reconstruction errors, the framework can effectively detect foreign objects. Comprehensive studies under different performance criteria demonstrate that our proposed method outperforms several benchmarking methods. Moreover, the proposed framework can be utilized for data analytics via train Internet-of-Things (IoT) systems.",1
"Image-to-image translation has been revolutionized with GAN-based methods. However, existing methods lack the ability to preserve the identity of the source domain. As a result, synthesized images can often over-adapt to the reference domain, losing important structural characteristics and suffering from suboptimal visual quality. To solve these challenges, we propose a novel frequency domain image translation (FDIT) framework, exploiting frequency information for enhancing the image generation process. Our key idea is to decompose the image into low-frequency and high-frequency components, where the high-frequency feature captures object structure akin to the identity. Our training objective facilitates the preservation of frequency information in both pixel space and Fourier spectral space. We broadly evaluate FDIT across five large-scale datasets and multiple tasks including image translation and GAN inversion. Extensive experiments and ablations show that FDIT effectively preserves the identity of the source image, and produces photo-realistic images. FDIT establishes state-of-the-art performance, reducing the average FID score by 5.6% compared to the previous best method.",0
"GAN-based techniques have transformed image-to-image translation, but their current limitations include the inability to maintain the identity of the source domain. Consequently, the generated images can excessively adapt to the reference domain, resulting in the loss of crucial structural attributes and low visual quality. In addressing these challenges, we present a new approach called frequency domain image translation (FDIT) that employs frequency data to enhance image generation. Our method breaks down images into low and high-frequency components, with the latter capturing object structure similar to identity. Our training objective aims to preserve frequency information in both pixel and Fourier spectral space. We comprehensively evaluate FDIT on five large-scale datasets and various tasks, including GAN inversion and image translation. Our experiments and analyses demonstrate that FDIT effectively preserves the source image's identity, producing realistic images. FDIT outperforms previous methods, reducing the average FID score by 5.6%.",1
"We propose a novel universal detector for detecting images generated by using CNNs. In this paper, properties of checkerboard artifacts in CNN-generated images are considered, and the spectrum of images is enhanced in accordance with the properties. Next, a classifier is trained by using the enhanced spectrums to judge a query image to be a CNN-generated ones or not. In addition, an ensemble of the proposed detector with emphasized spectrums and a conventional detector is proposed to improve the performance of these methods. In an experiment, the proposed ensemble is demonstrated to outperform a state-of-the-art method under some conditions.",0
"Our proposal introduces a unique universal detector capable of detecting CNN-generated images. This paper examines the characteristics of checkerboard artifacts in CNN-generated images and enhances the image spectrum accordingly. The enhanced spectrums are then used to train a classifier that can determine if a query image is CNN-generated or not. Furthermore, we suggest an ensemble of the proposed detector and a traditional detector to enhance the performance of these methods. Our experiment shows that the proposed ensemble surpasses a state-of-the-art method under certain conditions.",1
"Furnishing and rendering an indoor scene is a common but tedious task for interior design: an artist needs to observe the space, create a conceptual design, build a 3D model, and perform rendering. In this paper, we introduce a new problem of domain-specific image synthesis using generative modeling, namely neural scene decoration. Given a photograph of an empty indoor space, we aim to synthesize a new image of the same space that is fully furnished and decorated. Neural scene decoration can be applied in practice to efficiently generate conceptual but realistic interior designs, bypassing the traditional multi-step and time-consuming pipeline. Our attempt to neural scene decoration in this paper is a generative adversarial neural network that takes the input photograph and directly produce the image of the desired furnishing and decorations. Our network contains a novel image generator that transforms an initial point-based object layout into a realistic photograph. We demonstrate the performance of our proposed method by showing that it outperforms the baselines built upon previous works on image translations both qualitatively and quantitatively. Our user study further validates the plausibility and aesthetics in the generated designs.",0
"Decorating and furnishing indoor spaces is a common but tedious task for interior designers. They must observe the space, create a conceptual design, build a 3D model, and perform rendering. In this paper, we introduce a new problem called neural scene decoration, which involves domain-specific image synthesis using generative modeling. Our goal is to generate a fully furnished and decorated image of an empty indoor space, eliminating the need for traditional, time-consuming pipelines. We have developed a generative adversarial neural network that takes an input photograph and produces the desired image. Our network includes a novel image generator that transforms an initial point-based object layout into a realistic photograph. We demonstrate the effectiveness of our approach by outperforming previous works on image translations both qualitatively and quantitatively. Additionally, our user study confirms that the designs generated by our network are plausible and aesthetically pleasing.",1
"Recent image generation models show remarkable generation performance. However, they mirror strong location preference in datasets, which we call spatial bias. Therefore, generators render poor samples at unseen locations and scales. We argue that the generators rely on their implicit positional encoding to render spatial content. From our observations, the generator's implicit positional encoding is translation-variant, making the generator spatially biased. To address this issue, we propose injecting explicit positional encoding at each scale of the generator. By learning the spatially unbiased generator, we facilitate the robust use of generators in multiple tasks, such as GAN inversion, multi-scale generation, generation of arbitrary sizes and aspect ratios. Furthermore, we show that our method can also be applied to denoising diffusion probabilistic models.",0
"Remarkable performance has been demonstrated by recent image generation models. However, they display a spatial bias, showing a preference for strong locations in datasets. As a result, generators are unable to produce good samples at unseen scales and locations. Our argument is that the generators are reliant on their implicit positional encoding to render spatial content, which is translation-variant, leading to spatial bias. To overcome this issue, we suggest the injection of explicit positional encoding at each generator scale to learn a spatially unbiased generator. This will enable the robust use of generators in various tasks, including GAN inversion, multi-scale generation, and generation of arbitrary sizes and aspect ratios. Additionally, we demonstrate that our approach can be applied to denoising diffusion probabilistic models.",1
"Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",0
"Is it possible to train a generative model to create images within a specific domain solely based on a text prompt, without any visual input? Essentially, can an image generator be trained blindly? Through the use of large-scale Contrastive-Language-Image-Pre-training (CLIP) models, we introduce a text-based technique that enables a generative model to adapt to new domains without requiring any images from those domains. Our method leverages natural language prompts and only a few minutes of training to modify a generator across a wide range of domains with varying styles and shapes, some of which would be challenging or unfeasible using existing methods. We conduct a comprehensive series of experiments and comparisons across numerous domains, demonstrating the efficacy of our approach and highlighting that the shifted models retain the latent-space properties that make generative models desirable for downstream tasks.",1
"Machine learning tools are becoming increasingly powerful and widely used. Unfortunately membership attacks, which seek to uncover information from data sets used in machine learning, have the potential to limit data sharing. In this paper we consider an approach to increase the privacy protection of data sets, as applied to face recognition. Using an auxiliary face recognition model, we build on the StyleGAN generative adversarial network and feed it with latent codes combining two distinct sub-codes, one encoding visual identity factors, and, the other, non-identity factors. By independently varying these vectors during image generation, we create a synthetic data set of fictitious face identities. We use this data set to train a face recognition model. The model performance degrades in comparison to the state-of-the-art of face verification. When tested with a simple membership attack our model provides good privacy protection, however the model performance degrades in comparison to the state-of-the-art of face verification. We find that the addition of a small amount of private data greatly improves the performance of our model, which highlights the limitations of using synthetic data to train machine learning models.",0
"The use of machine learning tools is on the rise, as they become more powerful and widely adopted. However, this has led to an increase in membership attacks, which aim to uncover data from machine learning data sets, posing a threat to data sharing. This paper proposes a solution to enhance the privacy protection of face recognition data sets. By leveraging an auxiliary face recognition model, we utilize the StyleGAN generative adversarial network to create synthetic data sets of fictitious face identities. This is done by combining two distinct sub-codes, one encoding visual identity features, and the other encoding non-identity features. We then train a face recognition model using this synthetic data set, resulting in a slight decline in performance compared to the state-of-the-art of face verification. However, our model provides good privacy protection when tested with a simple membership attack. We also discovered that adding a small amount of private data can significantly improve the performance of our model, highlighting the limitations of using synthetic data to train machine learning models.",1
"Generating photo-realistic images from a text description is a challenging problem in computer vision. Previous works have shown promising performance to generate synthetic images conditional on text by Generative Adversarial Networks (GANs). In this paper, we focus on the category-consistent and relativistic diverse constraints to optimize the diversity of synthetic images. Based on those constraints, a category-consistent and relativistic diverse conditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images simultaneously. We use the attention loss and diversity loss to improve the sensitivity of the GAN to word attention and noises. Then, we employ the relativistic conditional loss to estimate the probability of relatively real or fake for synthetic images, which can improve the performance of basic conditional loss. Finally, we introduce a category-consistent loss to alleviate the over-category issues between K synthetic images. We evaluate our approach using the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the extensive experiments demonstrate superiority of the proposed method in comparison with state-of-the-art methods in terms of photorealistic and diversity of the generated synthetic images.",0
"Computer vision faces a difficult challenge in generating photo-realistic images from a text description. However, prior research has demonstrated promising results in producing synthetic images based on text through Generative Adversarial Networks (GANs). This study centers on enhancing the diversity of synthetic images through the implementation of category-consistent and relativistic diverse constraints. To achieve this, a category-consistent and relativistic diverse conditional GAN (CRD-CGAN) is proposed to simultaneously synthesize $K$ photo-realistic images. Attention loss and diversity loss are employed to increase the GAN's sensitivity to word attention and noise. Additionally, relativistic conditional loss is utilized to estimate the probability of relatively real or fake synthetic images and improve the basic conditional loss's performance. Finally, the introduction of a category-consistent loss helps alleviate the over-category issues between the K synthetic images. The effectiveness of this approach is evaluated using the Birds-200-2011, Oxford-102 flower, and MSCOCO 2014 datasets. The results of extensive experiments demonstrate the superiority of the proposed method in generating photorealistic and diverse synthetic images compared to state-of-the-art methods.",1
"We conduct a subjective experiment to compare the performance of traditional image coding methods and learning-based image coding methods. HEVC and VVC, the state-of-the-art traditional coding methods, are used as the representative traditional methods. The learning-based methods used contain not only CNN-based methods, but also a GAN-based method, all of which are advanced or typical. Single Stimuli (SS), which is also called Absolute Category Rating (ACR), is adopted as the methodology of the experiment to obtain perceptual quality of images. Additionally, we utilize some typical and frequently used objective quality metrics to evaluate the coding methods in the experiment as comparison. The experiment shows that CNN-based and GAN-based methods can perform better than traditional methods in low bit-rates. In high bit-rates, however, it is hard to verify whether CNN-based methods are superior to traditional methods. Because the GAN method does not provide models with high target bit-rates, we cannot exactly tell the performance of the GAN method in high bit-rates. Furthermore, some popular objective quality metrics have not shown the ability well to measure quality of images generated by learning-based coding methods, especially the GAN-based one.",0
"To compare the performance of traditional and learning-based image coding methods, we conducted a subjective experiment. The traditional methods used were HEVC and VVC, while the learning-based methods included CNN-based and GAN-based approaches. We used the Single Stimuli (SS) methodology, also known as Absolute Category Rating (ACR), to assess the perceptual quality of the images. Additionally, we utilized commonly used objective quality metrics for comparison. The results revealed that in low bit-rates, CNN-based and GAN-based methods outperformed traditional methods. However, in high bit-rates, it was challenging to determine if CNN-based methods were better. The GAN-based method did not provide models for high target bit-rates, making it difficult to evaluate its performance. Furthermore, common objective quality metrics were not effective in measuring the quality of images generated by learning-based coding methods, particularly the GAN-based method.",1
"In this paper, we demonstrated a practical application of realistic river image generation using deep learning. Specifically, we explored a generative adversarial network (GAN) model capable of generating high-resolution and realistic river images that can be used to support modeling and analysis in surface water estimation, river meandering, wetland loss, and other hydrological research studies. First, we have created an extensive repository of overhead river images to be used in training. Second, we incorporated the Progressive Growing GAN (PGGAN), a network architecture that iteratively trains smaller-resolution GANs to gradually build up to a very high resolution to generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler GAN architectures, difficulties arose in terms of exponential increase of training time and vanishing/exploding gradient issues, which the PGGAN implementation seemed to significantly reduce. The results presented in this study show great promise in generating high-quality images and capturing the details of river structure and flow to support hydrological research, which often requires extensive imagery for model performance.",0
"This paper showcases the practical application of deep learning in generating realistic river images. Our focus was on a generative adversarial network (GAN) model that can create high-quality river images for various hydrological research studies, such as surface water estimation, river meandering, and wetland loss. We began by collecting a vast collection of aerial river images to use for training. We then explored the Progressive Growing GAN (PGGAN) architecture, which trains smaller-resolution GANs iteratively to create high-resolution (1024x1024) synthetic river images. The use of simpler GAN models resulted in exponential increases in training time and gradient issues, which the PGGAN implementation helped to mitigate. Our study demonstrates the potential of generating high-quality river images with intricate details to facilitate hydrological research that requires extensive imagery for model performance.",1
"Latest Generative Adversarial Networks (GANs) are gathering outstanding results through a large-scale training, thus employing models composed of millions of parameters requiring extensive computational capabilities. Building such huge models undermines their replicability and increases the training instability. Moreover, multi-channel data, such as images or audio, are usually processed by realvalued convolutional networks that flatten and concatenate the input, often losing intra-channel spatial relations. To address these issues related to complexity and information loss, we propose a family of quaternion-valued generative adversarial networks (QGANs). QGANs exploit the properties of quaternion algebra, e.g., the Hamilton product, that allows to process channels as a single entity and capture internal latent relations, while reducing by a factor of 4 the overall number of parameters. We show how to design QGANs and to extend the proposed approach even to advanced models.We compare the proposed QGANs with real-valued counterparts on several image generation benchmarks. Results show that QGANs are able to obtain better FID scores than real-valued GANs and to generate visually pleasing images. Furthermore, QGANs save up to 75% of the training parameters. We believe these results may pave the way to novel, more accessible, GANs capable of improving performance and saving computational resources.",0
"Generative Adversarial Networks (GANs) have achieved remarkable results with large-scale training, using models with millions of parameters that require significant computational power. However, building such models can hinder their replicability and lead to training instability. Additionally, real-valued convolutional networks used to process multi-channel data like images or audio often lose intra-channel spatial relations. To address these issues, we propose quaternion-valued GANs (QGANs) that utilize the properties of quaternion algebra to process channels as a single entity and capture internal latent relations. This reduces the overall number of parameters by a factor of 4 while improving information retention. We show how to design QGANs and extend the approach to advanced models. Comparisons with real-valued GANs on image generation benchmarks reveal that QGANs generate visually pleasing images and obtain better FID scores while saving up to 75% of training parameters. These results suggest that QGANs may lead to novel and accessible GANs that improve performance and save computational resources.",1
"We propose a new method to detect deepfake images using the cue of the source feature inconsistency within the forged images. It is based on the hypothesis that images' distinct source features can be preserved and extracted after going through state-of-the-art deepfake generation processes. We introduce a novel representation learning approach, called pair-wise self-consistency learning (PCL), for training ConvNets to extract these source features and detect deepfake images. It is accompanied by a new image synthesis approach, called inconsistency image generator (I2G), to provide richly annotated training data for PCL. Experimental results on seven popular datasets show that our models improve averaged AUC over the state of the art from 96.45% to 98.05% in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset evaluation.",0
"Our proposed method utilizes source feature inconsistency within deepfake images to detect them. We believe that even after undergoing advanced deepfake generation processes, images still retain distinct source features that can be extracted. To achieve this, we introduce a unique representation learning approach called pair-wise self-consistency learning (PCL) that trains ConvNets to identify these source features and flag deepfake images. Additionally, we introduce an image synthesis method called inconsistency image generator (I2G) that generates annotated training data for PCL. Our models demonstrate superior performance on seven popular datasets, with an average AUC improvement from 96.45% to 98.05% in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset evaluation compared to the state of the art.",1
"In most interactive image generation tasks, given regions of interest (ROI) by users, the generated results are expected to have adequate diversities in appearance while maintaining correct and reasonable structures in original images. Such tasks become more challenging if only limited data is available. Recently proposed generative models complete training based on only one image. They pay much attention to the monolithic feature of the sample while ignoring the actual semantic information of different objects inside the sample. As a result, for ROI-based generation tasks, they may produce inappropriate samples with excessive randomicity and without maintaining the related objects' correct structures. To address this issue, this work introduces a MOrphologic-structure-aware Generative Adversarial Network named MOGAN that produces random samples with diverse appearances and reliable structures based on only one image. For training for ROI, we propose to utilize the data coming from the original image being augmented and bring in a novel module to transform such augmented data into knowledge containing both structures and appearances, thus enhancing the model's comprehension of the sample. To learn the rest areas other than ROI, we employ binary masks to ensure the generation isolated from ROI. Finally, we set parallel and hierarchical branches of the mentioned learning process. Compared with other single image GAN schemes, our approach focuses on internal features including the maintenance of rational structures and variation on appearance. Experiments confirm a better capacity of our model on ROI-based image generation tasks than its competitive peers.",0
"When users provide regions of interest (ROI) for interactive image generation tasks, the resulting images should display sufficient diversity in their appearance while maintaining the correct structures from the original images. These tasks become more difficult with limited data. Recently introduced generative models train on only one image and prioritize the sample's overall appearance over the specific semantic information of its objects. As a result, they may produce inappropriate samples for ROI-based generation tasks that lack reliable structure and have excessive randomness. To address this issue, we introduce MOGAN, a MOrphologic-structure-aware Generative Adversarial Network that produces random samples with diverse appearances and reliable structures based on only one image. For ROI training, we use data augmentation and a novel module to transform the augmented data into knowledge containing both structures and appearances, enhancing the model's understanding of the sample. To learn the areas outside of the ROI, we use binary masks to isolate the generation. Finally, we use parallel and hierarchical branches to facilitate the learning process. Compared to other single image GAN schemes, our approach prioritizes internal features such as maintaining rational structures and variation on appearance. Experiments show that our model performs better on ROI-based image generation tasks than its competitors.",1
"One of the important research topics in image generative models is to disentangle the spatial contents and styles for their separate control. Although StyleGAN can generate content feature vectors from random noises, the resulting spatial content control is primarily intended for minor spatial variations, and the disentanglement of global content and styles is by no means complete. Inspired by a mathematical understanding of normalization and attention, here we present a novel hierarchical adaptive Diagonal spatial ATtention (DAT) layers to separately manipulate the spatial contents from styles in a hierarchical manner. Using DAT and AdaIN, our method enables coarse-to-fine level disentanglement of spatial contents and styles. In addition, our generator can be easily integrated into the GAN inversion framework so that the content and style of translated images from multi-domain image translation tasks can be flexibly controlled. By using various datasets, we confirm that the proposed method not only outperforms the existing models in disentanglement scores, but also provides more flexible control over spatial features in the generated images.",0
"One of the key areas of study in image generative models is to separate the control of spatial contents and styles. Although StyleGAN can create content feature vectors from random noise, it only allows for minor spatial variations and doesn't fully disentangle global content and styles. Drawing inspiration from mathematical concepts of normalization and attention, we introduce a new technique called hierarchical adaptive Diagonal spatial ATtention (DAT) layers to manipulate spatial contents and styles separately in a hierarchical manner. By using DAT and AdaIN, our approach enables a coarse-to-fine level disentanglement of spatial contents and styles. Furthermore, our generator can easily integrate into the GAN inversion framework, allowing for flexible control of content and style in multi-domain image translation tasks. Our method not only outperforms existing models in disentanglement scores, but also provides more flexible control over spatial features in generated images, as confirmed by various datasets.",1
"Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. To address this challenge, we propose a novel representational learning-based STE model, referred to as RewriteNet that employs textual information as well as visual information. We assume that the scene text image can be decomposed into content and style features where the former represents the text information and style represents scene text characteristics such as font, alignment, and background. Under this assumption, we propose a method to separately encode content and style features of the input image by introducing the scene text recognizer that is trained by text information. Then, a text-edited image is generated by combining the style feature from the original image and the content feature from the target text. Unlike previous works that are only able to use synthetic images in the training phase, we also exploit real-world images by proposing a self-supervised training scheme, which bridges the domain gap between synthetic and real data. Our experiments demonstrate that RewriteNet achieves better quantitative and qualitative performance than other comparisons. Moreover, we validate that the use of text information and the self-supervised training scheme improves text switching performance. The implementation and dataset will be publicly available.",0
"Converting text in a scene image to desired text while preserving the original style, known as Scene Text Editing (STE), is a difficult task due to the complex interplay between text and style. To tackle this challenge, we present a new representational learning-based STE model called RewriteNet that utilizes both textual and visual information. We assume that scene text images can be broken down into content and style features, where the former represents the text and the latter embodies scene text characteristics like font, alignment, and background. To encode content and style features separately, we employ the scene text recognizer, which is trained on textual information. We then generate a text-edited image by combining the style feature of the original image with the content feature of the target text. Unlike previous approaches that only use synthetic images during training, we also leverage real-world images with a self-supervised training scheme, which bridges the gap between synthetic and real data. Our experiments show that RewriteNet outperforms other methods both quantitatively and qualitatively, and that the use of text information and the self-supervised training scheme improves text switching performance. The implementation and dataset will be publicly available.",1
"Generative adversarial networks (GANs) nowadays are capable of producing images of incredible realism. One concern raised is whether the state-of-the-art GAN's learned distribution still suffers from mode collapse, and what to do if so. Existing diversity tests of samples from GANs are usually conducted qualitatively on a small scale, and/or depends on the access to original training data as well as the trained model parameters. This paper explores to diagnose GAN intra-mode collapse and calibrate that, in a novel black-box setting: no access to training data, nor the trained model parameters, is assumed. The new setting is practically demanded, yet rarely explored and significantly more challenging. As a first stab, we devise a set of statistical tools based on sampling, that can visualize, quantify, and rectify intra-mode collapse. We demonstrate the effectiveness of our proposed diagnosis and calibration techniques, via extensive simulations and experiments, on unconditional GAN image generation (e.g., face and vehicle). Our study reveals that the intra-mode collapse is still a prevailing problem in state-of-the-art GANs and the mode collapse is diagnosable and calibratable in black-box settings. Our codes are available at: https://github.com/VITA-Group/BlackBoxGANCollapse.",0
"GANs are currently capable of generating highly realistic images, but there is concern over whether the learned distribution of state-of-the-art GANs suffers from mode collapse and how to address this issue. Existing diversity tests for GANs are limited in scope and often rely on access to training data and model parameters. This paper presents a novel approach to diagnosing and calibrating intra-mode collapse in GANs in a black-box setting, where no training data or model parameters are available. The authors propose a set of statistical tools based on sampling to visualize, quantify, and rectify intra-mode collapse. The effectiveness of the proposed approach is demonstrated through simulations and experiments on unconditional GAN image generation for faces and vehicles. The study reveals that intra-mode collapse remains a significant problem in state-of-the-art GANs, but it can be diagnosed and calibrated in black-box settings. The authors provide their code on GitHub.",1
"Handwritten text recognition in low resource scenarios, such as manuscripts with rare alphabets, is a challenging problem. The main difficulty comes from the very few annotated data and the limited linguistic information (e.g. dictionaries and language models). Thus, we propose a few-shot learning-based handwriting recognition approach that significantly reduces the human labor annotation process, requiring only few images of each alphabet symbol. First, our model detects all symbols of a given alphabet in a textline image, then a decoding step maps the symbol similarity scores to the final sequence of transcribed symbols. Our model is first pretrained on synthetic line images generated from any alphabet, even though different from the target domain. A second training step is then applied to diminish the gap between the source and target data. Since this retraining would require annotation of thousands of handwritten symbols together with their bounding boxes, we propose to avoid such human effort through an unsupervised progressive learning approach that automatically assigns pseudo-labels to the non-annotated data. The evaluation on different manuscript datasets show that our model can lead to competitive results with a significant reduction in human effort.",0
"Recognizing handwritten text in low resource scenarios, particularly in manuscripts with rare alphabets, poses a difficult challenge due to the scarcity of annotated data and limited linguistic information available, such as dictionaries and language models. To address this challenge, we propose a few-shot learning-based approach for handwriting recognition that reduces the need for extensive human labor in the annotation process by utilizing only a few images of each alphabet symbol. Our approach involves detecting all symbols of a given alphabet in a textline image and using a decoding step to map the symbol similarity scores to the final sequence of transcribed symbols. Our model is first pre-trained on synthetic line images generated from any alphabet, even if different from the target domain, and then re-trained to reduce the gap between the source and target data. To avoid the need for annotating thousands of handwritten symbols and their bounding boxes, we propose an unsupervised progressive learning approach that automatically assigns pseudo-labels to the non-annotated data. Our model has been evaluated on different manuscript datasets and has shown competitive results with a significant reduction in human effort.",1
"Synthetic data is becoming increasingly common for training computer vision models for a variety of tasks. Notably, such data has been applied in tasks related to humans such as 3D pose estimation where data is either difficult to create or obtain in realistic settings. Comparatively, there has been less work into synthetic animal data and it's uses for training models. Consequently, we introduce a parametric canine model, DynaDog+T, for generating synthetic canine images and data which we use for a common computer vision task, binary segmentation, which would otherwise be difficult due to the lack of available data.",0
"The use of synthetic data is becoming more prevalent in the training of computer vision models for various tasks. This type of data has been particularly useful in tasks that involve humans, such as 3D pose estimation, where obtaining realistic data is challenging. However, there has been less exploration of synthetic animal data and its potential applications in training models. Thus, we present a parametric canine model, DynaDog+T, which allows for the generation of synthetic canine images and data. This model is used in a common computer vision task, binary segmentation, which would be difficult to perform without adequate data.",1
"For successful scene text recognition (STR) models, synthetic text image generators have alleviated the lack of annotated text images from the real world. Specifically, they generate multiple text images with diverse backgrounds, font styles, and text shapes and enable STR models to learn visual patterns that might not be accessible from manually annotated data. In this paper, we introduce a new synthetic text image generator, SynthTIGER, by analyzing techniques used for text image synthesis and integrating effective ones under a single algorithm. Moreover, we propose two techniques that alleviate the long-tail problem in length and character distributions of training data. In our experiments, SynthTIGER achieves better STR performance than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST). Our ablation study demonstrates the benefits of using sub-components of SynthTIGER and the guideline on generating synthetic text images for STR models. Our implementation is publicly available at https://github.com/clovaai/synthtiger.",0
"Synthetic text image generators have been instrumental in the success of scene text recognition (STR) models by addressing the shortage of annotated text images in the real world. These generators produce multiple text images with varying backgrounds, font styles, and text shapes, allowing STR models to learn visual patterns that may not be accessible through manual annotation. This paper introduces a new synthetic text image generator called SynthTIGER, which combines effective techniques for text image synthesis. Additionally, two methods are proposed to alleviate the long-tail problem in length and character distributions of training data. Our experiments show that SynthTIGER outperforms the use of synthetic datasets MJ and ST. We provide an ablation study that demonstrates the benefits of using sub-components of SynthTIGER and offer guidelines for generating synthetic text images for STR models. Our implementation is available publicly at https://github.com/clovaai/synthtiger.",1
"We propose an audio-driven talking-head method to generate photo-realistic talking-head videos from a single reference image. In this work, we tackle two key challenges: (i) producing natural head motions that match speech prosody, and (ii) maintaining the appearance of a speaker in a large head motion while stabilizing the non-face regions. We first design a head pose predictor by modeling rigid 6D head movements with a motion-aware recurrent neural network (RNN). In this way, the predicted head poses act as the low-frequency holistic movements of a talking head, thus allowing our latter network to focus on detailed facial movement generation. To depict the entire image motions arising from audio, we exploit a keypoint based dense motion field representation. Then, we develop a motion field generator to produce the dense motion fields from input audio, head poses, and a reference image. As this keypoint based representation models the motions of facial regions, head, and backgrounds integrally, our method can better constrain the spatial and temporal consistency of the generated videos. Finally, an image generation network is employed to render photo-realistic talking-head videos from the estimated keypoint based motion fields and the input reference image. Extensive experiments demonstrate that our method produces videos with plausible head motions, synchronized facial expressions, and stable backgrounds and outperforms the state-of-the-art.",0
"Our proposal involves using audio as the driving force to create realistic talking-head videos from a single reference image. We address two main challenges: (i) generating natural head movements that correspond to speech prosody, and (ii) stabilizing non-face regions while maintaining the speaker's appearance during large head movements. To achieve this, we first create a head pose predictor using a motion-aware recurrent neural network (RNN) to model 6D head movements. This allows our subsequent network to focus on generating detailed facial movements. We use a keypoint-based dense motion field representation to capture the audio-induced image motions and develop a motion field generator to produce these fields from input audio, head poses, and the reference image. By using this representation, we maintain spatial and temporal consistency in the generated videos. Finally, an image generation network renders photo-realistic talking-head videos from the estimated motion fields and reference image. Our experiments show that our method produces videos with plausible head motions, synchronized facial expressions, and stable backgrounds, surpassing the state-of-the-art.",1
"Image-to-image translation models have shown remarkable ability on transferring images among different domains. Most of existing work follows the setting that the source domain and target domain keep the same at training and inference phases, which cannot be generalized to the scenarios for translating an image from an unseen domain to another unseen domain. In this work, we propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem, which aims to learn a model that can translate samples from image domains that are not observed during training. Accordingly, we propose a framework called ZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model each domain with domain-specific feature distribution that is semantically consistent on vision and attribute modalities. Then the domain-invariant features are disentangled with an shared encoder for image generation. We carry out extensive experiments on CUB and FLO datasets, and the results demonstrate the effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows significant accuracy improvements over state-of-the-art zero-shot learning methods on CUB and FLO.",0
"Models for image-to-image translation have demonstrated impressive capabilities in transferring images across different domains. However, most current approaches assume that the source and target domains remain the same during both training and inference stages, which limits their applicability in scenarios where an image needs to be translated from an unseen domain to another unseen domain. To address this issue, we propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem, which aims to develop a model capable of translating images from domains that were not observed during training. To achieve this, we introduce a framework called ZstGAN that employs an adversarial training scheme to model each domain with a domain-specific feature distribution that is semantically consistent on both vision and attribute modalities. The framework then disentangles domain-invariant features with a shared encoder for image generation. We conducted extensive experiments on the CUB and FLO datasets, and the results demonstrate the effectiveness of our method in tackling the UZSIT task. Additionally, ZstGAN outperforms state-of-the-art zero-shot learning methods on CUB and FLO with significant accuracy improvements.",1
"Place recognition is indispensable for a drift-free localization system. Due to the variations of the environment, place recognition using single-modality has limitations. In this paper, we propose a bi-modal place recognition method, which can extract a compound global descriptor from the two modalities, vision and LiDAR. Specifically, we first build the elevation image generated from 3D points as a structural representation. Then, we derive the correspondences between 3D points and image pixels that are further used in merging the pixel-wise visual features into the elevation map grids. In this way, we fuse the structural features and visual features in the consistent bird-eye view frame, yielding a semantic representation, namely CORAL. And the whole network is called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD has superior performance against other state-of-the-art methods. We also demonstrate that our network can be generalized to other scenes and sensor configurations on cross-city datasets.",0
"For a localization system that doesn't drift, place recognition is essential. However, the environment's variations limit place recognition that relies on a single modality. This paper proposes a bi-modal place recognition method that can generate a combined global descriptor from both vision and LiDAR modalities. The method involves creating an elevation image from 3D points as a structural representation and establishing correspondences between 3D points and image pixels to merge pixel-wise visual features into elevation map grids. This fusion results in a semantic representation known as CORAL, which is achieved in a consistent bird-eye view frame. The entire network is called CORAL-VLAD, and it outperforms other state-of-the-art methods based on comparisons on the Oxford RobotCar. Additionally, the network's generalizability is demonstrated on cross-city datasets with varying sensor configurations and scenes.",1
"Generating images from scene graphs is a challenging task that attracted substantial interest recently. Prior works have approached this task by generating an intermediate layout description of the target image. However, the representation of each object in the layout was generated independently, which resulted in high overlap, low coverage, and an overall blurry layout. We propose a novel method that alleviates these issues by generating the entire layout description gradually to improve inter-object dependency. We empirically show on the COCO-STUFF dataset that our approach improves the quality of both the intermediate layout and the final image. Our approach improves the layout coverage by almost 20 points and drops object overlap to negligible amounts.",0
"The generation of images from scene graphs has recently garnered significant attention due to its challenging nature. While previous works have attempted to tackle this task by creating an intermediate layout description of the intended image, they have encountered issues such as high overlap, low coverage, and blurry layouts. This is because each object in the layout was generated independently, leading to a lack of inter-object dependency. To overcome these problems, we propose a new method that gradually generates the entire layout description, resulting in improved inter-object dependency. Our empirical results on the COCO-STUFF dataset demonstrate that our approach enhances both the intermediate layout and the final image quality. We achieve a 20-point increase in layout coverage and negligible object overlap.",1
"Recent developments related to generative models have made it possible to generate diverse high-fidelity images. In particular, layout-to-image generation models have gained significant attention due to their capability to generate realistic complex images containing distinct objects. These models are generally conditioned on either semantic layouts or textual descriptions. However, unlike natural images, providing auxiliary information can be extremely hard in domains such as biomedical imaging and remote sensing. In this work, we propose a multi-object generation framework that can synthesize images with multiple objects without explicitly requiring their contextual information during the generation process. Based on a vector-quantized variational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial coherency within an image as well as semantic coherency between the objects and the background through two powerful autoregressive priors: PixelSNAIL and LayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent encodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the semantic distribution of the objects. An implicit advantage of our approach is that the generated samples are accompanied by object-level annotations. We demonstrate how coherency and fidelity are preserved with our method through experiments on the Multi-MNIST and CLEVR datasets; thereby outperforming state-of-the-art multi-object generative methods. The efficacy of our approach is demonstrated through application on medical imaging datasets, where we show that augmenting the training set with generated samples using our approach improves the performance of existing models.",0
"Recent advancements in generative models have enabled the creation of high-quality, diverse images. Layout-to-image generation models, in particular, have garnered attention for their ability to produce complex, realistic images with distinct objects. These models typically rely on semantic layouts or textual descriptions for conditioning. However, providing auxiliary information can prove challenging in domains like biomedical imaging and remote sensing. Our work proposes a multi-object generation framework that can synthesize images with multiple objects without explicitly requiring their contextual information during the generation process. Our model, built on a vector-quantized variational autoencoder backbone, leverages two powerful autoregressive priors, PixelSNAIL and LayoutPixelSNAIL, to preserve spatial coherency and semantic coherency between objects and the background. Our approach generates object-level annotations, and we demonstrate its effectiveness on the Multi-MNIST and CLEVR datasets, outperforming state-of-the-art multi-object generative methods. We also apply our approach to medical imaging datasets and show improved performance in existing models with the addition of generated samples.",1
"Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with applications in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at https://github.com/ardasahiner/ProCoGAN.",0
"Generative Adversarial Networks (GANs) are frequently utilized for modeling intricate data distributions. Both the generators and discriminators of GANs are typically modeled by neural networks, resulting in a complex optimization problem that is not transparent, non-convex, and non-concave for the generator and discriminator, respectively. Although these networks are usually optimized with gradient descent-ascent (GDA), it is uncertain if the optimization problem includes any saddle points, or if heuristic methods can detect them in practice. In this study, we examine the training of Wasserstein GANs using two-layer neural network discriminators from the perspective of convex duality, and for various generators, we reveal the conditions under which Wasserstein GANs can be exactly solved with convex optimization approaches or represented as convex-concave games. By employing this convex duality interpretation, we also demonstrate the impact of different discriminator activation functions. Our findings are supported by numerical results that highlight the power of the convex interpretation and its applications in the progressive training of convex architectures, such as linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments can be found at https://github.com/ardasahiner/ProCoGAN.",1
"We present a coupled Variational Auto-Encoder (VAE) method that improves the accuracy and robustness of the probabilistic inferences on represented data. The new method models the dependency between input feature vectors (images) and weighs the outliers with a higher penalty by generalizing the original loss function to the coupled entropy function, using the principles of nonlinear statistical coupling. We evaluate the performance of the coupled VAE model using the MNIST dataset. Compared with the traditional VAE algorithm, the output images generated by the coupled VAE method are clearer and less blurry. The visualization of the input images embedded in 2D latent variable space provides a deeper insight into the structure of new model with coupled loss function: the latent variable has a smaller deviation and a more compact latent space generates the output values. We analyze the histogram of the likelihoods of the input images using the generalized mean, which measures the model's accuracy as a function of the relative risk. The neutral accuracy, which is the geometric mean and is consistent with a measure of the Shannon cross-entropy, is improved. The robust accuracy, measured by the -2/3 generalized mean, is also improved.",0
"Our proposed approach is a coupled Variational Auto-Encoder (VAE) technique that enhances the precision and resilience of probabilistic inferences on represented data. To achieve this, we have integrated the dependency between input feature vectors, i.e., images, and provided a higher weightage to the outliers by modifying the original loss function to a coupled entropy function, based on the principles of nonlinear statistical coupling. We have assessed the performance of the coupled VAE model using the MNIST dataset and compared it with the traditional VAE algorithm. The results show that the images generated by the coupled VAE method are more distinct and less blurry. The visualization of the input images embedded in 2D latent variable space provides a deeper understanding of the structure of the new model, which has a smaller deviation and a more compact latent space, generating output values. We have analyzed the histogram of the likelihoods of the input images using the generalized mean, which measures the accuracy of the model as a function of the relative risk. The neutral accuracy, i.e., the geometric mean, which is consistent with a measure of the Shannon cross-entropy, has been improved. The robust accuracy, measured by the -2/3 generalized mean, has also been enhanced.",1
"Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets.",0
"The competitive performance of Vision Transformers (ViTs) in image recognition tasks with less vision-specific inductive biases has been recently observed. In this study, we aim to evaluate the extension of this observation to image generation. To achieve this, we integrate the ViT architecture into generative adversarial networks (GANs). However, we encountered instability during training due to the poor interaction between existing regularization methods for GANs and self-attention. To address this issue, we present novel regularization techniques for GANs with ViTs. Our approach, ViTGAN, achieves performance comparable to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets, as demonstrated empirically.",1
"In the present study, we propose to implement a new framework for estimating generative models via an adversarial process to extend an existing GAN framework and develop a white-box controllable image cartoonization, which can generate high-quality cartooned images/videos from real-world photos and videos. The learning purposes of our system are based on three distinct representations: surface representation, structure representation, and texture representation. The surface representation refers to the smooth surface of the images. The structure representation relates to the sparse colour blocks and compresses generic content. The texture representation shows the texture, curves, and features in cartoon images. Generative Adversarial Network (GAN) framework decomposes the images into different representations and learns from them to generate cartoon images. This decomposition makes the framework more controllable and flexible which allows users to make changes based on the required output. This approach overcomes any previous system in terms of maintaining clarity, colours, textures, shapes of images yet showing the characteristics of cartoon images.",0
"Our study proposes a novel framework for estimating generative models through an adversarial process, building upon an existing GAN framework. Our aim is to create a white-box controllable image cartoonization system that can produce high-quality cartooned images and videos from real-world photos and videos. To achieve this, our system is based on three distinct representations: surface, structure, and texture. Surface representation pertains to the smooth surface of images, structure representation relates to sparse color blocks and compresses generic content, while texture representation captures the texture, curves, and features seen in cartoon images. By decomposing images into different representations and learning from them, our GAN framework becomes more flexible and controllable, enabling users to make desired changes. This approach overcomes previous systems by preserving the clarity, colors, textures, and shapes of images while presenting the typical characteristics of cartoon images.",1
"In many applications of computer graphics, art and design, it is desirable for a user to provide intuitive non-image input, such as text, sketch, stroke, graph or layout, and have a computer system automatically generate photo-realistic images that adhere to the input content. While classic works that allow such automatic image content generation have followed a framework of image retrieval and composition, recent advances in deep generative models such as generative adversarial networks (GANs), variational autoencoders (VAEs), and flow-based methods have enabled more powerful and versatile image generation tasks. This paper reviews recent works for image synthesis given intuitive user input, covering advances in input versatility, image generation methodology, benchmark datasets, and evaluation metrics. This motivates new perspectives on input representation and interactivity, cross pollination between major image generation paradigms, and evaluation and comparison of generation methods.",0
"The computer graphics, art, and design fields often require non-image input from users, such as text, sketches, graphs, or layouts, which can then be transformed into photo-realistic images by computer systems. Traditional methods for generating image content have relied on image retrieval and composition, but recent advancements in deep generative models, like GANs, VAEs, and flow-based methods, have expanded the possibilities for image generation. This review paper examines current works in image synthesis based on intuitive user input, including advances in input options, image generation techniques, benchmark datasets, and evaluation metrics. These developments suggest new approaches to input representation and user interaction, as well as opportunities for combining different image generation methods and comparing their effectiveness.",1
"Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must either be closer in the representation space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL) is one such representation learning algorithm that has achieved state-of-the-art results in self-supervised image classification on ImageNet under the linear evaluation protocol. However, the utility of the learnt features of BYOL to perform clustering is not explored. In this work, we study the clustering ability of BYOL and observe that features learnt using BYOL may not be optimal for clustering. We propose a novel consensus clustering based loss function, and train BYOL with the proposed loss in an end-to-end way that improves the clustering ability and outperforms similar clustering based methods on some popular computer vision datasets.",0
"Recent progress in deep clustering and unsupervised representation learning is founded on the notion that different perspectives of an input image, created through data augmentation methods, should either be positioned closer together in the representation space or assigned to a comparable cluster. Bootstrap Your Own Latent (BYOL) is an example of such a representation learning algorithm that has achieved exceptional outcomes in self-supervised image classification on ImageNet using the linear evaluation method. Despite this, the usefulness of the BYOL-learned features for clustering has not been investigated. In this research, we examine the clustering capability of BYOL and observe that the features acquired using BYOL may not be optimal for clustering. We propose a new loss function based on consensus clustering and train BYOL with this loss function in an end-to-end manner, resulting in improved clustering ability that outperforms similar clustering-based methods on popular computer vision datasets.",1
"Attention is a general reasoning mechanism than can flexibly deal with image information, but its memory requirements had made it so far impractical for high resolution image generation. We present Grid Partitioned Attention (GPA), a new approximate attention algorithm that leverages a sparse inductive bias for higher computational and memory efficiency in image domains: queries attend only to few keys, spatially close queries attend to close keys due to correlations. Our paper introduces the new attention layer, analyzes its complexity and how the trade-off between memory usage and model power can be tuned by the hyper-parameters.We will show how such attention enables novel deep learning architectures with copying modules that are especially useful for conditional image generation tasks like pose morphing. Our contributions are (i) algorithm and code1of the novel GPA layer, (ii) a novel deep attention-copying architecture, and (iii) new state-of-the art experimental results in human pose morphing generation benchmarks.",0
"Up until now, the memory requirements of attention, a general reasoning mechanism that is capable of handling image information, have made it impractical for high resolution image generation. However, our team has developed a new algorithm called Grid Partitioned Attention (GPA) that utilizes a sparse inductive bias to enhance computational and memory efficiency in image domains. This algorithm allows queries to attend to only a few keys and for spatially close queries to attend to close keys due to correlations. Our research introduces the GPA layer, examines its complexity, and explains how the hyper-parameters can be adjusted to optimize the trade-off between memory usage and model power. We demonstrate how this attention facilitates innovative deep learning architectures, including copying modules that are particularly useful for conditional image generation tasks such as pose morphing. Our contributions include the algorithm and code for the innovative GPA layer, a novel deep attention-copying architecture, and new experimental results that set a new standard in human pose morphing generation benchmarks.",1
"The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-70% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines.",0
"The healthcare and finance industries frequently utilize the imputation of missing values in time series. Although autoregressive models are commonly used for this purpose, score-based diffusion models have recently shown superior performance in tasks such as audio synthesis and image generation. Therefore, we introduce a new method for time series imputation called Conditional Score-based Diffusion models for Imputation (CSDI), which employs score-based diffusion models that are specifically designed for imputation and can take advantage of correlations between observed values. Our experiments on healthcare and environmental data demonstrate that CSDI provides a 40-70% improvement over existing probabilistic imputation methods on widely used performance metrics, and deterministic imputation by CSDI reduces errors by 5-20% compared to the best deterministic imputation methods. Additionally, CSDI can be used for time series interpolation and probabilistic forecasting and performs competitively with existing baselines.",1
"We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.",0
"Our study demonstrates that cascaded diffusion models can successfully produce high-quality images for the class-conditional ImageNet generation challenge, without the need for auxiliary image classifiers to enhance sample quality. The cascaded diffusion model involves a sequence of multiple diffusion models that generate images with increasing resolution, starting with a standard diffusion model at the lowest resolution and followed by one or more super-resolution diffusion models that progressively upscale the image and incorporate more detailed features. We discovered that the quality of the samples in a cascading pipeline depends heavily on conditioning augmentation, which is our proposed technique for augmenting the lower resolution conditioning inputs to the super-resolution models. Our experiments confirmed that conditioning augmentation can prevent error accumulation during sampling in a cascaded model, leading to cascading pipelines with FID scores of 1.48 at 64x64, 3.52 at 128x128, and 4.88 at 256x256 resolutions that surpass BigGAN-deep. Furthermore, our classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256 outperformed VQ-VAE-2.",1
"Generative Adversarial Networks (GANs) have become the de-facto standard in image synthesis. However, without considering the foreground-background decomposition, existing GANs tend to capture excessive content correlation between foreground and background, thus constraining the diversity in image generation. This paper presents a novel Foreground-Background Composition GAN (FBC-GAN) that performs image generation by generating foreground objects and background scenes concurrently and independently, followed by composing them with style and geometrical consistency. With this explicit design, FBC-GAN can generate images with foregrounds and backgrounds that are mutually independent in contents, thus lifting the undesirably learned content correlation constraint and achieving superior diversity. It also provides excellent flexibility by allowing the same foreground object with different background scenes, the same background scene with varying foreground objects, or the same foreground object and background scene with different object positions, sizes and poses. It can compose foreground objects and background scenes sampled from different datasets as well. Extensive experiments over multiple datasets show that FBC-GAN achieves competitive visual realism and superior diversity as compared with state-of-the-art methods.",0
"GANs have become the norm for creating images; however, existing GANs have a drawback in that they do not separate foreground and background composition, resulting in excessive content correlation. This constraint hinders diversity in image generation. A new approach, called Foreground-Background Composition GAN (FBC-GAN), has been developed to address this issue. FBC-GAN generates foreground objects and background scenes independently and concurrently, then composes them using style and geometrical consistency. This method eliminates the content correlation constraint and produces superior diversity. Moreover, FBC-GAN is flexible and can generate images with different object positions, sizes, and poses, and can compose foreground objects and background scenes from different datasets. Extensive experiments on multiple datasets demonstrate that FBC-GAN achieves competitive visual realism and superior diversity compared to other methods.",1
"Generative adversarial networks (GANs) have achieved great success in image translation and manipulation. However, high-fidelity image generation with faithful style control remains a grand challenge in computer vision. This paper presents a versatile image translation and manipulation framework that achieves accurate semantic and style guidance in image generation by explicitly building a correspondence. To handle the quadratic complexity incurred by building the dense correspondences, we introduce a bi-level feature alignment strategy that adopts a top-$k$ operation to rank block-wise features followed by dense attention between block features which reduces memory cost substantially. As the top-$k$ operation involves index swapping which precludes the gradient propagation, we propose to approximate the non-differentiable top-$k$ operation with a regularized earth mover's problem so that its gradient can be effectively back-propagated. In addition, we design a novel semantic position encoding mechanism that builds up coordinate for each individual semantic region to preserve texture structures while building correspondences. Further, we design a novel confidence feature injection module which mitigates mismatch problem by fusing features adaptively according to the reliability of built correspondences. Extensive experiments show that our method achieves superior performance qualitatively and quantitatively as compared with the state-of-the-art. The code is available at \href{https://github.com/fnzhan/RABIT}{https://github.com/fnzhan/RABIT}.",0
"Although generative adversarial networks (GANs) have been successful in image translation and manipulation, generating high-quality images with precise style control remains a major challenge in computer vision. To address this issue, this study introduces a flexible image translation and manipulation framework that utilizes a correspondence-based approach to achieve accurate style and semantic guidance in image generation. However, building dense correspondences incurs a quadratic complexity. To reduce memory cost, this study proposes a bi-level feature alignment strategy that ranks block-wise features using a top-$k$ operation and employs dense attention between block features. Nonetheless, the top-$k$ operation precludes gradient propagation due to index swapping, thus approximating it using a regularized earth mover's problem is proposed to enable effective back-propagation of its gradient. Additionally, a novel semantic position encoding mechanism is devised to create coordinates for each semantic region, preserving texture structures while establishing correspondences, while a confidence feature injection module is designed to fuse features adaptively, mitigating mismatch problems based on the reliability of the built correspondences. The study's experiments show that the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively. The code is available at \href{https://github.com/fnzhan/RABIT}{https://github.com/fnzhan/RABIT}.",1
"Despite significant progress on current state-of-the-art image generation models, synthesis of document images containing multiple and complex object layouts is a challenging task. This paper presents a novel approach, called DocSynth, to automatically synthesize document images based on a given layout. In this work, given a spatial layout (bounding boxes with object categories) as a reference by the user, our proposed DocSynth model learns to generate a set of realistic document images consistent with the defined layout. Also, this framework has been adapted to this work as a superior baseline model for creating synthetic document image datasets for augmenting real data during training for document layout analysis tasks. Different sets of learning objectives have been also used to improve the model performance. Quantitatively, we also compare the generated results of our model with real data using standard evaluation metrics. The results highlight that our model can successfully generate realistic and diverse document images with multiple objects. We also present a comprehensive qualitative analysis summary of the different scopes of synthetic image generation tasks. Lastly, to our knowledge this is the first work of its kind.",0
"Although significant advances have been made in image generation models, producing document images with complex object layouts remains a difficult challenge. This study introduces a new method, called DocSynth, for automatically generating document images based on a specified layout. DocSynth is trained to produce a variety of realistic images consistent with the provided layout, making it a superior baseline model for creating synthetic document image datasets that can be used to augment real data during document layout analysis training. Different learning objectives were used to enhance the model's performance, and the generated images were compared to real data using standard evaluation metrics. The results demonstrate that DocSynth can generate diverse and lifelike document images with multiple objects. Additionally, we conducted a comprehensive qualitative analysis of the different types of synthetic image generation tasks, and to our knowledge, this is the first study of its kind.",1
"This paper studies probability distributions of penultimate activations of classification networks. We show that, when a classification network is trained with the cross-entropy loss, its final classification layer forms a Generative-Discriminative pair with a generative classifier based on a specific distribution of penultimate activations. More importantly, the distribution is parameterized by the weights of the final fully-connected layer, and can be considered as a generative model that synthesizes the penultimate activations without feeding input data. We empirically demonstrate that this generative model enables stable knowledge distillation in the presence of domain shift, and can transfer knowledge from a classifier to variational autoencoders and generative adversarial networks for class-conditional image generation.",0
"The objective of this research is to examine the likelihood distributions of second-to-last activations in classification networks. Our findings reveal that when a classification network is trained using cross-entropy loss, its final classification layer collaborates with a generative classifier that is based on a particular distribution of second-to-last activations, forming a Generative-Discriminative pair. Moreover, this distribution is parameterized by the weights of the final fully-connected layer and can be seen as a generative model that generates second-to-last activations in the absence of input data. Through empirical evidence, we demonstrate that this generative model facilitates steady knowledge distillation in the face of domain shift and facilitates knowledge transfer from a classifier to variational autoencoders and generative adversarial networks that generate images based on classes.",1
"Interactive facial image manipulation attempts to edit single and multiple face attributes using a photo-realistic face and/or semantic mask as input. In the absence of the photo-realistic image (only sketch/mask available), previous methods only retrieve the original face but ignore the potential of aiding model controllability and diversity in the translation process. This paper proposes a sketch-to-image generation framework called S2FGAN, aiming to improve users' ability to interpret and flexibility of face attribute editing from a simple sketch. The proposed framework modifies the constrained latent space semantics trained on Generative Adversarial Networks (GANs). We employ two latent spaces to control the face appearance and adjust the desired attributes of the generated face. Instead of constraining the translation process by using a reference image, the users can command the model to retouch the generated images by involving the semantic information in the generation process. In this way, our method can manipulate single or multiple face attributes by only specifying attributes to be changed. Extensive experimental results on CelebAMask-HQ dataset empirically shows our superior performance and effectiveness on this task. Our method successfully outperforms state-of-the-art methods on attribute manipulation by exploiting greater control of attribute intensity.",0
"The aim of interactive facial image manipulation is to modify single or multiple face attributes using a photo-realistic face and/or semantic mask as input. Previous methods have only been able to retrieve the original face when a photo-realistic image is not available. However, this approach ignores the potential for aiding model controllability and diversity in the translation process. To address this issue, the proposed framework, called S2FGAN, utilizes a sketch-to-image generation approach to enhance the user's ability to interpret and flexibly edit face attributes from a simple sketch. The framework modifies the constrained latent space semantics trained on Generative Adversarial Networks (GANs) and employs two latent spaces to control the face appearance and adjust the desired attributes of the generated face. Unlike previous methods that constrain the translation process using a reference image, our method involves the semantic information in the generation process, allowing for greater control of attribute intensity and the manipulation of single or multiple face attributes by only specifying attributes to be changed. The effectiveness of our method is demonstrated through extensive experimental results on CelebAMask-HQ dataset, which outperforms state-of-the-art methods on attribute manipulation.",1
"While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? In this work, we test this hypothesis and propose a simple and effective approach based on GANs for semantic part segmentation that requires as few as one label example along with an unlabeled dataset. Our key idea is to leverage a trained GAN to extract pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that GANs representation is ""readily discriminative"" and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning that is applicable to many other tasks. More results are available at https://repurposegans.github.io/.",0
"Although GANs have been successful in generating realistic images, their potential for tasks unrelated to synthesis has not been fully explored. Can GANs learn meaningful object structures while reproducing them? This study aims to test this hypothesis and presents a simple and effective approach to semantic part segmentation using GANs. The method only requires one labeled example and an unlabeled dataset. The approach involves using a trained GAN to extract pixel-wise representations from the input image, which are used as feature vectors for a segmentation network. The experiments show that the GANs representations are easily distinguishable and produce results comparable to those from supervised baselines trained with significantly more labels. This repurposing of GANs could lead to a new class of unsupervised representation learning applicable to many other tasks. Additional results can be found at https://repurposegans.github.io/.",1
"In this paper, we present a non-parametric structured latent variable model for image generation, called NP-DRAW, which sequentially draws on a latent canvas in a part-by-part fashion and then decodes the image from the canvas. Our key contributions are as follows. 1) We propose a non-parametric prior distribution over the appearance of image parts so that the latent variable ``what-to-draw'' per step becomes a categorical random variable. This improves the expressiveness and greatly eases the learning compared to Gaussians used in the literature. 2) We model the sequential dependency structure of parts via a Transformer, which is more powerful and easier to train compared to RNNs used in the literature. 3) We propose an effective heuristic parsing algorithm to pre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show that our method significantly outperforms previous structured image models like DRAW and AIR and is competitive to other generic generative models. Moreover, we show that our model's inherent compositionality and interpretability bring significant benefits in the low-data learning regime and latent space editing. Code is available at https://github.com/ZENGXH/NPDRAW.",0
"NP-DRAW is a non-parametric structured latent variable model for image generation that is presented in this paper. The model utilizes a sequential drawing technique on a latent canvas that is decoded to produce the final image. The paper highlights three key contributions. Firstly, a non-parametric prior distribution is proposed to improve expressiveness and simplify learning. Secondly, a Transformer is used to model the sequential dependency structure of image parts. This is shown to be more effective and easier to train than RNNs used in previous literature. Finally, an effective heuristic parsing algorithm is proposed to pre-train the prior. The model outperforms previous models such as DRAW and AIR, and is competitive with other generic generative models when applied to MNIST, Omniglot, CIFAR-10, and CelebA datasets. The model's inherent compositionality and interpretability are also shown to bring significant benefits in the low-data learning regime and latent space editing. Code for NP-DRAW is available at https://github.com/ZENGXH/NPDRAW.",1
"This work tackles the issue of fairness in the context of generative procedures, such as image super-resolution, which entail different definitions from the standard classification setting. Moreover, while traditional group fairness definitions are typically defined with respect to specified protected groups -- camouflaging the fact that these groupings are artificial and carry historical and political motivations -- we emphasize that there are no ground truth identities. For instance, should South and East Asians be viewed as a single group or separate groups? Should we consider one race as a whole or further split by gender? Choosing which groups are valid and who belongs in them is an impossible dilemma and being ""fair"" with respect to Asians may require being ""unfair"" with respect to South Asians. This motivates the introduction of definitions that allow algorithms to be \emph{oblivious} to the relevant groupings.   We define several intuitive notions of group fairness and study their incompatibilities and trade-offs. We show that the natural extension of demographic parity is strongly dependent on the grouping, and \emph{impossible} to achieve obliviously. On the other hand, the conceptually new definition we introduce, Conditional Proportional Representation, can be achieved obliviously through Posterior Sampling. Our experiments validate our theoretical results and achieve fair image reconstruction using state-of-the-art generative models.",0
"The aim of this study is to address the problem of fairness in generative processes, such as image super-resolution, which differ from the standard classification scenario. The conventional approach to defining group fairness is to identify protected groups, but this conceals the fact that these groupings are artificial and influenced by historical and political factors. It is impossible to determine ground truth identities, as the classification of individuals into groups is subjective. For example, should South and East Asians be treated as a single group or separate ones? Additionally, should one race be considered as a whole or divided by gender? This dilemma highlights the need for definitions that do not depend on groupings. We propose several notions of group fairness and examine their trade-offs and incompatibilities. Our research shows that demographic parity, which is an extension of previous notions of fairness, is strongly influenced by groupings and cannot be achieved without considering them. However, our novel definition of Conditional Proportional Representation can be achieved without considering groupings, using Posterior Sampling. Our experiments confirm that our theoretical results are valid and demonstrate that our approach achieves fair image reconstruction using state-of-the-art generative models.",1
"Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard softmax classifier can be reinterpreted as an energy-based model (EBM) for the joint distribution p(x,y); the resulting model can be optimized to improve calibration, robustness, and out-of-distribution detection, while generating samples rivaling the quality of recent GAN-based approaches. However, the softmax classifier that JEM exploits is inherently discriminative and its latent feature space is not well formulated as probabilistic distributions, which may hinder its potential for image generation and incur training instability. We hypothesize that generative classifiers, such as Linear Discriminant Analysis (LDA), might be more suitable for image generation since generative classifiers model the data generation process explicitly. This paper therefore investigates an LDA classifier for image classification and generation. In particular, the Max-Mahalanobis Classifier (MMC), a special case of LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be trained discriminatively, generatively, or jointly for image classification and generation. Extensive experiments on multiple datasets show that GMMC achieves state-of-the-art discriminative and generative performances, while outperforming JEM in calibration, adversarial robustness, and out-of-distribution detection by a significant margin. Our source code is available at https://github.com/sndnyang/GMMC.",0
"The Joint Energy-based Model (JEM) developed by Grathwohl et al. demonstrates that a standard softmax classifier can be viewed as an energy-based model (EBM) for p(x,y). This reinterpreted model can be optimized to enhance calibration, robustness, and detection of out-of-distribution cases, with generated samples that are comparable to those produced by GAN-based methods. However, the use of a discriminative softmax classifier in JEM may limit its potential for image generation and lead to training instability due to the inadequate formulation of its latent feature space as probabilistic distributions. To address this, we propose that generative classifiers, such as Linear Discriminant Analysis (LDA), may be better suited for image generation as they explicitly model the data generation process. In this paper, we investigate the use of the Max-Mahalanobis Classifier (MMC), a type of LDA, for both image classification and generation. Our Generative MMC (GMMC) can be trained in a discriminative, generative, or joint manner, and we demonstrate through extensive experiments on multiple datasets that GMMC outperforms JEM in calibration, adversarial robustness, and out-of-distribution detection by a significant margin, while achieving state-of-the-art performances in both discriminative and generative tasks. The source code for our work is publicly available at https://github.com/sndnyang/GMMC.",1
"We present a novel data generation tool for document processing. The tool focuses on providing a maximal level of visual information in a normal type document, ranging from character position to paragraph-level position. It also enables working with a large dataset on low-resource languages as well as providing a mean of processing thorough full-level information of the documented text. The data generation tools come with a dataset of 320000 Vietnamese synthetic document images and an instruction to generate a dataset of similar size in other languages. The repository can be found at: https://github.com/tson1997/SDL-Document-Image-Generation",0
"Our latest innovation is a data generation tool designed for document processing. This tool aims to offer the highest amount of visual information possible in a standard document, including character and paragraph-level positions. Moreover, it allows for working with vast datasets in low-resource languages and provides a means of thoroughly processing full-level text information. As part of this data generation tool, we have included a dataset of 320,000 synthetic document images in Vietnamese, along with instructions on how to create a dataset of equivalent size in other languages. The repository for this tool is accessible at: https://github.com/tson1997/SDL-Document-Image-Generation.",1
"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.",0
"Traditionally, images are viewed as 2D pixel arrays in most learning systems. However, there is a new trend that is gaining popularity where a 2D image is represented as an implicit neural representation (INR). An INR is a Multi-Layer Perceptron (MLP) that predicts an RGB pixel value based on its (x,y) coordinate. This paper proposes two innovative architectural techniques, factorized multiplicative modulation and multi-scale INRs, for constructing image decoders that are based on INRs. These techniques were used to create a state-of-the-art continuous image GAN. Previous attempts to use INRs for image generation were limited to MNIST-like datasets and were not capable of scaling to complex real-world data. Our proposed INR-GAN architecture significantly enhances the performance of continuous image generators, reducing the gap between continuous image GANs and pixel-based ones. Additionally, we explore several exciting properties of the INR-based decoders, such as out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, the ability to extrapolate outside of image boundaries, and a strong geometric prior. The project page can be found at https://universome.github.io/inr-gan.",1
"Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames.This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.",0
"The development of technology has resulted in the creation of methods that can produce desired multimedia visuals. Deep learning has been extensively researched in generating images in various fields, whereas generating videos, especially with conditional inputs, remains a complex and less explored area. To address this issue, we intend to train our model to generate a video that corresponds to a given text description. Our proposed training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), progresses frame-by-frame to produce a full-length video. Initially, we focus on creating a high-quality video frame while learning the relationship between the text and the image. Subsequently, the model is trained gradually on more consecutive frames, which stabilizes the training process and enables the creation of high-resolution videos based on conditional text descriptions. Our proposed method has been tested on various datasets, and the qualitative and quantitative experimental results demonstrate its effectiveness.",1
"In this paper, we propose a novel encoder, called ShapeEditor, for high-resolution, realistic and high-fidelity face exchange. First of all, in order to ensure sufficient clarity and authenticity, our key idea is to use an advanced pretrained high-quality random face image generator, i.e. StyleGAN, as backbone. Secondly, we design ShapeEditor, a two-step encoder, to make the swapped face integrate the identity and attribute of the input faces. In the first step, we extract the identity vector of the source image and the attribute vector of the target image respectively; in the second step, we map the concatenation of identity vector and attribute vector into the $\mathcal{W+}$ potential space. In addition, for learning to map into the latent space of StyleGAN, we propose a set of self-supervised loss functions with which the training data do not need to be labeled manually. Extensive experiments on the test dataset show that the results of our method not only have a great advantage in clarity and authenticity than other state-of-the-art methods, but also reflect the sufficient integration of identity and attribute.",0
"This paper introduces ShapeEditor, a new encoder that facilitates high-resolution, realistic, and high-fidelity face exchange. Our approach leverages StyleGAN, an advanced pretrained high-quality random face image generator, as the backbone to ensure clarity and authenticity. Furthermore, we design ShapeEditor as a two-step encoder that integrates the identity and attribute of input faces. Specifically, we extract the identity vector of the source image and the attribute vector of the target image in the first step, and then map the concatenation of identity vector and attribute vector into the $\mathcal{W+}$ potential space in the second step. To facilitate learning, we propose a set of self-supervised loss functions that do not require manual labeling of training data. Our experiments on the test dataset show that our approach outperforms other state-of-the-art methods in terms of clarity, authenticity, and integration of identity and attribute.",1
"Since human-labeled samples are free for the target set, unsupervised person re-identification (Re-ID) has attracted much attention in recent years, by additionally exploiting the source set. However, due to the differences on camera styles, illumination and backgrounds, there exists a large gap between source domain and target domain, introducing a great challenge on cross-domain matching. To tackle this problem, in this paper we propose a novel method named Dual-stream Reciprocal Disentanglement Learning (DRDL), which is quite efficient in learning domain-invariant features. In DRDL, two encoders are first constructed for id-related and id-unrelated feature extractions, which are respectively measured by their associated classifiers. Furthermore, followed by an adversarial learning strategy, both streams reciprocally and positively effect each other, so that the id-related features and id-unrelated features are completely disentangled from a given image, allowing the encoder to be powerful enough to obtain the discriminative but domain-invariant features. In contrast to existing approaches, our proposed method is free from image generation, which not only reduces the computational complexity remarkably, but also removes redundant information from id-related features. Extensive experiments substantiate the superiority of our proposed method compared with the state-of-the-arts. The source code has been released in https://github.com/lhf12278/DRDL.",0
"Unsupervised person re-identification (Re-ID) has gained attention in recent years due to the availability of human-labeled samples for the target set. However, the differences in camera styles, illumination, and backgrounds between the source and target domains pose a significant challenge for cross-domain matching. To address this problem, we propose a novel method called Dual-stream Reciprocal Disentanglement Learning (DRDL), which efficiently learns domain-invariant features. Our approach involves constructing two encoders for id-related and id-unrelated feature extractions, respectively measured by their associated classifiers. By employing an adversarial learning strategy, both streams positively influence each other to disentangle the id-related and id-unrelated features from a given image and obtain discriminative, domain-invariant features. Unlike existing methods, DRDL does not require image generation, reducing computational complexity and removing redundant information from id-related features. Extensive experiments demonstrate the superiority of our proposed method compared to the state-of-the-art approaches. The source code is available at https://github.com/lhf12278/DRDL.",1
"Pseudo-normality synthesis, which computationally generates a pseudo-normal image from an abnormal one (e.g., with lesions), is critical in many perspectives, from lesion detection, data augmentation to clinical surgery suggestion. However, it is challenging to generate high-quality pseudo-normal images in the absence of the lesion information. Thus, expensive lesion segmentation data have been introduced to provide lesion information for the generative models and improve the quality of the synthetic images. In this paper, we aim to alleviate the need of a large amount of lesion segmentation data when generating pseudo-normal images. We propose a Semi-supervised Medical Image generative LEarning network (SMILE) which not only utilizes limited medical images with segmentation masks, but also leverages massive medical images without segmentation masks to generate realistic pseudo-normal images. Extensive experiments show that our model outperforms the best state-of-the-art model by up to 6% for data augmentation task and 3% in generating high-quality images. Moreover, the proposed semi-supervised learning achieves comparable medical image synthesis quality with supervised learning model, using only 50 of segmentation data.",0
"The process of creating a pseudo-normal image from an abnormal one, known as pseudo-normality synthesis, is essential for various purposes such as identifying lesions, augmenting data, and suggesting clinical surgeries. However, generating high-quality pseudo-normal images without the lesion information is difficult. Consequently, expensive lesion segmentation data is used to improve the quality of synthetic images. This study aims to reduce the need for a significant amount of lesion segmentation data in generating pseudo-normal images. The Semi-supervised Medical Image generative LEarning network (SMILE) proposed in this paper utilizes both limited medical images with segmentation masks and a vast number of images without segmentation masks to produce realistic pseudo-normal images. The results of extensive experiments demonstrate that SMILE outperforms the best state-of-the-art model by 6% for data augmentation and 3% for generating high-quality images. Additionally, the proposed semi-supervised learning method achieves a comparable medical image synthesis quality with a supervised learning model using only 50 segmentation data.",1
"Generative adversarial networks (GANs) have gained considerable attention owing to their ability to reproduce images. However, they can recreate training images faithfully despite image degradation in the form of blur, noise, and compression, generating similarly degraded images. To solve this problem, the recently proposed noise robust GAN (NR-GAN) provides a partial solution by demonstrating the ability to learn a clean image generator directly from noisy images using a two-generator model comprising image and noise generators. However, its application is limited to noise, which is relatively easy to decompose owing to its additive and reversible characteristics, and its application to irreversible image degradation, in the form of blur, compression, and combination of all, remains a challenge. To address these problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that can learn a clean image generator directly from degraded images without knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator model composed of image, blur-kernel, noise, and quality-factor generators. However, in contrast to NR-GAN, to address irreversible characteristics, we introduce masking architectures adjusting degradation strength values in a data-driven manner using bypasses before and after degradation. Furthermore, to suppress uncertainty caused by the combination of blur, noise, and compression, we introduce adaptive consistency losses imposing consistency between irreversible degradation processes according to the degradation strengths. We demonstrate the effectiveness of BNCR-GAN through large-scale comparative studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we demonstrate the applicability of BNCR-GAN in image restoration.",0
"The ability of Generative Adversarial Networks (GANs) to reproduce images has attracted much attention. However, GANs can create degraded images that are similar to the training images, despite image degradation through blur, noise, and compression. The recently proposed Noise Robust GAN (NR-GAN) partially addresses this problem by demonstrating the ability to learn a clean image generator directly from noisy images using a two-generator model. However, NR-GAN is limited to noise, which is easy to decompose. Therefore, we propose a Blur, Noise, and Compression Robust GAN (BNCR-GAN) that can learn a clean image generator directly from degraded images without knowledge of degradation parameters. BNCR-GAN uses a multiple-generator model and introduces masking architectures to adjust degradation strength values in a data-driven manner. Furthermore, we introduce adaptive consistency losses to impose consistency between irreversible degradation processes according to the degradation strengths. We demonstrate the effectiveness of BNCR-GAN through comparative studies on CIFAR-10 and generality analysis on FFHQ. Additionally, we demonstrate the applicability of BNCR-GAN in image restoration.",1
"Variational inference (VI) plays an essential role in approximate Bayesian inference due to its computational efficiency and broad applicability. Crucial to the performance of VI is the selection of the associated divergence measure, as VI approximates the intractable distribution by minimizing this divergence. In this paper we propose a meta-learning algorithm to learn the divergence metric suited for the task of interest, automating the design of VI methods. In addition, we learn the initialization of the variational parameters without additional cost when our method is deployed in the few-shot learning scenarios. We demonstrate our approach outperforms standard VI on Gaussian mixture distribution approximation, Bayesian neural network regression, image generation with variational autoencoders and recommender systems with a partial variational autoencoder.",0
"The efficiency and versatility of variational inference (VI) make it a crucial tool in approximate Bayesian inference. The divergence measure used in VI is essential to its success, as it approximates an otherwise intractable distribution. In this study, we propose a meta-learning algorithm that automates the design of VI methods by learning the most suitable divergence metric for the task at hand. Additionally, we demonstrate that our approach can also learn the initialization of variational parameters without incurring extra costs in few-shot learning scenarios. Our method outperforms standard VI in various applications, including Gaussian mixture distribution approximation, Bayesian neural network regression, image generation with variational autoencoders, and recommender systems with partial variational autoencoders.",1
"Providing a human-understandable explanation of classifiers' decisions has become imperative to generate trust in their use for day-to-day tasks. Although many works have addressed this problem by generating visual explanation maps, they often provide noisy and inaccurate results forcing the use of heuristic regularization unrelated to the classifier in question. In this paper, we propose a new general perspective of the visual explanation problem overcoming these limitations. We show that visual explanation can be produced as the difference between two generated images obtained via two specific conditional generative models. Both generative models are trained using the classifier to explain and a database to enforce the following properties: (i) All images generated by the first generator are classified similarly to the input image, whereas the second generator's outputs are classified oppositely. (ii) Generated images belong to the distribution of real images. (iii) The distances between the input image and the corresponding generated images are minimal so that the difference between the generated elements only reveals relevant information for the studied classifier. Using symmetrical and cyclic constraints, we present two different approximations and implementations of the general formulation. Experimentally, we demonstrate significant improvements w.r.t the state-of-the-art on three different public data sets. In particular, the localization of regions influencing the classifier is consistent with human annotations.",0
"It has become increasingly important to provide understandable explanations for the decisions made by classifiers in order to establish trust in their use for everyday tasks. While some studies have attempted to address this issue by creating visual explanation maps, these often produce inaccurate and noisy results, necessitating the application of heuristic regularization that is unrelated to the specific classifier being used. This paper proposes a new approach to the visual explanation problem that overcomes these limitations. By generating two images using conditional generative models that are trained with the classifier and a database, the visual explanation can be produced as the difference between these two images. The generated images must meet several criteria, including being classified similarly to the input image by the first generator and oppositely by the second generator, belonging to the distribution of real images, and having minimal distances between the input image and the corresponding generated images. Two approximations and implementations of this approach are presented, and experimental results demonstrate significant improvements over existing methods on three public datasets, with the location of regions that influence the classifier being consistent with human annotations.",1
"We propose a novel and unified Cycle in Cycle Generative Adversarial Network (C2GAN) for generating human faces, hands, bodies, and natural scenes. Our proposed C2GAN is a cross-modal model exploring the joint exploitation of the input image data and guidance data in an interactive manner. C2GAN contains two different generators, i.e., an image-generation generator and a guidance-generation generator. Both generators are mutually connected and trained in an end-to-end fashion and explicitly form three cycled subnets, i.e., one image generation cycle and two guidance generation cycles. Each cycle aims at reconstructing the input domain and simultaneously produces a useful output involved in the generation of another cycle. In this way, the cycles constrain each other implicitly providing complementary information from both image and guidance modalities and bringing an extra supervision gradient across the cycles, facilitating a more robust optimization of the whole model. Extensive results on four guided image-to-image translation subtasks demonstrate that the proposed C2GAN is effective in generating more realistic images compared with state-of-the-art models. The code is available at https://github.com/Ha0Tang/C2GAN.",0
"Our C2GAN offers a fresh approach to generating images of human faces, hands, bodies, and natural scenes. It is a cross-modal model that exploits joint input image data and guidance data in an interactive way. The model comprises two generators, an image-generation generator, and a guidance-generation generator, which are mutually connected and trained end-to-end. The generators form three cycled subnets, including one image generation cycle and two guidance generation cycles, with each cycle reconstructing the input domain and producing a useful output for another cycle. This ensures that the cycles implicitly constrain each other, providing complementary information from both image and guidance modalities, and bringing an extra supervision gradient across the cycles for more robust optimization of the entire model. We present extensive results on four guided image-to-image translation subtasks, which demonstrate that our C2GAN is more effective in generating realistic images compared to state-of-the-art models. Interested parties can access our code at https://github.com/Ha0Tang/C2GAN.",1
"Deep visual models are susceptible to adversarial perturbations to inputs. Although these signals are carefully crafted, they still appear noise-like patterns to humans. This observation has led to the argument that deep visual representation is misaligned with human perception. We counter-argue by providing evidence of human-meaningful patterns in adversarial perturbations. We first propose an attack that fools a network to confuse a whole category of objects (source class) with a target label. Our attack also limits the unintended fooling by samples from non-sources classes, thereby circumscribing human-defined semantic notions for network fooling. We show that the proposed attack not only leads to the emergence of regular geometric patterns in the perturbations, but also reveals insightful information about the decision boundaries of deep models. Exploring this phenomenon further, we alter the `adversarial' objective of our attack to use it as a tool to `explain' deep visual representation. We show that by careful channeling and projection of the perturbations computed by our method, we can visualize a model's understanding of human-defined semantic notions. Finally, we exploit the explanability properties of our perturbations to perform image generation, inpainting and interactive image manipulation by attacking adversarialy robust `classifiers'.In all, our major contribution is a novel pragmatic adversarial attack that is subsequently transformed into a tool to interpret the visual models. The article also makes secondary contributions in terms of establishing the utility of our attack beyond the adversarial objective with multiple interesting applications.",0
"Adversarial perturbations can affect the accuracy of deep visual models. These perturbations often appear as noise to humans, leading to the belief that deep visual representation is not aligned with human perception. However, we argue against this by presenting evidence of human-meaningful patterns in adversarial perturbations. Our proposed attack is designed to confuse a network into misclassifying a whole category of objects with a target label, while limiting unintended fooling by non-source classes. This approach results in the emergence of regular geometric patterns and provides insights into the decision boundaries of deep models. We further demonstrate the utility of our attack by using it as a tool to explain deep visual representation. By channeling and projecting the perturbations, we can visualize a model's understanding of human-defined semantic notions. Furthermore, we leverage the perturbations for image generation, inpainting, and interactive image manipulation. Our work presents a novel pragmatic adversarial attack that can also be used for multiple applications beyond the adversarial objective.",1
"Although hierarchical structures are popular in recent vision transformers, they require sophisticated designs and massive datasets to work well. In this work, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture with minor code changes upon the original vision transformer and obtains improved performance compared to existing methods. Our empirical results show that the proposed method NesT converges faster and requires much less training data to achieve good generalization. For example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves $82.3\%/83.8\%$ accuracy evaluated on $224\times 224$ image size, outperforming previous methods with up to $57\%$ parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves $96\%$ accuracy using a single GPU, setting a new state of the art for vision transformers. Beyond image classification, we extend the key idea to image generation and show NesT leads to a strong decoder that is 8$\times$ faster than previous transformer based generators. Furthermore, we also propose a novel method for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.",0
"Recent vision transformers have adopted hierarchical structures, but they require complex designs and large datasets to function effectively. This study explores the concept of nesting basic local transformers on non-overlapping image blocks and combining them hierarchically. The block aggregation function is crucial in facilitating cross-block non-local information communication. This observation led to the development of a simplified architecture with minor code modifications to the original vision transformer, resulting in improved performance compared to existing methods. Empirical results demonstrate that the proposed method, NesT, converges faster and requires less training data to achieve good generalization. For instance, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves $82.3\%/83.8\%$ accuracy for $224\times 224$ image size, outperforming previous methods with up to $57\%$ parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves $96\%$ accuracy using a single GPU, setting a new state of the art for vision transformers. In addition, the key idea is extended to image generation, and NesT is shown to lead to a strong decoder that is 8$\times$ faster than previous transformer-based generators. Lastly, a novel method for visually interpreting the learned model is proposed, and the source code is available at https://github.com/google-research/nested-transformer.",1
"Interpreting how does deep neural networks (DNNs) make predictions is a vital field in artificial intelligence, which hinders wide applications of DNNs. Visualization of learned representations helps we humans understand the vision of DNNs. In this work, visualized images that can activate the neural network to the target classes are generated by back-propagation method. Here, rotation and scaling operations are applied to introduce the transformation invariance in the image generating process, which we find a significant improvement on visualization effect. Finally, we show some cases that such method can help us to gain insight into neural networks.",0
"The comprehension of how deep neural networks (DNNs) make predictions is crucial in the realm of artificial intelligence, as it limits the widespread use of DNNs. The visualization of learned representations assists us in comprehending the DNNs' vision. To generate visual images that can activate the neural network to the target classes, we utilize the back-propagation method. In this study, we apply rotation and scaling operations to introduce transformation invariance in the image generation process, which results in a significant improvement in visualization effectiveness. Finally, we present examples of how this method can provide us with insight into neural networks.",1
"Generative Adversarial Networks (GANs) have demonstrated unprecedented success in various image generation tasks. The encouraging results, however, come at the price of a cumbersome training process, during which the generator and discriminator are alternately updated in two stages. In this paper, we investigate a general training scheme that enables training GANs efficiently in only one stage. Based on the adversarial losses of the generator and discriminator, we categorize GANs into two classes, Symmetric GANs and Asymmetric GANs, and introduce a novel gradient decomposition method to unify the two, allowing us to train both classes in one stage and hence alleviate the training effort. We also computationally analyze the efficiency of the proposed method, and empirically demonstrate that, the proposed method yields a solid $1.5\times$ acceleration across various datasets and network architectures. Furthermore, we show that the proposed method is readily applicable to other adversarial-training scenarios, such as data-free knowledge distillation. The code is available at https://github.com/zju-vipa/OSGAN.",0
"Various image generation tasks have seen unprecedented success through the use of Generative Adversarial Networks (GANs). However, this success comes with the drawback of a challenging training process, where the generator and discriminator are updated separately in two stages. This paper explores a new training approach that allows for the efficient training of GANs in a single stage. By categorizing GANs into two classes, Symmetric GANs and Asymmetric GANs, based on the adversarial losses of the generator and discriminator, we introduce a novel gradient decomposition method that unifies the two classes. This method reduces the training effort and has been computationally analyzed for efficiency. Empirical evidence shows that this approach yields a 1.5x acceleration across various datasets and network architectures. Additionally, we demonstrate the applicability of this method to other adversarial-training scenarios, such as data-free knowledge distillation. The code for this approach can be found at https://github.com/zju-vipa/OSGAN.",1
"Recent work introduced progressive network growing as a promising way to ease the training for large GANs, but the model design and architecture-growing strategy still remain under-explored and needs manual design for different image data. In this paper, we propose a method to dynamically grow a GAN during training, optimizing the network architecture and its parameters together with automation. The method embeds architecture search techniques as an interleaving step with gradient-based training to periodically seek the optimal architecture-growing strategy for the generator and discriminator. It enjoys the benefits of both eased training because of progressive growing and improved performance because of broader architecture design space. Experimental results demonstrate new state-of-the-art of image generation. Observations in the search procedure also provide constructive insights into the GAN model design such as generator-discriminator balance and convolutional layer choices.",0
"Recently, progressive network growing has been identified as a promising method to facilitate the training of large GANs. However, the design of the model and architecture-growing strategy have not been thoroughly explored, and manual design is required for different image data. This paper proposes a dynamic GAN growth method that optimizes the network architecture and its parameters through automation during training. Architecture search techniques are embedded into the method as an interleaving step with gradient-based training to periodically search for the optimal architecture-growing strategy for the generator and discriminator. The method combines the benefits of both progressive growing and improved performance due to a broader architecture design space. Experimental results demonstrate new state-of-the-art image generation, and observations during the search procedure provide valuable insights into GAN model design, such as generator-discriminator balance and convolutional layer choices.",1
"Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.",0
"Many problems in computer vision and robotics are now commonly addressed with deep learning methods. However, acquiring high-quality training data is a challenging task due to the potential for human error, the time-consuming nature of data acquisition, and the high cost associated with it. Although simulation-based solutions have been gaining popularity, there remains a significant gap between simulated and real world data. This study presents a new technique for enhancing synthetic image data by utilizing unsupervised image-to-image translation, which applies the style of actual images to simulated ones using open source frameworks. The generated dataset is combined with traditional augmentation techniques and subsequently employed in a neural network model that runs in real-time on autonomous soccer robots. Our analysis indicates that this approach leads to a substantial improvement compared to models that use data generated solely through simulation.",1
"State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. We propose to perform reverse engineering of GMs to infer the model hyperparameters from the images generated by these models. We define a novel problem, ""model parsing"", as estimating GM network architectures and training loss functions by examining their generated images -- a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a fake image dataset with $100$K images generated by $100$ GMs. Extensive experiments show encouraging results in parsing the hyperparameters of the unseen models. Finally, our fingerprint estimation can be leveraged for deepfake detection and image attribution, as we show by reporting SOTA results on both the recent Celeb-DF and image attribution benchmarks.",0
"Our proposal involves reverse engineering State-of-the-art (SOTA) Generative Models (GMs) to determine the model hyperparameters used to create photo-realistic images that are difficult for humans to differentiate from authentic photos. We introduce a new problem called ""model parsing,"" which involves estimating GM network architectures and training loss functions by analyzing the images generated by these models. This task is challenging for humans, so we propose a two-component framework to address this issue. The first component is the Fingerprint Estimation Network (FEN), which trains with four constraints to estimate a GM fingerprint from a generated image that possesses the desired properties. The second component is the Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To assess our approach, we create a dataset comprising 100K fake images generated by 100 GMs and conduct extensive experiments that yield promising results in parsing the hyperparameters of the unseen models. Our fingerprint estimation technique can also be utilized for deepfake detection and image attribution, as we demonstrate by reporting state-of-the-art results on both the Celeb-DF and image attribution benchmarks.",1
"Human motion retargeting aims to transfer the motion of one person in a ""driving"" video or set of images to another person. Existing efforts leverage a long training video from each target person to train a subject-specific motion transfer model. However, the scalability of such methods is limited, as each model can only generate videos for the given target subject, and such training videos are labor-intensive to acquire and process. Few-shot motion transfer techniques, which only require one or a few images from a target, have recently drawn considerable attention. Methods addressing this task generally use either 2D or explicit 3D representations to transfer motion, and in doing so, sacrifice either accurate geometric modeling or the flexibility of an end-to-end learned representation. Inspired by the Transformable Bottleneck Network, which renders novel views and manipulations of rigid objects, we propose an approach based on an implicit volumetric representation of the image content, which can then be spatially manipulated using volumetric flow fields. We address the challenging question of how to aggregate information across different body poses, learning flow fields that allow for combining content from the appropriate regions of input images of highly non-rigid human subjects performing complex motions into a single implicit volumetric representation. This allows us to learn our 3D representation solely from videos of moving people. Armed with both 3D object understanding and end-to-end learned rendering, this categorically novel representation delivers state-of-the-art image generation quality, as shown by our quantitative and qualitative evaluations.",0
"The goal of human motion retargeting is to transfer the movements of one person in a ""driving"" video or set of images to another person. However, current methods require a long training video from each target person to create a subject-specific motion transfer model, making it difficult to scale. Recently, few-shot motion transfer techniques have gained attention, but they either sacrifice accurate geometric modeling or the flexibility of an end-to-end learned representation. To address these issues, we propose an approach based on an implicit volumetric representation of the image content, which can be spatially manipulated using volumetric flow fields. We tackle the challenge of aggregating information across different body poses, allowing us to learn our 3D representation solely from videos of moving people. Our novel representation delivers state-of-the-art image generation quality, as demonstrated by our quantitative and qualitative evaluations.",1
"Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 31.87 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256 \times 256$, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions.",0
"The Transformer is an example of an attention-based model that can model long-range dependencies effectively. However, the quadratic complexity of the self-attention operation makes it challenging to use for high-resolution image generation using GANs. To tackle this issue, we propose two solutions in this paper. Firstly, we suggest using multi-axis blocked self-attention instead of standard global self-attention in low-resolution stages, which allows for efficient mixing of local and global attention. Secondly, we remove self-attention in high-resolution stages and only keep multi-layer perceptrons similar to the implicit neural function. To further enhance the model's performance, we introduce a self-modulation component based on cross-attention. Our resulting model, called HiT, has a linear computational complexity with respect to the image size, making it ideal for synthesizing high definition images. Our experiments show that HiT achieves state-of-the-art FID scores of 31.87 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256 \times 256$, respectively, with reasonable throughput. We believe that HiT is a significant achievement for GAN generators that are entirely free of convolutions.",1
"Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks.",0
"The field of machine learning has seen a significant development in generative modeling. Unsupervised image generation has been successful with the use of Deep polynomial neural networks (PNNs), which map input vectors (noise) to synthesized images. However, PNNs have not been as successful in conditional generation tasks like super-resolution. This is because current PNNs only focus on single-variable polynomial expansions, which do not work well with two-variable inputs like noise and conditional variables. In this article, we present CoPE, a general framework that enables polynomial expansion of two input variables while capturing their auto- and cross-correlations. CoPE can easily be expanded to include more input variables. We evaluated CoPE in five tasks across eight datasets, including class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, and attribute-guided generation. The comprehensive evaluation shows that CoPE can be a valuable tool to tackle various conditional generation tasks.",1
"Although much progress has been made recently in 3D face reconstruction, most previous work has been devoted to predicting accurate and fine-grained 3D shapes. In contrast, relatively little work has focused on generating high-fidelity face textures. Compared with the prosperity of photo-realistic 2D face image generation, high-fidelity 3D face texture generation has yet to be studied. In this paper, we proposed a novel UV map generation model that predicts the UV map from a single face image. The model consists of a UV sampler and a UV generator. By selectively sampling the input face image's pixels and adjusting their relative locations, the UV sampler generates an incomplete UV map that could faithfully reconstruct the original face. Missing textures in the incomplete UV map are further full-filled by the UV generator. The training is based on pseudo ground truth blended by the 3DMM texture and the input face texture, thus weakly supervised. To deal with the artifacts in the imperfect pseudo UV map, multiple partial UV map discriminators are leveraged.",0
"Despite recent advancements in 3D face reconstruction, previous research has primarily focused on predicting precise and intricate 3D shapes, with little attention given to producing high-quality face textures. While 2D face image generation has flourished with photo-realistic results, generating high-fidelity 3D face textures has yet to be explored. This article introduces a new UV map generation model that anticipates the UV map from a single face image. The model comprises a UV sampler and a UV generator, which selectively samples the input face image's pixels and adjusts their relative positions to create an incomplete UV map that accurately reconstructs the original face. The incomplete UV map is then filled with the missing textures by the UV generator. The training of the model is weakly supervised, relying on pseudo ground truth blended by the 3DMM texture and the input face texture. To address the artifacts in the imperfect pseudo UV map, multiple partial UV map discriminators are utilized.",1
"Recent advances in image inpainting have shown impressive results for generating plausible visual details on rather simple backgrounds. However, for complex scenes, it is still challenging to restore reasonable contents as the contextual information within the missing regions tends to be ambiguous. To tackle this problem, we introduce pretext tasks that are semantically meaningful to estimating the missing contents. In particular, we perform knowledge distillation on pretext models and adapt the features to image inpainting. The learned semantic priors ought to be partially invariant between the high-level pretext task and low-level image inpainting, which not only help to understand the global context but also provide structural guidance for the restoration of local textures. Based on the semantic priors, we further propose a context-aware image inpainting model, which adaptively integrates global semantics and local features in a unified image generator. The semantic learner and the image generator are trained in an end-to-end manner. We name the model SPL to highlight its ability to learn and leverage semantic priors. It achieves the state of the art on Places2, CelebA, and Paris StreetView datasets.",0
"Although recent developments in image inpainting have produced impressive outcomes for generating realistic visual elements on simple backgrounds, restoring sensible content for complex scenes remains a challenge. This is due to the ambiguous contextual information present in the missing regions. To address this issue, we introduce semantically meaningful pretext tasks to estimate the missing contents. We perform knowledge distillation on pretext models and adapt the features to image inpainting. The learned semantic priors are partially invariant between the high-level pretext task and low-level image inpainting, which helps understand the global context, and provides structural guidance for restoring local textures. We propose a context-aware image inpainting model that adaptively integrates global semantics and local features in a unified image generator, based on the semantic priors. The semantic learner and the image generator are trained in an end-to-end manner. We call the model SPL, highlighting its ability to learn and leverage semantic priors, and it achieves the state of the art on Places2, CelebA, and Paris StreetView datasets.",1
"Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",0
"Generating high-dimensional images using conditional generative models has various applications, but obtaining supervision signals from conditions to images can be a costly process. The Diffusion-Decoding models with Contrastive representations (D2C) approach described in this paper offers a solution by training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C leverages a learned diffusion-based prior over the latent representations to enhance generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints using as few as 100 labeled examples. D2C outperforms state-of-the-art VAEs and diffusion models in conditional generation from new labels. Additionally, D2C generations for conditional image manipulation are much faster to produce and preferred by 50%-60% of human evaluators in a double-blind study over StyleGAN2 ones.",1
"Virtual try-on methods aim to generate images of fashion models wearing arbitrary combinations of garments. This is a challenging task because the generated image must appear realistic and accurately display the interaction between garments. Prior works produce images that are filled with artifacts and fail to capture important visual details necessary for commercial applications. We propose Outfit Visualization Net (OVNet) to capture these important details (e.g. buttons, shading, textures, realistic hemlines, and interactions between garments) and produce high quality multiple-garment virtual try-on images. OVNet consists of 1) a semantic layout generator and 2) an image generation pipeline using multiple coordinated warps. We train the warper to output multiple warps using a cascade loss, which refines each successive warp to focus on poorly generated regions of a previous warp and yields consistent improvements in detail. In addition, we introduce a method for matching outfits with the most suitable model and produce significant improvements for both our and other previous try-on methods. Through quantitative and qualitative analysis, we demonstrate our method generates substantially higher-quality studio images compared to prior works for multi-garment outfits. An interactive interface powered by this method has been deployed on fashion e-commerce websites and received overwhelmingly positive feedback.",0
"The main objective of virtual try-on techniques is to create images of fashion models wearing various combinations of clothing. This task is quite difficult since the images must look realistic and accurately depict how the different garments interact with each other. Previous attempts at generating such images have been plagued with errors and do not showcase the vital visual details that are necessary for commercial applications. Our proposed solution, the Outfit Visualization Net (OVNet), addresses these challenges by incorporating a semantic layout generator and an image generation pipeline that uses multiple coordinated warps to create high-quality virtual try-on images. We train the warper to output multiple warps using a cascade loss, which gradually improves the quality of the generated image by refining poorly generated regions. Additionally, we introduce a method for matching outfits with the most appropriate model, resulting in significant improvements over prior try-on methods. Our method has been widely deployed on fashion e-commerce websites and has received positive feedback for its ability to generate high-quality studio images of multi-garment outfits.",1
"Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases,such as image generation, OOD detection, and compositional generation.",0
"The method of training energy-based models known as contrastive divergence is widely used but has stability issues. To address this, we propose an adaptation that examines a challenging-to-calculate gradient term often omitted for convenience. Our research demonstrates the numerical significance of this gradient term and its importance in preventing training instabilities while remaining feasible to estimate. Additionally, we explore how data augmentation and multi-scale processing can enhance model robustness and generation quality. Our empirical evaluations reveal improved performance on various benchmarks and use cases, including image generation, OOD detection, and compositional generation, indicating enhanced model stability.",1
"Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. We study two types of noise processes, statistical image models and deep generative models under different random initializations. Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations. Datasets, models, and code are available at https://mbaradad.github.io/learning_with_noise.",0
"Massive datasets are utilized to train current vision systems, which are associated with significant costs such as curation expenses, human biases, and concerns regarding privacy and usage rights. To mitigate these expenses, there is an increasing interest in using low-cost data sources like unlabeled images. This study takes it one step further by exploring the possibility of dispensing with real image datasets altogether and instead learning from noise processes. The study examines a range of image generation models that generate images from basic random processes and uses them as training data for a visual representation learner with a contrastive loss. The study investigates two types of noise processes, statistical image models, and deep generative models with different random initializations. The results indicate that while it is crucial for the noise to capture particular structural properties of real data, good performance can still be achieved even with processes that are not realistic. Additionally, the study finds that diversity is a crucial feature for learning effective representations. The datasets, models, and code are available at https://mbaradad.github.io/learning_with_noise.",1
"Unsupervised domain adaptation for object detection is a challenging problem with many real-world applications. Unfortunately, it has received much less attention than supervised object detection. Models that try to address this task tend to suffer from a shortage of annotated training samples. Moreover, existing methods of feature alignments are not sufficient to learn domain-invariant representations. To address these limitations, we propose a novel augmented feature alignment network (AFAN) which integrates intermediate domain image generation and domain-adversarial training into a unified framework. An intermediate domain image generator is proposed to enhance feature alignments by domain-adversarial training with automatically generated soft domain labels. The synthetic intermediate domain images progressively bridge the domain divergence and augment the annotated source domain training data. A feature pyramid alignment is designed and the corresponding feature discriminator is used to align multi-scale convolutional features of different semantic levels. Last but not least, we introduce a region feature alignment and an instance discriminator to learn domain-invariant features for object proposals. Our approach significantly outperforms the state-of-the-art methods on standard benchmarks for both similar and dissimilar domain adaptations. Further extensive experiments verify the effectiveness of each component and demonstrate that the proposed network can learn domain-invariant representations.",0
"Object detection through unsupervised domain adaptation poses a complex challenge with immense practical implications. However, this area has not received as much attention as supervised object detection, partly due to a lack of annotated training samples that hinders the feature alignment of existing models. Consequently, we propose a novel augmented feature alignment network (AFAN) that incorporates domain-adversarial training and intermediate domain image generation into a unified framework. Our approach generates synthetic intermediate domain images that help bridge the domain divergence, augment source domain training data, and enhance feature alignments through domain-adversarial training with automatically generated soft domain labels. We also introduce feature pyramid alignment, region feature alignment, and instance discriminator to learn domain-invariant features for object proposals. Our method outperforms state-of-the-art techniques on standard benchmarks for both similar and dissimilar domain adaptations, and extensive experiments validate the efficacy of each component.",1
"Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple ""views"" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more ""model zoos"" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is released on our project page: https://ali-design.github.io/GenRep/",0
"The advancement of generative models has led to the production of highly realistic images that are almost indistinguishable from the original data. This development poses a question on the necessity of datasets if generative models are sufficiently capable. To explore this, we conducted a study on learning general-purpose visual representations from a black-box generative model rather than directly from data. We trained representations from the samples produced by an off-the-shelf image generator without access to its training data. We utilized the latent space of the generator to generate multiple ""views"" of the same semantic content and compared various representation learning methods that can be employed in this setting. We discovered that multiview data obtained from contrastive methods can be naturally used to identify positive and negative pairs. Our findings showed that the representations obtained were comparable to those learned directly from real data, but the sampling strategy applied and the training method used were critical for good performance. We anticipate a future where generative models will serve as a compressed and organized copy of datasets, replacing unwieldy, missing, or private data. This paper proposes several techniques for addressing visual representation learning in such a future, and the code is available on our project page: https://ali-design.github.io/GenRep/.",1
"The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1 for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.",0
"The aim of text-to-image synthesis systems is to produce high-quality scenes that are coherent, realistic, and accurately reflect the text description they are based on. To achieve this, our XMC-GAN system focuses on maximizing the mutual information between the image and the text. By utilizing multiple contrastive losses, we capture both inter-modality and intra-modality correspondences. Our system includes an attentional self-modulation generator, which enhances text-image correspondence, and a contrastive discriminator, which serves as a critic and feature encoder for contrastive learning. Compared to previous models, XMC-GAN produces superior results on three challenging datasets. Specifically, on MS-COCO, our system outperforms the state-of-the-art FID from 24.70 to 9.33, and is preferred by people for image quality and image-text alignment, compared to three other recent models. Additionally, XMC-GAN performs well on the Localized Narratives dataset, improving the state-of-the-art FID from 48.70 to 14.12. Finally, we demonstrate our system's strength on the difficult Open Images dataset, achieving a strong benchmark FID score of 26.91.",1
"As a generic modeling tool, Convolutional Neural Networks (CNNs) have been widely employed in image generation and translation tasks. However, when fed with a flat input, current CNN models may fail to generate vivid results due to the spatially shared convolution kernels. We call it the flatness degradation of CNNs. Unfortunately, such degradation is the greatest obstacles to generate a spatially-variant output from a flat input, which has been barely discussed in the previous literature. To tackle this problem, we propose a model agnostic solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in for any CNN generation model. The key idea is to break the flat input condition while keeping the intactness of the original information. Specifically, the NIB perturbs the input data symmetrically with a noise map and reassembles them in the feature domain as driven by the objective function. Extensive experiments show that existing CNN models equipped with NIB survive from the flatness degradation and are able to generate visually better results with richer details in some specific image generation tasks given flat inputs, e.g. semantic image synthesis, data-hidden image generation, and deep neural dithering.",0
"Convolutional Neural Networks (CNNs) are commonly used for image generation and translation tasks. However, when given a flat input, current CNN models may struggle to produce high-quality results due to the convolution kernels being shared spatially. This is known as the flatness degradation of CNNs, which has been largely ignored in previous literature. To overcome this challenge, we propose a model-agnostic solution called the Noise Incentive Block (NIB). This plug-in can be applied to any CNN generation model, and it perturbs the input data symmetrically with a noise map before reassembling them in the feature domain according to the objective function. Our experiments demonstrate that existing CNN models equipped with NIB can generate visually superior results with richer details in certain image generation tasks that involve flat inputs, such as semantic image synthesis, data-hidden image generation, and deep neural dithering.",1
"Generating videos predicting the future of a given sequence has been an area of active research in recent years. However, an essential problem remains unsolved: most of the methods require large computational cost and memory usage for training. In this paper, we propose a novel method for generating future prediction videos with less memory usage than the conventional methods. This is a critical stepping stone in the path towards generating videos with high image quality, similar to that of generated images in the latest works in the field of image generation. We achieve high-efficiency by training our method in two stages: (1) image reconstruction to encode video frames into latent variables, and (2) latent variable prediction to generate the future sequence. Our method achieves an efficient compression of video into low-dimensional latent variables by decomposing each frame according to its hierarchical structure. That is, we consider that video can be separated into background and foreground objects, and that each object holds time-varying and time-independent information independently. Our experiments show that the proposed method can efficiently generate future prediction videos, even for complex datasets that cannot be handled by previous methods.",0
"In recent years, there has been active research in generating videos that predict the future of a given sequence. However, a major issue that persists is the high computational cost and memory usage required for most of the existing methods. Therefore, this paper proposes a new approach for generating future prediction videos that uses less memory than conventional methods. This is a critical advancement towards generating videos of high image quality, similar to the latest works in image generation. Our method achieves high efficiency by training in two stages: first, image reconstruction encodes video frames into latent variables, and second, latent variable prediction generates the future sequence. Our approach compresses video into low-dimensional latent variables by decomposing each frame according to its hierarchical structure. Specifically, we separate video into background and foreground objects, with each object holding time-varying and time-independent information independently. Our experiments demonstrate that our proposed method can efficiently generate future prediction videos, even for complex datasets that previous methods cannot handle.",1
"We applied Deep Learning algorithm known as Generative Adversarial Networks (GANs) to perform solar image-to-image translation. That is, from Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager(HMI) line of sight magnetogram images to SDO/Atmospheric Imaging Assembly(AIA) 0304-{\AA} images. The Ultraviolet(UV)/Extreme Ultraviolet(EUV) observations like the SDO/AIA0304-{\AA} images were only made available to scientists in the late 1990s even though the magenetic field observations like the SDO/HMI have been available since the 1970s. Therefore by leveraging Deep Learning algorithms like GANs we can give scientists access to complete datasets for analysis. For generating high resolution solar images we use the Pix2PixHD and Pix2Pix algorithms. The Pix2PixHD algorithm was specifically designed for high resolution image generation tasks, and the Pix2Pix algorithm is by far the most widely used image to image translation algorithm. For training and testing we used the data for the year 2012, 2013 and 2014. The results show that our deep learning models are capable of generating high resolution(1024 x 1024 pixels) AIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson Correlation Coefficient of the images generated by Pix2PixHD and original images is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate images. The results we get for our Pix2PixHD model is better than the results obtained by previous works done by others to generate AIA0304 images. Thus, we can use these models to generate AIA0304 images when the AIA0304 data is not available which can be used for understanding space weather and giving researchers the capability to predict solar events such as Solar Flares and Coronal Mass Ejections. As far as we know, our work is the first attempt to leverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image translation.",0
"We utilized the Generative Adversarial Networks (GANs) Deep Learning algorithm to perform solar image-to-image translation, converting Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI) line of sight magnetogram images to SDO/Atmospheric Imaging Assembly (AIA) 0304-{\AA} images. Although magnetic field observations like SDO/HMI have been available since the 1970s, Ultraviolet (UV)/Extreme Ultraviolet (EUV) observations, such as SDO/AIA0304-{\AA} images, were only accessible to scientists in the late 1990s. By leveraging GANs, we can provide researchers with complete datasets for analysis. For high-resolution solar image generation, we used the Pix2PixHD and Pix2Pix algorithms. The former is specifically designed for high-resolution image generation tasks, while the latter is widely used for image-to-image translation. Our training and testing data spanned the years 2012-2014, and the results indicate that our models are capable of generating high-resolution (1024 x 1024 pixels) AIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson Correlation Coefficient of the images generated by Pix2PixHD and the original images is as high as 0.99, while this number is 0.962 if Pix2Pix is used. Our Pix2PixHD model outperforms previous works in generating AIA0304 images. Therefore, our models can be used to generate AIA0304 images in cases where the data is unavailable, aiding in the understanding of space weather and enabling the prediction of solar events such as Solar Flares and Coronal Mass Ejections. Our work is the first attempt to employ the Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image translation.",1
"Our work focuses on unsupervised and generative methods that address the following goals: (a) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (b) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (c) developing anomaly detection methods that leverage representations learned in (a). For (a), we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For (b), we derive an analytical result (Lemma 1) that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in our loss function, we then show empirically that, for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement trade space, when compared to other state of the art methods, with quality assessed via the Frechet Inception Distance (FID), and disentanglement via mutual information gap. For (c), we design several systems for anomaly detection exploiting representations learned in (a), and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. The above contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI.",0
"Our research is centered on unsupervised and generative techniques that aim to achieve three objectives. Firstly, we aim to learn unsupervised generative representations that uncover the latent factors that control image semantic attributes. Secondly, we aim to investigate how this ability to control attributes is related to the issue of disentangling latent factors. Lastly, we strive to create anomaly detection methods that utilize the representations learned in the first objective. To achieve the first objective, we propose a network architecture that combines multiscale generative models with mutual information maximization. To address the second objective, we introduce a concept clarification between the ability of generative networks to control semantic attributes and the ability to disentangle latent space representations. We show that the maximization of semantic attribute control leads to the disentanglement of latent factors. We then demonstrate through empirical evidence that our approach outperforms other state-of-the-art methods in terms of quality and disentanglement trade space. Furthermore, we design several systems for anomaly detection that leverage the representations learned in the first objective and show that they outperform other generative and discriminative algorithms. Our research in representation learning has potential applications in addressing other critical problems in computer vision, such as AI bias and privacy.",1
"Recently, AutoRegressive (AR) models for the whole image generation empowered by transformers have achieved comparable or even better performance to Generative Adversarial Networks (GANs). Unfortunately, directly applying such AR models to edit/change local image regions, may suffer from the problems of missing global information, slow inference speed, and information leakage of local guidance. To address these limitations, we propose a novel model -- image Local Autoregressive Transformer (iLAT), to better facilitate the locally guided image synthesis. Our iLAT learns the novel local discrete representations, by the newly proposed local autoregressive (LA) transformer of the attention mask and convolution mechanism. Thus iLAT can efficiently synthesize the local image regions by key guidance information. Our iLAT is evaluated on various locally guided image syntheses, such as pose-guided person image synthesis and face editing. Both the quantitative and qualitative results show the efficacy of our model.",0
"Recently, AR models enhanced by transformers for generating entire images have demonstrated comparable or superior performance to GANs. However, applying these AR models directly to modify local image regions can encounter issues such as missing global information, sluggish inference speed, and local guidance information leakage. To overcome these limitations, we present a new model called image Local Autoregressive Transformer (iLAT) that facilitates locally guided image synthesis. Our iLAT learns novel local discrete representations via a new local autoregressive (LA) transformer that incorporates an attention mask and convolution mechanism. This enables iLAT to efficiently synthesize local image regions using key guidance information. We evaluate our iLAT on a range of locally guided image synthesis tasks, including pose-guided person image synthesis and face editing, and demonstrate its effectiveness through both quantitative and qualitative results.",1
"Despite the recent popularity of neural network-based solvers for optimal transport (OT), there is no standard quantitative way to evaluate their performance. In this paper, we address this issue for quadratic-cost transport -- specifically, computation of the Wasserstein-2 distance, a commonly-used formulation of optimal transport in machine learning. To overcome the challenge of computing ground truth transport maps between continuous measures needed to assess these solvers, we use input-convex neural networks (ICNN) to construct pairs of measures whose ground truth OT maps can be obtained analytically. This strategy yields pairs of continuous benchmark measures in high-dimensional spaces such as spaces of images. We thoroughly evaluate existing optimal transport solvers using these benchmark measures. Even though these solvers perform well in downstream tasks, many do not faithfully recover optimal transport maps. To investigate the cause of this discrepancy, we further test the solvers in a setting of image generation. Our study reveals crucial limitations of existing solvers and shows that increased OT accuracy does not necessarily correlate to better results downstream.",0
"The popularity of neural network-based solvers for optimal transport (OT) has increased recently, but there is currently no standard method for evaluating their performance. To address this issue for quadratic-cost transport, we focus on computing the Wasserstein-2 distance, which is commonly used in machine learning. To overcome the challenge of computing ground truth transport maps between continuous measures, we use input-convex neural networks (ICNN) to construct pairs of measures whose ground truth OT maps can be obtained analytically. This approach enables us to create pairs of continuous benchmark measures in high-dimensional spaces, such as image spaces, and thoroughly evaluate existing optimal transport solvers using these benchmarks. Although these solvers perform well in downstream tasks, many of them do not accurately recover optimal transport maps. To investigate the reasons for this discrepancy, we test the solvers in an image generation setting. Our study reveals important limitations of existing solvers and demonstrates that increased OT accuracy does not necessarily lead to improved downstream results.",1
"Fooling people with highly realistic fake images generated with Deepfake or GANs brings a great social disturbance to our society. Many methods have been proposed to detect fake images, but they are vulnerable to adversarial perturbations -- intentionally designed noises that can lead to the wrong prediction. Existing methods of attacking fake image detectors usually generate adversarial perturbations to perturb almost the entire image. This is redundant and increases the perceptibility of perturbations. In this paper, we propose a novel method to disrupt the fake image detection by determining key pixels to a fake image detector and attacking only the key pixels, which results in the $L_0$ and the $L_2$ norms of adversarial perturbations much less than those of existing works. Experiments on two public datasets with three fake image detectors indicate that our proposed method achieves state-of-the-art performance in both white-box and black-box attacks.",0
"The use of Deepfake or GANs to deceive people with extremely realistic fake images causes significant social unrest in our society. Although various techniques have been suggested to identify phony images, they are susceptible to adversarial perturbations. These perturbations, deliberately created noises that can result in incorrect predictions, can be used to attack fake image detectors. Current methods of attacking these detectors typically involve perturbing almost the entire image with adversarial perturbations, which is unnecessary and can make the perturbations more noticeable. In this paper, we introduce a new approach that targets only key pixels for a fake image detector and attacks those pixels only. This results in adversarial perturbations with much lower L0 and L2 norms than those of existing methods. Our experiments on two public datasets with three fake image detectors demonstrate that our proposed method achieves state-of-the-art performance in both white-box and black-box attacks.",1
"Controllable person image generation aims to produce realistic human images with desirable attributes (e.g., the given pose, cloth textures or hair style). However, the large spatial misalignment between the source and target images makes the standard architectures for image-to-image translation not suitable for this task. Most of the state-of-the-art architectures avoid the alignment step during the generation, which causes many artifacts, especially for person images with complex textures. To solve this problem, we introduce a novel Spatially-Adaptive Warped Normalization (SAWN), which integrates a learned flow-field to warp modulation parameters. This allows us to align person spatial-adaptive styles with pose features efficiently. Moreover, we propose a novel self-training part replacement strategy to refine the pretrained model for the texture-transfer task, significantly improving the quality of the generated cloth and the preservation ability of irrelevant regions. Our experimental results on the widely used DeepFashion dataset demonstrate a significant improvement of the proposed method over the state-of-the-art methods on both pose-transfer and texture-transfer tasks. The source code is available at https://github.com/zhangqianhui/Sawn.",0
"The objective of controllable person image generation is to create realistic human images with desired attributes such as pose, clothing texture, and hair style. However, due to the considerable spatial misalignment between source and target images, standard architectures for image-to-image translation are not appropriate for this task. Most state-of-the-art architectures avoid the alignment step during generation, which results in numerous artifacts, especially for person images with complex textures. To address this issue, we propose a novel approach called Spatially-Adaptive Warped Normalization (SAWN), which employs a learned flow-field to warp modulation parameters. This enables us to efficiently align person spatial-adaptive styles with pose features. Additionally, we introduce a self-training part replacement strategy to refine the pretrained model for the texture-transfer task, resulting in significant improvements in the quality of generated clothing and the preservation ability of irrelevant regions. Our experiments on the widely used DeepFashion dataset demonstrate that our proposed method outperforms state-of-the-art methods on both pose-transfer and texture-transfer tasks. The source code for our method is available at https://github.com/zhangqianhui/Sawn.",1
"In this paper, we propose Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGN employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution. MGVAE constructs a hierarchical generative model based on MGN to variationally autoencode the hierarchy of coarsened graphs. Our proposed framework is end-to-end permutation equivariant with respect to node ordering. Our methods have been successful with several generative tasks including link prediction on citation graphs, unsupervised molecular representation learning to predict molecular properties, molecular generation, general graph generation and graph-based image generation.",0
"The Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) are introduced in this paper as a means to learn and generate graphs in a multiresolution and equivariant manner. MGN implements higher order message passing to encode the graph while partitioning it into mutually exclusive clusters and coarsening it into a lower resolution at each level of resolution. On the other hand, MGVAE utilizes MGN to develop a hierarchical generative model that can variationally autoencode the hierarchy of coarsened graphs. Our framework is end-to-end permutation equivariant with respect to node ordering. Our techniques have demonstrated success in various generative tasks, such as link prediction on citation graphs, unsupervised molecular representation learning for predicting molecular properties, molecular generation, general graph generation, and graph-based image generation.",1
"The objective of person re-identification (re-ID) is to retrieve a person's images from an image gallery, given a single instance of the person of interest. Despite several advancements, learning discriminative identity-sensitive and viewpoint invariant features for robust Person Re-identification is a major challenge owing to the large pose variation of humans. This paper proposes a re-ID pipeline that utilizes the image generation capability of Generative Adversarial Networks combined with pose clustering and feature fusion to achieve pose invariant feature learning. The objective is to model a given person under different viewpoints and large pose changes and extract the most discriminative features from all the appearances. The pose transformational GAN (pt-GAN) module is trained to generate a person's image in any given pose. In order to identify the most significant poses for discriminative feature extraction, a Pose Clustering module is proposed. The given instance of the person is modelled in varying poses and these features are effectively combined through the Feature Fusion Network. The final re-ID model consisting of these 3 sub-blocks, alleviates the pose dependence in person re-ID. Also, The proposed model is robust to occlusion, scale, rotation and illumination, providing a framework for viewpoint invariant feature learning. The proposed method outperforms the state-of-the-art GAN based models in 4 benchmark datasets. It also surpasses the state-of-the-art models that report higher re-ID accuracy in terms of improvement over baseline.",0
"Person re-identification (re-ID) aims to retrieve a person's images from a gallery, given a single instance of the person. Despite advancements, learning identity-sensitive and viewpoint invariant features for robust Person Re-identification is a challenge due to humans' large pose variation. This paper proposes a re-ID pipeline that combines Generative Adversarial Networks' image generation capability with pose clustering and feature fusion to achieve pose invariant feature learning. The pose transformational GAN (pt-GAN) module generates a person's image in any given pose. To identify significant poses for discriminative feature extraction, a Pose Clustering module is proposed. The Feature Fusion Network effectively combines features from varying poses, creating a final re-ID model that alleviates pose dependence. The proposed method outperforms state-of-the-art GAN based models in 4 benchmark datasets and surpasses models reporting higher re-ID accuracy in terms of baseline improvement. Additionally, the proposed model is robust to occlusion, scale, rotation, and illumination, providing a framework for viewpoint invariant feature learning.",1
"Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at 67%-74% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator play a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",0
"The use of deep generative adversarial networks (GANs) has become increasingly popular in many situations. However, their high parameter complexity makes them challenging to use in resource-constrained real-world applications. Despite this issue, GAN compression has not been thoroughly explored. Previous attempts to apply compression techniques have yielded unsatisfactory results due to the instability of GAN training. In contrast, the lottery ticket hypothesis has been successful in locating sparse subnetworks that can achieve full model performance in discriminative models. This study investigates the existence of such trainable subnetworks in deep GANs for the first time. The results show that matching subnetworks at 67%-74% sparsity can be found in various GANs. The discriminator's pruning has little impact on the quality and existence of matching subnetworks, but the initialization weights play a significant role. Furthermore, the study demonstrates the transferability of these subnetworks to new tasks. The experimental results show that the found subnetworks outperform current state-of-the-art GAN compression methods in both image generation and image-to-image translation GANs. The code is available at https://github.com/VITA-Group/GAN-LTH.",1
"Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.",0
"Deep feature representation for visual tasks has increasingly relied on attention mechanisms, particularly self-attention. Self-attention enhances features by calculating weighted sums of features at each position, utilizing pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention is limited by its quadratic complexity and disregard for potential correlation between different samples. This study proposes an innovative attention mechanism called external attention, which utilizes two external, small, learnable, shared memories. External attention can be easily implemented using two cascaded linear layers and two normalization layers, and can conveniently replace self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. The multi-head mechanism is further incorporated into external attention to develop an all-MLP architecture, external attention MLP (EAMLP), for image classification. Comprehensive experiments on various visual tasks, including image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis, demonstrate that our approach provides results comparable or superior to the self-attention mechanism and its variants, with significantly lower computational and memory costs.",1
"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS COCO, outperforms previous GAN-based models and a recent similar work DALL-E.",0
"Generating images from text in the general domain has been a challenge for a long time, requiring a robust generative model and cross-modal understanding. To tackle this problem, we present CogView, a Transformer with VQ-VAE tokenizer containing 4 billion parameters. Additionally, we showcase various finetuning approaches for downstream tasks such as style learning, super-resolution, text-image ranking, and fashion design. We also introduce methods to enhance pretraining stability, such as removing NaN losses. CogView (zero-shot) sets a new benchmark in FID for blurred MS COCO, surpassing previous GAN-based models and a recent comparable work, DALL-E.",1
"We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, designed to generate high-quality synthetic images for research in computer vision and deep learning. Our tool enables the description and manipulation of complex dynamic 3D scenes containing object meshes, materials, textures, lighting, volumetric data (e.g., smoke), and backgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors, can also be generated. In this work, we discuss design goals, architecture, and performance. We demonstrate the use of data generated by path tracing for training an object detector and pose estimator, showing improved performance in sim-to-real transfer in situations that are difficult for traditional raster-based renderers. We offer this tool as an easy-to-use, performant, high-quality renderer for advancing research in synthetic data generation and deep learning.",0
"Our Python-based renderer, utilizing NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, has been developed for the purpose of generating top-notch synthetic images to facilitate research in the fields of computer vision and deep learning. Our renderer is empowered to handle intricate and dynamic 3D scenes that are replete with object meshes, lighting, textures, volumetric data (e.g., smoke), and backgrounds. Additionally, we are able to produce metadata such as material properties, 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, and optical flow vectors. In this study, we delve into our design goals, architecture, and performance. We showcase how the use of data derived from path tracing has improved the effectiveness of object detection and pose estimation in sim-to-real transfer scenarios that are challenging for traditional raster-based renderers. We are proud to offer our tool as a high-quality, high-performance, and user-friendly renderer that can further augment research in synthetic data generation and deep learning.",1
"Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN's discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN's stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on an unconditional image generation task, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores and FID.",0
"The Generative adversarial network (GAN) has emerged as a crucial neural network model for unsupervised machine learning. Various discriminator loss functions have been developed to train the GAN's discriminators, all of which follow a similar structure - a summation of real and fake losses that only depends on actual and generated data, correspondingly. However, the problem with equally weighted losses is that training may benefit one loss while harming the other, leading to instability and mode collapse. In this study, we present a new collection of discriminator loss functions called adaptive weighted loss functions or aw-loss functions, which utilize a weighted sum of real and fake parts. By using the gradients of the loss's real and fake parts, we can adjust weights to train the discriminator in a manner that improves GAN stability. Our method can be applied to any discriminator model with a loss that is a summation of real and fake parts. Our experiments demonstrate the effectiveness of our loss functions in enhancing baseline results for unconditional image generation tasks on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores and FID.",1
"Existing vision-based action recognition is susceptible to occlusion and appearance variations, while wearable sensors can alleviate these challenges by capturing human motion with one-dimensional time-series signal. For the same action, the knowledge learned from vision sensors and wearable sensors, may be related and complementary. However, there exists significantly large modality difference between action data captured by wearable-sensor and vision-sensor in data dimension, data distribution and inherent information content. In this paper, we propose a novel framework, named Semantics-aware Adaptive Knowledge Distillation Networks (SAKDN), to enhance action recognition in vision-sensor modality (videos) by adaptively transferring and distilling the knowledge from multiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher modalities and uses RGB videos as student modality. To preserve local temporal relationship and facilitate employing visual deep learning model, we transform one-dimensional time-series signals of wearable sensors to two-dimensional images by designing a gramian angular field based virtual image generation model. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion Module to adaptively fuse intermediate representation knowledge from different teacher networks. Finally, to fully exploit and transfer the knowledge of multiple well-trained teacher networks to the student network, we propose a novel Graph-guided Semantically Discriminative Mapping loss, which utilizes graph-guided ablation analysis to produce a good visual explanation highlighting the important regions across modalities and concurrently preserving the interrelations of original data. Experimental results on Berkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness of our proposed SAKDN.",0
"Current vision-based action recognition is vulnerable to occlusions and changes in appearance. Wearable sensors can help overcome these issues by capturing human motion through one-dimensional time-series signals. The knowledge derived from both vision and wearable sensors may be complementary, but there are significant differences in data dimension, distribution, and information content. In this study, we introduce a new method called Semantics-aware Adaptive Knowledge Distillation Networks (SAKDN) to improve action recognition in vision-based sensors (videos) by transferring and distilling knowledge from multiple wearable sensors. SAKDN employs multiple wearable sensors as teacher modalities and RGB videos as a student modality. To ensure the preservation of local temporal relationships and facilitate the use of visual deep learning models, we transform wearable sensor signals into two-dimensional images using a gramian angular field-based virtual image generation model. We then build a Similarity-Preserving Adaptive Multi-modal Fusion Module to adaptively fuse intermediate representation knowledge from different teacher networks. Finally, we propose a Graph-guided Semantically Discriminative Mapping loss to fully exploit and transfer the knowledge from multiple teacher networks to the student network. This method utilizes graph-guided ablation analysis to produce a visual explanation that highlights the significant regions across modalities while preserving the interrelations of original data. Our experimental results on Berkeley-MHAD, UTD-MHAD, and MMAct datasets demonstrate the effectiveness of our proposed SAKDN.",1
"Deep learning models are widely used for image analysis. While they offer high performance in terms of accuracy, people are concerned about if these models inappropriately make inferences using irrelevant features that are not encoded from the target object in a given image. To address the concern, we propose a metamorphic testing approach that assesses if a given inference is made based on irrelevant features. Specifically, we propose two novel metamorphic relations to detect such inappropriate inferences. We applied our approach to 10 image classification models and 10 object detection models, with three large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the top-5 correct predictions made by the image classification models are subject to inappropriate inferences using irrelevant features. The corresponding rate for the object detection models is over 8.5%. Based on the findings, we further designed a new image generation strategy that can effectively attack existing models. Comparing with a baseline approach, our strategy can double the success rate of attacks.",0
"Although deep learning models are highly accurate in image analysis, there is a concern that they may draw inappropriate inferences based on irrelevant features not related to the target object in the image. To address this concern, we propose a metamorphic testing approach that evaluates whether such inferences are made. We introduce two new metamorphic relations to identify such inferences and apply our approach to 10 image classification and 10 object detection models using ImageNet, COCO, and Pascal VOC datasets. Our results show that over 5.3% of top-5 correct predictions made by image classification models and over 8.5% by object detection models involve inappropriate inferences. Based on these findings, we develop a new image generation strategy that can effectively attack existing models, doubling the success rate compared to a baseline approach.",1
"Malicious application of deepfakes (i.e., technologies can generate target faces or face attributes) has posed a huge threat to our society. The fake multimedia content generated by deepfake models can harm the reputation and even threaten the property of the person who has been impersonated. Fortunately, the adversarial watermark could be used for combating deepfake models, leading them to generate distorted images. The existing methods require an individual training process for every facial image, to generate the adversarial watermark against a specific deepfake model, which are extremely inefficient. To address this problem, we propose a universal adversarial attack method on deepfake models, to generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark) that can protect thousands of facial images from multiple deepfake models. Specifically, we first propose a cross-model universal attack pipeline by attacking multiple deepfake models and combining gradients from these models iteratively. Then we introduce a batch-based method to alleviate the conflict of adversarial watermarks generated by different facial images. Finally, we design a more reasonable and comprehensive evaluation method for evaluating the effectiveness of the adversarial watermark. Experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by deepfake models and successfully protect facial images from deepfakes in real scenes.",0
"The misuse of deepfake technology, which can create convincing fake faces and attributes, presents a significant danger to our society. These fabricated multimedia materials can ruin a person's reputation and even put their property at risk. However, there is hope in fighting against such deepfake models through the use of an adversarial watermark, which can cause the models to produce distorted images. Unfortunately, the current approach to creating these watermarks is highly inefficient, requiring extensive training for each facial image. To solve this issue, we propose a universal adversarial attack method that can produce a Cross-Model Universal Adversarial Watermark (CMUA-Watermark) to protect thousands of facial images from multiple deepfake models. Our approach involves attacking multiple models and combining their gradients iteratively, as well as using a batch-based method to address conflicts between different adversarial watermarks. We also introduce a more comprehensive and reasonable evaluation method to assess the effectiveness of the watermark. Our experiments demonstrate that the CMUA-Watermark can successfully distort fake facial images generated by deepfake models and protect real facial images from deepfakes.",1
"Noise injection has been proved to be one of the key technique advances in generating high-fidelity images. Despite its successful usage in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on the geodesic normal coordinates. Guided by our theories, we find that the existing method is incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.",0
"The utilization of noise injection has been shown to be a significant advancement in producing high-quality images. Despite its effectiveness in GANs, its validity remains uncertain. This research introduces a geometric framework to analyze the function of noise injection in GANs. By utilizing Riemannian geometry, the noise injection framework is modeled as fuzzy equivalence on the geodesic normal coordinates. Our findings suggest that the current approach is insufficient and a new noise injection strategy is developed. Our experiments on image generation and GAN inversion support the superiority of our approach.",1
"Generative adversarial networks (GANs) are a class of generative models with two antagonistic neural networks: a generator and a discriminator. These two neural networks compete against each other through an adversarial process that can be modeled as a stochastic Nash equilibrium problem. Since the associated training process is challenging, it is fundamental to design reliable algorithms to compute an equilibrium. In this paper, we propose a stochastic relaxed forward-backward (SRFB) algorithm for GANs and we show convergence to an exact solution when an increasing number of data is available. We also show convergence of an averaged variant of the SRFB algorithm to a neighborhood of the solution when only few samples are available. In both cases, convergence is guaranteed when the pseudogradient mapping of the game is monotone. This assumption is among the weakest known in the literature. Moreover, we apply our algorithm to the image generation problem.",0
"GANs are generative models that utilize two neural networks, a generator and a discriminator, in an adversarial process that can be represented as a stochastic Nash equilibrium problem. Due to the difficulty of training these models, it is crucial to develop dependable algorithms to compute an equilibrium. This paper introduces the stochastic relaxed forward-backward (SRFB) algorithm for GANs, which demonstrates convergence to an exact solution as the number of available data increases. Furthermore, when only a few samples are available, an averaged variant of the SRFB algorithm converges to a neighborhood of the solution. In both scenarios, convergence is ensured when the game's pseudogradient mapping is monotone, which is one of the weakest assumptions in the literature. Lastly, our algorithm is applied to image generation.",1
"Intrinsic Image Decomposition is an open problem of generating the constituents of an image. Generating reflectance and shading from a single image is a challenging task specifically when there is no ground truth. There is a lack of unsupervised learning approaches for decomposing an image into reflectance and shading using a single image. We propose a neural network architecture capable of this decomposition using physics-based parameters derived from the image. Through experimental results, we show that (a) the proposed methodology outperforms the existing deep learning-based IID techniques and (b) the derived parameters improve the efficacy significantly. We conclude with a closer analysis of the results (numerical and example images) showing several avenues for improvement.",0
"The problem of generating the components of an image, known as Intrinsic Image Decomposition, remains unsolved. It is particularly difficult to generate the reflectance and shading of an image from a single source when there is no established truth. The absence of unsupervised learning techniques for decomposing an image into reflectance and shading using one single image is notable. Our proposed neural network architecture can perform this decomposition using physics-based parameters extracted from the image. Our experimental results demonstrate that our methodology outperforms existing deep learning-based Intrinsic Image Decomposition techniques, and that the extracted parameters significantly improve efficacy. We conclude by examining the results in greater detail, including numerical and example images, and identifying several areas for further improvement.",1
"Contact pressure between the human body and its surroundings has important implications. For example, it plays a role in comfort, safety, posture, and health. We present a method that infers contact pressure between a human body and a mattress from a depth image. Specifically, we focus on using a depth image from a downward facing camera to infer pressure on a body at rest in bed occluded by bedding, which is directly applicable to the prevention of pressure injuries in healthcare. Our approach involves augmenting a real dataset with synthetic data generated via a soft-body physics simulation of a human body, a mattress, a pressure sensing mat, and a blanket. We introduce a novel deep network that we trained on an augmented dataset and evaluated with real data. The network contains an embedded human body mesh model and uses a white-box model of depth and pressure image generation. Our network successfully infers body pose, outperforming prior work. It also infers contact pressure across a 3D mesh model of the human body, which is a novel capability, and does so in the presence of occlusion from blankets.",0
"The pressure that exists between a person's body and their environment has significant implications, including comfort, safety, posture, and health. Our study introduces a technique for estimating the contact pressure between a person's body and a mattress using a depth image. Our focus is on using a downward-facing camera to determine the pressure on a body at rest in bed, even when it is covered by bedding. This technique can be used to prevent pressure injuries in healthcare. We combined real and synthetic data to create a dataset, which we used to train a unique deep network that includes a model of a human body mesh. The network uses a white-box model of depth and pressure image generation to infer body pose and contact pressure across a 3D mesh model of the human body. Our technique outperformed previous methods and was able to accurately estimate contact pressure even when occluded by blankets.",1
"There has been a rise in the use of Machine Learning as a Service (MLaaS) Vision APIs as they offer multiple services including pre-built models and algorithms, which otherwise take a huge amount of resources if built from scratch. As these APIs get deployed for high-stakes applications, it's very important that they are robust to different manipulations. Recent works have only focused on typical adversarial attacks when evaluating the robustness of vision APIs. We propose two new aspects of adversarial image generation methods and evaluate them on the robustness of Google Cloud Vision API's optical character recognition service and object detection APIs deployed in real-world settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and Microsoft Azure's Computer Vision API. Specifically, we go beyond the conventional small-noise adversarial attacks and introduce secret embedding and transparent adversarial examples as a simpler way to evaluate robustness. These methods are so straightforward that even non-specialists can craft such attacks. As a result, they pose a serious threat where APIs are used for high-stakes applications. Our transparent adversarial examples successfully evade state-of-the art object detections APIs such as Azure Cloud Vision (attack success rate 52%) and Google Cloud Vision (attack success rate 36%). 90% of the images have a secret embedded text that successfully fools the vision of time-limited humans but is detected by Google Cloud Vision API's optical character recognition. Complementing to current research, our results provide simple but unconventional methods on robustness evaluation.",0
"Machine Learning as a Service (MLaaS) Vision APIs are increasingly popular due to their ability to provide pre-built models and algorithms, saving resources that would otherwise be used to build them from scratch. However, as these APIs are being deployed for high-stakes applications, it is essential that they are robust to various manipulations. Current studies have only focused on typical adversarial attacks when assessing the APIs' robustness. To address this gap, we propose two new adversarial image generation methods and evaluate them on the Google Cloud Vision API's optical character recognition service and object detection APIs in real-world settings. Our methods go beyond conventional small-noise adversarial attacks and introduce secret embedding and transparent adversarial examples, providing a simpler way to evaluate robustness. These methods are so easy to use that even non-specialists can create such attacks, posing a significant threat in high-stakes applications. Our transparent adversarial examples successfully evade state-of-the-art object detection APIs such as Azure Cloud Vision (attack success rate 52%) and Google Cloud Vision (attack success rate 36%). Additionally, 90% of the images have a secret embedded text that fools human vision but is detected by the Google Cloud Vision API's optical character recognition. Our results provide unconventional but straightforward methods for evaluating robustness, complementing current research.",1
"The content based image retrieval aims to find the similar images from a large scale dataset against a query image. Generally, the similarity between the representative features of the query image and dataset images is used to rank the images for retrieval. In early days, various hand designed feature descriptors have been investigated based on the visual cues such as color, texture, shape, etc. that represent the images. However, the deep learning has emerged as a dominating alternative of hand-designed feature engineering from a decade. It learns the features automatically from the data. This paper presents a comprehensive survey of deep learning based developments in the past decade for content based image retrieval. The categorization of existing state-of-the-art methods from different perspectives is also performed for greater understanding of the progress. The taxonomy used in this survey covers different supervision, different networks, different descriptor type and different retrieval type. A performance analysis is also performed using the state-of-the-art methods. The insights are also presented for the benefit of the researchers to observe the progress and to make the best choices. The survey presented in this paper will help in further research progress in image retrieval using deep learning.",0
"The objective of content-based image retrieval is to locate similar images in a vast dataset in response to a query image. Typically, the images in the dataset are ranked for retrieval based on the similarity between the representative features of the query image and the dataset images. In the past, various feature descriptors that were manually designed based on visual cues such as color, texture, shape, etc. were studied. However, in the last decade, deep learning has emerged as a dominant alternative for feature engineering, as it can learn features from data automatically. This paper presents a comprehensive survey of deep learning-based advancements in content-based image retrieval over the past decade. The survey categorizes existing state-of-the-art methods from various perspectives, including different supervision, different networks, different descriptor types, and different retrieval types, to provide a better understanding of the progress. The paper also performs a performance analysis using state-of-the-art methods and presents insights to aid researchers in observing the progress and making informed choices. This survey will be valuable for further research into image retrieval using deep learning.",1
"Generative Adversarial Networks (GANs) have made great success in synthesizing high-quality images. However, how to steer the generation process of a well-trained GAN model and customize the output image is much less explored. It has been recently found that modulating the input latent code used in GANs can reasonably alter some variation factors in the output image, but such manipulation usually presents to change the entire image as a whole. In this work, we propose an effective approach, termed as LoGAN, to support local editing of the output image. Concretely, we introduce two operators, i.e., content modulation and style modulation, together with a priority mask to facilitate the precise control of the intermediate generative features. Taking bedroom synthesis as an instance, we are able to seamlessly remove, insert, shift, and rotate the individual objects inside a room. Furthermore, our method can completely clear out a room and then refurnish it with customized furniture and styles. Experimental results show the great potentials of steering the image generation of pre-trained GANs for versatile image editing.",0
"GANs have achieved significant progress in generating high-quality images. However, there is limited exploration on how to direct the generation process of a well-trained GAN model and personalize the output image. Recent studies have revealed that modulating the input latent code in GANs can modify some variation factors in the output image, but this manipulation typically alters the entire image. This research proposes an effective solution, called LoGAN, to enable local editing of the output image. The approach introduces two operators, namely content modulation and style modulation, along with a priority mask to enable precise control of the intermediate generative features. Using bedroom synthesis as an example, the method can seamlessly remove, insert, shift, and rotate individual objects in a room. Moreover, it can clear out a room and refurnish it with custom furniture and styles. The experimental results demonstrate the potential of steering pre-trained GANs for versatile image editing.",1
"Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic images. However, despite current success, it still faces important challenges when applied to small domains. Existing methods use transfer learning for I2I translation, but they still require the learning of millions of parameters from scratch. This drawback severely limits its application on small domains. In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we propose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former finetunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxiliary GAN that further facilitates the training of deep I2I systems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID improves on several datasets with over 25 points.",0
"In recent years, image-to-image (I2I) translation has made significant progress and can now generate lifelike images of high quality. However, it still faces significant challenges when applied to smaller domains. Current transfer learning methods for I2I translation require the learning of millions of parameters from scratch, limiting their usefulness in small domains. To overcome this limitation, we propose a new transfer learning method for I2I translation (TransferI2I) that separates the learning process into two steps: image generation and I2I translation. We introduce two new techniques for image generation: source-target initialization and self-initialization of the adaptor layer. These techniques provide better initialization for the I2I translation step. We also introduce an auxiliary GAN for training deep I2I systems from small datasets. Our experiments on Animal faces, Birds, and Foods datasets show that TransferI2I outperforms existing methods, with mFID improving by over 25 points on several datasets.",1
"Image content is a predominant factor in marketing campaigns, websites and banners. Today, marketers and designers spend considerable time and money in generating such professional quality content. We take a step towards simplifying this process using Generative Adversarial Networks (GANs). We propose a simple and novel conditioning strategy which allows generation of images conditioned on given semantic attributes using a generator trained for an unconditional image generation task. Our approach is based on modifying latent vectors, using directional vectors of relevant semantic attributes in latent space. Our method is designed to work with both discrete (binary and multi-class) and continuous image attributes. We show the applicability of our proposed approach, named Directional GAN, on multiple public datasets, with an average accuracy of 86.4% across different attributes.",0
"Marketing campaigns, websites, and banners heavily rely on image content. As a result, marketers and designers invest a significant amount of time and money in producing high-quality visuals. To streamline this process, we introduce the use of Generative Adversarial Networks (GANs). Our approach simplifies the image generation task by implementing a novel conditioning strategy that utilizes semantic attributes. By modifying latent vectors with directional vectors of relevant semantic attributes in latent space, our generator can create images that align with the given attributes. Our method works with both discrete and continuous image attributes. We demonstrate the effectiveness of our approach, called Directional GAN, on various public datasets, achieving an average accuracy of 86.4% across different attributes.",1
"Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN.",0
"Restoring faces that are severely degraded in the wild is a highly challenging task. Directly training a deep neural network (DNN) is not effective due to the complexity and severity of the problem. Generative adversarial network (GAN) based methods are better but tend to produce over-smoothed restorations. To address this, we propose a new method that involves learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder. We then fine-tune the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to control the global face structure, local face details, and background of the reconstructed image. Our proposed GAN prior embedded network (GPEN) is easy-to-implement and generates visually photo-realistic results. Our experiments showed that GPEN outperforms state-of-the-art BFR methods both quantitatively and qualitatively, especially for restoring severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN.",1
"Face recognition is a popular and well-studied area with wide applications in our society. However, racial bias had been proven to be inherent in most State Of The Art (SOTA) face recognition systems. Many investigative studies on face recognition algorithms have reported higher false positive rates of African subjects cohorts than the other cohorts. Lack of large-scale African face image databases in public domain is one of the main restrictions in studying the racial bias problem of face recognition. To this end, we collect a face image database namely CASIA-Face-Africa which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras are utilized to capture the face images under various illumination settings. Demographic attributes and facial expressions of the subjects are also carefully recorded. For landmark detection, each face image in the database is manually labeled with 68 facial keypoints. A group of evaluation protocols are constructed according to different applications, tasks, partitions and scenarios. The performances of SOTA face recognition algorithms without re-training are reported as baselines. The proposed database along with its face landmark annotations, evaluation protocols and preliminary results form a good benchmark to study the essential aspects of face biometrics for African subjects, especially face image preprocessing, face feature analysis and matching, facial expression recognition, sex/age estimation, ethnic classification, face image generation, etc. The database can be downloaded from our http://www.cripacsir.cn/dataset/",0
"Although face recognition is widely used in society and has been extensively studied, most State Of The Art (SOTA) systems exhibit inherent racial bias, as studies have shown that false positive rates are higher for African subjects than for other cohorts. The lack of large-scale African face image databases in the public domain has been a major barrier to studying the issue of racial bias in face recognition. To address this, we have created a face image database called CASIA-Face-Africa, which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras were used to capture the images under different lighting conditions, and demographic attributes and facial expressions were carefully documented. Each image was manually labeled with 68 facial keypoints for landmark detection, and evaluation protocols were developed for different applications, tasks, partitions, and scenarios. The performances of SOTA face recognition algorithms were used as baselines without re-training. This database provides a valuable benchmark for studying various aspects of face biometrics for African subjects, including image preprocessing, feature analysis and matching, expression recognition, sex/age estimation, ethnic classification, and image generation. Access to the database is available at http://www.cripacsir.cn/dataset/.",1
"Generative Adversarial Networks have recently shown promise for video generation, building off of the success of image generation while also addressing a new challenge: time. Although time was analyzed in some early work, the literature has not adequately grown with temporal modeling developments. We study the effects of Neural Differential Equations to model the temporal dynamics of video generation. The paradigm of Neural Differential Equations presents many theoretical strengths including the first continuous representation of time within video generation. In order to address the effects of Neural Differential Equations, we investigate how changes in temporal models affect generated video quality. Our results give support to the usage of Neural Differential Equations as a simple replacement for older temporal generators. While keeping run times similar and decreasing parameter count, we produce a new state-of-the-art model in 64$\times$64 pixel unconditional video generation, with an Inception Score of 15.20.",0
"Video generation using Generative Adversarial Networks has shown promise and builds on the success of image generation. However, it faces the challenge of modeling time, which has not been adequately addressed in the literature. To tackle this, we explore the use of Neural Differential Equations to model temporal dynamics in video generation. This approach offers theoretical advantages, including the first continuous representation of time in video generation. We investigate the effects of these equations on video quality and find that they can replace older temporal generators while maintaining similar run times and decreasing parameter count. Our method achieves state-of-the-art results in 64$\times$64 pixel unconditional video generation, with an Inception Score of 15.20.",1
"We propose PD-GAN, a probabilistic diverse GAN for image inpainting. Given an input image with arbitrary hole regions, PD-GAN produces multiple inpainting results with diverse and visually realistic content. Our PD-GAN is built upon a vanilla GAN which generates images based on random noise. During image generation, we modulate deep features of input random noise from coarse-to-fine by injecting an initially restored image and the hole regions in multiple scales. We argue that during hole filling, the pixels near the hole boundary should be more deterministic (i.e., with higher probability trusting the context and initially restored image to create natural inpainting boundary), while those pixels lie in the center of the hole should enjoy more degrees of freedom (i.e., more likely to depend on the random noise for enhancing diversity). To this end, we propose spatially probabilistic diversity normalization (SPDNorm) inside the modulation to model the probability of generating a pixel conditioned on the context information. SPDNorm dynamically balances the realism and diversity inside the hole region, making the generated content more diverse towards the hole center and resemble neighboring image content more towards the hole boundary. Meanwhile, we propose a perceptual diversity loss to further empower PD-GAN for diverse content generation. Experiments on benchmark datasets including CelebA-HQ, Places2 and Paris Street View indicate that PD-GAN is effective for diverse and visually realistic image restoration.",0
"PD-GAN is a diverse and probabilistic Generative Adversarial Network that we developed for image inpainting. It is capable of producing multiple inpainting results, which are visually realistic and diverse, when provided with an input image containing arbitrary hole regions. PD-GAN is derived from a vanilla GAN, which generates images based on random noise. During image generation, we modulate deep features of input random noise from coarse-to-fine by injecting an initially restored image and the hole regions in multiple scales. To ensure that the pixels near the hole boundary are more deterministic while those in the center of the hole enjoy more degrees of freedom, we introduce spatially probabilistic diversity normalization (SPDNorm) inside the modulation. This normalization dynamically balances the realism and diversity inside the hole region, making the generated content more diverse towards the hole center and resemble neighboring image content more towards the hole boundary. Additionally, we propose a perceptual diversity loss to further enhance PD-GAN for diverse content generation. Our experiments on benchmark datasets, including CelebA-HQ, Places2, and Paris Street View, demonstrate that PD-GAN is effective for diverse and visually realistic image restoration.",1
"Many applications of deep learning for image generation use perceptual losses for either training or fine-tuning of the generator networks. The use of perceptual loss however incurs repeated forward-backward passes in a large image classification network as well as a considerable memory overhead required to store the activations of this network. It is therefore desirable or sometimes even critical to get rid of these overheads.   In this work, we propose a way to train generator networks using approximations of perceptual loss that are computed without forward-backward passes. Instead, we use a simpler perceptual gradient network that directly synthesizes the gradient field of a perceptual loss. We introduce the concept of proxy targets, which stabilize the predicted gradient, meaning that learning with it does not lead to divergence or oscillations. In addition, our method allows interpretation of the predicted gradient, providing insight into the internals of perceptual loss and suggesting potential ways to improve it in future work.",0
"Perceptual losses are commonly used in deep learning for image generation, but they require repeated forward-backward passes in a large image classification network, leading to significant memory overhead. To address this issue, we propose a method to train generator networks using approximations of perceptual loss computed without forward-backward passes. Instead, we use a simpler perceptual gradient network to synthesize the gradient field of a perceptual loss. We stabilize the predicted gradient using proxy targets, preventing divergence or oscillations during the learning process. Furthermore, our approach allows for the interpretation of the predicted gradient, enabling us to gain insight into the inner workings of perceptual loss and offering potential avenues for future improvement.",1
"In this paper, we focus on deep clustering and unsupervised representation learning for images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be closer in the representation space (exemplar consistency), and/or similar images have a similar cluster assignment (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learnt to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a clustering algorithm. We define a clustering loss by performing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, Consensus Clustering using Unsupervised Representation Learning (ConCURL) improves the clustering performance over state-of-the art methods on four out of five image datasets. Further, we extend the evaluation procedure for clustering to reflect the challenges in real world clustering tasks, such as clustering performance in the case of distribution shift. We also perform a detailed ablation study for a deeper understanding of the algorithm.",0
"This paper discusses deep clustering and unsupervised representation learning for images. Recent progress in this field suggests that different views of an input image should be closer in the representation space, and similar images should have similar cluster assignments. In addition to these notions of exemplar and population consistency, we introduce consensus consistency. This ensures that representations induce similar partitions for variations in the representation space, clustering algorithms, and initializations. We develop an end-to-end learning framework that integrates all three consistencies, called Consensus Clustering using Unsupervised Representation Learning (ConCURL). Our algorithm outperforms state-of-the-art methods on four out of five image datasets, and we extend the evaluation procedure to reflect real-world clustering challenges, such as distribution shift. We also conduct an in-depth ablation study for a better understanding of the algorithm.",1
"Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.",0
"The fields of image and video synthesis share a common goal of creating content from random noise. While progress has been made in improving image-based models to handle diverse image content, high resolutions, and exceptional quality, achieving similar results in video synthesis remains problematic. Our approach addresses this issue by utilizing modern image generators to produce high-resolution videos. Our framework involves discovering a trajectory in the latent space of a fixed image generator, which not only yields high-quality videos but is also computationally efficient. We introduce a motion generator that disentangles motion and content, allowing for a wide range of applications, including content and motion manipulation. Additionally, we propose a novel task, cross-domain video synthesis, which generates moving objects for which the desired video data is unavailable. Our experiments demonstrate the superiority of our methods over existing video generation techniques. The code is available at https://github.com/snap-research/MoCoGAN-HD.",1
"Evaluating image generation models such as generative adversarial networks (GANs) is a challenging problem. A common approach is to compare the distributions of the set of ground truth images and the set of generated test images. The Frech\'et Inception distance is one of the most widely used metrics for evaluation of GANs, which assumes that the features from a trained Inception model for a set of images follow a normal distribution. In this paper, we argue that this is an over-simplified assumption, which may lead to unreliable evaluation results, and more accurate density estimation can be achieved using a truncated generalized normal distribution. Based on this, we propose a novel metric for accurate evaluation of GANs, named TREND (TRuncated gEneralized Normal Density estimation of inception embeddings). We demonstrate that our approach significantly reduces errors of density estimation, which consequently eliminates the risk of faulty evaluation results. Furthermore, we show that the proposed metric significantly improves robustness of evaluation results against variation of the number of image samples.",0
"Assessing image generation models, such as generative adversarial networks (GANs), poses a difficult challenge. A common method is to compare the distributions of a set of ground truth images with that of a set of generated test images. The widely used Frech\'et Inception distance metric assumes that features from a trained Inception model for a group of images follow a normal distribution. However, this simplistic assumption may produce unreliable evaluation results. To achieve more accurate density estimation, we propose using a truncated generalized normal distribution, which we demonstrate in our novel metric named TREND (TRuncated gEneralized Normal Density estimation of inception embeddings). Our approach significantly reduces density estimation errors and minimizes the risk of faulty evaluation outcomes. Additionally, we prove that our metric improves the evaluation results' robustness against changes in the number of image samples.",1
"We propose a novel approach for few-shot talking-head synthesis. While recent works in neural talking heads have produced promising results, they can still produce images that do not preserve the identity of the subject in source images. We posit this is a result of the entangled representation of each subject in a single latent code that models 3D shape information, identity cues, colors, lighting and even background details. In contrast, we propose to factorize the representation of a subject into its spatial and style components. Our method generates a target frame in two steps. First, it predicts a dense spatial layout for the target image. Second, an image generator utilizes the predicted layout for spatial denormalization and synthesizes the target frame. We experimentally show that this disentangled representation leads to a significant improvement over previous methods, both quantitatively and qualitatively.",0
"Our proposed approach for few-shot talking-head synthesis introduces a new method that addresses the issue of neural talking heads generating images that fail to maintain the identity of the subject from the source images. We believe that this is due to the use of a single latent code that represents each subject's 3D shape information, identity cues, colors, lighting, and background details, resulting in an entangled representation. In contrast, we suggest factorizing the representation of a subject into spatial and style components. Our method involves predicting a dense spatial layout for the target image and then utilizing this layout for spatial denormalization and synthesizing the target frame. Our experimental results demonstrate that this disentangled representation produces a significant improvement over previous methods, both quantitatively and qualitatively.",1
"Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.",0
"Person re-identification (re-ID) involves matching subject images from different camera views in a multi-camera surveillance system. One of the biggest challenges in person re-ID is the variations in pose across the camera network, which greatly affects a person's appearance. The current development data do not have enough pose variations to train person re-ID systems effectively. This paper proposes an end-to-end pose-driven attention-guided generative adversarial network that generates multiple poses of a person. The network attentively learns and transfers the subject pose through an attention mechanism, while a semantic-consistency loss preserves the semantic information of the person during pose transfer. To ensure the fine details of the image are realistic after pose translation, an appearance discriminator is used, and a pose discriminator ensures that the transferred images have the exact same pose as the target pose. Incorporating this proposed approach into a person re-identification framework can achieve realistic pose transferred images and state-of-the-art re-identification results.",1
"Conditional generative adversarial networks (cGANs) target at synthesizing diverse images given the input conditions and latent codes, but unfortunately, they usually suffer from the issue of mode collapse. To solve this issue, previous works mainly focused on encouraging the correlation between the latent codes and their generated images, while ignoring the relations between images generated from various latent codes. The recent MSGAN tried to encourage the diversity of the generated image but only considers ""negative"" relations between the image pairs. In this paper, we propose a novel DivCo framework to properly constrain both ""positive"" and ""negative"" relations between the generated images specified in the latent space. To the best of our knowledge, this is the first attempt to use contrastive learning for diverse conditional image synthesis. A novel latent-augmented contrastive loss is introduced, which encourages images generated from adjacent latent codes to be similar and those generated from distinct latent codes to be dissimilar. The proposed latent-augmented contrastive loss is well compatible with various cGAN architectures. Extensive experiments demonstrate that the proposed DivCo can produce more diverse images than state-of-the-art methods without sacrificing visual quality in multiple unpaired and paired image generation tasks.",0
"Conditional generative adversarial networks (cGANs) aim to generate a variety of images based on input conditions and latent codes. However, cGANs often suffer from mode collapse. Previous works focused mainly on the correlation between latent codes and their generated images, while neglecting the relations between images generated from different latent codes. The MSGAN attempted to increase image diversity, but only considered ""negative"" relations between image pairs. This paper introduces the DivCo framework, which constrains both ""positive"" and ""negative"" relations in the latent space to encourage diverse image synthesis. The proposed latent-augmented contrastive loss encourages similar images from adjacent latent codes and dissimilar images from distinct codes. The DivCo framework is compatible with various cGAN architectures and generates more diverse images without sacrificing visual quality in multiple image generation tasks. This is the first attempt to use contrastive learning for diverse conditional image synthesis.",1
"A text to image generation (T2I) model aims to generate photo-realistic images which are semantically consistent with the text descriptions. Built upon the recent advances in generative adversarial networks (GANs), existing T2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) The condition batch normalization methods are applied on the whole image feature maps equally, ignoring the local semantics; (2) The text encoder is fixed during training, which should be trained with the image generator jointly to learn better text representations for image generation. To address these limitations, we propose a novel framework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion so that the text encoder can exploit better text information. Concretely, we introduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a mask map in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code is available at https://github.com/wtliao/text2image.",0
"The objective of a T2I model is to create images that are both semantically consistent with the text descriptions and appear to be real. Although existing T2I models have advanced significantly due to their reliance on GAN technology, a more thorough examination of the images they generate has revealed two significant shortcomings. The first limitation is that the condition batch normalization methods are applied uniformly across the entire image feature maps, without regard for local semantics. The second limitation is that the text encoder remains fixed during training, necessitating the joint training of the text encoder with the image generator to achieve better text representations for image generation. To overcome these limitations, we have introduced a novel Semantic-Spatial Aware GAN framework that is trained end-to-end, allowing the text encoder to utilize better text information. Specifically, we have developed a Semantic-Spatial Aware Convolution Network that (1) learns semantic-adaptive transformations based on the text to effectively fuse text features and image features, and (2) learns a weakly-supervised mask map that depends on the present text-image fusion process to guide the transformation spatially. Our experiments on the challenging COCO and CUB bird datasets have shown that our method outperforms the latest state-of-the-art approaches in terms of both visual fidelity and alignment with input text description. Our code is available at https://github.com/wtliao/text2image.",1
"Several regularization methods have recently been introduced which force the latent activations of an autoencoder or deep neural network to conform to either a Gaussian or hyperspherical distribution, or to minimize the implicit rank of the distribution in latent space. In the present work, we introduce a novel regularizing loss function which simulates a pairwise repulsive force between items and an attractive force of each item toward the origin. We show that minimizing this loss function in isolation achieves a hyperspherical distribution. Moreover, when used as a regularizing term, the scaling factor can be adjusted to allow greater flexibility and tolerance of eccentricity, thus allowing the latent variables to be stratified according to their relative importance, while still promoting diversity. We apply this method of Eccentric Regularization to an autoencoder, and demonstrate its effectiveness in image generation, representation learning and downstream classification tasks.",0
"New regularization techniques have been developed to enforce Gaussian or hyperspherical distributions in the latent activations of autoencoders or deep neural networks, or to decrease the implicit rank of the distribution in latent space. Our study introduces a fresh regularizing loss function that mimics a pairwise repulsive force between items and an attractive force towards the origin for each item. We discovered that minimizing this loss function alone leads to a hyperspherical distribution. Additionally, when used as a regularizing term, the scaling factor can be tweaked to enhance flexibility and tolerance for eccentricity, allowing the latent variables to be classified based on their relative importance while promoting diversity. We applied this Eccentric Regularization method to an autoencoder and demonstrated its effectiveness in image generation, representation learning, and downstream classification tasks.",1
"Image quality assessment (IQA) aims to assess the perceptual quality of images. The outputs of the IQA algorithms are expected to be consistent with human subjective perception. In image restoration and enhancement tasks, images generated by generative adversarial networks (GAN) can achieve better visual performance than traditional CNN-generated images, although they have spatial shift and texture noise. Unfortunately, the existing IQA methods have unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment. To this end, we propose the reference-oriented deformable convolution, which can improve the performance of an IQA network on GAN-based distortion by adaptively considering this misalignment. We further propose a patch-level attention module to enhance the interaction among different patch regions, which are processed independently in previous patch-based methods. The modified residual block is also proposed by applying modifications to the classic residual block to construct a patch-region-based baseline called WResNet. Equipping this baseline with the two proposed modules, we further propose Region-Adaptive Deformable Network (RADN). The experiment results on the NTIRE 2021 Perceptual Image Quality Assessment Challenge dataset show the superior performance of RADN, and the ensemble approach won fourth place in the final testing phase of the challenge. Code is available at https://github.com/IIGROUP/RADN.",0
"The objective of IQA is to evaluate the visual quality of images, with the expectation that the results of the algorithms are in line with human perception. While generative adversarial networks (GAN) can produce images with superior visual quality compared to traditional CNN-generated images in restoration and enhancement tasks, they are prone to spatial shift and texture noise. However, current IQA methods are not effective in assessing GAN-based distortions due to their low tolerance for spatial misalignment. To address this issue, we propose the use of reference-oriented deformable convolution to enhance the performance of IQA networks in GAN-based distortions by accommodating misalignments. We also introduce a patch-level attention module to improve interactions among different patch regions, which were previously processed independently in patch-based methods. Furthermore, we suggest modifications to the classic residual block to create a patch-region-based baseline known as WResNet. Incorporating the two proposed modules, we present the Region-Adaptive Deformable Network (RADN), which exhibits superior performance based on the results of the NTIRE 2021 Perceptual Image Quality Assessment Challenge dataset. Our ensemble approach secured fourth place in the final testing phase of the challenge. The code for our proposed method is available at https://github.com/IIGROUP/RADN.",1
"We consider image completion from the perspective of amortized inference in an image generative model. We leverage recent state of the art variational auto-encoder architectures that have been shown to produce photo-realistic natural images at non-trivial resolutions. Through amortized inference in such a model we can train neural artifacts that produce diverse, realistic image completions even when the vast majority of an image is missing. We demonstrate superior sample quality and diversity compared to prior art on the CIFAR-10 and FFHQ-256 datasets. We conclude by describing and demonstrating an application that requires an in-painting model with the capabilities ours exhibits: the use of Bayesian optimal experimental design to select the most informative sequence of small field of view x-rays for chest pathology detection.",0
"Our approach to image completion involves utilizing amortized inference in an image generative model. We utilize cutting-edge variational auto-encoder architectures that have demonstrated exceptional performance in generating lifelike images at high resolutions. By incorporating amortized inference into this model, we can develop neural artifacts capable of generating diverse, realistic image completions, even when a significant portion of the image is missing. Our experiments on the CIFAR-10 and FFHQ-256 datasets demonstrate that our method produces superior sample quality and diversity compared to previous techniques. Additionally, we showcase an application that necessitates an in-painting model with the same capabilities as ours - using Bayesian optimal experimental design to select the most informative sequence of small field of view x-rays for chest pathology detection.",1
"The widespread dissemination of forged images generated by Deepfake techniques has posed a serious threat to the trustworthiness of digital information. This demands effective approaches that can detect perceptually convincing Deepfakes generated by advanced manipulation techniques. Most existing approaches combat Deepfakes with deep neural networks by mapping the input image to a binary prediction without capturing the consistency among different pixels. In this paper, we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection. We achieve this with transformer models, which have recently demonstrated superior performance in modeling dependencies between pixels for a variety of recognition tasks in computer vision. In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale transformer that operates on patches of different sizes to detect the local inconsistency at different spatial levels. To improve the detection results and enhance the robustness of our method to image compression, M2TR also takes frequency information, which is further combined with RGB features using a cross modality fusion module. Developing and evaluating Deepfake detection methods requires large-scale datasets. However, we observe that samples in existing benchmarks contain severe artifacts and lack diversity. This motivates us to introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. On three Deepfake datasets, we conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods.",0
"The proliferation of doctored images produced through Deepfake techniques has presented a significant challenge to the credibility of digital information. Therefore, it is imperative to devise effective methods that can identify Deepfakes generated through sophisticated manipulation techniques that are visually convincing. Although current approaches employ deep neural networks to combat Deepfakes, they do not capture consistency across pixels, resulting in limited success. In this paper, we propose a Multi-modal Multi-scale TRansformer (M2TR) that utilizes transformer models to capture subtle manipulation artifacts at different spatial levels. The M2TR applies a multi-scale transformer to various-sized patches to detect local inconsistencies and combines RGB features with frequency information via a cross-modality fusion module to enhance robustness against image compression. To evaluate our method, we introduce a high-quality dataset, SR-DF, consisting of 4,000 DeepFake videos produced through cutting-edge face-swapping and facial reenactment techniques. We conduct extensive experiments on three Deepfake datasets and demonstrate that our approach outperforms existing Deepfake detection methods.",1
"Neural networks-based learning of the distribution of non-dispatchable renewable electricity generation from sources such as photovoltaics (PV) and wind as well as load demands has recently gained attention. Normalizing flow density models have performed particularly well in this task due to the training through direct log-likelihood maximization. However, research from the field of image generation has shown that standard normalizing flows can only learn smeared-out versions of manifold distributions and can result in the generation of noisy data. To avoid the generation of time series data with unrealistic noise, we propose a dimensionality-reducing flow layer based on the linear principal component analysis (PCA) that sets up the normalizing flow in a lower-dimensional space. We train the resulting principal component flow (PCF) on data of PV and wind power generation as well as load demand in Germany in the years 2013 to 2015. The results of this investigation show that the PCF preserves critical features of the original distributions, such as the probability density and frequency behavior of the time series. The application of the PCF is, however, not limited to renewable power generation but rather extends to any data set, time series, or otherwise, which can be efficiently reduced using PCA.",0
"Recently, there has been growing interest in using neural networks to learn the distribution of non-dispatchable renewable electricity generation from sources like photovoltaics (PV) and wind, as well as load demands. One approach that has proven successful in this area is the use of normalizing flow density models, which are trained through direct log-likelihood maximization. However, research in image generation has shown that standard normalizing flows can only learn smeared-out versions of manifold distributions, leading to noisy data generation. To avoid this issue and generate time series data that accurately reflects the original distribution, we propose a dimensionality-reducing flow layer based on linear principal component analysis (PCA) that sets up the normalizing flow in a lower-dimensional space. We trained the resulting principal component flow (PCF) on data from PV and wind power generation as well as load demand in Germany from 2013 to 2015 and found that the PCF preserves critical features of the original distributions, including probability density and frequency behavior of the time series. The PCF can be applied to any data set, time series, or otherwise, that can be efficiently reduced using PCA.",1
"Despite all the challenges and limitations, vision-based vehicle speed detection is gaining research interest due to its great potential benefits such as cost reduction, and enhanced additional functions. As stated in a recent survey [1], the use of learning-based approaches to address this problem is still in its infancy. One of the main difficulties is the need for a large amount of data, which must contain the input sequences and, more importantly, the output values corresponding to the actual speed of the vehicles. Data collection in this context requires a complex and costly setup to capture the images from the camera synchronized with a high precision speed sensor to generate the ground truth speed values. In this paper we explore, for the first time, the use of synthetic images generated from a driving simulator (e.g., CARLA) to address vehicle speed detection using a learning-based approach. We simulate a virtual camera placed over a stretch of road, and generate thousands of images with variability corresponding to multiple speeds, different vehicle types and colors, and lighting and weather conditions. Two different approaches to map the sequence of images to an output speed (regression) are studied, including CNN-GRU and 3D-CNN. We present preliminary results that support the high potential of this approach to address vehicle speed detection.",0
"Despite facing challenges and limitations, there is growing research interest in vision-based vehicle speed detection due to its potential benefits, such as cost reduction and improved functionality. However, a recent survey revealed that the use of learning-based methods to tackle this issue is still in its early stages, with one of the main obstacles being the requirement for a significant amount of data, including input sequences and the corresponding vehicle speed values. Collecting such data is complex and costly and necessitates a setup that can capture images from a camera synchronized with a high-precision speed sensor to generate ground truth speed values. This study is the first to examine the applicability of synthetic images generated from a driving simulator, such as CARLA, to address vehicle speed detection using a learning-based approach. The authors simulated a virtual camera positioned over a road section and generated thousands of images with variability corresponding to multiple speeds, different vehicle types and colors, and various lighting and weather conditions. They investigated two different approaches for mapping the image sequence to an output speed (regression), including CNN-GRU and 3D-CNN, and presented preliminary findings that demonstrate the high potential of this approach.",1
"This paper studies the neural architecture search (NAS) problem for developing efficient generator networks. Compared with deep models for visual recognition tasks, generative adversarial network (GAN) are usually designed to conduct various complex image generation. We first discover an intact search space of generator networks including three dimensionalities, i.e., path, operator, channel for fully excavating the network performance. To reduce the huge search cost, we explore a coarse-to-fine search strategy which divides the overall search process into three sub-optimization problems accordingly. In addition, a fair supernet training approach is utilized to ensure that all sub-networks can be updated fairly and stably. Experiments results on benchmarks show that we can provide generator networks with better image quality and lower computational costs over the state-of-the-art methods. For example, with our method, it takes only about 8 GPU hours on the entire edges-to-shoes dataset to get a 2.56 MB model with a 24.13 FID score and 10 GPU hours on the entire Urban100 dataset to get a 1.49 MB model with a 24.94 PSNR score.",0
"This study focuses on the problem of neural architecture search (NAS) and its application in developing efficient generator networks. While deep models are typically used for visual recognition tasks, generative adversarial networks (GANs) are designed for complex image generation. We begin by identifying an extensive search space for generator networks, which encompasses three dimensions: path, operator, and channel. To mitigate the high cost of searching this space, we employ a coarse-to-fine search strategy, dividing the process into three sub-optimization problems. We also use a fair supernet training approach to ensure all sub-networks are updated evenly and reliably. Our experimental results demonstrate that our method outperforms state-of-the-art techniques, providing generator networks with improved image quality and lower computational costs. For instance, on the edges-to-shoes dataset, we can obtain a 2.56 MB model with a 24.13 FID score in just 8 GPU hours, and on the Urban100 dataset, we can achieve a 1.49 MB model with a 24.94 PSNR score in 10 GPU hours.",1
"The existing text-guided image synthesis methods can only produce limited quality results with at most \mbox{$\text{256}^2$} resolution and the textual instructions are constrained in a small Corpus. In this work, we propose a unified framework for both face image generation and manipulation that produces diverse and high-quality images with an unprecedented resolution at 1024 from multimodal inputs. More importantly, our method supports open-world scenarios, including both image and text, without any re-training, fine-tuning, or post-processing. To be specific, we propose a brand new paradigm of text-guided image generation and manipulation based on the superior characteristics of a pretrained GAN model. Our proposed paradigm includes two novel strategies. The first strategy is to train a text encoder to obtain latent codes that align with the hierarchically semantic of the aforementioned pretrained GAN model. The second strategy is to directly optimize the latent codes in the latent space of the pretrained GAN model with guidance from a pretrained language model. The latent codes can be randomly sampled from a prior distribution or inverted from a given image, which provides inherent supports for both image generation and manipulation from multi-modal inputs, such as sketches or semantic labels, with textual guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",0
"Current methods for generating images based on textual descriptions are limited in quality and resolution, with text constraints from a small corpus. Our proposed unified framework for face image generation and manipulation produces high-quality images at 1024 resolution from multimodal inputs, without the need for re-training or fine-tuning. Our approach is based on a pretrained GAN model and includes two novel strategies: training a text encoder to obtain latent codes aligned with the hierarchical semantics of the GAN model, and optimizing latent codes in the GAN model's latent space with guidance from a pretrained language model. Our method supports image generation and manipulation from various inputs, such as sketches or semantic labels, with textual guidance. To facilitate this, we introduce the Multi-Modal CelebA-HQ dataset, which includes real face images and corresponding semantic segmentation maps, sketches, and textual descriptions. Extensive experiments demonstrate the superior performance of our method. Code and data are available at https://github.com/weihaox/TediGAN.",1
"It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found here https://github.com/ppope/dimensions.",0
"The prevailing belief is that natural image data displays a low-dimensional structure, despite conventional pixel representations having high dimensionality. This belief is the foundation of the success of deep learning in computer vision. Our study employs dimension estimation tools on popular datasets to explore the importance of low-dimensional structures in deep learning. Our findings indicate that natural image datasets possess a very low intrinsic dimension in comparison to the high number of pixels in the images. Moreover, we discovered that neural networks find it easier to learn low-dimensional datasets, and models that solve these tasks exhibit superior generalization from training to test data. We also developed a technique to validate our dimension estimation tools on synthetic data created by GANs, allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Our experiments' code may be accessed at https://github.com/ppope/dimensions.",1
"This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves $46.4$ $AP$ (and $65.1$ $AP_{50}$), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in other visual detection tasks, namely, video object detection, instance segmentation, 3D object detection and keypoint detection for human pose estimation, and an additional ``labels to photo`` image generation task, where the integration of our voting module consistently improves performance in all cases. Code is available at \url{https://github.com/nerminsamet/houghnet}.",0
"HoughNet, a novel object detection technique, is introduced in this paper. It is a one-stage, anchor-free, voting-based, bottom-up approach that relies on the Generalized Hough Transform. The method determines the presence of an object at a particular location by summing up the votes cast on that location, collected from both near and long-distance locations based on a log-polar vote field. This voting mechanism enables HoughNet to incorporate both near and long-range, class-conditional evidence for visual recognition, which enhances current object detection methods that rely on only local evidence. HoughNet's best model achieves $46.4$ $AP$ (and $65.1$ $AP_{50}$) on the COCO dataset, which is comparable to state-of-the-art methods in bottom-up object detection and better than most major one-stage and two-stage techniques. The effectiveness of the proposal is further validated in other visual detection tasks, including video object detection, instance segmentation, 3D object detection, keypoint detection for human pose estimation, and an additional ""labels to photo"" image generation task, where the integration of the voting module consistently improves performance. The code is publicly available at \url{https://github.com/nerminsamet/houghnet}.",1
"Human pose transfer has received great attention due to its wide applications, yet is still a challenging task that is not well solved. Recent works have achieved great success to transfer the person image from the source to the target pose. However, most of them cannot well capture the semantic appearance, resulting in inconsistent and less realistic textures on the reconstructed results. To address this issue, we propose a new two-stage framework to handle the pose and appearance translation. In the first stage, we predict the target semantic parsing maps to eliminate the difficulties of pose transfer and further benefit the latter translation of per-region appearance style. In the second one, with the predicted target semantic maps, we suggest a new person image generation method by incorporating the region-adaptive normalization, in which it takes the per-region styles to guide the target appearance generation. Extensive experiments show that our proposed SPGNet can generate more semantic, consistent, and photo-realistic results and perform favorably against the state of the art methods in terms of quantitative and qualitative evaluation. The source code and model are available at https://github.com/cszy98/SPGNet.git.",0
"Human pose transfer is a complex task that has garnered significant interest due to its broad range of applications. Although recent research has made progress in transferring images of people from one pose to another, most existing methods struggle to accurately capture the semantic appearance of the subject, resulting in less realistic textures in the final output. To address this issue, we propose a new two-stage framework that focuses on both pose and appearance translation. In the first stage, our approach predicts target semantic parsing maps to simplify the pose transfer process and facilitate per-region appearance style translation. In the second stage, we introduce a new person image generation method that uses region-adaptive normalization and per-region styles to guide the generation of the target appearance. Our experiments demonstrate that our proposed SPGNet produces more semantic, consistent, and photo-realistic results and outperforms state-of-the-art methods in both quantitative and qualitative evaluations. The source code and model can be found at https://github.com/cszy98/SPGNet.git.",1
"Learning to generate new images for a novel category based on only a few images, named as few-shot image generation, has attracted increasing research interest. Several state-of-the-art works have yielded impressive results, but the diversity is still limited. In this work, we propose a novel Delta Generative Adversarial Network (DeltaGAN), which consists of a reconstruction subnetwork and a generation subnetwork. The reconstruction subnetwork captures intra-category transformation, i.e., ""delta"", between same-category pairs. The generation subnetwork generates sample-specific ""delta"" for an input image, which is combined with this input image to generate a new image within the same category. Besides, an adversarial delta matching loss is designed to link the above two subnetworks together. Extensive experiments on five few-shot image datasets demonstrate the effectiveness of our proposed method.",0
"The concept of few-shot image generation, which involves creating new images for a category with limited source images, has become a popular area of research. While there have been impressive advances in this field, the variety of generated images is still limited. Our team has developed a Delta Generative Adversarial Network (DeltaGAN) that comprises two subnetworks: a reconstruction subnetwork that captures the differences between images within a category, and a generation subnetwork that combines this information with input images to generate new images within the same category. We have also created an adversarial delta matching loss that connects the two subnetworks. Our method has been extensively tested on five different few-shot image datasets, and the results show its effectiveness.",1
"Training generative models, such as GANs, on a target domain containing limited examples (e.g., 10) can easily result in overfitting. In this work, we seek to utilize a large source domain for pretraining and transfer the diversity information from source to target. We propose to preserve the relative similarities and differences between instances in the source via a novel cross-domain distance consistency loss. To further reduce overfitting, we present an anchor-based strategy to encourage different levels of realism over different regions in the latent space. With extensive results in both photorealistic and non-photorealistic domains, we demonstrate qualitatively and quantitatively that our few-shot model automatically discovers correspondences between source and target domains and generates more diverse and realistic images than previous methods.",0
"Overfitting can occur easily when training generative models, such as GANs, on a target domain that has a limited number of examples, like 10. Thus, in this study, we aim to use a large source domain for pretraining and transfer the diversity information from source to target. We propose a new cross-domain distance consistency loss to maintain the relative similarities and differences between instances in the source. To further reduce overfitting, we introduce an anchor-based strategy that encourages different levels of realism across various areas in the latent space. Through extensive results in both photorealistic and non-photorealistic domains, we show that our few-shot model discovers correspondences between source and target domains automatically. Additionally, it generates more diverse and realistic images than previous methods, both qualitatively and quantitatively.",1
"Generative Adversarial Networks (GANs) have shown satisfactory performance in synthetic image generation by devising complex network structure and adversarial training scheme. Even though GANs are able to synthesize realistic images, there exists a number of generated images with defective visual patterns which are known as artifacts. While most of the recent work tries to fix artifact generations by perturbing latent code, few investigate internal units of a generator to fix them. In this work, we devise a method that automatically identifies the internal units generating various types of artifact images. We further propose the sequential correction algorithm which adjusts the generation flow by modifying the detected artifact units to improve the quality of generation while preserving the original outline. Our method outperforms the baseline method in terms of FID-score and shows satisfactory results with human evaluation.",0
"By developing intricate network structures and adversarial training schemes, Generative Adversarial Networks (GANs) have demonstrated impressive results in generating synthetic images. However, despite their ability to produce realistic images, some generated images exhibit visual abnormalities known as artifacts. While current research focuses on rectifying artifact generation by manipulating latent code, few studies address the internal units of a generator. Our research presents a technique that automatically identifies the internal units responsible for various artifact images. Additionally, we introduce a sequential correction algorithm that modifies the detected artifact units to improve the quality of generation while maintaining the original outline. Our method surpasses the baseline approach in FID-score and yields satisfactory results based on human evaluation.",1
"Focus based methods have shown promising results for the task of depth estimation. However, most existing focus based depth estimation approaches depend on maximal sharpness of the focal stack. Out of focus information in the focal stack poses challenges for this task. In this paper, we propose a dynamically multi modal learning strategy which incorporates RGB data and the focal stack in our framework. Our goal is to deeply excavate the spatial correlation in the focal stack by designing the spatial correlation perception module and dynamically fuse multi modal information between RGB data and the focal stack in a adaptive way by designing the multi modal dynamic fusion module. The success of our method is demonstrated by achieving the state of the art performance on two datasets. Furthermore, we test our network on a set of different focused images generated by a smart phone camera to prove that the proposed method not only broke the limitation of only using light field data, but also open a path toward practical applications of depth estimation on common consumer level cameras data.",0
"While focus based methods have shown promise in depth estimation, existing approaches often rely on maximal sharpness of the focal stack, making out-of-focus information a challenge. To address this, we introduce a dynamically multi-modal learning strategy that incorporates RGB data and the focal stack into our framework. Our approach includes a spatial correlation perception module to deeply excavate the spatial correlation in the focal stack, as well as a multi-modal dynamic fusion module to adaptively fuse multi-modal information between RGB data and the focal stack. Our method achieves state-of-the-art performance on two datasets and demonstrates its applicability to consumer-level camera data.",1
"Event camera is an asynchronous, high frequency vision sensor with low power consumption, which is suitable for human action understanding task. It is vital to encode the spatial-temporal information of event data properly and use standard computer vision tool to learn from the data. In this work, we propose a timestamp image encoding 2D network, which takes the encoded spatial-temporal images with polarity information of the event data as input and output the action label. In addition, we propose a future timestamp image generator to generate futureaction information to aid the model to anticipate the human action when the action is not completed. Experiment results show that our method can achieve the same level of performance as those RGB-based benchmarks on real world action recognition,and also achieve the state of the art (SOTA) result on gesture recognition. Our future timestamp image generating model can effectively improve the prediction accuracy when the action is not completed. We also provide insight discussion on the importance of motion and appearance information in action recognition and anticipation.",0
"The event camera is a vision sensor that operates at high frequency and uses low power, making it ideal for understanding human actions. Proper encoding of the spatial-temporal information in event data is essential to effectively learn from it using standard computer vision tools. This study proposes a 2D network that encodes timestamp images to process the spatial-temporal data and polarity information of event data, producing an action label. A future timestamp image generator is also proposed to anticipate human action when it is unfinished. The experimental results show that this method achieves comparable performance to RGB-based benchmarks in real-world action recognition and achieves the state-of-the-art in gesture recognition. The future timestamp image generator significantly improves prediction accuracy when the action is unfinished. Finally, this study discusses the importance of motion and appearance information in action recognition and anticipation.",1
"We address the task of multi-view image-to-image translation for person image generation. The goal is to synthesize photo-realistic multi-view images with pose-consistency across all views. Our proposed end-to-end framework is based on a joint learning of multiple unpaired image-to-image translation models, one per camera viewpoint. The joint learning is imposed by constraints on the shared 3D human pose in order to encourage the 2D pose projections in all views to be consistent. Experimental results on the CMU-Panoptic dataset demonstrate the effectiveness of the suggested framework in generating photo-realistic images of persons with new poses that are more consistent across all views in comparison to a standard Image-to-Image baseline. The code is available at: https://github.com/sony-si/MultiView-Img2Img",0
"Our focus is on generating photo-realistic multi-view images of people while maintaining pose-consistency across all views. Our proposed approach involves using multiple unpaired image-to-image translation models, with one model per camera viewpoint. We enforce joint learning by imposing constraints on the shared 3D human pose, which promotes consistent 2D pose projections across all views. Our experimental results on the CMU-Panoptic dataset demonstrate the effectiveness of our framework in generating more consistent photo-realistic images of people with new poses compared to a standard Image-to-Image baseline. Interested individuals can access the code at: https://github.com/sony-si/MultiView-Img2Img",1
"Restoring the clean background from the superimposed images containing a noisy layer is the common crux of a classical category of tasks on image restoration such as image reflection removal, image deraining and image dehazing. These tasks are typically formulated and tackled individually due to the diverse and complicated appearance patterns of noise layers within the image. In this work we present the Deep-Masking Generative Network (DMGN), which is a unified framework for background restoration from the superimposed images and is able to cope with different types of noise. Our proposed DMGN follows a coarse-to-fine generative process: a coarse background image and a noise image are first generated in parallel, then the noise image is further leveraged to refine the background image to achieve a higher-quality background image. In particular, we design the novel Residual Deep-Masking Cell as the core operating unit for our DMGN to enhance the effective information and suppress the negative information during image generation via learning a gating mask to control the information flow. By iteratively employing this Residual Deep-Masking Cell, our proposed DMGN is able to generate both high-quality background image and noisy image progressively. Furthermore, we propose a two-pronged strategy to effectively leverage the generated noise image as contrasting cues to facilitate the refinement of the background image. Extensive experiments across three typical tasks for image background restoration, including image reflection removal, image rain steak removal and image dehazing, show that our DMGN consistently outperforms state-of-the-art methods specifically designed for each single task.",0
"The restoration of a clean background from images with a noisy layer is a common challenge in image restoration tasks such as image reflection removal, image deraining, and image dehazing. Due to the diverse and complex appearance patterns of noise layers within an image, these tasks are typically approached individually. In this study, we present the Deep-Masking Generative Network (DMGN), a unified framework that can handle different types of noise and restore the background from superimposed images. The DMGN employs a coarse-to-fine generative process, wherein a coarse background image and a noise image are generated in parallel, and the noise image is used to refine the background image, resulting in a higher-quality background image. We use the Residual Deep-Masking Cell, a novel core operating unit, to enhance the effective information and suppress the negative information during image generation. Our proposed DMGN can generate both high-quality background images and noisy images progressively by iteratively employing this Residual Deep-Masking Cell. Additionally, we propose a two-pronged strategy that effectively leverages the generated noise image as contrasting cues to facilitate the refinement of the background image. Our extensive experiments across three typical tasks for image background restoration demonstrate that the DMGN outperforms state-of-the-art methods that are designed for each single task.",1
"Solar cell electroluminescence (EL) defect segmentation is an interesting and challenging topic. Many methods have been proposed for EL defect detection, but these methods are still unsatisfactory due to the diversity of the defect and background. In this paper, we provide a new idea of using generative adversarial network (GAN) for defect segmentation. Firstly, the GAN-based method removes the defect region in the input defective image to get a defect-free image, while keeping the background almost unchanged. Then, the subtracted image is obtained by making difference between the defective input image with the generated defect-free image. Finally, the defect region can be segmented through thresholding the subtracted image. To keep the background unchanged before and after image generation, we propose a novel strong identity GAN (SIGAN), which adopts a novel strong identity loss to constraint the background consistency. The SIGAN can be used not only for defect segmentation, but also small-samples defective dataset augmentation. Moreover, we release a new solar cell EL image dataset named as EL-2019, which includes three types of images: crack, finger interruption and defect-free. Experiments on EL-2019 dataset show that the proposed method achieves 90.34% F-score, which outperforms many state-of-the-art methods in terms of solar cell defects segmentation results.",0
"The task of segmenting defects in solar cell electroluminescence (EL) is both interesting and challenging. While there have been many proposed methods for detecting EL defects, they have yet to provide satisfactory results due to the diverse nature of both the defects and the background. In this study, we introduce a new approach that utilizes a generative adversarial network (GAN) for defect segmentation. Our GAN-based method first removes the defect region from the input image to generate a defect-free image that maintains the background almost unchanged. By subtracting the input defective image from the generated defect-free image, we obtain a subtracted image that can be thresholded to segment the defect region. To ensure that the background remains consistent before and after image generation, we introduce a novel strong identity GAN (SIGAN) that adopts a strong identity loss. This approach can not only be used for defect segmentation, but also for augmenting small-samples defective datasets. Additionally, we have created a new solar cell EL image dataset called EL-2019, which includes three types of images: crack, finger interruption, and defect-free. Our experiments on the EL-2019 dataset show that our proposed method achieves a 90.34% F-score, outperforming many state-of-the-art methods in terms of solar cell defect segmentation.",1
"Pose-guided person image generation usually involves using paired source-target images to supervise the training, which significantly increases the data preparation effort and limits the application of the models. To deal with this problem, we propose a novel multi-level statistics transfer model, which disentangles and transfers multi-level appearance features from person images and merges them with pose features to reconstruct the source person images themselves. So that the source images can be used as supervision for self-driven person image generation. Specifically, our model extracts multi-level features from the appearance encoder and learns the optimal appearance representation through attention mechanism and attributes statistics. Then we transfer them to a pose-guided generator for re-fusion of appearance and pose. Our approach allows for flexible manipulation of person appearance and pose properties to perform pose transfer and clothes style transfer tasks. Experimental results on the DeepFashion dataset demonstrate our method's superiority compared with state-of-the-art supervised and unsupervised methods. In addition, our approach also performs well in the wild.",0
"The conventional approach to pose-guided person image generation involves using paired source-target images for training, which can be time-consuming and limit the model's applicability. To address this issue, we present a new multi-level statistics transfer model that isolates and transfers multi-level appearance features from person images and combines them with pose features to reconstruct the source person images. This way, the source images can be utilized to supervise self-driven person image generation. Our model leverages an attention mechanism and attributes statistics to extract multi-level features from the appearance encoder and learn the optimal appearance representation. We then transfer these representations to a pose-guided generator, which fuses the appearance and pose. Our method enables flexible manipulation of person appearance and pose to perform pose and clothes style transfer tasks. We conducted experiments on the DeepFashion dataset, which demonstrated that our approach outperforms other supervised and unsupervised methods. Moreover, our approach performs well in real-world scenarios.",1
"We propose a novel transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local writing style patterns. The proposed HWT captures the long and short range relationships within the style examples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder attention that enables style-content entanglement by gathering the style representation of each query character. To the best of our knowledge, we are the first to introduce a transformer-based generative network for styled handwritten text generation. Our proposed HWT generates realistic styled handwritten text images and significantly outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT generalizes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images.",0
"We present an innovative method for generating styled handwritten text images called the HWT approach, which uses a transformer-based model. Our approach aims to learn both style-content relationship and global/local writing style patterns. To achieve this, the HWT employs a self-attention mechanism to capture the long and short-range dependencies of style examples. Additionally, the transformer-based HWT has an encoder-decoder attention that enables style-content relationship by gathering the style representation of each query character. Our proposed method outperforms the state-of-the-art model in generating realistic styled handwritten text images, as demonstrated through qualitative, quantitative, and human-based evaluations. Moreover, the HWT generalizes well, handling arbitrary text length and any desired writing style, even in a few-shot setting where words and writing style are unseen during training.",1
"This paper studies the problem of learning the conditional distribution of a high-dimensional output given an input, where the output and input may belong to two different domains, e.g., the output is a photo image and the input is a sketch image. We solve this problem by cooperative training of a fast thinking initializer and slow thinking solver. The initializer generates the output directly by a non-linear transformation of the input as well as a noise vector that accounts for latent variability in the output. The slow thinking solver learns an objective function in the form of a conditional energy function, so that the output can be generated by optimizing the objective function, or more rigorously by sampling from the conditional energy-based model. We propose to learn the two models jointly, where the fast thinking initializer serves to initialize the sampling of the slow thinking solver, and the solver refines the initial output by an iterative algorithm. The solver learns from the difference between the refined output and the observed output, while the initializer learns from how the solver refines its initial output. We demonstrate the effectiveness of the proposed method on various conditional learning tasks, e.g., class-to-image generation, image-to-image translation, and image recovery. The advantage of our method over GAN-based methods is that our method is equipped with a slow thinking process that refines the solution guided by a learned objective function.",0
"The focus of this research is on learning the conditional distribution of a high-dimensional output given an input, where the input and output may belong to different domains, such as a sketch image and a photo image. Our solution to this problem involves cooperative training of a fast thinking initializer and a slow thinking solver. The initializer generates the output through a non-linear transformation of the input and a noise vector that accounts for variability in the output. The slow thinking solver learns an objective function in the form of a conditional energy function, which allows the output to be generated by optimizing the objective function or by sampling from the energy-based model. We propose a joint learning approach, where the fast thinking initializer initializes the sampling process of the slow thinking solver, and the solver refines the initial output through an iterative algorithm. The solver learns from the difference between the refined output and the observed output, while the initializer learns from how the solver refines its initial output. Our method is effective in various conditional learning tasks, such as class-to-image generation, image-to-image translation, and image recovery. Unlike GAN-based methods, our method employs a slow thinking process that refines the solution guided by a learned objective function.",1
"The Generative Adversarial Network (GAN) is a state-of-the-art technique in the field of deep learning. A number of recent papers address the theory and applications of GANs in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance, e.g., Inception Score (IS) and statistical metrics, e.g., Fr\'echet Inception Distance (FID). Here, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. We characterize the performance of a GAN as an image generator according to three aspects: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. A GAN should not generate a few different images repeatedly. Based on the three aspects of ideal GANs, we have designed the Likeness Score (LS) to evaluate GAN performance, and have applied it to evaluate several typical GANs. We compared our proposed measure with two commonly used GAN evaluation methods: IS and FID, and four additional measures. Furthermore, we discuss how these evaluations could help us deepen our understanding of GANs and improve their performance.",0
"The field of deep learning employs the advanced technique known as Generative Adversarial Network (GAN). Numerous papers have recently studied the theory and applications of GANs in image processing. However, there have been limited studies that directly assess the outputs of GANs, with most focusing on classification performance and statistical metrics. In this study, we propose a new method to evaluate GANs that involves analyzing the images they generate. We consider three essential aspects of image generation, including creativity, inheritance, and diversity. Our newly developed Likeness Score (LS) evaluates GAN performance based on these three aspects. We assess several typical GANs using the LS method and compare it with commonly used evaluation methods, such as IS and FID, as well as four additional measures. Finally, we discuss how these evaluations could contribute to a deeper understanding of GANs and enhance their performance.",1
"Face verification has come into increasing focus in various applications including the European Entry/Exit System, which integrates face recognition mechanisms. At the same time, the rapid advancement of biometric authentication requires extensive performance tests in order to inhibit the discriminatory treatment of travellers due to their demographic background. However, the use of face images collected as part of border controls is restricted by the European General Data Protection Law to be processed for no other reason than its original purpose. Therefore, this paper investigates the suitability of synthetic face images generated with StyleGAN and StyleGAN2 to compensate for the urgent lack of publicly available large-scale test data. Specifically, two deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR 29794-5) face image quality assessment algorithm is utilized to compare the applicability of synthetic face images compared to real face images extracted from the FRGC dataset. Finally, based on the analysis of impostor score distributions and utility score distributions, our experiments reveal negligible differences between StyleGAN vs. StyleGAN2, and further also minor discrepancies compared to real face images.",0
"The use of face verification has become increasingly important in different fields, such as the European Entry/Exit System, which incorporates face recognition methods. As biometric authentication continues to advance rapidly, it is necessary to conduct extensive performance tests to prevent discriminatory treatment of travelers based on their demographic background. However, the European General Data Protection Law restricts the use of face images collected during border controls for any other purpose than their original intention. As a result, this study aims to assess the suitability of synthetic face images created with StyleGAN and StyleGAN2 to address the lack of publicly available large-scale test data. Three face image quality assessment algorithms, including two deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR 29794-5), are utilized to compare the applicability of synthetic face images to real face images obtained from the FRGC dataset. After analyzing the impostor score distributions and utility score distributions, our experiments reveal minimal differences between StyleGAN and StyleGAN2 and minor discrepancies compared to real face images.",1
"Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in various image generation and synthesis tasks, yet their notoriously high computational cost hinders their efficient deployment on edge devices. Directly applying generic compression approaches yields poor results on GANs, which motivates a number of recent GAN compression works. While prior works mainly accelerate conditional GANs, e.g., pix2pix and CycleGAN, compressing state-of-the-art unconditional GANs has rarely been explored and is more challenging. In this paper, we propose novel approaches for unconditional GAN compression. We first introduce effective channel pruning and knowledge distillation schemes specialized for unconditional GANs. We then propose a novel content-aware method to guide the processes of both pruning and distillation. With content-awareness, we can effectively prune channels that are unimportant to the contents of interest, e.g., human faces, and focus our distillation on these regions, which significantly enhances the distillation quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2 by 11x with visually negligible image quality loss compared to the full-size model. More interestingly, when applied to various image manipulation tasks, our compressed model forms a smoother and better disentangled latent manifold, making it more effective for image editing.",0
"Various image generation and synthesis tasks rely heavily on Generative Adversarial Networks (GANs), such as StyleGAN2, but their high computational cost makes their deployment on edge devices challenging. Applying generic compression methods to GANs results in poor outcomes, which has led to recent research on GAN compression. Although previous works have focused on accelerating conditional GANs like pix2pix and CycleGAN, there has been limited exploration of compressing state-of-the-art unconditional GANs, which is more difficult. In this study, we propose new approaches for compressing unconditional GANs, including specialized channel pruning and knowledge distillation methods, along with a content-aware method to aid both processes. With content-awareness, we can effectively prune channels that are not relevant to the contents of interest while enhancing the distillation quality. Our compression method achieves substantial improvements over the state-of-the-art on StyleGAN2 and SN-GAN, reducing the FLOPs of StyleGAN2 by 11x with visually insignificant image quality loss compared to the full-size model. Additionally, our compressed model results in a smoother and better disentangled latent manifold, making it more effective for various image manipulation tasks.",1
"Conditional Generative Adversarial Networks (cGAN) were designed to generate images based on the provided conditions, \eg, class-level distributions. However, existing methods have used the same generating architecture for all classes. This paper presents a novel idea that adopts NAS to find a distinct architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability. We evaluate our approach on CIFAR10 and CIFAR100. Besides achieving better image generation quality in terms of FID scores, we discover several insights that are helpful in designing cGAN models. Code is available at https://github.com/PeterouZh/NAS_cGAN.",0
"The Conditional Generative Adversarial Networks (cGAN) were created to produce images based on given conditions, such as the distribution at a class level. However, current techniques use the same architecture for generating images across all classes. This study presents a fresh concept that utilizes NAS to find an individual architecture for each class. The search space consists of regular and class-modulated convolutions, with the latter designed to introduce class-specific information while avoiding a decrease in training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization to prevent the search cost from increasing with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm, and a moving average is applied for better stability. We assessed our approach on CIFAR10 and CIFAR100 and discovered several helpful insights for designing cGAN models, in addition to achieving better image generation quality in terms of FID scores. The code is available at https://github.com/PeterouZh/NAS_cGAN.",1
"This paper presents a new approach for synthesizing a novel street-view panorama given an overhead satellite image. Taking a small satellite image patch as input, our method generates a Google's omnidirectional street-view type panorama, as if it is captured from the same geographical location as the center of the satellite patch. Existing works tackle this task as an image generation problem which adopts generative adversarial networks to implicitly learn the cross-view transformations, while ignoring the domain relevance. In this paper, we propose to explicitly establish the geometric correspondences between the two-view images so as to facilitate the cross-view transformation learning. Specifically, we observe that when a 3D point in the real world is visible in both views, there is a deterministic mapping between the projected points in the two-view images given the height information of this 3D point. Motivated by this, we develop a novel Satellite to Street-view image Projection (S2SP) module which explicitly establishes such geometric correspondences and projects the satellite images to the street viewpoint. With these projected satellite images as network input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates images that better respect the scene geometry than existing approaches.",0
"The aim of this paper is to introduce a new method for synthesizing a street-view panorama from an overhead satellite image. Unlike existing approaches which use generative adversarial networks to learn cross-view transformations, our approach establishes explicit geometric correspondences between the two images in order to facilitate transformation learning. By developing a Satellite to Street-view image Projection (S2SP) module, we are able to project satellite images to street viewpoint and generate realistic street-view panoramas that are consistent with the satellite image. Our method is trained in an end-to-end manner and is shown to generate images that better respect the scene geometry than existing approaches, as demonstrated by experiments on two benchmark datasets.",1
"Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying distribution of the observed data. However, how the features learned from solving the task of image generation are applicable to other vision tasks remains seldom explored. In this work, we show that learning to synthesize images can bring remarkable hierarchical visual features that are generalizable across a wide range of applications. Specifically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise representation to train a novel hierarchical encoder. The visual feature produced by our encoder, termed as Generative Hierarchical Feature (GH-Feat), has strong transferability to both generative and discriminative tasks, including image editing, image harmonization, image classification, face verification, landmark detection, and layout prediction. Extensive qualitative and quantitative experimental results demonstrate the appealing performance of GH-Feat.",0
"Recently, Generative Adversarial Networks (GANs) have made significant advancements in image synthesis by understanding the distribution of the observed data. However, there is limited exploration on how the features learned from image generation can be applied to other vision tasks. Our study demonstrates that the process of learning to synthesize images can produce remarkable hierarchical visual features that are transferable to various applications. We use the pre-trained StyleGAN generator as a learned loss function and its layer-wise representation to train a novel hierarchical encoder. This encoder produces Generative Hierarchical Features (GH-Feat), which have strong transferability to generative and discriminative tasks, such as image editing, image harmonization, image classification, face verification, landmark detection, and layout prediction. Our extensive qualitative and quantitative experiments prove the promising performance of GH-Feat.",1
"The task of image generation started to receive some attention from artists and designers to inspire them in new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control on them. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user choice by performing several optimization steps to recover optimal parameters from the model's latent space. We tested several exploration methods starting with classic gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so that they can even be used without numerical criterion, without inspirational image, but with only with human preference. Thus, by iterating on one's preferences we could make robust Facial Composite or Fashion Generation algorithms. High resolution of the produced design generations are obtained using progressive growing of GANs. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",0
"Artists and designers have become interested in image generation as a means of generating new ideas. However, the process can be time-consuming due to the lack of tools available for utilizing deep generative models like Generative Adversarial Networks. In this study, we propose a simple approach to inspire creators with new generations learned from a dataset of their choice while providing some degree of control. Our method involves using an optimization technique to find the best latent parameters for generating a new image that closely matches an input image of the user's choice. We tested various optimization methods, including gradient descent and gradient-free optimizers. The latter can be used without numerical criteria or inspirational images and rely only on human preferences. By iterating on these preferences, we can develop robust algorithms for generating facial composites or fashion designs. We produce high-resolution images using progressive growing of GANs. Our results demonstrate that our approach is effective in retrieving satisfactory images for a variety of datasets.",1
"The interest of the deep learning community in image synthesis has grown massively in recent years. Nowadays, deep generative methods, and especially Generative Adversarial Networks (GANs), are leading to state-of-the-art performance, capable of synthesizing images that appear realistic. While the efforts for improving the quality of the generated images are extensive, most attempts still consider the generator part as an uncorroborated ""black-box"". In this paper, we aim to provide a better understanding and design of the image generation process. We interpret existing generators as implicitly relying on sparsity-inspired models. More specifically, we show that generators can be viewed as manifestations of the Convolutional Sparse Coding (CSC) and its Multi-Layered version (ML-CSC) synthesis processes. We leverage this observation by explicitly enforcing a sparsifying regularization on appropriately chosen activation layers in the generator, and demonstrate that this leads to improved image synthesis. Furthermore, we show that the same rationale and benefits apply to generators serving inverse problems, demonstrated on the Deep Image Prior (DIP) method.",0
"The deep learning community has shown significant interest in image synthesis in recent years, with deep generative methods, particularly Generative Adversarial Networks (GANs), leading to exceptional performance in creating realistic images. Despite the extensive efforts to improve the quality of generated images, the generator component is often seen as a ""black-box."" In this article, we aim to enhance the understanding and design of the image generation process. We interpret existing generators as implicitly utilizing sparsity-inspired models, specifically, the Convolutional Sparse Coding (CSC) and its Multi-Layered version (ML-CSC) synthesis processes. By enforcing sparsifying regularization on activation layers in the generator, we demonstrate improved image synthesis. Additionally, we apply the same rationale and benefits to generators serving inverse problems, as demonstrated by the Deep Image Prior (DIP) method.",1
"The significant progress on Generative Adversarial Networks (GANs) has facilitated realistic single-object image generation based on language input. However, complex-scene generation (with various interactions among multiple objects) still suffers from messy layouts and object distortions, due to diverse configurations in layouts and appearances. Prior methods are mostly object-driven and ignore their inter-relations that play a significant role in complex-scene images. This work explores relationship-aware complex-scene image generation, where multiple objects are inter-related as a scene graph. With the help of relationships, we propose three major updates in the generation framework. First, reasonable spatial layouts are inferred by jointly considering the semantics and relationships among objects. Compared to standard location regression, we show relative scales and distances serve a more reliable target. Second, since the relations between objects significantly influence an object's appearance, we design a relation-guided generator to generate objects reflecting their relationships. Third, a novel scene graph discriminator is proposed to guarantee the consistency between the generated image and the input scene graph. Our method tends to synthesize plausible layouts and objects, respecting the interplay of multiple objects in an image. Experimental results on Visual Genome and HICO-DET datasets show that our proposed method significantly outperforms prior arts in terms of IS and FID metrics. Based on our user study and visual inspection, our method is more effective in generating logical layout and appearance for complex-scenes.",0
"Generative Adversarial Networks (GANs) have made great strides in generating realistic single-object images based on language input, but creating complex-scene images with multiple objects and interactions remains challenging due to the variety of layouts and appearances. Previous methods focused on individual objects, ignoring their relationships, which are crucial for complex-scene images. Our work introduces relationship-aware complex-scene image generation, where multiple objects are inter-connected as a scene graph. We propose three major updates to the generation framework. First, we use relationships to infer spatial layouts that consider the semantics and relationships among objects. Second, we design a relation-guided generator to generate objects that reflect their relationships. Third, we propose a novel scene graph discriminator to ensure consistency between the generated image and the input scene graph. Our method produces plausible layouts and objects, respecting the interplay of multiple objects in an image. Our experimental results show that our proposed method outperforms prior methods in terms of IS and FID metrics and is more effective in generating logical layouts and appearances for complex-scenes.",1
"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",0
"Significant advancements in deep generative models have enabled the creation of photorealistic images. However, most of these approaches only operate in the two-dimensional image domain, neglecting the three-dimensional nature of the world. To address this limitation, some recent works propose generative models that are 3D-aware, meaning that they model scenes in 3D and then render them to the image plane in a differentiable manner. Although this approach results in impressive 3D consistency, it requires modeling the camera, assuming fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is necessary for real-world data, and the results may deteriorate if the data distribution is not matched. Our hypothesis is that by jointly learning a camera generator and an image generator, we can develop a more principled approach to 3D-aware image synthesis. Additionally, we propose decomposing the scene into a background and foreground model to create more efficient and disentangled scene representations. We train a 3D- and camera-aware generative model from raw, unposed image collections, which accurately recovers the image and camera data distribution. At test time, our model allows for explicit control over the camera, as well as the shape and appearance of the scene.",1
"Generative adversarial networks (GANs) have shown impressive results in both unconditional and conditional image generation. In recent literature, it is shown that pre-trained GANs, on a different dataset, can be transferred to improve the image generation from a small target data. The same, however, has not been well-studied in the case of conditional GANs (cGANs), which provides new opportunities for knowledge transfer compared to unconditional setup. In particular, the new classes may borrow knowledge from the related old classes, or share knowledge among themselves to improve the training. This motivates us to study the problem of efficient conditional GAN transfer with knowledge propagation across classes. To address this problem, we introduce a new GAN transfer method to explicitly propagate the knowledge from the old classes to the new classes. The key idea is to enforce the popularly used conditional batch normalization (BN) to learn the class-specific information of the new classes from that of the old classes, with implicit knowledge sharing among the new ones. This allows for an efficient knowledge propagation from the old classes to the new ones, with the BN parameters increasing linearly with the number of new classes. The extensive evaluation demonstrates the clear superiority of the proposed method over state-of-the-art competitors for efficient conditional GAN transfer tasks. The code is available at: https://github.com/mshahbazi72/cGANTransfer",0
"GANs have demonstrated impressive outcomes in generating both unconditional and conditional images. Recent research has exhibited that pre-trained GANs, which were trained on different datasets, can be utilized to enhance image generation from a limited target data. However, the same has not been extensively studied in the context of conditional GANs (cGANs). This presents new prospects for knowledge transfer in comparison to the unconditional setup, as new classes can acquire knowledge from related old classes or share knowledge among themselves to enhance training. Consequently, we are motivated to investigate the issue of efficient cGAN transfer with knowledge propagation across classes. To tackle this problem, we propose a novel GAN transfer approach that explicitly propagates knowledge from old classes to new ones. We achieve this by enforcing conditional batch normalization (BN) to learn class-specific information of new classes from old classes, with implicit knowledge sharing among new ones. This enables efficient knowledge propagation from old classes to new ones, with BN parameters increasing linearly with the number of new classes. Extensive evaluations demonstrate that our method outperforms state-of-the-art competitors for efficient conditional GAN transfer tasks. The code for this method is available at: https://github.com/mshahbazi72/cGANTransfer.",1
"Person image synthesis, e.g., pose transfer, is a challenging problem due to large variation and occlusion. Existing methods have difficulties predicting reasonable invisible regions and fail to decouple the shape and style of clothing, which limits their applications on person image editing. In this paper, we propose PISE, a novel two-stage generative model for Person Image Synthesis and Editing, which is able to generate realistic person images with desired poses, textures, or semantic layouts. For human pose transfer, we first synthesize a human parsing map aligned with the target pose to represent the shape of clothing by a parsing generator, and then generate the final image by an image generator. To decouple the shape and style of clothing, we propose joint global and local per-region encoding and normalization to predict the reasonable style of clothing for invisible regions. We also propose spatial-aware normalization to retain the spatial context relationship in the source image. The results of qualitative and quantitative experiments demonstrate the superiority of our model on human pose transfer. Besides, the results of texture transfer and region editing show that our model can be applied to person image editing.",0
"Generating realistic person images with desired poses, textures, or semantic layouts is a difficult task, particularly due to large variation and occlusion in person image synthesis such as pose transfer. The existing methods have limitations in predicting reasonable invisible regions and separating the shape and style of clothing, which restrict their use in person image editing. This paper introduces PISE, a two-stage generative model for Person Image Synthesis and Editing, which can generate realistic person images with desired poses, textures, or semantic layouts. For human pose transfer, the model first generates a human parsing map that aligns with the target pose to represent the shape of clothing, followed by image generation. To overcome the difficulties in separating the shape and style of clothing, the model uses per-region encoding and normalization to predict the reasonable style of clothing for invisible regions. Additionally, spatial-aware normalization is used to retain the spatial context relationship in the source image. The outcomes of qualitative and quantitative experiments show that our model is superior in human pose transfer. Furthermore, the model can also be applied to texture transfer and region editing in person image editing.",1
"Image generation has rapidly evolved in recent years. Modern architectures for adversarial training allow to generate even high resolution images with remarkable quality. At the same time, more and more effort is dedicated towards controlling the content of generated images. In this paper, we take one further step in this direction and propose a conditional generative adversarial network (GAN) that generates images with a defined number of objects from given classes. This entails two fundamental abilities (1) being able to generate high-quality images given a complex constraint and (2) being able to count object instances per class in a given image. Our proposed model modularly extends the successful StyleGAN2 architecture with a count-based conditioning as well as with a regression sub-network to count the number of generated objects per class during training. In experiments on three different datasets, we show that the proposed model learns to generate images according to the given multiple-class count condition even in the presence of complex backgrounds. In particular, we propose a new dataset, CityCount, which is derived from the Cityscapes street scenes dataset, to evaluate our approach in a challenging and practically relevant scenario.",0
"In recent years, there has been rapid development in image generation technology. With modern architectures for adversarial training, high resolution images of exceptional quality can now be generated. Additionally, there is a growing emphasis on controlling the content of generated images. In this study, we propose a conditional generative adversarial network (GAN) that can generate images with a specific number of objects from predetermined classes. This requires two essential abilities: (1) the ability to generate high-quality images while adhering to complex constraints and (2) the ability to count object instances per class in a given image. Our proposed model extends the successful StyleGAN2 architecture by incorporating count-based conditioning and a regression sub-network to count the number of generated objects per class during training. We conducted experiments on three different datasets, including a new dataset called CityCount derived from the Cityscapes street scenes dataset, to evaluate our model's ability to generate images according to the given multiple-class count condition, even in the presence of complex backgrounds.",1
"Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID).",0
"Large-scale image datasets power Generative Adversarial Networks (GANs) to produce impressive unconditional image generation results. However, generated images are still easy to detect, particularly on datasets with high variance such as bedrooms and churches. This paper suggests several enhancements to further advance image generation capabilities. A novel dual contrastive loss improves the discriminator's ability to learn generalized and distinct representations, thus incentivizing generation. Attention is also revisited, and several attention blocks are experimented with in the generator. Attention remains an important module for successful image generation, despite not being used in recent state-of-the-art models. Different attention architectures are studied in the discriminator, and a reference attention mechanism is proposed. By combining these remedies, the state-of-the-art Fr\'{e}chet Inception Distance (FID) is improved by at least 17.5% on various benchmark datasets. Even more significant improvements are observed on compositional synthetic scenes, with up to 47.5% improvement in FID.",1
"Localized Narratives is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TReCS, a sequential model that exploits this grounding to generate images. TReCS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.",0
"The dataset Localized Narratives contains descriptive language paired with mouse traces to provide visual grounding for phrases. Our proposed model, TReCS, utilizes this grounding to generate images. TReCS retrieves segmentation masks and predicts object labels aligned with mouse traces from the descriptions. These alignments are used to position masks, resulting in a fully covered segmentation canvas. The final image is generated using a segmentation-to-image generator with this canvas. Our approach outperforms existing direct text-to-image models in both automatic metrics and human evaluations, producing more photo-realistic images that better match the given descriptions.",1
"The moments (a.k.a., mean and standard deviation) of latent features are often removed as noise when training image recognition models, to increase stability and reduce training time. However, in the field of image generation, the moments play a much more central role. Studies have shown that the moments extracted from instance normalization and positional normalization can roughly capture style and shape information of an image. Instead of being discarded, these moments are instrumental to the generation process. In this paper we propose Moment Exchange, an implicit data augmentation method that encourages the model to utilize the moment information also for recognition models. Specifically, we replace the moments of the learned features of one training image by those of another, and also interpolate the target labels -- forcing the model to extract training signal from the moments in addition to the normalized features. As our approach is fast, operates entirely in feature space, and mixes different signals than prior methods, one can effectively combine it with existing augmentation approaches. We demonstrate its efficacy across several recognition benchmark data sets where it improves the generalization capability of highly competitive baseline networks with remarkable consistency.",0
"When training image recognition models, the mean and standard deviation of latent features are often removed to eliminate noise and improve stability and training time. However, in image generation, these moments are crucial in capturing style and shape information. In this paper, we introduce Moment Exchange, a data augmentation method that encourages recognition models to utilize moment information. Our method replaces the learned features' moments from one training image with those of another and interpolates target labels. This forces the model to extract training signal from moments and normalized features. Our approach is fast, operates entirely in feature space, and mixes different signals from prior methods. It can effectively combine with existing augmentation approaches. We demonstrate Moment Exchange's efficacy across several recognition benchmark data sets, improving the generalization capability of highly competitive baseline networks with remarkable consistency.",1
"Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-tosketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy.",0
"The synthesis and recognition of face photo-sketches have various applications in the fields of law enforcement and digital entertainment. While generative adversarial networks (GANs) have significantly improved image synthesis quality, they have not explicitly focused on recognition. In this study, we present the Identity-Aware CycleGAN (IACycleGAN) model that uses a new perceptual loss to supervise image generation. This model enhances the synthesis of key facial regions, such as eyes and nose, which are crucial for identity recognition. Additionally, we propose a mutual optimization procedure between the synthesis and recognition models, which iteratively generates better images and enhances recognition by using the triplet loss of generated and real samples. The proposed method outperforms several state-of-the-art methods in terms of synthetic image quality and photo-sketch recognition accuracy, as demonstrated through extensive experiments on CUFS and CUFSF databases for photo-to-sketch and sketch-to-photo tasks.",1
"While Generative Adversarial Networks (GANs) show increasing performance and the level of realism is becoming indistinguishable from natural images, this also comes with high demands on data and computation. We show that state-of-the-art GAN models -- such as they are being publicly released by researchers and industry -- can be used for a range of applications beyond unconditional image generation. We achieve this by an iterative scheme that also allows gaining control over the image generation process despite the highly non-linear latent spaces of the latest GAN models. We demonstrate that this opens up the possibility to re-use state-of-the-art, difficult to train, pre-trained GANs with a high level of control even if only black-box access is granted. Our work also raises concerns and awareness that the use cases of a published GAN model may well reach beyond the creators' intention, which needs to be taken into account before a full public release. Code is available at https://github.com/a514514772/hijackgan.",0
"Although Generative Adversarial Networks (GANs) are becoming increasingly realistic and their performance is improving, this advancement comes with a high demand for data and computation. Our study demonstrates that state-of-the-art GAN models, which are publicly available from researchers and industry, can have numerous applications beyond unconditional image generation. With an iterative approach, we have attained control over the image generation process, despite the non-linear latent spaces of the latest GAN models. Therefore, our research indicates that challenging pre-trained GANs can be reused with a high degree of control, even if only black-box access is provided. However, our work also highlights the need to consider the potential unintended uses of a publicly released GAN model before making it fully available. Our code is accessible at https://github.com/a514514772/hijackgan.",1
"The first step toward Seed Phenotyping i.e. the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of pa-rameters that form more complex traits is the identification of seed type. Generally, a plant re-searcher inspects the visual attributes of a seed such as size, shape, area, color and texture to identify the seed type, a process that is tedious and labor-intensive. Advances in the areas of computer vision and deep learning have led to the development of convolutional neural networks (CNN) that aid in classification using images. While they classify efficiently, a key bottleneck is the need for an extensive amount of labelled data to train the CNN before it can be put to the task of classification. The work leverages the concepts of Contrastive Learning and Domain Randomi-zation in order to achieve the same. Briefly, domain randomization is the technique of applying models trained on images containing simulated objects to real-world objects. The use of synthetic images generated from a representational sample crop of real-world images alleviates the need for a large volume of test subjects. As part of the work, synthetic image datasets of five different types of seed images namely, canola, rough rice, sorghum, soy and wheat are applied to three different self-supervised learning frameworks namely, SimCLR, Momentum Contrast (MoCo) and Build Your Own Latent (BYOL) where ResNet-50 is used as the backbone in each of the networks. When the self-supervised models are fine-tuned with only 5% of the labels from the synthetic dataset, results show that MoCo, the model that yields the best performance of the self-supervised learning frameworks in question, achieves an accuracy of 77% on the test dataset which is only ~13% less than the accuracy of 90% achieved by ResNet-50 trained on 100% of the labels.",0
"The initial stage of Seed Phenotyping involves identifying the seed type, which requires a plant researcher to visually inspect attributes such as size, shape, color, area, and texture. However, this process is time-consuming and laborious. Recent advancements in computer vision and deep learning have led to the development of convolutional neural networks (CNN) that aid in seed classification using images. However, the CNN requires a large amount of labeled data for training, which can be a bottleneck. To overcome this, the work employs the concepts of Contrastive Learning and Domain Randomization. Synthetic image datasets of five seed types are applied to three self-supervised learning frameworks using ResNet-50 as the backbone. When fine-tuned with only 5% of the labels from the synthetic dataset, the MoCo framework achieves an accuracy of 77% on the test dataset, which is only ~13% less than that achieved by ResNet-50 trained on 100% of the labels.",1
"Although Generative Adversarial Networks (GANs) are successfully applied to diverse fields, training GANs on synthetic aperture radar (SAR) data is a challenging task mostly due to speckle noise. On the one hands, in a learning perspective of human's perception, it is natural to learn a task by using various information from multiple sources. However, in the previous GAN works on SAR target image generation, the information on target classes has only been used. Due to the backscattering characteristics of SAR image signals, the shapes and structures of SAR target images are strongly dependent on their pose angles. Nevertheless, the pose angle information has not been incorporated into such generative models for SAR target images. In this paper, we firstly propose a novel GAN-based multi-task learning (MTL) method for SAR target image generation, called PeaceGAN that uses both pose angle and target class information, which makes it possible to produce SAR target images of desired target classes at intended pose angles. For this, the PeaceGAN has two additional structures, a pose estimator and an auxiliary classifier, at the side of its discriminator to combine the pose and class information more efficiently. In addition, the PeaceGAN is jointly learned in an end-to-end manner as MTL with both pose angle and target class information, thus enhancing the diversity and quality of generated SAR target images The extensive experiments show that taking an advantage of both pose angle and target class learning by the proposed pose estimator and auxiliary classifier can help the PeaceGAN's generator effectively learn the distributions of SAR target images in the MTL framework, so that it can better generate the SAR target images more flexibly and faithfully at intended pose angles for desired target classes compared to the recent state-of-the-art methods.",0
"Generating synthetic aperture radar (SAR) data using Generative Adversarial Networks (GANs) poses a challenge due to speckle noise. Previous GAN works have only utilized target class information, neglecting the importance of pose angle information in SAR image signal characteristics. To address this, we propose PeaceGAN, a GAN-based multi-task learning (MTL) method that uses both pose angle and target class information to generate SAR target images of desired classes at intended angles. PeaceGAN incorporates a pose estimator and auxiliary classifier at the discriminator to efficiently combine pose and class information. The end-to-end MTL framework enhances the diversity and quality of generated SAR target images. Our experiments demonstrate that PeaceGAN, with the added benefit of pose angle and target class learning, effectively generates SAR target images at intended angles and desired classes, surpassing current state-of-the-art methods.",1
"Generative Adversarial Networks (GANs) advance face synthesis through learning the underlying distribution of observed data. Despite the high-quality generated faces, some minority groups can be rarely generated from the trained models due to a biased image generation process. To study the issue, we first conduct an empirical study on a pre-trained face synthesis model. We observe that after training the GAN model not only carries the biases in the training data but also amplifies them to some degree in the image generation process. To further improve the fairness of image generation, we propose an interpretable baseline method to balance the output facial attributes without retraining. The proposed method shifts the interpretable semantic distribution in the latent space for a more balanced image generation while preserving the sample diversity. Besides producing more balanced data regarding a particular attribute (e.g., race, gender, etc.), our method is generalizable to handle more than one attribute at a time and synthesize samples of fine-grained subgroups. We further show the positive applicability of the balanced data sampled from GANs to quantify the biases in other face recognition systems, like commercial face attribute classifiers and face super-resolution algorithms.",0
"GANs have improved facial synthesis by learning the distribution of available data, resulting in high-quality generated faces. However, certain minority groups may not be adequately represented due to biased image generation processes. To address this issue, we conducted an empirical study on a pre-trained face synthesis model and found that biases in the training data were amplified during image generation. To enhance the fairness of the image generation process, we suggest an interpretable baseline method that balances the output facial attributes without requiring retraining. This approach changes the semantic distribution in the latent space to create a more balanced image generation process while preserving sample diversity. Our method is not limited to balancing a single attribute, and it can synthesize samples of fine-grained subgroups. Moreover, we demonstrate that balanced data generated from GANs can be used to measure biases in other face recognition systems, such as commercial face attribute classifiers and face super-resolution algorithms.",1
"In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity learns the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipulation. Our model can produce diverse and high-quality images with an unprecedented resolution at 1024. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or without instance guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",0
"The authors of this study introduce TediGAN, a fresh approach to generating and manipulating images using textual descriptions. The proposed method comprises three parts: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module converts genuine images into the latent space of a well-trained StyleGAN. The visual-linguistic similarity trains the text-image match by mapping the image and text into a shared embedding space. The instance-level optimization ensures identity preservation during manipulation. TediGAN can create high-quality images with unmatched resolution at 1024, using a style-mixing control mechanism that supports multi-modal input synthesis, such as sketches or semantic labels, with or without instance guidance. To support text-guided multi-modal synthesis, the authors introduce the Multi-Modal CelebA-HQ dataset, which includes actual face images, corresponding semantic segmentation maps, sketches, and text descriptions. The authors demonstrate the superior performance of their method with extensive experiments on this dataset. The code and data are available at https://github.com/weihaox/TediGAN.",1
"The conditional generative adversarial network (cGAN) is a powerful tool of generating high-quality images, but existing approaches mostly suffer unsatisfying performance or the risk of mode collapse. This paper presents Omni-GAN, a variant of cGAN that reveals the devil in designing a proper discriminator for training the model. The key is to ensure that the discriminator receives strong supervision to perceive the concepts and moderate regularization to avoid collapse. Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods (e.g., implicit neural representation, INR). Experiments validate the superior performance of Omni-GAN and Omni-INR-GAN in a wide range of image generation and restoration tasks. In particular, Omni-INR-GAN sets new records on the ImageNet dataset with impressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and 256, respectively, surpassing the previous records by 100+ points. Moreover, leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrary resolution, even up to x60+ higher resolution. Code is available.",0
"Although the conditional generative adversarial network (cGAN) is a useful tool for generating high-quality images, most current approaches suffer from poor performance or the risk of mode collapse. This article introduces Omni-GAN, a variation of cGAN that emphasizes the importance of designing an appropriate discriminator for model training. The key is to ensure that the discriminator receives strong supervision to understand concepts and moderate regularization to avoid collapse. Omni-GAN is simple to implement and can be easily integrated with standard encoding methods such as implicit neural representation (INR). Experiments demonstrate that Omni-GAN and Omni-INR-GAN are highly effective in a variety of image generation and restoration tasks. In particular, Omni-INR-GAN achieved impressive Inception scores of 262.85 and 343.22 for image sizes of 128 and 256, respectively, surpassing prior records by over 100 points on the ImageNet dataset. Additionally, by leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrarily high resolutions, even up to 60 times higher. The code is available for use.",1
"This paper proposes a method to extract the position and pose of vehicles in the 3D world from a single traffic camera. Most previous monocular 3D vehicle detection algorithms focused on cameras on vehicles from the perspective of a driver, and assumed known intrinsic and extrinsic calibration. On the contrary, this paper focuses on the same task using uncalibrated monocular traffic cameras. We observe that the homography between the road plane and the image plane is essential to 3D vehicle detection and the data synthesis for this task, and the homography can be estimated without the camera intrinsics and extrinsics. We conduct 3D vehicle detection by estimating the rotated bounding boxes (r-boxes) in the bird's eye view (BEV) images generated from inverse perspective mapping. We propose a new regression target called \textit{tailed~r-box} and a \textit{dual-view} network architecture which boosts the detection accuracy on warped BEV images. Experiments show that the proposed method can generalize to new camera and environment setups despite not seeing imaged from them during training.",0
"In this paper, a novel approach is presented for determining the position and pose of vehicles in the 3D world utilizing a single traffic camera. While previous monocular 3D vehicle detection algorithms have focused on cameras from the driver's perspective and assumed known intrinsic and extrinsic calibration, this paper concentrates on the same task using uncalibrated monocular traffic cameras. The authors identify the importance of the homography between the road plane and image plane for 3D vehicle detection and data synthesis, which can be estimated without camera intrinsics and extrinsics. The proposed method utilizes rotated bounding boxes (r-boxes) in bird's eye view (BEV) images generated through inverse perspective mapping and introduces a new regression target called the \textit{tailed r-box} and a \textit{dual-view} network architecture to enhance detection accuracy on warped BEV images. Experiments demonstrate the method's ability to generalize to new camera and environment setups, even without prior exposure to images from these setups during training.",1
"While generative adversarial networks (GANs) can successfully produce high-quality images, they can be challenging to control. Simplifying GAN-based image generation is critical for their adoption in graphic design and artistic work. This goal has led to significant interest in methods that can intuitively control the appearance of images generated by GANs. In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images' colors. We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN architecture to control the colors of GAN-generated images specified by a target color histogram feature. We then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network to keep the original image's content while changing the colors based on the given target histogram. We show that this histogram-based approach offers a better way to control GAN-generated and real images' colors while producing more compelling results compared to existing alternative strategies.",0
"Although generative adversarial networks (GANs) are capable of producing high-quality images, they can be difficult to manipulate. Simplifying the process of GAN-based image generation is crucial for their use in graphic design and art. As a result, there is considerable interest in techniques that enable intuitive control over the appearance of GAN-generated images. In this study, we introduce HistoGAN, a color histogram-based method for regulating the colors of GAN-generated images. We concentrate on color histograms because they provide an intuitive method for describing image color while remaining independent of domain-specific meanings. Our approach involves modifying the StyleGAN architecture to control the colors of GAN-generated images based on a target color histogram feature. We also describe extending HistoGAN to recolor real images using an encoder network trained alongside HistoGAN. The unsupervised model, ReHistoGAN, is designed to keep the original image's content while modifying its colors based on the provided target histogram. We demonstrate that our histogram-based method offers a more effective way to regulate the colors of GAN-generated and real images while producing more compelling results than current alternative strategies.",1
"Modern neural networks have been successful in many regression-based tasks such as face recognition, facial landmark detection, and image generation. In this work, we investigate an intuitive but understudied characteristic of modern neural networks, namely, the nonsmoothness. The experiments using synthetic data confirm that such operations as ReLU and max pooling in modern neural networks lead to nonsmoothness. We quantify the nonsmoothness using a feature named the sum of the magnitude of peaks (SMP) and model the input-output relationships for building blocks of modern neural networks. Experimental results confirm that our model can accurately predict the statistical behaviors of the nonsmoothness as it propagates through such building blocks as the convolutional layer, the ReLU activation, and the max pooling layer. We envision that the nonsmoothness feature can potentially be used as a forensic tool for regression-based applications of neural networks.",0
"Numerous regression-based tasks, such as face recognition, facial landmark detection, and image generation, have been accomplished by modern neural networks. This study delves into a less explored yet intuitive characteristic of these networks, which is nonsmoothness. Synthetic data experiments demonstrate that nonsmoothness is produced by operations like ReLU and max pooling in modern neural networks. To quantify nonsmoothness, we use a feature called the sum of the magnitude of peaks (SMP) and model the input-output relationships of building blocks in modern neural networks. Our experimental findings reveal that our model can accurately predict the statistical behavior of nonsmoothness as it propagates through building blocks like the convolutional layer, ReLU activation, and max pooling layer. We believe that nonsmoothness can potentially be employed as a forensic tool for regression-based neural network applications.",1
"We present a new method for few-shot human motion transfer that achieves realistic human image generation with only a small number of appearance inputs. Despite recent advances in single person motion transfer, prior methods often require a large number of training images and take long training time. One promising direction is to perform few-shot human motion transfer, which only needs a few of source images for appearance transfer. However, it is particularly challenging to obtain satisfactory transfer results. In this paper, we address this issue by rendering a human texture map to a surface geometry (represented as a UV map), which is personalized to the source person. Our geometry generator combines the shape information from source images, and the pose information from 2D keypoints to synthesize the personalized UV map. A texture generator then generates the texture map conditioned on the texture of source images to fill out invisible parts. Furthermore, we may fine-tune the texture map on the manifold of the texture generator from a few source images at the test time, which improves the quality of the texture map without over-fitting or artifacts. Extensive experiments show the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively. Our code is available at https://github.com/HuangZhiChao95/FewShotMotionTransfer.",0
"In this paper, we introduce a novel approach to human motion transfer that enables realistic image generation using only a limited number of appearance inputs. While previous methods for single person motion transfer have made significant strides, they often require extensive training data and take a long time to train. To address this limitation, we explore the concept of few-shot human motion transfer, which involves using only a handful of source images for appearance transfer. However, achieving satisfactory transfer results using this method is difficult. To overcome this challenge, we propose using a human texture map that is rendered to a surface geometry personalized to the source person and represented as a UV map. Our geometry generator uses shape information from source images and pose information from 2D keypoints to synthesize the personalized UV map, while a texture generator generates the texture map conditioned on the texture of source images to fill out invisible parts. Additionally, we can fine-tune the texture map on the texture generator manifold using only a few source images during testing to improve the texture map's quality without over-fitting or artifacts. Our experimental results demonstrate that our method outperforms other state-of-the-art methods both qualitatively and quantitatively. The code for our method is available at https://github.com/HuangZhiChao95/FewShotMotionTransfer.",1
"The recently introduced introspective variational autoencoder (IntroVAE) exhibits outstanding image generations, and allows for amortized inference using an image encoder. The main idea in IntroVAE is to train a VAE adversarially, using the VAE encoder to discriminate between generated and real data samples. However, the original IntroVAE loss function relied on a particular hinge-loss formulation that is very hard to stabilize in practice, and its theoretical convergence analysis ignored important terms in the loss. In this work, we take a step towards better understanding of the IntroVAE model, its practical implementation, and its applications. We propose the Soft-IntroVAE, a modified IntroVAE that replaces the hinge-loss terms with a smooth exponential loss on generated samples. This change significantly improves training stability, and also enables theoretical analysis of the complete algorithm. Interestingly, we show that the IntroVAE converges to a distribution that minimizes a sum of KL distance from the data distribution and an entropy term. We discuss the implications of this result, and demonstrate that it induces competitive image generation and reconstruction. Finally, we describe two applications of Soft-IntroVAE to unsupervised image translation and out-of-distribution detection, and demonstrate compelling results. Code and additional information is available on the project website -- https://taldatech.github.io/soft-intro-vae-web",0
"The IntroVAE model has been recently introduced and has shown excellent capabilities in generating images while enabling amortized inference using an image encoder. The key concept of this model is to train a VAE through adversarial methods, with the VAE encoder discriminating between real and generated data samples. However, the original loss function of IntroVAE relied on a hinge-loss formulation that is difficult to stabilize in practice, and its convergence analysis overlooked vital loss terms. This study aims to enhance our comprehension of the IntroVAE model, its practical implementation, and its applications. We propose the Soft-IntroVAE, which replaces the hinge-loss terms with a smooth exponential loss on generated samples. This results in better training stability and theoretical analysis of the whole algorithm. It was discovered that the IntroVAE converges to a distribution that minimizes a sum of KL distance from the data distribution and an entropy term, which has significant implications and induces competitive image generation and reconstruction. Lastly, Soft-IntroVAE was applied to unsupervised image translation and out-of-distribution detection, with promising results. More information and code can be found on the project website -- https://taldatech.github.io/soft-intro-vae-web.",1
"Synthesizing high-quality realistic images from text descriptions is a challenging task. Almost all existing text-to-image Generative Adversarial Networks employ stacked architecture as the backbone. They utilize cross-modal attention mechanisms to fuse text and image features, and introduce extra networks to ensure text-image semantic consistency. In this work, we propose a much simpler, but more effective text-to-image model than previous works. Corresponding to the above three limitations, we propose: 1) a novel one-stage text-to-image backbone which is able to synthesize high-quality images directly by one pair of generator and discriminator, 2) a novel fusion module called deep text-image fusion block which deepens the text-image fusion process in generator, 3) a novel target-aware discriminator composed of matching-aware gradient penalty and one-way output which promotes the generator to synthesize more realistic and text-image semantic consistent images without introducing extra networks. Compared with existing text-to-image models, our proposed method (i.e., DF-GAN) is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance. Extensive experiments on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models.",0
"Generating high-quality images that are realistic based on text descriptions presents a difficult challenge. Most text-to-image Generative Adversarial Networks that exist currently use a stacked architecture as their backbone. They employ cross-modal attention mechanisms to combine image and text features, and add extra networks to ensure the consistency of text-image semantics. In this study, we present a text-to-image model that is simpler and more effective than previous models, addressing the aforementioned limitations by proposing: 1) a new single-stage text-to-image backbone that can directly generate high-quality images using only one generator and discriminator pair, 2) a novel fusion module called the deep text-image fusion block that deepens the text-image fusion process in the generator, and 3) a new target-aware discriminator composed of a matching-aware gradient penalty and one-way output that encourages the generator to create more realistic and semantically consistent text-image images without requiring additional networks. Our proposed method (DF-GAN) is simpler than existing text-to-image models while still being more efficient at generating realistic and text-matching images, resulting in better performance. Extensive experiments performed on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of our proposed model compared to state-of-the-art models.",1
"We consider the task of photo-realistic unconditional image generation (generate high quality, diverse samples that carry the same visual content as the image) on mobile platforms using Generative Adversarial Networks (GANs). In this paper, we propose a novel approach to trade-off image generation accuracy of a GAN for the energy consumed (compute) at run-time called Scale-Energy Tradeoff GAN (SETGAN). GANs usually take a long time to train and consume a huge memory hence making it difficult to run on edge devices. The key idea behind SETGAN for an image generation task is for a given input image, we train a GAN on a remote server and use the trained model on edge devices. We use SinGAN, a single image unconditional generative model, that contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. During the training process, we determine the optimal number of scales for a given input image and the energy constraint from the target edge device. Results show that with SETGAN's unique client-server-based architecture, we were able to achieve a 56% gain in energy for a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale training, we obtain around 4x gain in training time on the server.",0
"The objective of our study is to generate high-quality images that are visually similar to the original image using Generative Adversarial Networks (GANs) on mobile devices. We have introduced a new approach, called Scale-Energy Tradeoff GAN (SETGAN), that balances image generation accuracy and energy consumption at runtime. GANs are known for their long training time and memory consumption, making it difficult to run them on edge devices. SETGAN addresses this issue by training a GAN on a remote server and using the trained model on edge devices. We have used SinGAN, a single image unconditional generative model, which contains a pyramid of fully convolutional GANs, to determine the optimal number of scales for a given input image and energy constraint from the target edge device during the training process. Our results indicate that the SETGAN architecture has led to a 56% energy gain with a 3% to 12% loss in SSIM accuracy. Additionally, the parallel multi-scale training has resulted in a four-fold reduction in training time on the server.",1
"A layout to image (L2I) generation model aims to generate a complicated image containing multiple objects (things) against natural background (stuff), conditioned on a given layout. Built upon the recent advances in generative adversarial networks (GANs), existing L2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) the object-to-object as well as object-to-stuff relations are often broken and (2) each object's appearance is typically distorted lacking the key defining characteristics associated with the object class. We argue that these are caused by the lack of context-aware object and stuff feature encoding in their generators, and location-sensitive appearance representation in their discriminators. To address these limitations, two new modules are proposed in this work. First, a context-aware feature transformation module is introduced in the generator to ensure that the generated feature encoding of either object or stuff is aware of other co-existing objects/stuff in the scene. Second, instead of feeding location-insensitive image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images to preserve location-sensitive information, resulting in much enhanced object appearance. Extensive experiments show that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks.",0
"The objective of a layout to image (L2I) generation model is to produce a complex image consisting of numerous objects set against a natural background, based on a given layout. Despite the impressive advancements in generative adversarial networks (GANs) used in existing L2I models, a closer examination of the images they generate exposes two main drawbacks: (1) the relationships between objects and between objects and the background are often disrupted, and (2) the appearance of each object is typically distorted, lacking the distinctive features that identify its class. We contend that these shortcomings are due to the absence of context-aware object and background feature encoding in their generators and location-sensitive appearance representation in their discriminators. To resolve these limitations, we introduce two new modules in this study. First, we introduce a context-aware feature transformation module in the generator, which ensures that the generated feature encoding of either object or background is aware of the presence of other co-existing objects/backgrounds in the scene. Second, instead of using location-insensitive image features to the discriminator, we use the Gram matrix derived from the feature maps of the generated object images to preserve location-sensitive information, resulting in a much improved object appearance. The proposed method is extensively tested and is shown to attain state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks.",1
"Image inpainting task requires filling the corrupted image with contents coherent with the context. This research field has achieved promising progress by using neural image inpainting methods. Nevertheless, there is still a critical challenge in guessing the missed content with only the context pixels. The goal of this paper is to fill the semantic information in corrupted images according to the provided descriptive text. Unique from existing text-guided image generation works, the inpainting models are required to compare the semantic content of the given text and the remaining part of the image, then find out the semantic content that should be filled for missing part. To fulfill such a task, we propose a novel inpainting model named Text-Guided Dual Attention Inpainting Network (TDANet). Firstly, a dual multimodal attention mechanism is designed to extract the explicit semantic information about the corrupted regions, which is done by comparing the descriptive text and complementary image areas through reciprocal attention. Secondly, an image-text matching loss is applied to maximize the semantic similarity of the generated image and the text. Experiments are conducted on two open datasets. Results show that the proposed TDANet model reaches new state-of-the-art on both quantitative and qualitative measures. Result analysis suggests that the generated images are consistent with the guidance text, enabling the generation of various results by providing different descriptions. Codes are available at https://github.com/idealwhite/TDANet",0
"The task of image inpainting involves replacing missing content in a corrupted image with contextually coherent information. Neural image inpainting methods have shown promise in this field, but the difficulty of guessing the missing content with only context pixels remains a critical challenge. This paper aims to fill semantic information in corrupted images according to descriptive text. Unlike existing text-guided image generation works, inpainting models must compare the semantic content of the given text and remaining parts of the image to identify the semantic content required to fill the missing part. To achieve this, the Text-Guided Dual Attention Inpainting Network (TDANet) model is proposed. This model uses a dual multimodal attention mechanism to extract semantic information about corrupted regions by comparing descriptive text and complementary image areas through reciprocal attention. An image-text matching loss is then applied to maximize the semantic similarity of the generated image and text. Experiments conducted on two open datasets show that the TDANet model outperforms existing models in both quantitative and qualitative measures. The generated images are consistent with the guidance text, enabling the generation of various results by providing different descriptions. The codes are available at https://github.com/idealwhite/TDANet.",1
"This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. We design a progressive generator which comprises a sequence of transfer blocks. Each block performs an intermediate transfer step by modeling the relationship between the condition and the target poses with attention mechanism. Two types of blocks are introduced, namely Pose-Attentional Transfer Block (PATB) and Aligned Pose-Attentional Transfer Bloc ~(APATB). Compared with previous works, our model generates more photorealistic person images that retain better appearance consistency and shape consistency compared with input images. We verify the efficacy of the model on the Market-1501 and DeepFashion datasets, using quantitative and qualitative measures. Furthermore, we show that our method can be used for data augmentation for the person re-identification task, alleviating the issue of data insufficiency. Code and pretrained models are available at https://github.com/tengteng95/Pose-Transfer.git.",0
"In this article, a novel generative adversarial network for pose transfer is proposed. The network aims to transfer the pose of a given individual to a desired target pose. The generator is designed in a progressive manner, consisting of a series of transfer blocks. Each block performs an intermediate transfer step by utilizing an attention mechanism to model the relationship between the condition and target poses. Two types of transfer blocks are introduced: Pose-Attentional Transfer Block (PATB) and Aligned Pose-Attentional Transfer Block (APATB). Our model generates more realistic images of individuals that are consistent in appearance and shape compared to input images, surpassing existing models. We demonstrate the effectiveness of our model using quantitative and qualitative measures on the Market-1501 and DeepFashion datasets. Moreover, our approach can be utilized for data augmentation in person re-identification, addressing the issue of insufficient data. The source code and pretrained models are available at https://github.com/tengteng95/Pose-Transfer.git.",1
"Whereas conventional state-of-the-art image processing systems of recording and output devices almost exclusively utilize square arranged methods, biological models, however, suggest an alternative, evolutionarily-based structure. Inspired by the human visual perception system, hexagonal image processing in the context of machine learning offers a number of key advantages that can benefit both researchers and users alike. The hexagonal deep learning framework Hexnet leveraged in this contribution serves therefore the generation of hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the results of our created test environment show, the proposed models can surpass current approaches of conventional image generation. While resulting in a reduction of the models' complexity in the form of trainable parameters, they furthermore allow an increase of test rates in comparison to their square counterparts.",0
"While conventional image processing systems mainly use square methods in their recording and output devices, biological models suggest an evolutionary-based hexagonal structure as an alternative. Hexagonal image processing, inspired by the human visual perception system, has several advantages for both researchers and users in the context of machine learning. Hexnet, a hexagonal deep learning framework, generates hexagonal images using hexagonal deep neural networks (H-DNN) and outperforms conventional image generation approaches, as demonstrated by our test environment. This approach reduces model complexity in terms of trainable parameters and increases test rates compared to square counterparts.",1
"How to improve generative modeling by better exploiting spatial regularities and coherence in images? We introduce a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs). In our spatial dependency networks (SDNs), feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating-based mechanism that distributes contextual information across 2-D space. We show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. Furthermore, we demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, we find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task. Our results suggest favoring spatial dependency over convolutional layers in various VAE settings. The accompanying source code is given at https://github.com/djordjemila/sdn.",0
"The focus of this study is on enhancing generative modeling through the optimization of spatial regularities and coherence in images. The authors propose a new neural network that can construct image generators, or decoders, which is then implemented in variational autoencoders (VAEs). The spatial dependency network (SDN) utilizes a sequential gating-based mechanism to compute feature maps in a spatially coherent manner at each level of a deep neural net. This results in a significant improvement in density estimation over baseline convolutional architectures and other models in the same class. Additionally, the SDN can be applied to large images, creating high-quality and coherent samples. The researchers also demonstrate that a powerful SDN decoder can enhance disentangled representation learning in a vanilla VAE setting, highlighting the importance of neural architectures in this task. Overall, the study suggests that spatial dependency layers may be more effective than convolutional layers in VAE settings, and the source code for the research is available at https://github.com/djordjemila/sdn.",1
"In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.",0
"Our work proposes a novel generative model that can automatically separate the global and local features of images in an unsupervised manner. This is achieved by incorporating a generative flow into the VAE framework to model the decoder. Our model uses the variational auto-encoding framework to learn a low-dimensional vector of latent variables that captures the global information of an image. This vector is then used as a conditional input to a flow-based invertible decoder with an architecture inspired by style transfer literature. Our model's effectiveness is demonstrated through experiments on standard image benchmarks, which show its ability to perform density estimation, image generation, and unsupervised representation learning. Importantly, our work shows that a generative model with a likelihood-based objective can learn decoupled representations without explicit supervision, using only architectural inductive biases. The code for our model is available at https://github.com/XuezheMax/wolf.",1
"In this paper, we design two fundamental differential operators for the derivation of rotation differential invariants of images. Each differential invariant obtained by using the new method can be expressed as a homogeneous polynomial of image partial derivatives, which preserve their values when the image is rotated by arbitrary angles. We produce all possible instances of homogeneous invariants up to the given order and degree, and discuss the independence of them in detail. As far as we know, no previous papers have published so many explicit forms of high-order rotation differential invariants of images. In the experimental part, texture classification and image patch verification are carried out on popular real databases. These rotation differential invariants are used as image feature vector. We mainly evaluate the effects of various factors on the performance of them. The experimental results also validate that they have better performance than some commonly used image features in some cases.",0
"This paper introduces two primary differential operators that can be utilized to derive rotation differential invariants of images. The use of these operators results in differential invariants that are expressed as homogeneous polynomials of the partial derivatives of the image. These invariants retain their values even when the image is rotated by any angle. All possible instances of homogeneous invariants up to the given order and degree are generated, and their independence is discussed in detail. This paper presents the most comprehensive collection of high-order rotation differential invariants of images to date. In the experimental section, texture classification and image patch verification are performed on popular real databases using these rotation differential invariants as image feature vectors. The paper assesses the effect of several factors on the performance of these invariants. The experimental results confirm that in some cases, these invariants perform better than commonly used image features.",1
"In this work, we study the image transformation problem by learning the underlying transformations from a collection of images using Generative Adversarial Networks (GANs). Specifically, we propose an unsupervised learning framework, termed as TrGAN, to project images onto a transformation space that is shared by the generator and the discriminator. Any two points in this projected space define a transformation that can guide the image generation process, leading to continuous semantic change. By projecting a pair of images onto the transformation space, we are able to adequately extract the semantic variation between them and further apply the extracted semantic to facilitating image editing, including not only transferring image styles (e.g., changing day to night) but also manipulating image contents (e.g., adding clouds in the sky). Code and models are available at https://genforce.github.io/trgan.",0
"The objective of our research is to explore the image transformation problem using Generative Adversarial Networks (GANs) by learning the fundamental transformations from a set of images. Our proposed framework, TrGAN, is an unsupervised learning approach that allows images to be projected onto a shared transformation space between the generator and discriminator. By defining the transformation between any two points in this projected space, we can generate images with continuous semantic changes. This process enables us to extract the semantic variation between a pair of images and apply it to image editing, such as transferring image styles or manipulating image contents. Interested individuals can access our code and models at https://genforce.github.io/trgan.",1
"Generative adversarial networks achieve great performance in photorealistic image synthesis in various domains, including human images. However, they usually employ latent vectors that encode the sampled outputs globally. This does not allow convenient control of semantically-relevant individual parts of the image, and is not able to draw samples that only differ in partial aspects, such as clothing style. We address these limitations and present a generative model for images of dressed humans offering control over pose, local body part appearance and garment style. This is the first method to solve various aspects of human image generation such as global appearance sampling, pose transfer, parts and garment transfer, and parts sampling jointly in a unified framework. As our model encodes part-based latent appearance vectors in a normalized pose-independent space and warps them to different poses, it preserves body and clothing appearance under varying posture. Experiments show that our flexible and general generative method outperforms task-specific baselines for pose-conditioned image generation, pose transfer and part sampling in terms of realism and output resolution.",0
"The photorealistic image synthesis achieved by generative adversarial networks is impressive in multiple domains, including human images. However, the networks typically use latent vectors that encode sampled outputs globally, which makes it difficult to control individual parts of the image with semantic relevance. Furthermore, the networks cannot generate samples that differ only in partial aspects, such as clothing style. To address these limitations, we present a generative model for dressed human images that allows control over pose, local body part appearance, and garment style. Our unified framework solves various aspects of human image generation, such as global appearance sampling, pose transfer, parts and garment transfer, and parts sampling. By encoding part-based latent appearance vectors in a normalized pose-independent space and warping them to different poses, our model preserves body and clothing appearance under varying posture. Our experiments demonstrate that our flexible and general generative method outperforms task-specific baselines in terms of realism and output resolution for pose-conditioned image generation, pose transfer, and part sampling.",1
"Convolutional neural networks (CNNs) have achieved beyond human-level accuracy in the image classification task and are widely deployed in real-world environments. However, CNNs show vulnerability to adversarial perturbations that are well-designed noises aiming to mislead the classification models. In order to defend against the adversarial perturbations, adversarially trained GAN (ATGAN) is proposed to improve the adversarial robustness generalization of the state-of-the-art CNNs trained by adversarial training. ATGAN incorporates adversarial training into standard GAN training procedure to remove obfuscated gradients which can lead to a false sense in defending against the adversarial perturbations and are commonly observed in existing GANs-based adversarial defense methods. Moreover, ATGAN adopts the image-to-image generator as data augmentation to increase the sample complexity needed for adversarial robustness generalization in adversarial training. Experimental results in MNIST SVHN and CIFAR-10 datasets show that the proposed method doesn't rely on obfuscated gradients and achieves better global adversarial robustness generalization performance than the adversarially trained state-of-the-art CNNs.",0
"Image classification tasks have been revolutionized by Convolutional Neural Networks (CNNs), surpassing human-level accuracy and becoming popular in real-world environments. Despite their success, CNNs are vulnerable to adversarial perturbations, which are designed to mislead classification models. In response, the Adversarially Trained GAN (ATGAN) has been introduced to enhance the generalization of state-of-the-art CNNs trained via adversarial methods. ATGAN incorporates adversarial training into GAN training to eliminate obfuscated gradients, which can create a false sense of defense against adversarial perturbations, a common issue with existing GAN-based defense methods. Additionally, ATGAN uses image-to-image generators as data augmentation to increase sample complexity and improve adversarial robustness generalization. Experimental results on MNIST, SVHN, and CIFAR-10 datasets demonstrate that ATGAN does not rely on obfuscated gradients and outperforms state-of-the-art CNNs in global adversarial robustness generalization.",1
"Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing. However, due to the high computational cost of large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the results of a single edit on edge devices, prohibiting interactive user experience. In this paper, we take inspirations from modern rendering software and propose Anycost GAN for interactive natural image editing. We train the Anycost GAN to support elastic resolutions and channels for faster image generation at versatile speeds. Running subsets of the full generator produce outputs that are perceptually similar to the full generator, making them a good proxy for preview. By using sampling-based multi-resolution training, adaptive-channel training, and a generator-conditioned discriminator, the anycost generator can be evaluated at various configurations while achieving better image quality compared to separately trained models. Furthermore, we develop new encoder training and latent code optimization techniques to encourage consistency between the different sub-generators during image projection. Anycost GAN can be executed at various cost budgets (up to 10x computation reduction) and adapt to a wide range of hardware and latency requirements. When deployed on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12x speedup, enabling interactive image editing. The code and demo are publicly available: https://github.com/mit-han-lab/anycost-gan.",0
"The use of generative adversarial networks (GANs) has allowed for the creation and alteration of photorealistic images. However, the computational cost associated with large-scale generators, such as StyleGAN2, often results in significant delays when editing images on edge devices, which limits the user experience. To address this issue, we drew inspiration from modern rendering software and proposed Anycost GAN, which supports elastic resolutions and channels, allowing for faster image generation at varying speeds. By utilizing subsets of the full generator, we were able to produce outputs that were perceptually similar to the full generator, making them an effective proxy for previewing edits. Through sampling-based multi-resolution training, adaptive-channel training, and a generator-conditioned discriminator, we achieved better image quality than models trained separately. Additionally, we developed new encoder training and latent code optimization techniques to promote consistency between sub-generators during image projection. Anycost GAN can be executed at different cost budgets (up to 10x computation reduction) and adapt to a wide range of hardware and latency requirements. When used on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12x faster speeds, allowing for interactive image editing. We have made the code and demo publicly available on Github: https://github.com/mit-han-lab/anycost-gan.",1
"StyleGAN is one of the state-of-the-art image generators which is well-known for synthesizing high-resolution and hyper-realistic face images. Though images generated by vanilla StyleGAN model are visually appealing, they sometimes contain prominent circular artifacts which severely degrade the quality of generated images. In this work, we provide a systematic investigation on how those circular artifacts are formed by studying the functionalities of different stages of vanilla StyleGAN architecture, with both mechanism analysis and extensive experiments. The key modules of vanilla StyleGAN that promote such undesired artifacts are highlighted. Our investigation also explains why the artifacts are usually circular, relatively small and rarely split into 2 or more parts. Besides, we propose a simple yet effective solution to remove the prominent circular artifacts for vanilla StyleGAN, by applying a novel pixel-instance normalization (PIN) layer.",0
"StyleGAN is a widely recognized image generator that is proficient in producing high-resolution and realistic face images. Although the visual appeal of the images generated by the standard StyleGAN model is impressive, they often exhibit circular defects that significantly reduce their quality. This study presents a thorough examination of how these circular artifacts are created by analyzing the various stages of the vanilla StyleGAN architecture through both mechanism analysis and comprehensive experiments. Our findings identify the primary modules of the vanilla StyleGAN model responsible for such undesirable artifacts and explain why they typically take on a circular shape, are relatively small, and rarely split into multiple parts. Additionally, we introduce a straightforward yet effective solution to eliminate the prominent circular artifacts in vanilla StyleGAN by implementing a novel Pixel-Instance Normalization (PIN) layer.",1
"Although highly accurate automated diagnostic techniques for melanoma have been reported, the realization of a system capable of providing diagnostic evidence based on medical indices remains an open issue because of difficulties in obtaining reliable training data. In this paper, we propose bulk production augmentation (BPA) to generate high-quality, diverse pseudo-skin tumor images with the desired structural malignant features for additional training images from a limited number of labeled images. The proposed BPA acts as an effective data augmentation in constructing the feature detector for the atypical pigment network (APN), which is a key structure in melanoma diagnosis. Experiments show that training with images generated by our BPA largely boosts the APN detection performance by 20.0 percentage points in the area under the receiver operating characteristic curve, which is 11.5 to 13.7 points higher than that of conventional CycleGAN-based augmentations in AUC.",0
"Despite the existence of highly precise automated diagnostic techniques for melanoma, creating a system that can provide diagnostic evidence based on medical indices has yet to be achieved due to the challenges of obtaining dependable training data. This study introduces a solution called bulk production augmentation (BPA), which generates high-quality and diverse pseudo-skin tumor images that possess the necessary structural malignant features for additional training images. BPA serves as an effective data augmentation that enhances the feature detector for the atypical pigment network (APN), a significant structure in melanoma diagnosis. Results from experiments reveal that utilizing BPA-generated images for training significantly improves APN detection performance by 20.0 percentage points in the area under the receiver operating characteristic curve, which is higher than that of traditional CycleGAN-based augmentations by 11.5 to 13.7 points in AUC.",1
"Low-quality face image restoration is a popular research direction in today's computer vision field. It can be used as a pre-work for tasks such as face detection and face recognition. At present, there is a lot of work to solve the problem of low-quality faces under various environmental conditions. This paper mainly focuses on the restoration of motion-blurred faces. In increasingly abundant mobile scenes, the fast recovery of motion-blurred faces can bring highly effective speed improvements in tasks such as face matching. In order to achieve this goal, a deblurring method for motion-blurred facial image signals based on generative adversarial networks(GANs) is proposed. It uses an end-to-end method to train a sharp image generator, i.e., a processor for motion-blurred facial images. This paper introduce the processing progress of motion-blurred images, the development and changes of GANs and some basic concepts. After that, it give the details of network structure and training optimization design of the image processor. Then we conducted a motion blur image generation experiment on some general facial data set, and used the pairs of blurred and sharp face image data to perform the training and testing experiments of the processor GAN, and gave some visual displays. Finally, MTCNN is used to detect the faces of the image generated by the deblurring processor, and compare it with the result of the blurred image. From the results, the processing effect of the deblurring processor on the motion-blurred picture has a significant improvement both in terms of intuition and evaluation indicators of face detection.",0
"The field of computer vision is currently focused on restoring low-quality face images, which serves as a pre-work for face detection and recognition. Addressing the issue of faces with low-quality due to varying environmental conditions is a major area of research. This study concentrates specifically on restoring motion-blurred faces, which can significantly enhance the speed and efficacy of face matching in mobile environments. To accomplish this, the paper proposes a deblurring method using generative adversarial networks (GANs) to generate sharp images of motion-blurred faces. The paper offers an overview of motion-blurred image processing, GANs' development and basic concepts, the image processor's network structure, and training optimization design. The study then presents the results of the motion blur image generation experiment on standard facial datasets and the visual displays of the training and testing experiments of the processor GAN. Finally, the paper evaluates the performance of the deblurring processor on the motion-blurred picture using MTCNN for face detection and compares it with the blurred image. The results show significant improvement in the processing effect of the deblurring processor on motion-blurred pictures in terms of face detection intuition and evaluation indicators.",1
"The field of face recognition (FR) has witnessed great progress with the surge of deep learning. Existing methods mainly focus on extracting discriminative features, and directly compute the cosine or L2 distance by the point-to-point way without considering the context information. In this study, we make a key observation that the local con-text represented by the similarities between the instance and its inter-class neighbors1plays an important role forFR. Specifically, we attempt to incorporate the local in-formation in the feature space into the metric, and pro-pose a unified framework calledInter-class DiscrepancyAlignment(IDA), with two dedicated modules, Discrepancy Alignment Operator(IDA-DAO) andSupport Set Estimation(IDA-SSE). IDA-DAO is used to align the similarity scores considering the discrepancy between the images and its neighbors, which is defined by adaptive support sets on the hypersphere. For practical inference, it is difficult to acquire support set during online inference. IDA-SSE can provide convincing inter-class neighbors by introducing virtual candidate images generated with GAN. Further-more, we propose the learnable IDA-SSE, which can implicitly give estimation without the need of any other images in the evaluation process. The proposed IDA can be incorporated into existing FR systems seamlessly and efficiently. Extensive experiments demonstrate that this frame-work can 1) significantly improve the accuracy, and 2) make the model robust to the face images of various distributions.Without bells and whistles, our method achieves state-of-the-art performance on multiple standard FR benchmarks.",0
"The use of deep learning has greatly advanced the field of face recognition (FR). However, current methods focus primarily on extracting discriminative features and computing the cosine or L2 distance without taking into account contextual information. This study recognizes the importance of local context, represented by similarities between instances and their inter-class neighbors, in FR. The proposed Inter-class Discrepancy Alignment (IDA) framework incorporates local information into the metric using two dedicated modules: Discrepancy Alignment Operator (IDA-DAO) and Support Set Estimation (IDA-SSE). IDA-DAO aligns similarity scores by considering image discrepancies and adaptive support sets on the hypersphere. IDA-SSE introduces virtual candidate images generated with GAN to provide convincing inter-class neighbors. The learnable IDA-SSE estimates support sets without requiring additional images during evaluation. The proposed IDA can be easily integrated into existing FR systems and achieves state-of-the-art performance on standard FR benchmarks.",1
"Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of function approximators based on polynomial expansions. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that $\Pi$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning. The source code is available at \url{https://github.com/grigorisg9gr/polynomial_nets}.",0
"Currently, Deep Convolutional Neural Networks (DCNNs) are the preferred method for both generative and discriminative learning in the fields of computer vision and machine learning. The success of DCNNs is due to the careful selection of their components, such as residual blocks, rectifiers, and advanced normalization techniques. In this paper, a new category of function approximators called $\Pi$-Nets is introduced, which are based on polynomial expansions. $\Pi$-Nets are polynomial neural networks that utilize a high-order polynomial of the input as output. The unknown parameters are represented by high-order tensors and estimated through a collective tensor factorization with shared factors. Three tensor decompositions are proposed that greatly reduce the number of parameters and can be efficiently implemented by hierarchical neural networks. Empirical evidence demonstrates that $\Pi$-Nets are highly expressive and can produce satisfactory results without non-linear activation functions in a variety of tasks, including images, graphs, and audio. When combined with activation functions, $\Pi$-Nets produce state-of-the-art results in image generation, face verification, and 3D mesh representation learning. The source code can be found at \url{https://github.com/grigorisg9gr/polynomial_nets}.",1
"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",0
"In the past, the primary emphasis of text-to-image generation has been on improving modeling assumptions for training on a pre-determined dataset. Such assumptions could entail intricate structures, auxiliary losses, or extra information like object part labels or segmentation masks offered during training. Our method, which employs a transformer to autoregressively model the text and image tokens as a unified data stream, presents a straightforward solution for this task. When evaluated in a zero-shot manner, our approach is comparable to prior domain-specific models, provided there is sufficient data and scale.",1
"As a new approach to train generative models, \emph{generative adversarial networks} (GANs) have achieved considerable success in image generation. This framework has also recently been applied to data with graph structures. We propose labeled-graph generative adversarial networks (LGGAN) to train deep generative models for graph-structured data with node labels. We test the approach on various types of graph datasets, such as collections of citation networks and protein graphs. Experiment results show that our model can generate diverse labeled graphs that match the structural characteristics of the training data and outperforms all alternative approaches in quality and generality. To further evaluate the quality of the generated graphs, we use them on a downstream task of graph classification, and the results show that LGGAN can faithfully capture the important aspects of the graph structure.",0
"Generative adversarial networks (GANs) have made significant strides in image generation and are now being used to train generative models for data with graph structures. Our proposal, labeled-graph generative adversarial networks (LGGAN), aims to train deep generative models for graph-structured data with node labels. We conducted experiments on different types of graph datasets, including citation networks and protein graphs, and our model generated diverse labeled graphs that matched the structural characteristics of the training data. Our approach outperformed alternative methods in terms of quality and generality. To further evaluate the quality of the generated graphs, we used them for graph classification, and the results revealed that LGGAN accurately captured the important aspects of the graph structure.",1
"Although training data is essential for machine learning, railway companies are facing difficulties in gathering adequate images of defective equipment due to their proactive replacement of would be defective equipment. Nevertheless, proactive replacement is indispensable for safe and undisturbed operation of public transport. In this research, we have developed a model using CycleGAN to generate artificial images of defective equipment instead of real images. By adopting these generated images as training data, we verified that these images are indistinguishable from real images and they play a vital role in enhancing the accuracy of the defect detection models.",0
"Machine learning heavily relies on training data. However, railway companies encounter challenges in obtaining sufficient images of faulty equipment as they opt for preemptive replacement of potentially defective equipment. Nonetheless, proactive replacement is crucial to ensure the safety and smooth operation of public transportation. To overcome this issue, we have created a model using CycleGAN to produce synthetic images of defective equipment instead of actual images. Our study confirms that these generated images are virtually identical to genuine images and significantly improve the precision of defect detection models when utilized as training data.",1
"Unpaired image-to-image translation refers to learning inter-image-domain mapping in an unsupervised manner. Existing methods often learn deterministic mappings without explicitly modelling the robustness to outliers or predictive uncertainty, leading to performance degradation when encountering unseen out-of-distribution (OOD) patterns at test time. To address this limitation, we propose a novel probabilistic method called Uncertainty-aware Generalized Adaptive Cycle Consistency (UGAC), which models the per-pixel residual by generalized Gaussian distribution, capable of modelling heavy-tailed distributions. We compare our model with a wide variety of state-of-the-art methods on two challenging tasks: unpaired image denoising in the natural image and unpaired modality prorogation in medical image domains. Experimental results demonstrate that our model offers superior image generation quality compared to recent methods in terms of quantitative metrics such as signal-to-noise ratio and structural similarity. Our model also exhibits stronger robustness towards OOD test data.",0
"The term unpaired image-to-image translation refers to the unsupervised learning of mapping between different image domains. However, existing methods often fall short as they learn deterministic mappings that do not account for outliers or predictive uncertainty. Consequently, when such methods encounter previously unseen out-of-distribution (OOD) patterns during testing, their performance suffers. To address this issue, we propose a novel probabilistic method called Uncertainty-aware Generalized Adaptive Cycle Consistency (UGAC). Our approach models per-pixel residual using a generalized Gaussian distribution that can handle heavy-tailed distributions. We evaluate our model against various state-of-the-art methods on two challenging tasks: unpaired image denoising in natural images and unpaired modality propagation in medical images. Our experimental results show that our model generates superior images compared to recent methods, as evidenced by quantitative metrics such as signal-to-noise ratio and structural similarity. Furthermore, our model exhibits greater robustness towards OOD test data.",1
"We present Sandwich Batch Normalization (SaBN), an embarrassingly easy improvement of Batch Normalization (BN) with only a few lines of code changes. SaBN is motivated by addressing the inherent feature distribution heterogeneity that one can be identified in many tasks, which can arise from data heterogeneity (multiple input domains) or model heterogeneity (dynamic architectures, model conditioning, etc.). Our SaBN factorizes the BN affine layer into one shared sandwich affine layer, cascaded by several parallel independent affine layers. Concrete analysis reveals that, during optimization, SaBN promotes balanced gradient norms while still preserving diverse gradient directions: a property that many application tasks seem to favor. We demonstrate the prevailing effectiveness of SaBN as a drop-in replacement in four tasks: $\textbf{conditional image generation}$, $\textbf{neural architecture search}$ (NAS), $\textbf{adversarial training}$, and $\textbf{arbitrary style transfer}$. Leveraging SaBN immediately achieves better Inception Score and FID on CIFAR-10 and ImageNet conditional image generation with three state-of-the-art GANs; boosts the performance of a state-of-the-art weight-sharing NAS algorithm significantly on NAS-Bench-201; substantially improves the robust and standard accuracies for adversarial defense; and produces superior arbitrary stylized results. We also provide visualizations and analysis to help understand why SaBN works. Codes are available at https://github.com/VITA-Group/Sandwich-Batch-Normalization.",0
"Introducing Sandwich Batch Normalization (SaBN), a simple and effective enhancement to Batch Normalization (BN) that requires minimal code changes. SaBN addresses the issue of feature distribution heterogeneity that often occurs in various tasks due to data or model heterogeneity. SaBN breaks down the BN affine layer into a shared sandwich affine layer followed by several parallel independent affine layers. SaBN promotes balanced gradient norms and preserves diverse gradient directions during optimization, making it suitable for numerous application tasks. SaBN is a drop-in replacement that improves the Inception Score and FID on CIFAR-10 and ImageNet conditional image generation with three state-of-the-art GANs, enhances the performance of a state-of-the-art weight-sharing NAS algorithm on NAS-Bench-201, improves the robust and standard accuracies for adversarial defense, and produces superior arbitrary stylized results. We provide visualizations and analysis to help understand SaBN's effectiveness and have made the codes available at https://github.com/VITA-Group/Sandwich-Batch-Normalization.",1
"Photo-realistic re-rendering of a human from a single image with explicit control over body pose, shape and appearance enables a wide range of applications, such as human appearance transfer, virtual try-on, motion imitation, and novel view synthesis. While significant progress has been made in this direction using learning-based image generation tools, such as GANs, existing approaches yield noticeable artefacts such as blurring of fine details, unrealistic distortions of the body parts and garments as well as severe changes of the textures. We, therefore, propose a new method for synthesising photo-realistic human images with explicit control over pose and part-based appearance, i.e., StylePoseGAN, where we extend a non-controllable generator to accept conditioning of pose and appearance separately. Our network can be trained in a fully supervised way with human images to disentangle pose, appearance and body parts, and it significantly outperforms existing single image re-rendering methods. Our disentangled representation opens up further applications such as garment transfer, motion transfer, virtual try-on, head (identity) swap and appearance interpolation. StylePoseGAN achieves state-of-the-art image generation fidelity on common perceptual metrics compared to the current best-performing methods and convinces in a comprehensive user study.",0
"Photo-realistic rendering of human images from a single source image with precise control over body shape, pose, and appearance has numerous applications such as virtual try-on, motion imitation, human appearance transfer, and novel view synthesis. However, current learning-based image generation tools such as GANs often produce unsatisfactory results, including distorted body parts, unrealistic garment textures, and blurred fine details. To address these issues, we have developed a new method called StylePoseGAN, which allows for explicit control over pose and appearance. By extending a non-controllable generator to accept separate conditioning for pose and appearance, we can train our network in a fully supervised way to disentangle appearance, pose, and body parts. Our disentangled representation offers further applications such as motion transfer, garment transfer, and head (identity) swap. StylePoseGAN outperforms existing single image re-rendering methods, achieving state-of-the-art image generation fidelity and performing well in a comprehensive user study.",1
"In this paper, we leverage advances in neural networks towards forming a neural rendering for controllable image generation, and thereby bypassing the need for detailed modeling in conventional graphics pipeline. To this end, we present Neural Graphics Pipeline (NGP), a hybrid generative model that brings together neural and traditional image formation models. NGP decomposes the image into a set of interpretable appearance feature maps, uncovering direct control handles for controllable image generation. To form an image, NGP generates coarse 3D models that are fed into neural rendering modules to produce view-specific interpretable 2D maps, which are then composited into the final output image using a traditional image formation model. Our approach offers control over image generation by providing direct handles controlling illumination and camera parameters, in addition to control over shape and appearance variations. The key challenge is to learn these controls through unsupervised training that links generated coarse 3D models with unpaired real images via neural and traditional (e.g., Blinn- Phong) rendering functions, without establishing an explicit correspondence between them. We demonstrate the effectiveness of our approach on controllable image generation of single-object scenes. We evaluate our hybrid modeling framework, compare with neural-only generation methods (namely, DCGAN, LSGAN, WGAN-GP, VON, and SRNs), report improvement in FID scores against real images, and demonstrate that NGP supports direct controls common in traditional forward rendering. Code is available at http://geometry.cs.ucl.ac.uk/projects/2021/ngp.",0
"In this paper, we utilize recent advancements in neural networks to create controllable images without relying on detailed modeling in traditional graphics pipelines. Our approach involves the Neural Graphics Pipeline (NGP), which combines neural and traditional image formation models to break down images into easily interpretable appearance feature maps. These maps provide direct control over image generation, including control over illumination and camera parameters, as well as shape and appearance variations. The challenge is to learn these controls through unsupervised training that links generated 3D models with real images using both neural and traditional rendering functions. We showcase the effectiveness of our approach on single-object scenes and compare our hybrid model with solely neural-based generation methods. Our results demonstrate an improvement in FID scores against real images, and we provide code available for use.",1
"Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. Existing methods typically employ intuitive, carefully hand-designed mechanisms for controlling such rotations. In this paper, we take a novel approach to address this issue by casting min-max optimization as a physical system. We leverage tools from physics to introduce LEAD (Least-Action Dynamics), a second-order optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete-time settings for bilinear games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements over baseline methods.",0
"The interest in two-player min-max games has been reignited by adversarial formulations such as generative adversarial networks (GANs). The convergence of such games is hindered by rotational dynamics, which is a major obstacle in their optimization. Current techniques rely on hand-designed mechanisms to control rotations. In this study, we present a unique approach to address this problem by treating min-max optimization as a physical system. We introduce LEAD (Least-Action Dynamics), a second-order optimizer for min-max games that uses tools from physics. We then use Lyapunov stability theory and spectral analysis to investigate LEAD's convergence properties in continuous and discrete-time settings for bilinear games, and we demonstrate linear convergence to the Nash equilibrium. Finally, we experimentally assess our technique on synthetic setups and CIFAR-10 image generation, revealing improvements over baseline methods.",1
"In this paper, we propose a novel approach to solve the pose guided person image generation task. We assume that the relation between pose and appearance information can be described by a simple matrix operation in hidden space. Based on this assumption, our method estimates a pose-invariant feature matrix for each identity, and uses it to predict the target appearance conditioned on the target pose. The estimation process is formulated as a p-norm regression problem in hidden space. By utilizing the differentiation of the solution of this regression problem, the parameters of the whole framework can be trained in an end-to-end manner. While most previous works are only applicable to the supervised training and single-shot generation scenario, our method can be easily adapted to unsupervised training and multi-shot generation. Extensive experiments on the challenging Market-1501 dataset show that our method yields competitive performance in all the aforementioned variant scenarios.",0
"This paper presents a new technique for solving the pose guided person image generation task. The approach assumes that the relationship between pose and appearance information can be explained through a simple matrix operation in hidden space. The proposed method estimates a pose-invariant feature matrix for each individual and employs it to predict the target appearance based on the target pose. The estimation process is formulated as a p-norm regression problem in hidden space, and the parameters of the entire framework can be trained end-to-end by utilizing the differentiation of the solution of this regression problem. Unlike previous methods that are only suitable for supervised training and single-shot generation scenarios, our approach can be effortlessly adapted to unsupervised training and multi-shot generation. Extensive experiments conducted on the challenging Market-1501 dataset illustrate that our method delivers competitive performance in all the aforementioned variant scenarios.",1
"Variational autoencoders (VAEs) often suffer from posterior collapse, which is a phenomenon in which the learned latent space becomes uninformative. This is often related to a hyperparameter resembling the data variance. It can be shown that an inappropriate choice of this parameter causes oversmoothness and leads to posterior collapse in the linearly approximated case and can be empirically verified for the general cases. Therefore, we propose AR-ELBO (Adaptively Regularized Evidence Lower BOund), which controls the smoothness of the model by adapting this variance parameter. In addition, we extend VAE with alternative parameterizations on the variance parameter to deal with non-uniform or conditional data variance. The proposed VAE extensions trained with AR-ELBO show improved Fr\'echet inception distance (FID) on images generated from the MNIST and CelebA datasets.",0
"Posterior collapse is a common issue with Variational autoencoders (VAEs) where the latent space becomes uninformative due to a hyperparameter resembling the data variance. Inappropriate selection of this parameter leads to oversmoothness and posterior collapse in the linearly approximated case, which can be empirically verified. To overcome this, we introduce AR-ELBO (Adaptively Regularized Evidence Lower BOund) that adjusts the smoothness of the model by adapting the variance parameter. Furthermore, we propose VAE extensions that use alternative parameterizations to handle non-uniform or conditional data variance. Our proposed VAE extensions trained with AR-ELBO exhibit improved Fr\'echet inception distance (FID) on images generated from the MNIST and CelebA datasets.",1
"Image generation from a single image using generative adversarial networks is quite interesting due to the realism of generated images. However, recent approaches need improvement for such realistic and diverse image generation, when the global context of the image is important such as in face, animal, and architectural image generation. This is mainly due to the use of fewer convolutional layers for mainly capturing the patch statistics and, thereby, not being able to capture global statistics very well. We solve this problem by using attention blocks at selected scales and feeding a random Gaussian blurred image to the discriminator for training. Our results are visually better than the state-of-the-art particularly in generating images that require global context. The diversity of our image generation, measured using the average standard deviation of pixels, is also better.",0
"The use of generative adversarial networks for generating images from a single image is fascinating because of the realistic quality of the images they produce. However, recent methods have room for improvement, especially when it comes to generating diverse and realistic images that require a consideration of global context, like faces, animals, and architecture. This is mainly due to the limited use of convolutional layers that only capture patch statistics and fail to capture global statistics effectively. To address this issue, we have implemented attention blocks at selected scales and introduced a random Gaussian blurred image to the discriminator during training. Our results are superior to the current state-of-the-art, particularly in generating images that require global context. Additionally, our image generation shows better diversity, as measured by the average standard deviation of pixels.",1
"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",0
"Generating data from noise is the essence of generative modeling, whereas creating noise from data is a simple task. Our approach involves using a stochastic differential equation (SDE) to smoothly transform complex data into a known prior distribution by gradually introducing noise. The reverse-time SDE transforms the prior distribution back into the original data by slowly eliminating the noise, relying solely on the time-dependent gradient field (also known as score) of the perturbed data distribution. We employ neural networks to accurately estimate these scores, and numerical SDE solvers to generate samples. Our framework encompasses previous approaches in score-based generative modeling and diffusion probabilistic modeling, providing new sampling processes and modeling capabilities. We introduce a predictor-corrector framework to correct errors in the discretized reverse-time SDE's evolution. We also derive an equivalent neural ODE that samples from the same distribution as the SDE and allows for exact likelihood computation and improved sampling efficiency. Additionally, we demonstrate a novel approach to solving inverse problems using score-based models, as seen in experiments on class-conditional generation, image inpainting, and colorization. With architectural improvements, our model achieves record-breaking performance for unconditional image generation on CIFAR-10, with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",1
"Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA)that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks.",0
"Data augmentation is a common technique used to expand datasets with artificial samples that follow the underlying data distribution. In order to increase the range of augmentations possible, we investigate negative data augmentation (NDA) methods that generate out-of-distribution samples. These negative samples can reveal information about the support of the data distribution, which can be utilized for generative modeling and representation learning. We propose a new training objective for GANs that includes NDA as a source of synthetic data for the discriminator. This objective can still recover the true data distribution under certain conditions, but it can also direct the generator away from generating samples without the desired structure. Our approach improves both conditional and unconditional image generation, as well as anomaly detection capabilities. We also apply the NDA strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving better performance on image classification, object detection, and action recognition tasks. These results indicate that knowledge of what does not qualify as valid data can be an effective form of weak supervision for various unsupervised learning tasks.",1
"We present two new metrics for evaluating generative models in the class-conditional image generation setting. These metrics are obtained by generalizing the two most popular unconditional metrics: the Inception Score (IS) and the Fre'chet Inception Distance (FID). A theoretical analysis shows the motivation behind each proposed metric and links the novel metrics to their unconditional counterparts. The link takes the form of a product in the case of IS or an upper bound in the FID case. We provide an extensive empirical evaluation, comparing the metrics to their unconditional variants and to other metrics, and utilize them to analyze existing generative models, thus providing additional insights about their performance, from unlearned classes to mode collapse.",0
"We introduce two novel metrics for assessing generative models in the context of class-conditional image generation. These metrics are based on the widely-used Inception Score (IS) and Fre'chet Inception Distance (FID) metrics, but are adapted to the class-conditional setting. We offer a theoretical explanation for each metric and demonstrate their connection to their unconditional counterparts. This connection is expressed through either a product (for IS) or an upper bound (for FID). To evaluate the effectiveness of these metrics, we conduct a thorough empirical analysis and compare them to other metrics, including their unconditional counterparts. Our analysis provides valuable insights into the performance of existing generative models, from unlearned classes to mode collapse.",1
"Reconciling symbolic and distributed representations is a crucial challenge that can potentially resolve the limitations of current deep learning. Remarkable advances in this direction have been achieved recently via generative object-centric representation models. While learning a recognition model that infers object-centric symbolic representations like bounding boxes from raw images in an unsupervised way, no such model can provide another important ability of a generative model, i.e., generating (sampling) according to the structure of learned world density. In this paper, we propose Generative Neurosymbolic Machines, a generative model that combines the benefits of distributed and symbolic representations to support both structured representations of symbolic components and density-based generation. These two crucial properties are achieved by a two-layer latent hierarchy with the global distributed latent for flexible density modeling and the structured symbolic latent map. To increase the model flexibility in this hierarchical structure, we also propose the StructDRAW prior. In experiments, we show that the proposed model significantly outperforms the previous structured representation models as well as the state-of-the-art non-structured generative models in terms of both structure accuracy and image generation quality. Our code, datasets, and trained models are available at https://github.com/JindongJiang/GNM",0
"The integration of symbolic and distributed representations is a significant obstacle that can potentially overcome the limitations of current deep learning. Recent advancements in generative object-centric representation models have made remarkable progress in this direction. However, while unsupervised learning of recognition models can infer object-centric symbolic representations like bounding boxes from raw images, it cannot generate (sample) according to the structure of learned world density, which is a crucial ability of a generative model. To address this issue, we introduce Generative Neurosymbolic Machines, a generative model that combines the advantages of distributed and symbolic representations to support both structured representations of symbolic components and density-based generation. Our model comprises a two-layer latent hierarchy with a global distributed latent for flexible density modeling and a structured symbolic latent map to achieve these two crucial properties. To enhance the model's flexibility in this hierarchical structure, we propose the StructDRAW prior. In our experiments, we demonstrate that our proposed model outperforms previous structured representation models as well as state-of-the-art non-structured generative models in terms of both structure accuracy and image generation quality. We have made our code, datasets, and trained models available at https://github.com/JindongJiang/GNM.",1
"In this paper we tackle the problem of pose guided person image generation, which aims to transfer a person image from the source pose to a novel target pose while maintaining the source appearance. Given the inefficiency of standard CNNs in handling large spatial transformation, we propose a structure-aware flow based method for high-quality person image generation. Specifically, instead of learning the complex overall pose changes of human body, we decompose the human body into different semantic parts (e.g., head, torso, and legs) and apply different networks to predict the flow fields for these parts separately. Moreover, we carefully design the network modules to effectively capture the local and global semantic correlations of features within and among the human parts respectively. Extensive experimental results show that our method can generate high-quality results under large pose discrepancy and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",0
"The objective of our paper is to address the issue of generating person images based on a specific pose, where the goal is to transfer the source image to a new pose without altering the original appearance. Due to the limitations of conventional CNNs in handling significant spatial transformations, we propose a structure-aware flow-based approach for producing high-quality person images. Instead of attempting to learn the complex pose changes of the entire human body, we divide it into distinct semantic components (such as the head, torso, and legs) and apply separate networks to predict the flow fields for each part. Additionally, we carefully design the network modules to effectively capture the local and global semantic correlations of features within and among the human parts. Our method produces superior results when compared to other state-of-the-art approaches, as demonstrated by extensive experimental results that validate our approach's ability to generate high-quality images while accommodating significant pose discrepancies.",1
"Conditional image generation is the task of generating diverse images using class label information. Although many conditional Generative Adversarial Networks (GAN) have shown realistic results, such methods consider pairwise relations between the embedding of an image and the embedding of the corresponding label (data-to-class relations) as the conditioning losses. In this paper, we propose ContraGAN that considers relations between multiple image embeddings in the same batch (data-to-data relations) as well as the data-to-class relations by using a conditional contrastive loss. The discriminator of ContraGAN discriminates the authenticity of given samples and minimizes a contrastive objective to learn the relations between training images. Simultaneously, the generator tries to generate realistic images that deceive the authenticity and have a low contrastive loss. The experimental results show that ContraGAN outperforms state-of-the-art-models by 7.3% and 7.7% on Tiny ImageNet and ImageNet datasets, respectively. Besides, we experimentally demonstrate that contrastive learning helps to relieve the overfitting of the discriminator. For a fair comparison, we re-implement twelve state-of-the-art GANs using the PyTorch library. The software package is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.",0
"Generating diverse images using class label information is the main goal of conditional image generation. While many conditional Generative Adversarial Networks (GAN) have produced realistic results, they only consider pairwise relations between the embedding of an image and the embedding of the corresponding label (data-to-class relations) as the conditioning losses. Our proposed ContraGAN, on the other hand, considers relations between multiple image embeddings in the same batch (data-to-data relations) as well as data-to-class relations by using a conditional contrastive loss. The discriminator of ContraGAN distinguishes the authenticity of given samples and minimizes a contrastive objective to learn the relations between training images. Meanwhile, the generator attempts to generate realistic images that deceive the authenticity and have a low contrastive loss. Our experimental results reveal that ContraGAN outperforms state-of-the-art models by 7.3% and 7.7% on Tiny ImageNet and ImageNet datasets, respectively. Additionally, we demonstrate that contrastive learning helps to alleviate overfitting of the discriminator. To ensure a fair comparison, we re-implemented twelve state-of-the-art GANs using the PyTorch library, and the software package is accessible at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.",1
"We present a framework to translate between 2D image views and 3D object shapes. Recent progress in deep learning enabled us to learn structure-aware representations from a scene. However, the existing literature assumes that pairs of images and 3D shapes are available for training in full supervision. In this paper, we propose SIST, a Self-supervised Image to Shape Translation framework that fulfills three tasks: (i) reconstructing the 3D shape from a single image; (ii) learning disentangled representations for shape, appearance and viewpoint; and (iii) generating a realistic RGB image from these independent factors. In contrast to the existing approaches, our method does not require image-shape pairs for training. Instead, it uses unpaired image and shape datasets from the same object class and jointly trains image generator and shape reconstruction networks. Our translation method achieves promising results, comparable in quantitative and qualitative terms to the state-of-the-art achieved by fully-supervised methods.",0
"A framework is presented in this paper that can translate between 2D image views and 3D object shapes. Recent advancements in deep learning have made it possible to learn structure-aware representations from a scene. However, current literature assumes that pairs of images and 3D shapes are available for training in full supervision. To address this limitation, the authors propose a Self-supervised Image to Shape Translation (SIST) framework that can perform three tasks: (i) reconstructing the 3D shape from a single image; (ii) learning disentangled representations for shape, appearance, and viewpoint; and (iii) generating a realistic RGB image from these independent factors. Unlike existing approaches, this method does not require image-shape pairs for training, but instead uses unpaired image and shape datasets from the same object class and trains image generator and shape reconstruction networks jointly. Results show that the proposed method achieves promising results, comparable in quantitative and qualitative terms to the state-of-the-art achieved by fully-supervised methods.",1
"Generative Adversarial Networks (GANs) are an unsupervised generative model that learns data distribution through adversarial training. However, recent experiments indicated that GANs are difficult to train due to the requirement of optimization in the high dimensional parameter space and the zero gradient problem. In this work, we propose a Self Sparse Generative Adversarial Network (Self-Sparse GAN) that reduces the parameter space and alleviates the zero gradient problem. In the Self-Sparse GAN, we design a Self-Adaptive Sparse Transform Module (SASTM) comprising the sparsity decomposition and feature-map recombination, which can be applied on multi-channel feature maps to obtain sparse feature maps. The key idea of Self-Sparse GAN is to add the SASTM following every deconvolution layer in the generator, which can adaptively reduce the parameter space by utilizing the sparsity in multi-channel feature maps. We theoretically prove that the SASTM can not only reduce the search space of the convolution kernel weight of the generator but also alleviate the zero gradient problem by maintaining meaningful features in the Batch Normalization layer and driving the weight of deconvolution layers away from being negative. The experimental results show that our method achieves the best FID scores for image generation compared with WGAN-GP on MNIST, Fashion-MNIST, CIFAR-10, STL-10, mini-ImageNet, CELEBA-HQ, and LSUN bedrooms, and the relative decrease of FID is 4.76% ~ 21.84%.",0
"The difficulty of training Generative Adversarial Networks (GANs) lies in the high dimensional parameter space and the zero gradient problem. To tackle these issues, we propose the Self-Sparse GAN, which utilizes the Self-Adaptive Sparse Transform Module (SASTM) to reduce the parameter space and alleviate the zero gradient problem. The SASTM decomposes and recombines multi-channel feature maps to obtain sparse feature maps, which can be applied following each deconvolution layer in the generator. The SASTM reduces the search space of the convolution kernel weight and maintains meaningful features in the Batch Normalization layer, driving the weight of deconvolution layers away from being negative. The experimental results show that our method achieves the best FID scores for image generation compared with WGAN-GP on various datasets with a relative decrease of FID of 4.76% ~ 21.84%.",1
"With the advent of generative adversarial networks, synthesizing images from textual descriptions has recently become an active research area. It is a flexible and intuitive way for conditional image generation with significant progress in the last years regarding visual realism, diversity, and semantic alignment. However, the field still faces several challenges that require further research efforts such as enabling the generation of high-resolution images with multiple objects, and developing suitable and reliable evaluation metrics that correlate with human judgement. In this review, we contextualize the state of the art of adversarial text-to-image synthesis models, their development since their inception five years ago, and propose a taxonomy based on the level of supervision. We critically examine current strategies to evaluate text-to-image synthesis models, highlight shortcomings, and identify new areas of research, ranging from the development of better datasets and evaluation metrics to possible improvements in architectural design and model training. This review complements previous surveys on generative adversarial networks with a focus on text-to-image synthesis which we believe will help researchers to further advance the field.",0
"Recently, the research area of synthesizing images from textual descriptions has gained momentum with the emergence of generative adversarial networks. This approach offers a flexible and intuitive way for conditional image generation, and has made significant progress in terms of visual realism, diversity, and semantic alignment. However, challenges remain, such as the need to develop techniques for generating high-resolution images with multiple objects, and reliable evaluation metrics that align with human judgement. In this review, we provide an overview of the state of the art of adversarial text-to-image synthesis models, their development over the past five years, and classify them based on the level of supervision. We also assess existing evaluation strategies, identify limitations, and suggest new research areas. Our review, which complements previous surveys on generative adversarial networks, aims to facilitate further advancements in the field.",1
"Convolutional neural networks are able to learn realistic image priors from numerous training samples in low-level image generation and restoration. We show that, for high-level image recognition tasks, we can further reconstruct ""realistic"" images of each category by leveraging intrinsic Batch Normalization (BN) statistics without any training data. Inspired by the popular VAE/GAN methods, we regard the zero-shot optimization process of synthetic images as generative modeling to match the distribution of BN statistics. The generated images serve as a calibration set for the following zero-shot network quantizations. Our method meets the needs for quantizing models based on sensitive information, \textit{e.g.,} due to privacy concerns, no data is available. Extensive experiments on benchmark datasets show that, with the help of generated data, our approach consistently outperforms existing data-free quantization methods.",0
"By utilizing numerous training samples, convolutional neural networks can acquire realistic image priors for low-level image generation and restoration. Our research demonstrates that, in addition to this, we can reconstruct ""realistic"" images of each category for high-level image recognition tasks without requiring training data by utilizing intrinsic Batch Normalization (BN) statistics. We employ generative modeling, inspired by popular VAE/GAN methods, to match the distribution of BN statistics in the zero-shot optimization process of synthetic images. These generated images serve as a calibration set for subsequent zero-shot network quantizations. Our approach fulfills the requirement for quantizing models based on confidential information where data is unavailable due to privacy concerns. Our method consistently outperforms existing data-free quantization methods on benchmark datasets through the use of generated data.",1
"Discrete latent spaces in variational autoencoders have been shown to effectively capture the data distribution for many real-world problems such as natural language understanding, human intent prediction, and visual scene representation. However, discrete latent spaces need to be sufficiently large to capture the complexities of real-world data, rendering downstream tasks computationally challenging. For instance, performing motion planning in a high-dimensional latent representation of the environment could be intractable. We consider the problem of sparsifying the discrete latent space of a trained conditional variational autoencoder, while preserving its learned multimodality. As a post hoc latent space reduction technique, we use evidential theory to identify the latent classes that receive direct evidence from a particular input condition and filter out those that do not. Experiments on diverse tasks, such as image generation and human behavior prediction, demonstrate the effectiveness of our proposed technique at reducing the discrete latent sample space size of a model while maintaining its learned multimodality.",0
"Variational autoencoders with discrete latent spaces have proven effective in capturing data distributions for real-world problems, including natural language understanding, human intent prediction, and visual scene representation. However, the size of the discrete latent space must be sufficiently large to handle the complexities of real-world data, making downstream tasks computationally challenging. For example, motion planning in a high-dimensional latent representation of an environment may become intractable. To address this issue, we propose a technique for sparsifying the discrete latent space of a trained conditional variational autoencoder while preserving its multimodality. We use evidential theory as a post hoc latent space reduction technique to identify the latent classes that receive direct evidence from a specific input condition and filter out those that do not. Our experiments on various tasks, such as image generation and human behavior prediction, demonstrate the effectiveness of our proposed technique in reducing the sample space size of a model's discrete latent space while maintaining its learned multimodality.",1
"Neural networks are prone to learning shortcuts -- they often model simple correlations, ignoring more complex ones that potentially generalize better. Prior works on image classification show that instead of learning a connection to object shape, deep classifiers tend to exploit spurious correlations with low-level texture or the background for solving the classification task. In this work, we take a step towards more robust and interpretable classifiers that explicitly expose the task's causal structure. Building on current advances in deep generative modeling, we propose to decompose the image generation process into independent causal mechanisms that we train without direct supervision. By exploiting appropriate inductive biases, these mechanisms disentangle object shape, object texture, and background; hence, they allow for generating counterfactual images. We demonstrate the ability of our model to generate such images on MNIST and ImageNet. Further, we show that the counterfactual images can improve out-of-distribution robustness with a marginal drop in performance on the original classification task, despite being synthetic. Lastly, our generative model can be trained efficiently on a single GPU, exploiting common pre-trained models as inductive biases.",0
"Shortcuts are often learned by neural networks, which can result in simple correlations being modeled while more complex ones that may be more generalizable are ignored. Previous research on image classification has shown that deep classifiers tend to rely on spurious correlations with low-level texture or the background instead of learning a connection to object shape. This study aims to create more solid and comprehensible classifiers by explicitly exposing the causal structure of the task. By disentangling object shape, object texture, and background with independent causal mechanisms that are trained without direct supervision, we propose to break down the image generation process. These mechanisms are able to generate counterfactual images by utilizing appropriate inductive biases. The model's ability to generate such images is demonstrated on MNIST and ImageNet. Furthermore, we show that despite being synthetic, the counterfactual images can enhance out-of-distribution robustness with only a slight decrease in performance on the original classification task. Lastly, our generative model can be trained efficiently on a single GPU by utilizing common pre-trained models as inductive biases.",1
"Blind image deblurring is a fundamental and challenging computer vision problem, which aims to recover both the blur kernel and the latent sharp image from only a blurry observation. Despite the superiority of deep learning methods in image deblurring have displayed, there still exists major challenge with various non-uniform motion blur. Previous methods simply take all the image features as the input to the decoder, which handles different degrees (e.g. large blur, small blur) simultaneously, leading to challenges for sharp image generation. To tackle the above problems, we present a deep two-branch network to deal with blurry images via a component divided module, which divides an image into two components based on the representation of blurry degree. Specifically, two component attentive blocks are employed to learn attention maps to exploit useful deblurring feature representations on both large and small blurry regions. Then, the blur-aware features are fed into two-branch reconstruction decoders respectively. In addition, a new feature fusion mechanism, orientation-based feature fusion, is proposed to merge sharp features of the two branches. Both qualitative and quantitative experimental results show that our method performs favorably against the state-of-the-art approaches.",0
"The task of blind image deblurring is a complex challenge for computer vision, as it involves restoring the clear image and blur kernel from an unclear input. While deep learning techniques have shown promise in this area, there remains a significant difficulty with handling non-uniform motion blur. Previous methods attempt to address this issue by treating all image features as equal, which creates problems in generating sharp images. In response, we propose a deep two-branch network that divides the image into two components based on the degree of blur. This is achieved through component attentive blocks that learn attention maps for both large and small blurry areas. The blur-aware features are then fed into two-branch reconstruction decoders, with a new orientation-based feature fusion mechanism used to merge the sharp features of the two branches. Our approach outperforms existing methods, as demonstrated through both qualitative and quantitative experiments.",1
"Supervised machine learning requires a large amount of labeled data to achieve proper test results. However, generating accurately labeled segmentation maps on remote sensing imagery, including images from synthetic aperture radar (SAR), is tedious and highly subjective. In this work, we propose to alleviate the issue of limited training data by generating synthetic SAR images with the pix2pix algorithm. This algorithm uses conditional Generative Adversarial Networks (cGANs) to generate an artificial image while preserving the structure of the input. In our case, the input is a segmentation mask, from which a corresponding synthetic SAR image is generated. We present different models, perform a comparative study and demonstrate that this approach synthesizes convincing glaciers in SAR images with promising qualitative and quantitative results.",0
"To obtain accurate results in supervised machine learning, a significant amount of labeled data is required. However, accurately labeling segmentation maps on remote sensing imagery, including synthetic aperture radar (SAR) images, is a challenging and subjective task. To address the issue of limited training data, we propose the use of the pix2pix algorithm to generate synthetic SAR images. This algorithm utilizes conditional Generative Adversarial Networks (cGANs) to produce an artificial image while maintaining the input's structure. In our case, we use a segmentation mask as input to generate a corresponding synthetic SAR image. We present various models, conduct a comparative analysis, and demonstrate that our approach synthesizes convincing glaciers in SAR images with promising qualitative and quantitative outcomes.",1
"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality.",0
"Over the past decade, deep learning has emerged as the standard for image processing. At the same time, there has been a growing interest in orbital activities such as satellite servicing and debris removal, which require spacecraft proximity operations. However, the use of deep learning for on-orbit visual navigation faces two significant challenges. Firstly, an effective system for model development is essential to streamline data curation, training, and evaluation. Secondly, a lack of labeled training data hinders the creation of robust deep learning models. To overcome these challenges, this paper introduces an open-source deep learning pipeline specifically designed for on-orbit visual navigation applications. The pipeline includes two custom software tools that are built on top of a cloud architecture. The first tool leverages Blender to generate labeled synthetic training data, while the second tool is a plugin-based framework for effective dataset curation and model training. The system has been used in the Texas Spacecraft Laboratory and has shown marked benefits in development speed and quality. The system's cloud-based computational resources also allow for the development of even the largest datasets and models from any machine.",1
"We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on $160 \times 160$ CelebA and $64 \times 64$ unconditional ImageNet.",0
"Our proposed approach offers a principled way to regularize the critic of GAN-like models using gradient-based methods. We achieve this by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD) while controlling the gradient of the critic to ensure a sensible loss function. Our method enforces exact, analytical gradient constraints without any additional cost, unlike existing techniques that rely on additive regularizers. The resulting loss function is continuous and backed by empirical evidence showing that it stabilizes and accelerates training, resulting in image generation models that surpass state-of-the-art approaches on $160 \times 160$ CelebA and $64 \times 64$ unconditional ImageNet.",1
"The biggest challenge faced by a Machine Learning Engineer is the lack of data they have, especially for 2-dimensional images. The image is processed to be trained into a Machine Learning model so that it can recognize patterns in the data and provide predictions. This research is intended to create a solution using the Cycle Generative Adversarial Networks (GANs) algorithm in overcoming the problem of lack of data. Then use Style Transfer to be able to generate a new image based on the given style. Based on the results of testing the resulting model has been carried out several improvements, previously the loss value of the photo generator: 3.1267, monet style generator: 3.2026, photo discriminator: 0.6325, and monet style discriminator: 0.6931 to photo generator: 2.3792, monet style generator: 2.7291, photo discriminator: 0.5956, and monet style discriminator: 0.4940. It is hoped that the research will make the application of this solution useful in the fields of Education, Arts, Information Technology, Medicine, Astronomy, Automotive and other important fields.",0
"The primary difficulty faced by Machine Learning Engineers pertains to insufficient data, particularly in the case of 2D images. These images undergo processing to enable training of Machine Learning models, which subsequently recognize data patterns and offer predictions. This study endeavors to surmount the dearth of data through the application of the Cycle Generative Adversarial Networks (GANs) algorithm. Additionally, Style Transfer is employed to produce a new image based on a specified style. Improvements were made to the resulting model based on testing, with the photo generator loss value reduced from 3.1267 to 2.3792, the monet style generator loss value from 3.2026 to 2.7291, the photo discriminator from 0.6325 to 0.5956 and the monet style discriminator from 0.6931 to 0.4940. This research aims to create a useful solution that can be applied in various crucial fields including Education, Arts, Information Technology, Medicine, Astronomy, Automotive, and others.",1
"Training GANs on videos is even more sophisticated than on images because videos have a distinguished dimension: time. While recent methods designed a dedicated architecture considering time, generated videos are still far from indistinguishable from real videos. In this paper, we introduce ArrowGAN framework, where the discriminators learns to classify arrow of time as an auxiliary task and the generators tries to synthesize forward-running videos. We argue that the auxiliary task should be carefully chosen regarding the target domain. In addition, we explore categorical ArrowGAN with recent techniques in conditional image generation upon ArrowGAN framework, achieving the state-of-the-art performance on categorical video generation. Our extensive experiments validate the effectiveness of arrow of time as a self-supervisory task, and demonstrate that all our components of categorical ArrowGAN lead to the improvement regarding video inception score and Frechet video distance on three datasets: Weizmann, UCFsports, and UCF-101.",0
"GAN training for videos is more complex compared to images due to the addition of time as a dimension. While some methods have been developed to address this, generated videos still lack the ability to pass as real. This research paper introduces the ArrowGAN framework, where the discriminator is trained to classify the arrow of time as an auxiliary task, and the generator aims to create forward-running videos. It is essential to carefully choose the auxiliary task based on the target domain. The study also explores the categorical ArrowGAN, integrating recent conditional image generation techniques, achieving top performance in categorical video generation. The results of the extensive experiments confirm the effectiveness of arrow of time as a self-supervisory task. The categorical ArrowGAN components lead to improvements in video inception score and Frechet video distance across three datasets: Weizmann, UCFsports, and UCF-101.",1
"Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\times$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .",0
"Different frameworks compete for deep generative learning based on likelihood, including normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models. VAEs provide quick and tractable sampling and convenient access to encoding networks, but they are presently outperformed by other models such as autoregressive models and normalizing flows. Our research explores the design of neural architectures for hierarchical VAEs, rather than focusing on statistical challenges. We introduce Nouveau VAE (NVAE), a deep hierarchical VAE that utilizes depth-wise separable convolutions and batch normalization, with a residual parameterization of Normal distributions and spectral regularization for training stabilization. NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on various datasets, such as MNIST, CIFAR-10, CelebA 64, and CelebA HQ, and provides a strong baseline on FFHQ. NVAE sets a record as the first successful VAE applied to natural images of up to 256$\times$256 pixels. The source code for NVAE is available at https://github.com/NVlabs/NVAE.",1
"We present a framework for training GANs with explicit control over generated images. We are able to control the generated image by settings exact attributes such as age, pose, expression, etc. Most approaches for editing GAN-generated images achieve partial control by leveraging the latent space disentanglement properties, obtained implicitly after standard GAN training. Such methods are able to change the relative intensity of certain attributes, but not explicitly set their values. Recently proposed methods, designed for explicit control over human faces, harness morphable 3D face models to allow fine-grained control capabilities in GANs. Unlike these methods, our control is not constrained to morphable 3D face model parameters and is extendable beyond the domain of human faces. Using contrastive learning, we obtain GANs with an explicitly disentangled latent space. This disentanglement is utilized to train control-encoders mapping human-interpretable inputs to suitable latent vectors, thus allowing explicit control. In the domain of human faces we demonstrate control over identity, age, pose, expression, hair color and illumination. We also demonstrate control capabilities of our framework in the domains of painted portraits and dog image generation. We demonstrate that our approach achieves state-of-the-art performance both qualitatively and quantitatively.",0
"Our framework provides a means of training GANs to generate images with explicit control. This control allows for precise manipulation of attributes such as age, pose, and expression. While many methods for editing GAN-generated images rely on latent space disentanglement to achieve partial control, our approach allows for explicit setting of attribute values. Unlike other methods that utilize morphable 3D face models for control, our approach is not limited to human faces and allows for fine-grained control beyond this domain. By using contrastive learning, we obtain GANs with a disentangled latent space, which enables training of control-encoders mapping interpretable inputs to suitable latent vectors. Our framework achieves superior performance in the domains of human faces, painted portraits, and dog image generation, both qualitatively and quantitatively.",1
"Iterative generative models, such as noise conditional score networks and denoising diffusion probabilistic models, produce high quality samples by gradually denoising an initial noise vector. However, their denoising process has many steps, making them 2-3 orders of magnitude slower than other generative models such as GANs and VAEs. In this paper, we establish a novel connection between knowledge distillation and image generation with a technique that distills a multi-step denoising process into a single step, resulting in a sampling speed similar to other single-step generative models. Our Denoising Student generates high quality samples comparable to GANs on the CIFAR-10 and CelebA datasets, without adversarial training. We demonstrate that our method scales to higher resolutions through experiments on 256 x 256 LSUN. Code and checkpoints are available at https://github.com/tcl9876/Denoising_Student",0
"Gradual denoising techniques used in iterative generative models, like noise conditional score networks and denoising diffusion probabilistic models, produce superior quality samples. However, their multiple denoising steps make them considerably slower than other generative models such as GANs and VAEs. This paper introduces a new technique that connects knowledge distillation and image generation. By distilling a multi-step denoising process into a single step, we achieve sampling speeds similar to single-step generative models. Our Denoising Student produces high-quality samples comparable to GANs on the CIFAR-10 and CelebA datasets, without adversarial training. We also demonstrate that our method scales to higher resolutions through experiments on 256 x 256 LSUN. Code and checkpoints are available at https://github.com/tcl9876/Denoising_Student.",1
"Federated learning is a new machine learning paradigm which allows data parties to build machine learning models collaboratively while keeping their data secure and private. While research efforts on federated learning have been growing tremendously in the past two years, most existing works still depend on pre-existing public datasets and artificial partitions to simulate data federations due to the lack of high-quality labeled data generated from real-world edge applications. Consequently, advances on benchmark and model evaluations for federated learning have been lagging behind. In this paper, we introduce a real-world image dataset. The dataset contains more than 900 images generated from 26 street cameras and 7 object categories annotated with detailed bounding box. The data distribution is non-IID and unbalanced, reflecting the characteristic real-world federated learning scenarios. Based on this dataset, we implemented two mainstream object detection algorithms (YOLO and Faster R-CNN) and provided an extensive benchmark on model performance, efficiency, and communication in a federated learning setting. Both the dataset and algorithms are made publicly available.",0
"Federated learning is a novel approach to machine learning that enables data parties to collaboratively construct machine learning models while ensuring the privacy and security of their data. Despite the significant growth in research on federated learning over the past two years, most existing studies rely on artificial partitions and pre-existing public datasets to simulate data federations, owing to the scarcity of high-quality labeled data produced by real-world edge applications. As a result, progress in benchmarking and evaluating models for federated learning has been slow. In this paper, we present a real-world image dataset comprising over 900 images from 26 street cameras and 7 object categories, annotated with detailed bounding boxes. The dataset reflects the non-IID and unbalanced data distribution typical of real-world federated learning scenarios. Using this dataset, we implemented two popular object detection algorithms, YOLO and Faster R-CNN, and conducted a comprehensive benchmark of model performance, efficiency, and communication in a federated learning environment. Both the dataset and algorithms are publicly available.",1
"Generative Adversarial Networks (GANs) are an arrange of two neural networks -- the generator and the discriminator -- that are jointly trained to generate artificial data, such as images, from random inputs. The quality of these generated images has recently reached such levels that can often lead both machines and humans into mistaking fake for real examples. However, the process performed by the generator of the GAN has some limitations when we want to condition the network to generate images from subcategories of a specific class. Some recent approaches tackle this \textit{conditional generation} by introducing extra information prior to the training process, such as image semantic segmentation or textual descriptions. While successful, these techniques still require defining beforehand the desired subcategories and collecting large labeled image datasets representing them to train the GAN from scratch. In this paper we present a novel and alternative method for guiding generic non-conditional GANs to behave as conditional GANs. Instead of re-training the GAN, our approach adds into the mix an encoder network to generate the high-dimensional random input vectors that are fed to the generator network of a non-conditional GAN to make it generate images from a specific subcategory. In our experiments, when compared to training a conditional GAN from scratch, our guided GAN is able to generate artificial images of perceived quality comparable to that of non-conditional GANs after training the encoder on just a few hundreds of images, which substantially accelerates the process and enables adding new subcategories seamlessly.",0
"Generative Adversarial Networks (GANs) consist of two neural networks, namely the generator and the discriminator, which are trained together to produce synthetic data, such as images, from random inputs. The generated images have now reached a level of quality that can deceive both humans and machines into mistaking them for real examples. However, when we want to generate images from specific subcategories of a particular class, the generator network of GANs has limitations. To overcome this, recent techniques have introduced additional information, such as image semantic segmentation and textual descriptions, prior to the training process. These methods require defining the desired subcategories beforehand and collecting large labeled image datasets to train the GAN from scratch. In this paper, we propose an innovative alternative approach to transform non-conditional GANs into conditional GANs. Instead of re-training the GAN, we introduce an encoder network to generate high-dimensional random input vectors that guide the generator network of a non-conditional GAN to produce images from specific subcategories. Our experiments show that our guided GAN method can generate synthetic images of comparable quality to that of non-conditional GANs after training the encoder on only a few hundred images, which significantly speeds up the process and enables seamless addition of new subcategories.",1
"The last few years have witnessed the great success of non-linear generative models in synthesizing high-quality photorealistic face images. Many recent 3D facial texture reconstruction and pose manipulation from a single image approaches still rely on large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Yet the collection of such a large scale high-resolution 3D texture dataset is still very costly and difficult to maintain age/ethnicity balance. Moreover, regression-based approaches suffer from generalization to the in-the-wild conditions and are unable to fine-tune to a target-image. In this work, we propose an unsupervised approach for one-shot 3D facial texture completion that does not require large-scale texture datasets, but rather harnesses the knowledge stored in 2D face generators. The proposed approach rotates an input image in 3D and fill-in the unseen regions by reconstructing the rotated image in a 2D face generator, based on the visible parts. Finally, we stitch the most visible textures at different angles in the UV image-plane. Further, we frontalize the target image by projecting the completed texture into the generator. The qualitative and quantitative experiments demonstrate that the completed UV textures and frontalized images are of high quality, resembles the original identity, can be used to train a texture GAN model for 3DMM fitting and improve pose-invariant face recognition.",0
"In recent years, non-linear generative models have achieved significant success in producing high-quality photorealistic face images. However, many current techniques for 3D facial texture reconstruction and pose manipulation from a single image still require large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Unfortunately, collecting such a dataset is expensive and maintaining age/ethnicity balance is difficult. Furthermore, regression-based approaches struggle to generalize to in-the-wild conditions and cannot fine-tune to a target image. To address these challenges, we propose an unsupervised approach for one-shot 3D facial texture completion that leverages the knowledge stored in 2D face generators rather than large-scale texture datasets. Our approach rotates an input image in 3D and fills in the unseen regions by reconstructing the rotated image in a 2D face generator based on the visible parts. We then stitch the most visible textures at different angles in the UV image-plane and frontalize the target image by projecting the completed texture into the generator. Qualitative and quantitative experiments demonstrate that our completed UV textures and frontalized images are of high quality, resemble the original identity, and can be used to train a texture GAN model for 3DMM fitting and improve pose-invariant face recognition.",1
"Recent advances in deep generative models for photo-realistic images have led to high quality visual results. Such models learn to generate data from a given training distribution such that generated images can not be easily distinguished from real images by the human eye. Yet, recent work on the detection of such fake images pointed out that they are actually easily distinguishable by artifacts in their frequency spectra. In this paper, we propose to generate images according to the frequency distribution of the real data by employing a spectral discriminator. The proposed discriminator is lightweight, modular and works stably with different commonly used GAN losses. We show that the resulting models can better generate images with realistic frequency spectra, which are thus harder to detect by this cue.",0
"In recent times, advancements in deep generative models have led to the production of high-quality photo-realistic images. These models are designed to learn how to generate data from a specified training distribution. As a result, the images they produce are often indistinguishable from real images to the human eye. However, recent research on detecting fake images revealed that these images can be easily distinguished from real ones by examining their frequency spectra for artifacts. This paper proposes a solution to this problem by generating images based on the frequency distribution of real data through the use of a spectral discriminator. This discriminator is lightweight, modular, and compatible with various commonly used GAN losses. The resulting models generate images with realistic frequency spectra, making them more difficult to detect.",1
"Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are: (1) the generation of high quality images, (2) diversity of image generation, and (3) stable training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state of the art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress towards addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress towards important computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Code related to GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review.",0
"Over the past few years, there has been a significant amount of research on Generative Adversarial Networks (GANs). These networks have had a significant impact on the field of computer vision, particularly in the areas of image generation, image-to-image translation, and facial attribute manipulation. Despite their progress thus far, applying GANs to real-world problems is still challenging, with three significant challenges being high-quality image generation, diverse image generation, and stable training. In this review, we provide a detailed overview of the state of the art in GAN-related research, with a focus on the progress made by popular GAN technologies in addressing these challenges. We use a taxonomy based on variations in GAN architectures and loss functions to structure our review. While there have been several reviews of GANs, none have focused on the practical challenges relevant to computer vision. Our objective is to provide a critical analysis of the status of GAN research in terms of progress towards important computer vision application requirements. Additionally, we discuss the most compelling applications in computer vision that have demonstrated considerable success using GANs, as well as suggestions for future research directions. Code related to the GAN-variants studied in this review is available on https://github.com/sheqi/GAN_Review.",1
"Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study.",0
"GANs have been highly successful in multiple areas such as medicine, natural language processing, and computer vision. They excel at transforming objects and individuals into specific shapes, and are adept at learning complex distributions to create meaningful samples. However, a thorough review of the field is currently lacking, particularly with regards to GANs loss-variant, evaluation metrics, image generation diversity, and stable training. In this survey, we aim to fill this gap by providing a comprehensive overview of adversarial models for image synthesis. We categorize the literature based on image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We also organize the studies according to their base models, architecture ideas, constraints, loss functions, evaluation metrics, and training datasets. Our review highlights the milestones in adversarial models and provides insights into the development path from model-based to data-driven methods. We also identify potential future research directions. A unique aspect of this review is that all software implementations of GAN methods and datasets are available in one place at https://github.com/pshams55/GAN-Case-Study.",1
"We present a generative model for controllable person image synthesis,as shown in Figure , which can be applied to pose-guided person image synthesis, $i.e.$, converting the pose of a source person image to the target pose while preserving the texture of that source person image, and clothing-guided person image synthesis, $i.e.$, changing the clothing texture of a source person image to the desired clothing texture. By explicitly establishing the dense correspondence between the target pose and the source image, we can effectively address the misalignment introduced by pose tranfer and generate high-quality images. Specifically, we first generate the target semantic map under the guidence of the target pose, which can provide more accurate pose representation and structural constraints during the generation process. Then, decomposed attribute encoder is used to extract the component features, which not only helps to establish a more accurate dense correspondence, but also realizes the clothing-guided person generation. After that, we will establish a dense correspondence between the target pose and the source image within the sharded domain. The source image feature is warped according to the dense correspondence to flexibly account for deformations. Finally, the network renders image based on the warped source image feature and the target pose. Experimental results show that our method is superior to state-of-the-art methods in pose-guided person generation and its effectiveness in clothing-guided person generation.",0
"In this paper, we introduce a generative model that enables the synthesis of person images with controllable attributes. Our model can be applied to two scenarios: pose-guided person image synthesis and clothing-guided person image synthesis. The former involves converting the pose of a source person image to a target pose while keeping the texture of the source image, while the latter involves changing the clothing texture of a source person image to a desired texture. To address the misalignment caused by pose transfer and generate high-quality images, we establish a dense correspondence between the target pose and the source image. We start by generating a target semantic map under the guidance of the target pose, which provides more accurate pose representation and structural constraints during the generation process. Then, we use a decomposed attribute encoder to extract component features that aid in establishing accurate dense correspondence and realizing clothing-guided person generation. Next, we establish dense correspondence between the target pose and the source image in the shared domain, and warp the source image feature based on this correspondence to account for deformations. Finally, the network renders the image based on the warped source image feature and the target pose. Our experimental results demonstrate that our method outperforms state-of-the-art methods in pose-guided person generation and is effective in clothing-guided person generation.",1
"Although significant progress has been made in synthesizing high-quality and visually realistic face images by unconditional Generative Adversarial Networks (GANs), there still lacks of control over the generation process in order to achieve semantic face editing. In addition, it remains very challenging to maintain other face information untouched while editing the target attributes. In this paper, we propose a novel learning framework, called GuidedStyle, to achieve semantic face editing on StyleGAN by guiding the image generation process with a knowledge network. Furthermore, we allow an attention mechanism in StyleGAN generator to adaptively select a single layer for style manipulation. As a result, our method is able to perform disentangled and controllable edits along various attributes, including smiling, eyeglasses, gender, mustache and hair color. Both qualitative and quantitative results demonstrate the superiority of our method over other competing methods for semantic face editing. Moreover, we show that our model can be also applied to different types of real and artistic face editing, demonstrating strong generalization ability.",0
"Despite significant progress in synthesizing high-quality and visually realistic face images using unconditional Generative Adversarial Networks (GANs), there is still a lack of control over the generation process to achieve semantic face editing and maintain other face information during attribute editing. To address this, we introduce a new learning framework, GuidedStyle, which guides the image generation process using a knowledge network to achieve semantic face editing on StyleGAN. Additionally, our method allows for an attention mechanism in the StyleGAN generator to selectively manipulate a single layer for style manipulation. As a result, our approach achieves disentangled and controllable edits for various attributes, including smiling, eyeglasses, gender, mustache, and hair color. Our method outperforms other competing approaches, demonstrating superior results in both qualitative and quantitative assessments. Furthermore, we demonstrate the generalizability of our model in different types of real and artistic face editing.",1
"Among the wide variety of image generative models, two models stand out: Variational Auto Encoders (VAE) and Generative Adversarial Networks (GAN). GANs can produce realistic images, but they suffer from mode collapse and do not provide simple ways to get the latent representation of an image. On the other hand, VAEs do not have these problems, but they often generate images less realistic than GANs. In this article, we explain that this lack of realism is partially due to a common underestimation of the natural image manifold dimensionality. To solve this issue we introduce a new framework that combines VAE and GAN in a novel and complementary way to produce an auto-encoding model that keeps VAEs properties while generating images of GAN-quality. We evaluate our approach both qualitatively and quantitatively on five image datasets.",0
"Out of the various image generative models available, Variational Auto Encoders (VAE) and Generative Adversarial Networks (GAN) are particularly notable. While GANs can create lifelike images, they are prone to mode collapse and lack simple methods for obtaining the latent representation of an image. Conversely, VAEs do not suffer from these issues but their generated images are often less realistic than those produced by GANs. This article proposes that the reason for this discrepancy is a common underestimation of the natural image manifold dimensionality. To address this problem, the authors introduce a new framework that combines VAE and GAN to create an auto-encoding model that retains VAE's advantageous features while producing images of GAN-quality. The effectiveness of this approach is evaluated qualitatively and quantitatively on five different image datasets.",1
"Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator's objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks.",0
"GANs can achieve greater versatility in downstream tasks if latent vectors can be effectively inferred during training. Recent frameworks like ALI and BiGAN have successfully incorporated methods for inference of latent variables in GANs by adversarial training an image generator and an encoder. Our approach builds on these frameworks by introducing multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge. We modify the discriminator's objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks. We also design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that matches all the distributions. Our framework includes a novel set of techniques for providing self-supervised feedback to the model based on properties like patch-level correspondence and cycle consistency of reconstructions. Experiments demonstrate the proposed approach's efficacy, scalability, and flexibility for various tasks.",1
"Video-based human motion transfer creates video animations of humans following a source motion. Current methods show remarkable results for tightly-clad subjects. However, the lack of temporally consistent handling of plausible clothing dynamics, including fine and high-frequency details, significantly limits the attainable visual quality. We address these limitations for the first time in the literature and present a new framework which performs high-fidelity and temporally-consistent human motion transfer with natural pose-dependent non-rigid deformations, for several types of loose garments. In contrast to the previous techniques, we perform image generation in three subsequent stages, synthesizing human shape, structure, and appearance. Given a monocular RGB video of an actor, we train a stack of recurrent deep neural networks that generate these intermediate representations from 2D poses and their temporal derivatives. Splitting the difficult motion transfer problem into subtasks that are aware of the temporal motion context helps us to synthesize results with plausible dynamics and pose-dependent detail. It also allows artistic control of results by manipulation of individual framework stages. In the experimental results, we significantly outperform the state-of-the-art in terms of video realism. Our code and data will be made publicly available.",0
"The process of video-based human motion transfer involves creating video animations of humans mimicking a source motion. While current methods produce impressive results for subjects wearing tight clothing, they struggle with managing the dynamics of loose clothing, including fine and high-frequency details. To address this issue, we introduce a new framework that offers high-quality and consistent human motion transfer with natural pose-dependent non-rigid deformations for various types of loose garments. Our approach involves three stages of image generation, which synthesize human shape, structure, and appearance. By training a stack of recurrent deep neural networks to generate intermediate representations from 2D poses and their temporal derivatives, we can tackle the challenging motion transfer problem by breaking it down into smaller, more manageable subtasks. This approach allows us to produce results with plausible dynamics and pose-dependent detail, while also providing artistic control over each framework stage. In our experimental results, our framework significantly outperforms the state-of-the-art in terms of video realism. We will make our code and data publicly available.",1
"In remote sensing images, the existence of the thin cloud is an inevitable and ubiquitous phenomenon that crucially reduces the quality of imageries and limits the scenarios of application. Therefore, thin cloud removal is an indispensable procedure to enhance the utilization of remote sensing images. Generally, even though contaminated by thin clouds, the pixels still retain more or less surface information. Hence, different from thick cloud removal, thin cloud removal algorithms normally concentrate on inhibiting the cloud influence rather than substituting the cloud-contaminated pixels. Meanwhile, considering the surface features obscured by the cloud are usually similar to adjacent areas, the dependency between each pixel of the input is useful to reconstruct contaminated areas. In this paper, to make full use of the dependencies between pixels of the image, we propose a Multi-Head Linear Attention Generative Adversarial Network (MLAGAN) for Thin Cloud Removal. The MLA-GAN is based on the encoding-decoding framework consisting of multiple attention-based layers and deconvolutional layers. Compared with six deep learning-based thin cloud removal benchmarks, the experimental results on the RICE1 and RICE2 datasets demonstrate that the proposed framework MLA-GAN has dominant advantages in thin cloud removal.",0
"The presence of thin clouds in remote sensing images is a common problem that significantly reduces their quality and limits their potential applications. Thus, removing thin clouds is an essential process to improve the usefulness of these images. While pixels contaminated by thin clouds still contain surface information, the focus of thin cloud removal algorithms is on minimizing cloud influence rather than replacing these pixels. Additionally, since obscured surface features are similar to surrounding areas, utilizing the relationships between pixels can help reconstruct contaminated regions. This paper proposes a Multi-Head Linear Attention Generative Adversarial Network (MLAGAN) for Thin Cloud Removal, which leverages pixel dependencies through an encoding-decoding framework with attention-based layers and deconvolutional layers. Compared to six other deep learning-based thin cloud removal models, experiments on the RICE1 and RICE2 datasets demonstrate that MLA-GAN outperforms them significantly.",1
"UAVs have become an essential photogrammetric measurement as they are affordable, easily accessible and versatile. Aerial images captured from UAVs have applications in small and large scale texture mapping, 3D modelling, object detection tasks, DTM and DSM generation etc. Photogrammetric techniques are routinely used for 3D reconstruction from UAV images where multiple images of the same scene are acquired. Developments in computer vision and deep learning techniques have made Single Image Depth Estimation (SIDE) a field of intense research. Using SIDE techniques on UAV images can overcome the need for multiple images for 3D reconstruction. This paper aims to estimate depth from a single UAV aerial image using deep learning. We follow a self-supervised learning approach, Self-Supervised Monocular Depth Estimation (SMDE), which does not need ground truth depth or any extra information other than images for learning to estimate depth. Monocular video frames are used for training the deep learning model which learns depth and pose information jointly through two different networks, one each for depth and pose. The predicted depth and pose are used to reconstruct one image from the viewpoint of another image utilising the temporal information from videos. We propose a novel architecture with two 2D CNN encoders and a 3D CNN decoder for extracting information from consecutive temporal frames. A contrastive loss term is introduced for improving the quality of image generation. Our experiments are carried out on the public UAVid video dataset. The experimental results demonstrate that our model outperforms the state-of-the-art methods in estimating the depths.",0
"UAVs have become crucial for photogrammetric measurements due to their affordability, accessibility, and versatility. The aerial images captured from these vehicles have various applications, such as small and large-scale texture mapping, 3D modelling, object detection, DTM and DSM generation, etc. Photogrammetric techniques are commonly used for 3D reconstruction from multiple images of one scene. However, advancements in computer vision have led to intense research in the field of Single Image Depth Estimation (SIDE), which can reconstruct 3D models from one image. This study aims to estimate depth from a single UAV aerial image using deep learning, specifically the Self-Supervised Monocular Depth Estimation (SMDE) approach. The model uses monocular video frames for training, which jointly learns depth and pose information through two separate networks. The predicted depth and pose are then used to reconstruct an image from the viewpoint of another image using temporal information from videos. The proposed architecture uses two 2D CNN encoders and a 3D CNN decoder to extract information from consecutive frames, with a contrastive loss term introduced to improve image generation quality. The study's experiments were conducted using the public UAVid video dataset and showed that the proposed model outperforms state-of-the-art methods in estimating depths.",1
"Generative adversarial networks (GANs) provide state-of-the-art results in image generation. However, despite being so powerful, they still remain very challenging to train. This is in particular caused by their highly non-convex optimization space leading to a number of instabilities. Among them, mode collapse stands out as one of the most daunting ones. This undesirable event occurs when the model can only fit a few modes of the data distribution, while ignoring the majority of them. In this work, we combat mode collapse using second-order gradient information. To do so, we analyse the loss surface through its Hessian eigenvalues, and show that mode collapse is related to the convergence towards sharp minima. In particular, we observe how the eigenvalues of the $G$ are directly correlated with the occurrence of mode collapse. Finally, motivated by these findings, we design a new optimization algorithm called nudged-Adam (NuGAN) that uses spectral information to overcome mode collapse, leading to empirically more stable convergence properties.",0
"Despite their ability to produce cutting-edge results in image generation, Generative adversarial networks (GANs) are still challenging to train due to the highly non-convex optimization space that leads to various instabilities. Mode collapse, in particular, is a significant challenge where the model can only fit a few modes of the data distribution, ignoring the majority of them. This study addresses mode collapse and proposes a new optimization algorithm named nudged-Adam (NuGAN) that utilizes second-order gradient information to overcome this issue. The study analyzes the loss surface through its Hessian eigenvalues and finds that mode collapse is related to convergence towards sharp minima. The eigenvalues of G are also directly correlated with the occurrence of mode collapse. The proposed NuGAN algorithm utilizes spectral information to overcome mode collapse, resulting in more stable convergence properties.",1
"Automatic histopathology image segmentation is crucial to disease analysis. Limited available labeled data hinders the generalizability of trained models under the fully supervised setting. Semi-supervised learning (SSL) based on generative methods has been proven to be effective in utilizing diverse image characteristics. However, it has not been well explored what kinds of generated images would be more useful for model training and how to use such images. In this paper, we propose a new data guided generative method for histopathology image segmentation by leveraging the unlabeled data distributions. First, we design an image generation module. Image content and style are disentangled and embedded in a clustering-friendly space to utilize their distributions. New images are synthesized by sampling and cross-combining contents and styles. Second, we devise an effective data selection policy for judiciously sampling the generated images: (1) to make the generated training set better cover the dataset, the clusters that are underrepresented in the original training set are covered more; (2) to make the training process more effective, we identify and oversample the images of ""hard cases"" in the data for which annotated training data may be scarce. Our method is evaluated on glands and nuclei datasets. We show that under both the inductive and transductive settings, our SSL method consistently boosts the performance of common segmentation models and attains state-of-the-art results.",0
"The segmentation of histopathology images automatically is critical for analyzing diseases. However, the limited availability of labeled data hinders the generalizability of fully supervised trained models. Generative semi-supervised learning (SSL) methods have been proven to be effective in utilizing various image characteristics, but it is unclear what types of generated images would be most useful for model training and how to use them. In this paper, we propose a novel data-guided generative method for histopathology image segmentation that leverages the distributions of unlabeled data. We first design an image generation module that disentangles image content and style and embeds them in a clustering-friendly space to synthesize new images by sampling and cross-combining contents and styles. We then devise an effective data selection policy that samples generated images judiciously by covering clusters that are underrepresented in the original training set and oversampling ""hard cases"" for which annotated training data may be scarce. Our method is evaluated on glands and nuclei datasets, and we demonstrate that our SSL method consistently improves the performance of common segmentation models and achieves state-of-the-art results under both inductive and transductive settings.",1
"Generative adversarial networks are the state of the art approach towards learned synthetic image generation. Although early successes were mostly unsupervised, bit by bit, this trend has been superseded by approaches based on labelled data. These supervised methods allow a much finer-grained control of the output image, offering more flexibility and stability. Nevertheless, the main drawback of such models is the necessity of annotated data. In this work, we introduce an novel framework that benefits from two popular learning techniques, adversarial training and representation learning, and takes a step towards unsupervised conditional GANs. In particular, our approach exploits the structure of a latent space (learned by the representation learning) and employs it to condition the generative model. In this way, we break the traditional dependency between condition and label, substituting the latter by unsupervised features coming from the latent space. Finally, we show that this new technique is able to produce samples on demand keeping the quality of its supervised counterpart.",0
"The latest advanced method for creating artificial images is through generative adversarial networks. While initial successes were achieved without supervision, there has been a shift towards using labeled data for greater control and stability. However, the downside is that annotated data is required. This study proposes a new approach that combines adversarial training and representation learning to create unsupervised conditional GANs. By utilizing the structure of a learned latent space, our method can condition the generative model without relying on labeled data. Our results demonstrate that this technique can produce high-quality images on demand, similar to supervised methods.",1
"With the proliferation of face image manipulation (FIM) techniques such as Face2Face and Deepfake, more fake face images are spreading over the internet, which brings serious challenges to public confidence. Face image forgery detection has made considerable progresses in exposing specific FIM, but it is still in scarcity of a robust fake face detector to expose face image forgeries under complex scenarios such as with further compression, blurring, scaling, etc. Due to the relatively fixed structure, convolutional neural network (CNN) tends to learn image content representations. However, CNN should learn subtle manipulation traces for image forensics tasks. Thus, we propose an adaptive manipulation traces extraction network (AMTEN), which serves as pre-processing to suppress image content and highlight manipulation traces. AMTEN exploits an adaptive convolution layer to predict manipulation traces in the image, which are reused in subsequent layers to maximize manipulation artifacts by updating weights during the back-propagation pass. A fake face detector, namely AMTENnet, is constructed by integrating AMTEN with CNN. Experimental results prove that the proposed AMTEN achieves desirable pre-processing. When detecting fake face images generated by various FIM techniques, AMTENnet achieves an average accuracy up to 98.52%, which outperforms the state-of-the-art works. When detecting face images with unknown post-processing operations, the detector also achieves an average accuracy of 95.17%.",0
"The internet is flooded with fake face images due to the increasing popularity of face image manipulation techniques like Face2Face and Deepfake. This has led to a loss of public trust and confidence. While there has been progress in detecting specific types of manipulated images, there is a shortage of a reliable detector that can identify fake face images under complex scenarios such as scaling, compression, and blurring. Convolutional neural networks (CNN) are good at learning image content representations, but they struggle with identifying subtle manipulation traces required for image forensics tasks. To address this issue, we present an adaptive manipulation traces extraction network (AMTEN) that suppresses image content and highlights manipulation traces. AMTEN uses an adaptive convolution layer to predict manipulation traces, which are then incorporated into subsequent layers to maximize manipulation artifacts. We integrate AMTEN with CNN to create a fake face detector, AMTENnet, which achieves an average accuracy of 98.52% when detecting fake face images generated by various FIM techniques. It also achieves an average accuracy of 95.17% when detecting face images with unknown post-processing operations, outperforming existing state-of-the-art works.",1
"Generative Adversarial Networks (GANs) have made releasing of synthetic images a viable approach to share data without releasing the original dataset. It has been shown that such synthetic data can be used for a variety of downstream tasks such as training classifiers that would otherwise require the original dataset to be shared. However, recent work has shown that the GAN models and their synthetically generated data can be used to infer the training set membership by an adversary who has access to the entire dataset and some auxiliary information. Current approaches to mitigate this problem (such as DPGAN) lead to dramatically poorer generated sample quality than the original non--private GANs. Here we develop a new GAN architecture (privGAN), where the generator is trained not only to cheat the discriminator but also to defend membership inference attacks. The new mechanism provides protection against this mode of attack while leading to negligible loss in downstream performances. In addition, our algorithm has been shown to explicitly prevent overfitting to the training set, which explains why our protection is so effective. The main contributions of this paper are: i) we propose a novel GAN architecture that can generate synthetic data in a privacy preserving manner without additional hyperparameter tuning and architecture selection, ii) we provide a theoretical understanding of the optimal solution of the privGAN loss function, iii) we demonstrate the effectiveness of our model against several white and black--box attacks on several benchmark datasets, iv) we demonstrate on three common benchmark datasets that synthetic images generated by privGAN lead to negligible loss in downstream performance when compared against non--private GANs.",0
"The use of Generative Adversarial Networks (GANs) has made it possible to share data through synthetic images instead of the original dataset. This strategy has been proven effective for training classifiers and other downstream tasks. However, recent studies have revealed that GAN models and their synthetic data can be exploited by adversaries to infer the training set membership. Efforts to address this issue, such as DPGAN, have resulted in lower quality synthetic samples compared to non-private GANs. To solve this problem, we introduce a new GAN framework called privGAN that not only trains the generator to deceive the discriminator but also defends against membership inference attacks. Our approach has shown to prevent overfitting while maintaining high downstream performance. The contributions of our work include proposing a novel GAN architecture that preserves privacy without additional hyperparameter tuning, providing a theoretical understanding of the privGAN loss function, demonstrating the effectiveness of our model against various attacks on benchmark datasets, and showing that privGAN-generated synthetic images have minimal impact on downstream performance compared to non-private GANs.",1
"Human pose transfer, as a misaligned image generation task, is very challenging. Existing methods cannot effectively utilize the input information, which often fail to preserve the style and shape of hair and clothes. In this paper, we propose an adaptive human pose transfer network with two hierarchical deformation levels. The first level generates human semantic parsing aligned with the target pose, and the second level generates the final textured person image in the target pose with the semantic guidance. To avoid the drawback of vanilla convolution that treats all the pixels as valid information, we use gated convolution in both two levels to dynamically select the important features and adaptively deform the image layer by layer. Our model has very few parameters and is fast to converge. Experimental results demonstrate that our model achieves better performance with more consistent hair, face and clothes with fewer parameters than state-of-the-art methods. Furthermore, our method can be applied to clothing texture transfer.",0
"Generating aligned images for human pose transfer poses a significant challenge due to misalignments. Current approaches fail to preserve the style and shape of hair and clothes, struggling to utilize input information effectively. This paper presents an adaptive human pose transfer network with two hierarchical deformation levels. The first level aligns human semantic parsing with the target pose, while the second level generates a final textured person image using semantic guidance. To overcome the limitations of vanilla convolution, which treats all pixels as valid information, we use gated convolution in both levels to select important features and adaptively deform the image layer by layer. Our model has few parameters, enabling fast convergence. Experimental results demonstrate that our approach produces more consistent hair, face, and clothes with fewer parameters than state-of-the-art methods. Additionally, the method can be applied to clothing texture transfer.",1
"Generative adversarial training (GAT) is a recently introduced adversarial defense method. Previous works have focused on empirical evaluations of its application to training robust predictive models. In this paper we focus on theoretical understanding of the GAT method and extending its application to generative modeling and out-of-distribution detection. We analyze the optimal solutions of the maximin formulation employed by the GAT objective, and make a comparative analysis of the minimax formulation employed by GANs. We use theoretical analysis and 2D simulations to understand the convergence property of the training algorithm. Based on these results, we develop an incremental generative training algorithm, and conduct comprehensive evaluations of the algorithm's application to image generation and adversarial out-of-distribution detection. Our results suggest that generative adversarial training is a promising new direction for the above applications.",0
"The GAT method is a new adversarial defense technique that has been primarily evaluated through empirical studies for developing robust predictive models. However, this paper aims to explore the theoretical aspects of GAT and its potential application in generative modeling and detecting out-of-distribution anomalies. We investigate the optimal solutions of GAT's maximin formulation and compare it with GAN's minimax formulation. Through theoretical analysis and 2D simulations, we analyze the convergence behavior of GAT's training algorithm. Based on our findings, we introduce an incremental generative training algorithm and evaluate its effectiveness in generating images and detecting adversarial out-of-distribution anomalies. Our results suggest that GAT is a promising approach for these applications.",1
"Temporal consistency is crucial for extending image processing pipelines to the video domain, which is often enforced with flow-based warping error over adjacent frames. Yet for human video synthesis, such scheme is less reliable due to the misalignment between source and target video as well as the difficulty in accurate flow estimation. In this paper, we propose an effective intrinsic temporal regularization scheme to mitigate these issues, where an intrinsic confidence map is estimated via the frame generator to regulate motion estimation via temporal loss modulation. This creates a shortcut for back-propagating temporal loss gradients directly to the front-end motion estimator, thus improving training stability and temporal coherence in output videos. We apply our intrinsic temporal regulation to single-image generator, leading to a powerful ""INTERnet"" capable of generating $512\times512$ resolution human action videos with temporal-coherent, realistic visual details. Extensive experiments demonstrate the superiority of proposed INTERnet over several competitive baselines.",0
"Ensuring consistency over time is vital for extending image processing pipelines to videos, typically achieved through flow-based warping error between adjacent frames. However, when generating human videos, this approach is less dependable due to misalignment and inaccurate flow estimation. To address this, we present an intrinsic temporal regularization technique that utilizes a confidence map generated by the frame generator to regulate motion estimation through temporal loss modulation. This approach enables direct back-propagation of temporal loss gradients to the motion estimator, improving training stability and temporal coherence in the output videos. Our method is applied to a single-image generator, resulting in the powerful ""INTERnet"" capable of generating $512\times512$ resolution human action videos with realistic, temporal-coherent visual details. Extensive experiments demonstrate the superiority of our proposed method over several competitive baselines.",1
"To detect bias in face recognition networks, it can be useful to probe a network under test using samples in which only specific attributes vary in some controlled way. However, capturing a sufficiently large dataset with specific control over the attributes of interest is difficult. In this work, we describe a simulator that applies specific head pose and facial expression adjustments to images of previously unseen people. The simulator first fits a 3D morphable model to a provided image, applies the desired head pose and facial expression controls, then renders the model into an image. Next, a conditional Generative Adversarial Network (GAN) conditioned on the original image and the rendered morphable model is used to produce the image of the original person with the new facial expression and head pose. We call this conditional GAN -- MorphGAN. Images generated using MorphGAN conserve the identity of the person in the original image, and the provided control over head pose and facial expression allows test sets to be created to identify robustness issues of a facial recognition deep network with respect to pose and expression. Images generated by MorphGAN can also serve as data augmentation when training data are scarce. We show that by augmenting small datasets of faces with new poses and expressions improves the recognition performance by up to 9% depending on the augmentation and data scarcity.",0
"One way to identify bias in face recognition networks is to test the network with controlled samples where only specific attributes vary. However, gathering a dataset with control over these attributes is challenging. This study introduces a simulator that modifies images of unfamiliar people by adjusting their head pose and facial expressions using a 3D morphable model. The simulator then uses a conditional GAN, called MorphGAN, to produce new images of the same person with different expressions and poses. MorphGAN can create test sets to evaluate the network's robustness to pose and expression, and the generated images can also serve as data augmentation. The study found that augmenting small datasets with MorphGAN images improved recognition performance by up to 9% depending on the level of augmentation and data scarcity.",1
"Autonomous agents, such as driverless cars, require large amounts of labeled visual data for their training. A viable approach for acquiring such data is training a generative model with collected real data, and then augmenting the collected real dataset with synthetic images from the model, generated with control of the scene layout and ground truth labeling. In this paper we propose Full-Glow, a fully conditional Glow-based architecture for generating plausible and realistic images of novel street scenes given a semantic segmentation map indicating the scene layout. Benchmark comparisons show our model to outperform recent works in terms of the semantic segmentation performance of a pretrained PSPNet. This indicates that images from our model are, to a higher degree than from other models, similar to real images of the same kinds of scenes and objects, making them suitable as training data for a visual semantic segmentation or object recognition system.",0
"To train autonomous agents, like driverless cars, a substantial amount of labeled visual data is necessary. One viable method for obtaining such data involves training a generative model with actual data, and then augmenting that dataset with synthetic images produced by the model. The synthetic images are created with control over the scene layout and labeling of ground truth. This paper introduces Full-Glow, a fully conditional Glow-based architecture designed to generate realistic images of new street scenes based on a semantic segmentation map that indicates the scene's layout. Our model surpasses recent works in terms of semantic segmentation performance, as demonstrated by benchmark comparisons with a pretrained PSPNet. This suggests that our images are more similar to real images of the same scenes and objects, making them appropriate for training a visual semantic segmentation or object recognition system.",1
"While GAN is a powerful model for generating images, its inability to infer a latent space directly limits its use in applications requiring an encoder. Our paper presents a simple architectural setup that combines the generative capabilities of GAN with an encoder. We accomplish this by combining the encoder with the discriminator using shared weights, then training them simultaneously using a new loss term. We model the output of the encoder latent space via a GMM, which leads to both good clustering using this latent space and improved image generation by the GAN. Our framework is generic and can be easily plugged into any GAN strategy. In particular, we demonstrate it both with Vanilla GAN and Wasserstein GAN, where in both it leads to an improvement in the generated images in terms of both the IS and FID scores. Moreover, we show that our encoder learns a meaningful representation as its clustering results are competitive with the current GAN-based state-of-the-art in clustering.",0
"Although GAN is proficient in creating images, its deficiency in directly deducing a latent space constrains its applicability in encoder-dependent applications. Our study proposes a straightforward structural configuration that merges GAN's generative capabilities with an encoder. We accomplish this by combining the encoder and the discriminator, using shared weights, and training them simultaneously with a new loss term. We shape the encoder's latent space output through a GMM, resulting in excellent clustering and improved image generation by GAN. Our approach is universally applicable and can be easily integrated into any GAN strategy. We demonstrate its efficacy with both Vanilla GAN and Wasserstein GAN, which leads to superior generated images in terms of IS and FID scores. Additionally, we exhibit that our encoder learns a significant representation by producing clustering results that are on par with the current GAN-based state-of-the-art in clustering.",1
"The recent explosion in applications of machine learning to satellite imagery often rely on visible images and therefore suffer from a lack of data during the night. The gap can be filled by employing available infra-red observations to generate visible images. This work presents how deep learning can be applied successfully to create those images by using U-Net based architectures. The proposed methods show promising results, achieving a structural similarity index (SSIM) up to 86\% on an independent test set and providing visually convincing output images, generated from infra-red observations.",0
"Machine learning applications to satellite imagery have experienced a surge in popularity, but they face a challenge in accessing data during nighttime due to their reliance on visible images. However, this issue can be resolved with the utilization of available infra-red observations to generate visible images. This study demonstrates the successful application of deep learning through U-Net based architectures to create these images. The methods proposed exhibit encouraging outcomes, with a structural similarity index (SSIM) of up to 86\% on an independent test set and visually convincing output images generated from infra-red observations.",1
"Style analysis of artwork in computer vision predominantly focuses on achieving results in target image generation through optimizing understanding of low level style characteristics such as brush strokes. However, fundamentally different techniques are required to computationally understand and control qualities of art which incorporate higher level style characteristics. We study style representations learned by neural network architectures incorporating these higher level characteristics. We find variation in learned style features from incorporating triplets annotated by art historians as supervision for style similarity. Networks leveraging statistical priors or pretrained on photo collections such as ImageNet can also derive useful visual representations of artwork. We align the impact of these expert human knowledge, statistical, and photo realism priors on style representations with art historical research and use these representations to perform zero-shot classification of artists. To facilitate this work, we also present the first large-scale dataset of portraits prepared for computational analysis.",0
"The main focus of style analysis in computer vision is to achieve targeted image generation by understanding low-level style features like brush strokes. However, higher-level style characteristics require different techniques for computational control and understanding. Our research involves studying style representations learned by neural networks that incorporate these higher-level characteristics. By using triplets annotated by art historians for style similarity supervision, we were able to observe variations in learned style features. Additionally, networks that employ statistical priors or are pretrained on photo collections like ImageNet can derive useful visual representations of artwork. We align these expert human knowledge, statistical, and photo realism priors on style representations with art historical research and utilize them for performing zero-shot classification of artists. To facilitate this work, we present the first large-scale dataset of portraits that's prepared for computational analysis.",1
"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.",0
"Our study presents a new approach called dynamic neural radiance fields to model and depict the appearance and movements of a human face. The ability to digitally model and reconstruct a talking human is crucial for various applications, particularly for telepresence applications in AR or VR, where an accurate reproduction of the appearance from different viewpoints and head-poses is necessary. Unlike existing techniques that explicitly model the geometry and material properties or rely solely on images, we introduce an implicit representation of the head using scene representation networks. To address facial movements, we combine our scene representation network with a low-dimensional morphable model that enables explicit control over pose and expressions. Our approach employs volumetric rendering to generate images from this hybrid representation and demonstrates that a dynamic neural scene representation can be learned from monocular input data without requiring a specialized capture setup. Our experiments reveal that the learned volumetric representation enables photo-realistic image generation, surpassing the quality of state-of-the-art video-based reenactment methods.",1
"Multilabel conditional image generation is a challenging problem in computer vision. In this work we propose Multi-ingredient Pizza Generator (MPG), a conditional Generative Neural Network (GAN) framework for synthesizing multilabel images. We design MPG based on a state-of-the-art GAN structure called StyleGAN2, in which we develop a new conditioning technique by enforcing intermediate feature maps to learn scalewise label information. Because of the complex nature of the multilabel image generation problem, we also regularize synthetic image by predicting the corresponding ingredients as well as encourage the discriminator to distinguish between matched image and mismatched image. To verify the efficacy of MPG, we test it on Pizza10, which is a carefully annotated multi-ingredient pizza image dataset. MPG can successfully generate photo-realist pizza images with desired ingredients. The framework can be easily extend to other multilabel image generation scenarios.",0
"Generating multilabel images through conditional image generation is a difficult task in the field of computer vision. Our proposal, Multi-ingredient Pizza Generator (MPG), is a Generative Neural Network (GAN) framework that is conditional and is specifically designed to synthesize multilabel images. We have created MPG using StyleGAN2, which is a state-of-the-art GAN structure. We have developed a new conditioning technique in MPG that enforces intermediate feature maps to learn scalewise label information. Due to the complexity of the multilabel image generation problem, we have regularized synthetic images by predicting the corresponding ingredients and encouraging the discriminator to differentiate between matched and mismatched images. We have tested MPG on Pizza10, which is a meticulously annotated multi-ingredient pizza image dataset, and found that it can successfully generate photorealistic pizza images with desired ingredients. This framework can be easily applied to other multilabel image generation scenarios.",1
"Few-shot image generation seeks to generate more data of a given domain, with only few available training examples. As it is unreasonable to expect to fully infer the distribution from just a few observations (e.g., emojis), we seek to leverage a large, related source domain as pretraining (e.g., human faces). Thus, we wish to preserve the diversity of the source domain, while adapting to the appearance of the target. We adapt a pretrained model, without introducing any additional parameters, to the few examples of the target domain. Crucially, we regularize the changes of the weights during this adaptation, in order to best preserve the information of the source dataset, while fitting the target. We demonstrate the effectiveness of our algorithm by generating high-quality results of different target domains, including those with extremely few examples (e.g., <10). We also analyze the performance of our method with respect to some important factors, such as the number of examples and the dissimilarity between the source and target domain.",0
"The objective of few-shot image generation is to produce more data for a specified domain, despite having limited training samples. Since it is impractical to deduce the entire distribution from a small number of observations like emojis, we aim to utilize a large, related source domain for pretraining, such as human faces. Our goal is to maintain the variety of the source domain while adjusting to the appearance of the target. We modify a pretrained model, without introducing new parameters, to suit the few examples available for the target domain. To ensure that the information from the source dataset is retained while fitting the target, we regulate the changes in the weights during adaptation. We demonstrate the efficacy of our approach by generating high-quality outcomes for different target domains, even those with very few examples (less than ten). We also scrutinize the performance of our method with regard to crucial elements like the number of examples and the dissimilarity between the source and target domain.",1
"In this work, we propose a novel Cyclic Image Translation Generative Adversarial Network (CIT-GAN) for multi-domain style transfer. To facilitate this, we introduce a Styling Network that has the capability to learn style characteristics of each domain represented in the training dataset. The Styling Network helps the generator to drive the translation of images from a source domain to a reference domain and generate synthetic images with style characteristics of the reference domain. The learned style characteristics for each domain depend on both the style loss and domain classification loss. This induces variability in style characteristics within each domain. The proposed CIT-GAN is used in the context of iris presentation attack detection (PAD) to generate synthetic presentation attack (PA) samples for classes that are under-represented in the training set. Evaluation using current state-of-the-art iris PAD methods demonstrates the efficacy of using such synthetically generated PA samples for training PAD methods. Further, the quality of the synthetically generated samples is evaluated using Frechet Inception Distance (FID) score. Results show that the quality of synthetic images generated by the proposed method is superior to that of other competing methods, including StarGan.",0
"Our work introduces a new approach to multi-domain style transfer, called the Cyclic Image Translation Generative Adversarial Network (CIT-GAN). To achieve this, we incorporate a Styling Network that learns the style characteristics of each domain in the training dataset to help the generator translate images from a source domain to a reference domain and create synthetic images with the reference domain's style characteristics. The learned style characteristics depend on the style loss and domain classification loss, which leads to style variability within each domain. Our proposed CIT-GAN is applied to iris presentation attack detection (PAD) to generate synthetic presentation attack (PA) samples for under-represented classes in the training set. Our evaluation shows that using synthetically generated PA samples for training PAD methods is effective. We also evaluate the quality of the synthetic images using the Frechet Inception Distance (FID) score, and our results show that the quality of the images generated by our method is superior to other methods, including StarGan.",1
"We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.",0
"Using StyleGAN2, a cutting-edge image generation architecture, we investigate and assess the latent style space. We utilize models that were pre-trained on various datasets. Our research illustrates that the StyleSpace, which consists of channel-wise style parameters, is significantly more disentangled than other intermediate latent spaces studied in prior research. Furthermore, we introduce a technique for identifying a vast collection of style channels that each control a distinct visual attribute in an exceptionally localized and disentangled manner. Additionally, we propose a straightforward approach for detecting style channels that govern a specific attribute using a pretrained classifier or a small number of example images. Our experiments demonstrate that manipulating visual attributes through StyleSpace controls is better disentangled than those suggested in previous works. To demonstrate this, we employ a newly developed Attribute Dependency metric. Lastly, we showcase the effectiveness of StyleSpace controls in manipulating genuine images. Our research provides a pathway to semantically meaningful and well-disentangled image manipulations through user-friendly and intuitive interfaces.",1
"Despite recent impressive results on single-object and single-domain image generation, the generation of complex scenes with multiple objects remains challenging. In this paper, we start with the idea that a model must be able to understand individual objects and relationships between objects in order to generate complex scenes well. Our layout-to-image-generation method, which we call Object-Centric Generative Adversarial Network (or OC-GAN), relies on a novel Scene-Graph Similarity Module (SGSM). The SGSM learns representations of the spatial relationships between objects in the scene, which lead to our model's improved layout-fidelity. We also propose changes to the conditioning mechanism of the generator that enhance its object instance-awareness. Apart from improving image quality, our contributions mitigate two failure modes in previous approaches: (1) spurious objects being generated without corresponding bounding boxes in the layout, and (2) overlapping bounding boxes in the layout leading to merged objects in images. Extensive quantitative evaluation and ablation studies demonstrate the impact of our contributions, with our model outperforming previous state-of-the-art approaches on both the COCO-Stuff and Visual Genome datasets. Finally, we address an important limitation of evaluation metrics used in previous works by introducing SceneFID -- an object-centric adaptation of the popular Fr{\'e}chet Inception Distance metric, that is better suited for multi-object images.",0
"Although single-object and single-domain image generation has had impressive results, creating complex scenes with multiple objects is still a challenge. In this paper, we believe that in order to generate complex scenes well, a model must comprehend individual objects and their relationships with one another. Our method, Object-Centric Generative Adversarial Network (OC-GAN), uses a Scene-Graph Similarity Module (SGSM) to learn representations of the spatial relationships between objects in a scene, leading to improved layout-fidelity. We also suggest modifications to the generator's conditioning mechanism in order to enhance its object instance-awareness. Our contributions address two failure modes in previous approaches, such as the generation of spurious objects without corresponding bounding boxes in the layout and overlapping bounding boxes leading to merged objects in images. Through extensive quantitative evaluation and ablation studies, we have demonstrated the effectiveness of our approach, with our model surpassing previous state-of-the-art methods on both the COCO-Stuff and Visual Genome datasets. Finally, we introduce SceneFID, an object-centric adaptation of the popular Fr{\'e}chet Inception Distance metric, to overcome the limitations of evaluation metrics used in previous works and better suit multi-object images.",1
"Recent studies have shown remarkable success in face image generations. However, most of the existing methods only generate face images from random noise, and cannot generate face images according to the specific attributes. In this paper, we focus on the problem of face synthesis from attributes, which aims at generating faces with specific characteristics corresponding to the given attributes. To this end, we propose a novel attributes aware face image generator method with generative adversarial networks called AFGAN. Specifically, we firstly propose a two-path embedding layer and self-attention mechanism to convert binary attribute vector to rich attribute features. Then three stacked generators generate $64 \times 64$, $128 \times 128$ and $256 \times 256$ resolution face images respectively by taking the attribute features as input. In addition, an image-attribute matching loss is proposed to enhance the correlation between the generated images and input attributes. Extensive experiments on CelebA demonstrate the superiority of our AFGAN in terms of both qualitative and quantitative evaluations.",0
"Face image generation has seen significant advancements in recent studies. However, the majority of current methods only generate face images from arbitrary noise and are unable to produce face images with specific attributes. This paper addresses the issue of face synthesis from attributes, aiming to generate faces with particular characteristics that match given attributes. To achieve this, we introduce a new attributes-aware face image generator method called AFGAN, which utilizes generative adversarial networks. Our approach includes a two-path embedding layer and self-attention mechanism to convert binary attribute vectors into rich attribute features. Then, three stacked generators produce $64 \times 64$, $128 \times 128$, and $256 \times 256$ resolution face images using the attribute features as input. Additionally, we propose an image-attribute matching loss to improve the correlation between the generated images and input attributes. Our experiments on CelebA demonstrate the superior performance of AFGAN in both qualitative and quantitative evaluations.",1
"With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study.",0
"Recent years have seen remarkable progress in artificial intelligence (AI), with breakthroughs achieved through the application of deep learning. Tasks such as object detection, reading comprehension, and video games have been performed with superhuman performance. Generative Modeling has also been employed to generate paintings and music, while research in Natural Language Processing has advanced significantly with the emergence of pre-trained contextual neural language models. Despite these exciting developments, AI still falls short of human creativity, which is often considered the ultimate challenge for AI. Drawing inspiration from Chinese calligraphy and the Abstract Expressionist movement, we present a novel creative framework that employs Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks with intrinsic meaning and aesthetic value. Our approach differs from existing work in image captioning and text-to-image generation, which rely on text descriptions of images. We have also released a Chinese calligraphy image dataset and conducted a user study to demonstrate our framework through a prototype system.",1
"We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the wholes cene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects' appearance, such as shadow and lighting, and provides control over each object's 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity).",0
"BlockGAN is a novel image generative model that can learn object-aware 3D scene representations directly from unlabelled 2D images. Most current scene representation learning methods either disregard the scene background or treat the whole scene as one entity. Other approaches that consider scene compositionality treat scene objects as mere image patches or 2D layers with alpha maps. To overcome these limitations, we drew inspiration from the computer graphics pipeline and devised BlockGAN to first generate 3D features of background and foreground objects, then combine them into 3D features for the entire scene, and subsequently render them into realistic images. This approach enables BlockGAN to reason over occlusion and interaction between objects' appearance, such as shadow and lighting, while allowing for control over each object's 3D pose and identity, all while maintaining image realism. BlockGAN is trained end-to-end, utilizing only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments validate that using explicit 3D features for objects allows BlockGAN to learn disentangled representations in terms of both objects (foreground and background) and their properties (pose and identity).",1
"We study the problem of generating point clouds of 3D objects. Instead of discretizing the object into 3D voxels with huge computational cost and resolution limitations, we propose a novel geometry image based generator (GIG) to convert the 3D point cloud generation problem to a 2D geometry image generation problem. Since the geometry image is a completely regular 2D array that contains the surface points of the 3D object, it leverages both the regularity of the 2D array and the geodesic neighborhood of the 3D surface. Thus, one significant benefit of our GIG is that it allows us to directly generate the 3D point clouds using efficient 2D image generation networks. Experiments on both rigid and non-rigid 3D object datasets have demonstrated the promising performance of our method to not only create plausible and novel 3D objects, but also learn a probabilistic latent space that well supports the shape editing like interpolation and arithmetic.",0
"Our focus is on generating point clouds for 3D objects. To avoid the high computational cost and resolution limitations associated with discretizing the object into 3D voxels, we propose a new method called the geometry image based generator (GIG). This approach converts the 3D point cloud generation problem into a 2D geometry image generation problem. By using a regular 2D array that contains the surface points of the 3D object, the GIG leverages the regularity of the 2D array and the geodesic neighborhood of the 3D surface. One major advantage of our GIG is that it enables the direct generation of 3D point clouds using efficient 2D image generation networks. Our experiments with both rigid and non-rigid 3D object datasets demonstrate the promising performance of our method. It not only creates plausible and novel 3D objects, but also learns a probabilistic latent space that supports shape editing such as interpolation and arithmetic.",1
"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.",0
"Currently utilized image generator networks heavily rely on spatial convolutions and, in some cases, self-attention blocks to synthesize images in a step-by-step approach. In contrast, our team has developed a novel architecture for image generators that independently computes the color value for each pixel using a random latent vector and pixel coordinate. There is no use of spatial convolutions or other operations that communicate information across pixels during image synthesis. We have assessed the modeling capabilities of these generators when trained adversarially and have found that they produce similar quality images compared to state-of-the-art convolutional generators. Additionally, we have explored several unique properties that arise from the new architecture.",1
"The restricted Boltzmann machine (RBM) is a representative generative model based on the concept of statistical mechanics. In spite of the strong merit of interpretability, unavailability of backpropagation makes it less competitive than other generative models. Here we derive differentiable loss functions for both binary and multinary RBMs. Then we demonstrate their learnability and performance by generating colored face images.",0
"The concept of statistical mechanics forms the basis for the restricted Boltzmann machine (RBM), a notable generative model. However, the RBM's lack of backpropagation hinders its competitiveness compared to other models, despite being highly interpretable. To address this issue, we have established differentiable loss functions for binary and multinary RBMs and showcased their effectiveness in generating colored face images by demonstrating their learnability and performance.",1
"In the last few years, several works have tackled the problem of novel view synthesis from stereo images or even from a single picture. However, previous methods are computationally expensive, specially for high-resolution images. In this paper, we address the problem of generating a multiplane image (MPI) from a single high-resolution picture. We present the adaptive-MPI representation, which allows rendering novel views with low computational requirements. To this end, we propose an adaptive slicing algorithm that produces an MPI with a variable number of image planes. We present a new lightweight CNN for depth estimation, which is learned by knowledge distillation from a larger network. Occluded regions in the adaptive-MPI are inpainted also by a lightweight CNN. We show that our method is capable of producing high-quality predictions with one order of magnitude less parameters compared to previous approaches. The robustness of our method is evidenced on challenging pictures from the Internet.",0
"Over the past few years, various studies have tackled the issue of generating new perspectives from stereo images or a single image. However, these earlier methods are computationally intensive, particularly for high-resolution images. This article focuses on creating a multiplane image (MPI) from a single, high-resolution picture. We introduce the adaptive-MPI representation, which allows for the generation of new viewpoints with low computational requirements. To achieve this, we propose an adaptive slicing algorithm that produces an MPI with a varying number of image planes. We also present a new, lightweight CNN for depth estimation, which is learned through knowledge distillation from a larger network. In addition, occluded areas in the adaptive-MPI are filled in using another lightweight CNN. Our approach is capable of producing high-quality predictions with significantly fewer parameters compared to previous methods. We demonstrate the robustness of our approach by testing it on challenging images from the Internet.",1
"Gaze redirection aims at manipulating the gaze of a given face image with respect to a desired direction (i.e., a reference angle) and it can be applied to many real life scenarios, such as video-conferencing or taking group photos. However, previous work on this topic mainly suffers of two limitations: (1) Low-quality image generation and (2) Low redirection precision. In this paper, we propose to alleviate these problems by means of a novel gaze redirection framework which exploits both a numerical and a pictorial direction guidance, jointly with a coarse-to-fine learning strategy. Specifically, the coarse branch learns the spatial transformation which warps input image according to desired gaze. On the other hand, the fine-grained branch consists of a generator network with conditional residual image learning and a multi-task discriminator. This second branch reduces the gap between the previously warped image and the ground-truth image and recovers finer texture details. Moreover, we propose a numerical and pictorial guidance module~(NPG) which uses a pictorial gazemap description and numerical angles as an extra guide to further improve the precision of gaze redirection. Extensive experiments on a benchmark dataset show that the proposed method outperforms the state-of-the-art approaches in terms of both image quality and redirection precision. The code is available at https://github.com/jingjingchen777/CFGR",0
"The goal of gaze redirection is to alter the direction in which a face image is looking by a specified reference angle. This technique has practical applications in scenarios like video-conferencing or group photos. However, previous research in this area has been limited by poor image quality and insufficient redirection precision. To address these issues, we suggest a novel gaze redirection framework that combines numerical and pictorial guidance with a coarse-to-fine learning strategy. The coarse branch transforms the input image to match the desired gaze, while the fine-grained branch uses a generator network and multi-task discriminator to refine the image and recover texture details. Additionally, we introduce a numerical and pictorial guidance module (NPG) that uses gazemap descriptions and numerical angles to improve the precision of gaze redirection. Our experiments on a benchmark dataset show that our method outperforms existing approaches in terms of both image quality and redirection precision. The code can be accessed at https://github.com/jingjingchen777/CFGR.",1
"We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.",0
"Our goal is to develop image generation models that can effectively generate images in new domains with limited examples. Our initial focus is on examining the generalization capabilities of established image generators, and we have found that autoencoders are particularly adept at generalizing to new domains, even when trained on heavily restricted data. We have utilized this knowledge to create a robust, unsupervised algorithm for generating images with limited data, which incorporates a unique training process that involves recovering an image from various data augmentations. Our Augmentation-Interpolative AutoEncoders are capable of producing realistic images of new objects using only a few reference images, surpassing both prior interpolative models and supervised few-shot image generators. Our approach is straightforward, efficient, and broadly applicable, and does not require category labels or other forms of supervision during training.",1
"While Generative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has a low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in Mimicry: https://github.com/kwotsin/mimicry.",0
"Despite being crucial to many generative modelling applications, Generative Adversarial Networks (GANs) are plagued with various issues. In this research, we present a systematic framework that effectively addresses two major issues of GANs, namely, discriminator catastrophic forgetting and generator mode collapse. Our proposed solution involves the utilization of contrastive learning and mutual information maximization techniques to enhance GAN performance for image synthesis across five datasets. Through extensive analyses, we have identified the sources of improvements and demonstrated significant stability in GAN training. Our approach outperforms the state-of-the-art SSGAN method and shows no inferior performance on image domains such as faces, which is a common issue with existing methods. Our method is simple to implement, practical, and requires only one auxiliary objective with low computational cost. It also performs robustly across a wide range of training settings and datasets without the need for hyperparameter tuning. For reproducibility, we have made our code available in Mimicry: https://github.com/kwotsin/mimicry.",1
"Generative Adversarial Networks (GANs) have become predominant in image generation tasks. Their success is attributed to the training regime which employs two models: a generator G and discriminator D that compete in a minimax zero sum game. Nonetheless, GANs are difficult to train due to their sensitivity to hyperparameter and parameter initialisation, which often leads to vanishing gradients, non-convergence, or mode collapse, where the generator is unable to create samples with different variations. In this work, we propose a novel Generative Adversarial Stacked Convolutional Autoencoder(GASCA) model and a generative adversarial gradual greedy layer-wise learning algorithm de-signed to train Adversarial Autoencoders in an efficient and incremental manner. Our training approach produces images with significantly lower reconstruction error than vanilla joint training.",0
"The use of Generative Adversarial Networks (GANs) has become widespread in generating images due to their effective training method that involves a generator, G, and a discriminator, D, competing in a minimax zero-sum game. However, GANs are challenging to train due to their sensitivity to hyperparameter and parameter initialization, which can result in vanishing gradients, non-convergence, or mode collapse. As a solution, we propose a novel model, Generative Adversarial Stacked Convolutional Autoencoder (GASCA), and a gradual greedy layer-wise learning algorithm to train Adversarial Autoencoders efficiently and incrementally. Our training approach produces images with significantly lower reconstruction error than vanilla joint training.",1
"Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. Visualization of the attention maps shows that the channel-aware attention module is able to localize the discriminative regions, while the pixel-aware attention module has the ability to capture the globally visual contents for the generation of an image.",0
"Many text-to-image generation methods currently in use employ a multi-stage modular architecture that has several drawbacks. Firstly, training multiple networks leads to longer run times and may impact the stability and convergence of the generative model. Secondly, these approaches do not consider the quality of early-stage generator images. Lastly, numerous discriminators need to be trained. To address these issues, we propose the Dual Attention Generative Adversarial Network (DTGAN), which uses only one generator/discriminator pair to synthesize high-quality and semantically consistent images. Our model incorporates channel-aware and pixel-aware attention modules that guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and attention weights. We also introduce Conditional Adaptive Instance-Layer Normalization (CAdaILN), which allows our attention modules to flexibly control shape and texture changes based on the input natural-language description. Additionally, we utilize a new type of visual loss to enhance image resolution and ensure vivid shape and perceptually uniform color distributions of generated images. Our experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to state-of-the-art models that use a multi-stage framework. Our visualization of the attention maps illustrates that the channel-aware attention module can localize discriminative regions, while the pixel-aware attention module can capture globally visual contents for image generation.",1
"Recently there has been an interest in the potential of learning generative models from a single image, as opposed to from a large dataset. This task is of practical significance, as it means that generative models can be used in domains where collecting a large dataset is not feasible. However, training a model capable of generating realistic images from only a single sample is a difficult problem. In this work, we conduct a number of experiments to understand the challenges of training these methods and propose some best practices that we found allowed us to generate improved results over previous work in this space. One key piece is that unlike prior single image generation methods, we concurrently train several stages in a sequential multi-stage manner, allowing us to learn models with fewer stages of increasing image resolution. Compared to a recent state of the art baseline, our model is up to six times faster to train, has fewer parameters, and can better capture the global structure of images.",0
"Lately, there has been a growing interest in the capability of learning generative models from a solitary image, rather than a vast dataset. This is significant because it enables the use of generative models in areas where gathering a massive dataset is not feasible. Despite this, creating a model that can produce realistic images from only one sample is challenging. In this study, we conducted various experiments to comprehend the difficulties of training these methods and recommend some best practices that yielded more satisfactory results than previous research in this field. One critical aspect is that, unlike prior single image generation techniques, we trained multiple stages in a sequential multi-stage approach, enabling us to learn models with fewer stages of increasing image resolution. Our model is up to six times faster to train, has fewer parameters, and can better capture the global structure of images compared to a recent state-of-the-art baseline.",1
"In recent years there has been a growing interest in image generation through deep learning. While an important part of the evaluation of the generated images usually involves visual inspection, the inclusion of human perception as a factor in the training process is often overlooked. In this paper we propose an alternative perceptual regulariser for image-to-image translation using conditional generative adversarial networks (cGANs). To do so automatically (avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance (NLPD) to measure the perceptual similarity between the generated image and the original image. The NLPD is based on the principle of normalising the value of coefficients with respect to a local estimate of mean energy at different scales and has already been successfully tested in different experiments involving human perception. We compare this regulariser with the originally proposed L1 distance and note that when using NLPD the generated images contain more realistic values for both local and global contrast. We found that using NLPD as a regulariser improves image segmentation accuracy on generated images as well as improving two no-reference image quality metrics.",0
"Deep learning-based image generation has gained significant interest in recent years. However, the impact of human perception in the training process is often overlooked, despite being crucial in evaluating the quality of the generated images. In this study, we suggest a novel perceptual regulariser for image-to-image translation using conditional generative adversarial networks (cGANs), which automatically measures the perceptual similarity between the generated and original images, without the need for visual inspection. We use the Normalised Laplacian Pyramid Distance (NLPD), a coefficient normalisation method based on estimating mean energy at different scales, which has been effectively used in experiments involving human perception. Our comparison of NLPD with the originally proposed L1 distance shows that NLPD produces more realistic values for both local and global contrast in generated images. We also observed that using NLPD as a regulariser enhances image segmentation accuracy and two no-reference image quality metrics.",1
"For humans, visual understanding is inherently generative: given a 3D shape, we can postulate how it would look in the world; given a 2D image, we can infer the 3D structure that likely gave rise to it. We can thus translate between the 2D visual and 3D structural modalities of a given object. In the context of computer vision, this corresponds to a learnable module that serves two purposes: (i) generate a realistic rendering of a 3D object (shape-to-image translation) and (ii) infer a realistic 3D shape from an image (image-to-shape translation). In this paper, we learn such a module while being conscious of the difficulties in obtaining large paired 2D-3D datasets. By leveraging generative domain translation methods, we are able to define a learning algorithm that requires only weak supervision, with unpaired data. The resulting model is not only able to perform 3D shape, pose, and texture inference from 2D images, but can also generate novel textured 3D shapes and renders, similar to a graphics pipeline. More specifically, our method (i) infers an explicit 3D mesh representation, (ii) utilizes example shapes to regularize inference, (iii) requires only an image mask (no keypoints or camera extrinsics), and (iv) has generative capabilities. While prior work explores subsets of these properties, their combination is novel. We demonstrate the utility of our learned representation, as well as its performance on image generation and unpaired 3D shape inference tasks.",0
"Humans possess an innate ability to generate visual understanding by imagining how a 3D shape would appear in the real world or by deducing the 3D structure that might have led to a 2D image. Consequently, we can switch between the 2D visual and 3D structural forms of an object. In the realm of computer vision, a trainable module has been created to achieve two objectives: (i) realistically produce a rendering of a 3D object (shape-to-image translation) and (ii) deduce a realistic 3D shape from an image (image-to-shape translation). The module is developed while keeping in mind the difficulty of obtaining large paired 2D-3D datasets, and generative domain translation techniques are utilized to define a learning algorithm, which requires only weak supervision with unpaired data. The resulting model can not only infer 3D shape, pose, and texture from 2D images but also generate unique textured 3D shapes and renders, similar to a graphics pipeline. The method infers an explicit 3D mesh representation, uses example shapes to regulate inference, requires only an image mask (no keypoints or camera extrinsics), and has generative abilities. Although previous studies have explored some of these features separately, their amalgamation is innovative. The usefulness and performance of the learned representation are demonstrated through image generation and unpaired 3D shape inference tasks.",1
"Generative adversarial networks have achieved remarkable performance on various tasks but suffer from training instability. Despite many training strategies proposed to improve training stability, this issue remains as a challenge. In this paper, we investigate the training instability from the perspective of adversarial samples and reveal that adversarial training on fake samples is implemented in vanilla GANs, but adversarial training on real samples has long been overlooked. Consequently, the discriminator is extremely vulnerable to adversarial perturbation and the gradient given by the discriminator contains non-informative adversarial noises, which hinders the generator from catching the pattern of real samples. Here, we develop adversarial symmetric GANs (AS-GANs) that incorporate adversarial training of the discriminator on real samples into vanilla GANs, making adversarial training symmetrical. The discriminator is therefore more robust and provides more informative gradient with less adversarial noise, thereby stabilizing training and accelerating convergence. The effectiveness of the AS-GANs is verified on image generation on CIFAR-10 , CelebA, and LSUN with varied network architectures. Not only the training is more stabilized, but the FID scores of generated samples are consistently improved by a large margin compared to the baseline. The bridging of adversarial samples and adversarial networks provides a new approach to further develop adversarial networks.",0
"Although generative adversarial networks have shown impressive performance on various tasks, their training stability remains a significant challenge despite attempts to improve it with different strategies. In this paper, we examine the issue of training instability through the lens of adversarial samples and discover that while vanilla GANs implement adversarial training on fake samples, they overlook adversarial training on real samples. As a result, the discriminator is highly vulnerable to adversarial perturbations, and the gradient provided by the discriminator contains non-informative adversarial noises that hinder the generator from capturing the pattern of real samples. To address this, we introduce adversarial symmetric GANs (AS-GANs), which integrate adversarial training of the discriminator on real samples into vanilla GANs, creating symmetrical adversarial training. This makes the discriminator more robust, providing a more informative gradient with less adversarial noise, which stabilizes training and accelerates convergence. Our experiments on various network architectures for image generation on CIFAR-10, CelebA, and LSUN demonstrate the effectiveness of AS-GANs, as not only is the training more stable, but the FID scores of generated samples are consistently improved by a large margin compared to the baseline. This bridging of adversarial samples and adversarial networks provides a new direction for the development of adversarial networks.",1
"We propose a novel unsupervised approach based on a two-stage object-centric adversarial framework that only needs object regions for detecting frame-level local anomalies in videos. The first stage consists in learning the correspondence between the current appearance and past gradient images of objects in scenes deemed normal, allowing us to either generate the past gradient from current appearance or the reverse. The second stage extracts the partial reconstruction errors between real and generated images (appearance and past gradient) with normal object behaviour, and trains a discriminator in an adversarial fashion. In inference mode, we employ the trained image generators with the adversarially learned binary classifier for outputting region-level anomaly detection scores. We tested our method on four public benchmarks, UMN, UCSD, Avenue and ShanghaiTech and our proposed object-centric adversarial approach yields competitive or even superior results compared to state-of-the-art methods.",0
"Our proposed approach is a unique unsupervised method that utilizes a two-stage object-centric adversarial framework to detect local anomalies in videos solely based on object regions. During the first stage, we establish a connection between the current appearance and past gradient images of objects in normal scenes, which enables us to generate either past gradient from current appearance or the reverse. The second stage involves extracting partial reconstruction errors between actual and generated images (appearance and past gradient) that display normal object behaviour. We use this information to train a discriminator in an adversarial manner. During inference mode, we employ the adversarially learned binary classifier with the trained image generators to produce region-level anomaly detection scores. Our method was tested on four public benchmarks, namely UMN, UCSD, Avenue and ShanghaiTech, and we found that our novel object-centric adversarial approach produces competitive or superior results compared to other state-of-the-art methods.",1
"Given the ever-increasing computational costs of modern machine learning models, we need to find new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. Therefore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse domains, (ii) enabling controlled content synthesis by allowing modification in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between fixed representations without having to learn or finetune them. This allows users to utilize various existing domain-specific expert models from the literature that had been trained with extensive computational resources. Experiments on diverse conditional image synthesis tasks, competitive image modification results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own.",0
"As modern machine learning models become more computationally expensive, it is necessary to explore new ways to reuse expert models and leverage the resources invested in their creation. Recent studies have shown that the value of these large models lies in the representations they learn. Thus, we propose a model that can relate different existing representations and address this challenge using a conditionally invertible network. This network is capable of providing generic transfer between diverse domains, enabling controlled content synthesis through modifications in other domains, and facilitating the diagnosis of existing representations by translating them into interpretable domains, such as images. Our domain transfer network can translate between fixed representations without the need to learn or finetune them. This feature allows users to utilize various existing domain-specific expert models from literature that have been trained with extensive computational resources. We conducted experiments on diverse conditional image synthesis tasks, competitive image modification results, and image-to-image and text-to-image generation to demonstrate the generic applicability of our approach. For instance, we were able to perform text-to-image generation by translating between BERT and BigGAN, which neither expert model could achieve independently.",1
"Pose guided person image generation means to generate a photo-realistic person image conditioned on an input person image and a desired pose. This task requires spatial manipulation of the source image according to the target pose. However, the generative adversarial networks (GANs) widely used for image generation and translation rely on spatially local and translation equivariant operators, i.e., convolution, pooling and unpooling, which cannot handle large image deformation. This paper introduces a novel two-stream appearance transfer network (2s-ATN) to address this challenge. It is a multi-stage architecture consisting of a source stream and a target stream. Each stage features an appearance transfer module and several two-stream feature fusion modules. The former finds the dense correspondence between the two-stream feature maps and then transfers the appearance information from the source stream to the target stream. The latter exchange local information between the two streams and supplement the non-local appearance transfer. Both quantitative and qualitative results indicate the proposed 2s-ATN can effectively handle large spatial deformation and occlusion while retaining the appearance details. It outperforms prior states of the art on two widely used benchmarks.",0
"The process of pose guided person image generation involves creating a realistic image of a person based on an input image and desired pose. This requires manipulating the input image to match the desired pose. However, traditional methods such as generative adversarial networks (GANs) are limited in their ability to handle significant image deformation. To overcome this challenge, a new approach called the two-stream appearance transfer network (2s-ATN) has been introduced. This multi-stage architecture includes a source stream and a target stream, each with appearance transfer and feature fusion modules. These modules work together to transfer appearance information between the two streams, exchange local information, and handle large spatial deformation and occlusion while retaining appearance details. The proposed 2s-ATN has outperformed previous state-of-the-art methods on two commonly used benchmarks.",1
"Blind motion deblurring involves reconstructing a sharp image from an observation that is blurry. It is a problem that is ill-posed and lies in the categories of image restoration problems. The training data-based methods for image deblurring mostly involve training models that take a lot of time. These models are data-hungry i.e., they require a lot of training data to generate satisfactory results. Recently, there are various image feature learning methods developed which relieve us of the need for training data and perform image restoration and image synthesis, e.g., DIP, InGAN, and SinGAN. SinGAN is a generative model that is unconditional and could be learned from a single natural image. This model primarily captures the internal distribution of the patches which are present in the image and is capable of generating samples of varied diversity while preserving the visual content of the image. Images generated from the model are very much like real natural images. In this paper, we focus on blind motion deblurring through SinGAN architecture.",0
"The process of blind motion deblurring entails the reconstruction of a clear image from a blurry observation, a task classified under image restoration problems. However, this is a challenging task as it is ill-posed. The conventional training data-based methods for image deblurring require extensive time and large amounts of data. However, recent advancements in image feature learning methods, such as DIP, InGAN, and SinGAN, have alleviated the need for training data for image synthesis and restoration. SinGAN is an unconditional generative model that can learn from a single natural image and primarily captures the internal distribution of patches in the images, thereby producing diverse samples while maintaining visual content. Generated images from SinGAN closely resemble real natural images. This paper focuses on blind motion deblurring using the SinGAN architecture.",1
"In contrast to great success of memory-consuming face editing methods at a low resolution, to manipulate high-resolution (HR) facial images, i.e., typically larger than 7682 pixels, with very limited memory is still challenging. This is due to the reasons of 1) intractable huge demand of memory; 2) inefficient multi-scale features fusion. To address these issues, we propose a NOVEL pixel translation framework called Cooperative GAN(CooGAN) for HR facial image editing. This framework features a local path for fine-grained local facial patch generation (i.e., patch-level HR, LOW memory) and a global path for global lowresolution (LR) facial structure monitoring (i.e., image-level LR, LOW memory), which largely reduce memory requirements. Both paths work in a cooperative manner under a local-to-global consistency objective (i.e., for smooth stitching). In addition, we propose a lighter selective transfer unit for more efficient multi-scale features fusion, yielding higher fidelity facial attributes manipulation. Extensive experiments on CelebAHQ well demonstrate the memory efficiency as well as the high image generation quality of the proposed framework.",0
"While memory-consuming face editing methods have had great success at low resolutions, manipulating high-resolution (HR) facial images, which are typically larger than 7682 pixels, with limited memory remains challenging. This is due to two reasons: the intractable demand for memory and inefficient multi-scale feature fusion. To address these issues, we propose a novel pixel translation framework, Cooperative GAN (CooGAN), for HR facial image editing. This framework features a local path for generating fine-grained local facial patches (i.e., patch-level HR, low memory) and a global path for monitoring global low-resolution (LR) facial structure (i.e., image-level LR, low memory), which significantly reduces memory requirements. Both paths work cooperatively under a local-to-global consistency objective for smooth stitching. Additionally, we propose a lighter selective transfer unit for more efficient multi-scale feature fusion, resulting in higher fidelity facial attribute manipulation. Extensive experiments on CelebAHQ demonstrate the memory efficiency and high image generation quality of our proposed framework.",1
"This paper reviews the novel concept of controllable variational autoencoder (ControlVAE), discusses its parameter tuning to meet application needs, derives its key analytic properties, and offers useful extensions and applications. ControlVAE is a new variational autoencoder (VAE) framework that combines the automatic control theory with the basic VAE to stabilize the KL-divergence of VAE models to a specified value. It leverages a non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to dynamically tune the weight of the KL-divergence term in the evidence lower bound (ELBO) using the output KL-divergence as feedback. This allows us to precisely control the KL-divergence to a desired value (set point), which is effective in avoiding posterior collapse and learning disentangled representations. In order to improve the ELBO over the regular VAE, we provide simplified theoretical analysis to inform setting the set point of KL-divergence for ControlVAE. We observe that compared to other methods that seek to balance the two terms in VAE's objective, ControlVAE leads to better learning dynamics. In particular, it can achieve a good trade-off between reconstruction quality and KL-divergence. We evaluate the proposed method on three tasks: image generation, language modeling and disentangled representation learning. The results show that ControlVAE can achieve much better reconstruction quality than the other methods for comparable disentanglement. On the language modeling task, ControlVAE can avoid posterior collapse (KL vanishing) and improve the diversity of generated text. Moreover, our method can change the optimization trajectory, improving the ELBO and the reconstruction quality for image generation.",0
"In this article, the new concept of ControlVAE is discussed, which is a variational autoencoder (VAE) framework that employs automatic control theory to stabilize the KL-divergence of VAE models to a specified value. This is achieved by using a non-linear PI controller to dynamically adjust the weight of the KL-divergence term in the evidence lower bound (ELBO) based on the output KL-divergence as feedback. This allows precise control of the KL-divergence to avoid posterior collapse and achieve disentangled representations. The article provides simplified theoretical analysis to guide the setting of the KL-divergence set point for ControlVAE, which results in better learning dynamics compared to other methods that balance the two terms in VAE's objective. The proposed method is evaluated on three tasks, including image generation, language modeling, and disentangled representation learning, and is found to achieve better reconstruction quality and disentanglement than other methods. Additionally, ControlVAE can avoid posterior collapse and improve the diversity of generated text in language modeling, and can change the optimization trajectory to improve ELBO and reconstruction quality in image generation.",1
"Generative adversarial networks (GANs) are one of the most powerful generative models, but always require a large and balanced dataset to train. Traditional GANs are not applicable to generate minority-class images in a highly imbalanced dataset. Balancing GAN (BAGAN) is proposed to mitigate this problem, but it is unstable when images in different classes look similar, e.g. flowers and cells. In this work, we propose a supervised autoencoder with an intermediate embedding model to disperse the labeled latent vectors. With the improved autoencoder initialization, we also build an architecture of BAGAN with gradient penalty (BAGAN-GP). Our proposed model overcomes the unstable issue in original BAGAN and converges faster to high quality generations. Our model achieves high performance on the imbalanced scale-down version of MNIST Fashion, CIFAR-10, and one small-scale medical image dataset.",0
"GANs are a potent generative model that necessitates a substantial and well-balanced dataset for effective training. However, traditional GANs cannot create images of minority classes in a highly imbalanced dataset. To address this issue, a Balancing GAN (BAGAN) was introduced, but it remains unstable when the images of various classes are similar, such as flowers and cells. To resolve this instability, we propose a supervised autoencoder that utilizes an intermediate embedding model to scatter the labeled latent vectors. Additionally, we have developed an improved autoencoder initialization for the BAGAN architecture with gradient penalty (BAGAN-GP). Our new model is more stable and produces higher quality generations at a faster rate than the original BAGAN. Our model successfully performs on imbalanced datasets such as MNIST Fashion, CIFAR-10, and a small-scale medical image dataset.",1
"Despite the recent success of face image generation with GANs, conditional hair editing remains challenging due to the under-explored complexity of its geometry and appearance. In this paper, we present MichiGAN (Multi-Input-Conditioned Hair Image GAN), a novel conditional image generation method for interactive portrait hair manipulation. To provide user control over every major hair visual factor, we explicitly disentangle hair into four orthogonal attributes, including shape, structure, appearance, and background. For each of them, we design a corresponding condition module to represent, process, and convert user inputs, and modulate the image generation pipeline in ways that respect the natures of different visual attributes. All these condition modules are integrated with the backbone generator to form the final end-to-end network, which allows fully-conditioned hair generation from multiple user inputs. Upon it, we also build an interactive portrait hair editing system that enables straightforward manipulation of hair by projecting intuitive and high-level user inputs such as painted masks, guiding strokes, or reference photos to well-defined condition representations. Through extensive experiments and evaluations, we demonstrate the superiority of our method regarding both result quality and user controllability. The code is available at https://github.com/tzt101/MichiGAN.",0
"The complexity of geometry and appearance in hair editing makes it challenging despite the recent success of GANs in face image generation. This paper introduces a new method called MichiGAN, which is a Multi-Input-Conditioned Hair Image GAN that enables interactive portrait hair manipulation while providing user control over all major hair visual factors. The disentanglement of hair into four orthogonal attributes - shape, structure, appearance, and background - is explicitly designed to create corresponding condition modules that process and convert user inputs and modulate the image generation pipeline. These modules are integrated with the backbone generator to form an end-to-end network that allows fully-conditioned hair generation from multiple user inputs. An interactive portrait hair editing system is also built to enable straightforward manipulation of hair using intuitive and high-level user inputs such as painted masks, guiding strokes, or reference photos. Experimental evaluations demonstrate the superiority of our method in terms of result quality and user controllability. The code is available at https://github.com/tzt101/MichiGAN.",1
"Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) often suffer from inferior performance due to unstable training, especially for text generation. To solve this issue, we propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that improves discriminator training by downplaying bad-quality fake samples. Moreover, our variational GAN framework can provably overcome the training issue in many GANs that an optimal discriminator cannot provide any informative gradient to training generator. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.",0
"GANs have achieved success in various vision-related tasks, but their training can be unstable, leading to poor performance in text generation. To address this, we propose a new training framework for variational GAN, which delivers superior training stability. Our inspiration comes from the connection between GANs and reinforcement learning, leading to two mechanisms: probability ratio clipping that prevents excessively large updates in generator training, and sample re-weighting that downplays bad-quality fake samples to improve discriminator training. Our variational GAN framework can overcome the issue of an optimal discriminator providing no informative gradient to the generator's training. By implementing our training approach in different GAN architectures, we observe significant improvements in various tasks such as text generation, text style transfer, and image generation.",1
"Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as birds and human faces, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the image quality and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82% to 84.79% when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.",0
"The production of images based on written descriptions has garnered significant attention recently. Although existing models can create lifelike images of single items, such as birds or human faces, generating images with multiple objects remains challenging. This study proposes a method to enhance the quality and alignment of generated images in Text-to-Image (T2I) synthesis by integrating Visual Question Answering (VQA) and leveraging the VQA 2.0 dataset. By merging question and answer pairs, we create extra training examples and utilize a conventional VQA model to supply the T2I model with an auxiliary learning signal. Our approach encourages generated images from QA pairs to appear realistic and also minimizes an external VQA loss. Compared to the baseline, our method reduces the FID from 27.84 to 25.38 and boosts the R-prec. from 83.82% to 84.79%, demonstrating that a standard VQA model can enhance T2I synthesis effectively.",1
"Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\left(n^{1.5}d\right)$ from $O\left(n^2d\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.",0
"Recently, self-attention has become popular for various sequence modeling issues. Despite its effectiveness, self-attention requires quadratic compute and memory with respect to sequence length. To address this problem, previous methods have focused on attending to local sliding windows or a small set of locations irrelevant to the content. Our proposed solution is to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This approach combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention. Our model outperforms comparable sparse attention models on language modeling and image generation while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the PG-19 data-set with a 22 layer Routing Transformer model trained on sequences of length 8192.",1
"Autoregressive models use chain rule to define a joint probability distribution as a product of conditionals. These conditionals need to be normalized, imposing constraints on the functional families that can be used. To increase flexibility, we propose autoregressive conditional score models (AR-CSM) where we parameterize the joint distribution in terms of the derivatives of univariate log-conditionals (scores), which need not be normalized. To train AR-CSM, we introduce a new divergence between distributions named Composite Score Matching (CSM). For AR-CSM models, this divergence between data and model distributions can be computed and optimized efficiently, requiring no expensive sampling or adversarial training. Compared to previous score matching algorithms, our method is more scalable to high dimensional data and more stable to optimize. We show with extensive experimental results that it can be applied to density estimation on synthetic data, image generation, image denoising, and training latent variable models with implicit encoders.",0
"Autoregressive models utilize the chain rule to establish a joint probability distribution that consists of a product of conditionals. These conditionals must be normalized, which limits the functional families available for use. In order to enhance flexibility, our proposed solution is autoregressive conditional score models (AR-CSM), which parameterize the joint distribution in terms of univariate log-conditionals' derivatives (scores), which do not require normalization. To train AR-CSM, we introduce a new divergence between distributions called Composite Score Matching (CSM). This divergence between data and model distributions can be calculated and optimized efficiently for AR-CSM models, without costly sampling or adversarial training. Our approach is more scalable for high-dimensional data and more stable to optimize than previous score matching algorithms, as demonstrated by extensive experimental results. Our method can be used for density estimation on synthetic data, image generation, image denoising, and training latent variable models with implicit encoders.",1
"We propose a novel approach for generating unrestricted adversarial examples by manipulating fine-grained aspects of image generation. Unlike existing unrestricted attacks that typically hand-craft geometric transformations, we learn stylistic and stochastic modifications leveraging state-of-the-art generative models. This allows us to manipulate an image in a controlled, fine-grained manner without being bounded by a norm threshold. Our approach can be used for targeted and non-targeted unrestricted attacks on classification, semantic segmentation and object detection models. Our attacks can bypass certified defenses, yet our adversarial images look indistinguishable from natural images as verified by human evaluation. Moreover, we demonstrate that adversarial training with our examples improves performance of the model on clean images without requiring any modifications to the architecture. We perform experiments on LSUN, CelebA-HQ and COCO-Stuff as high resolution datasets to validate efficacy of our proposed approach.",0
"We suggest a new method of creating adversarial examples that are not limited by norm thresholds. Rather than using geometric transformations, we use up-to-date generative models to make stylistic and stochastic changes to the images. This enables us to manipulate the images in a precise and fine-grained manner for targeted or non-targeted attacks on classification, object detection, and semantic segmentation models. Despite being able to bypass certified defenses, our adversarial images are visually indistinguishable from natural images, as confirmed by human evaluation. Furthermore, we demonstrate that adversarial training using our examples enhances the model's performance on clean images without altering the architecture. We validate the effectiveness of our approach on high-resolution datasets such as LSUN, CelebA-HQ, and COCO-Stuff through experiments.",1
"Generative Adversarial Networks (GANs) have achieved state-of-the-art performance for several image generation and manipulation tasks. Different works have improved the limited understanding of the latent space of GANs by embedding images into specific GAN architectures to reconstruct the original images. We present a novel StyleGAN-based autoencoder architecture, which can reconstruct images with very high quality across several data domains. We demonstrate a previously unknown grade of generalizablility by training the encoder and decoder independently and on different datasets. Furthermore, we provide new insights about the significance and capabilities of noise inputs of the well-known StyleGAN architecture. Our proposed architecture can handle up to 40 images per second on a single GPU, which is approximately 28x faster than previous approaches. Finally, our model also shows promising results, when compared to the state-of-the-art on the image denoising task, although it was not explicitly designed for this task.",0
"GANs have proven to be highly effective for generating and manipulating images, with many studies improving our understanding of their latent space. To further this understanding, we introduce a unique autoencoder architecture based on StyleGAN. This architecture is capable of producing high-quality reconstructions across various data domains and exhibits an unprecedented level of generalizability, even when trained on different datasets. Additionally, we provide new insights into the noise inputs of the StyleGAN architecture and demonstrate that our proposed model is significantly faster than previous methods. We also show that our model produces promising results for image denoising, despite not being specifically designed for this task.",1
"Image generation with explicit condition or label generally works better than unconditional methods. In modern GAN frameworks, both generator and discriminator are formulated to model the conditional distribution of images given with labels. In this paper, we provide an alternative formulation of GAN which models the joint distribution of images and labels. There are two advantages in this joint formulation over conditional approaches. The first advantage is that the joint formulation is more robust to label noises if it's properly modeled. This alleviates the burden of making noise-free labels and allows the use of weakly-supervised labels in image generation. The second is that we can use any kinds of weak labels or image features that have correlations with the original image data to enhance unconditional image generation. We will show the effectiveness of our joint formulation on CIFAR10, CIFAR100, and STL dataset with the state-of-the-art GAN architecture.",0
"Explicitly conditioned or labeled image generation methods are generally more effective than unconditional methods. In modern GAN frameworks, both the generator and discriminator are designed to model the conditional distribution of images based on their labels. However, this paper presents an alternative GAN formulation that models the joint distribution of images and labels. This joint formulation offers two advantages over conditional approaches. Firstly, it is more resistant to label noise if properly modeled, thus eliminating the need for noise-free labels and allowing for the use of weakly-supervised labels in image generation. Secondly, any weak labels or image features that are correlated with the original image data can be used to enhance unconditional image generation. The effectiveness of this joint formulation is demonstrated on the CIFAR10, CIFAR100, and STL datasets using state-of-the-art GAN architecture.",1
"Generative Adversarial Networks (GANs) coupled with self-supervised tasks have shown promising results in unconditional and semi-supervised image generation. We propose a self-supervised approach (LT-GAN) to improve the generation quality and diversity of images by estimating the GAN-induced transformation (i.e. transformation induced in the generated images by perturbing the latent space of generator). Specifically, given two pairs of images where each pair comprises of a generated image and its transformed version, the self-supervision task aims to identify whether the latent transformation applied in the given pair is same to that of the other pair. Hence, this auxiliary loss encourages the generator to produce images that are distinguishable by the auxiliary network, which in turn promotes the synthesis of semantically consistent images with respect to latent transformations. We show the efficacy of this pretext task by improving the image generation quality in terms of FID on state-of-the-art models for both conditional and unconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets. Moreover, we empirically show that LT-GAN helps in improving controlled image editing for CelebA-HQ and ImageNet over baseline models. We experimentally demonstrate that our proposed LT self-supervision task can be effectively combined with other state-of-the-art training techniques for added benefits. Consequently, we show that our approach achieves the new state-of-the-art FID score of 9.8 on conditional CIFAR-10 image generation.",0
"The combination of Generative Adversarial Networks (GANs) and self-supervised tasks has produced promising results in the generation of both unconditional and semi-supervised images. Our proposal, the LT-GAN, utilizes a self-supervised approach to enhance the quality and diversity of generated images. This is achieved by estimating the GAN-induced transformation through perturbations in the latent space of the generator. Our self-supervision task involves identifying whether the latent transformation applied in a pair of generated images is the same as that in another pair. This task encourages the generator to produce images that are distinguishable by the auxiliary network, resulting in semantically consistent images with respect to latent transformations. We demonstrate the effectiveness of our approach on state-of-the-art models for both conditional and unconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets, as well as controlled image editing. We also show that our proposed self-supervision task can be effectively combined with other training techniques. Our approach achieved the new state-of-the-art FID score of 9.8 on conditional CIFAR-10 image generation.",1
"Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.",0
"The artificial intelligence field faces a major challenge in establishing disentangled data representations. These latent representations are essential for generative models to have control over the various factors involved in the synthesis process. However, current approaches necessitate significant supervision and training, which can negatively impact the quality of the output. To address this issue, we propose a method in which disentangled data representation can be learned with minimal supervision, utilizing pre-trained networks. Our approach involves separating the processes of disentanglement and synthesis by utilizing a pre-trained unconditional image generator such as StyleGAN. By mapping data into its latent space, we leverage the network's high quality and expressive latent space without the need for additional training. We apply our method to the challenging domain of human heads and demonstrate its effectiveness through qualitative and quantitative evaluation, showcasing its success in de-identification and maintaining temporal identity coherency in image sequences. Our method surpasses existing approaches that require more supervision and training, successfully disentangling identity from other facial attributes.",1
"Our ability to sample realistic natural images, particularly faces, has advanced by leaps and bounds in recent years, yet our ability to exert fine-tuned control over the generative process has lagged behind. If this new technology is to find practical uses, we need to achieve a level of control over generative networks which, without sacrificing realism, is on par with that seen in computer graphics and character animation. To this end we propose ConfigNet, a neural face model that allows for controlling individual aspects of output images in semantically meaningful ways and that is a significant step on the path towards finely-controllable neural rendering. ConfigNet is trained on real face images as well as synthetic face renders. Our novel method uses synthetic data to factorize the latent space into elements that correspond to the inputs of a traditional rendering pipeline, separating aspects such as head pose, facial expression, hair style, illumination, and many others which are very hard to annotate in real data. The real images, which are presented to the network without labels, extend the variety of the generated images and encourage realism. Finally, we propose an evaluation criterion using an attribute detection network combined with a user study and demonstrate state-of-the-art individual control over attributes in the output images.",0
"In recent years, sampling natural images, especially faces, has made great strides. However, our ability to precisely control the generative process has not kept pace. In order for this new technology to be practical, we must achieve a level of control that is comparable to that seen in computer graphics and character animation, while still maintaining realism. To achieve this, we introduce ConfigNet, a neural face model that allows for meaningful control over individual aspects of output images. ConfigNet is trained on a combination of real face images and synthetic face renders, using a novel method that separates various aspects (such as head pose, facial expression, hair style, and illumination) that are difficult to annotate in real data. The real images extend the variety of the generated images and enhance realism. We also propose an evaluation criterion using an attribute detection network and a user study, demonstrating state-of-the-art control over attributes in the output images. ConfigNet is a significant step towards finely-controllable neural rendering.",1
"In recent years, text-guided image manipulation has gained increasing attention in the image generation research field. Recent works have proposed to deal with a simplified setting where the input image only has a single object and the text modification is acquired by swapping image captions or labels. In this paper, we study a setting that allows users to edit an image with multiple objects using complex text instructions. In this image generation task, the inputs are a reference image and an instruction in natural language that describes desired modifications to the input image. We propose a GAN-based method to tackle this problem. The key idea is to treat text as neural operators to locally modify the image feature. We show that the proposed model performs favorably against recent baselines on three public datasets.",0
"The image generation research field has recently shown increased interest in text-guided image manipulation. Prior works have focused on a simplified setting where the input image only features a single object, and modifications are made by swapping image captions or labels. However, our study considers a more complex setting, where users can edit an image with multiple objects using detailed text instructions. Our proposed solution is a GAN-based method that treats text as neural operators to modify image features locally. We demonstrate the effectiveness of our model on three public datasets, outperforming recent baselines.",1
"The focus of this survey is on the analysis of two modalities of multimodal deep learning: image and text. Unlike classic reviews of deep learning where monomodal image classifiers such as VGG, ResNet and Inception module are central topics, this paper will examine recent multimodal deep models and structures, including auto-encoders, generative adversarial nets and their variants. These models go beyond the simple image classifiers in which they can do uni-directional (e.g. image captioning, image generation) and bi-directional (e.g. cross-modal retrieval, visual question answering) multimodal tasks. Besides, we analyze two aspects of the challenge in terms of better content understanding in deep multimodal applications. We then introduce current ideas and trends in deep multimodal feature learning, such as feature embedding approaches and objective function design, which are crucial in overcoming the aforementioned challenges. Finally, we include several promising directions for future research.",0
"In this survey, the focus will be on the examination of two forms of multimodal deep learning, namely image and text. Unlike traditional reviews of deep learning that center around monomodal image classifiers like VGG, ResNet, and Inception module, this paper will investigate recent multimodal deep models and structures, including auto-encoders, generative adversarial nets, and their variants. These models surpass the basic image classifiers as they can perform uni-directional tasks like image captioning and image generation, as well as bi-directional tasks like cross-modal retrieval and visual question answering. Furthermore, we analyze two aspects of the challenge of achieving better content understanding in deep multimodal applications. We also introduce current ideas and trends in deep multimodal feature learning, such as feature embedding approaches and objective function design, which are vital to overcoming the aforementioned challenges. Lastly, we highlight several promising areas for future research.",1
"Semantic editing on segmentation map has been proposed as an intermediate interface for image generation, because it provides flexible and strong assistance in various image generation tasks. This paper aims to improve quality of edited segmentation map conditioned on semantic inputs. Even though recent studies apply global and local adversarial losses extensively to generate images for higher image quality, we find that they suffer from the misalignment of the boundary area in the mask area. To address this, we propose MExGAN for semantic editing on segmentation map, which uses a novel Multi-Expansion (MEx) loss implemented by adversarial losses on MEx areas. Each MEx area has the mask area of the generation as the majority and the boundary of original context as the minority. To boost convenience and stability of MEx loss, we further propose an Approximated MEx (A-MEx) loss. Besides, in contrast to previous model that builds training data for semantic editing on segmentation map with part of the whole image, which leads to model performance degradation, MExGAN applies the whole image to build the training data. Extensive experiments on semantic editing on segmentation map and natural image inpainting show competitive results on four datasets.",0
"The use of semantic editing on segmentation maps has been suggested as an intermediary interface for image generation due to its versatility and strong support in various image generation tasks. This study aims to enhance the quality of edited segmentation maps based on semantic inputs. While previous research extensively uses global and local adversarial losses to generate high-quality images, they suffer from the misalignment of boundary areas in the mask region. To address this issue, we propose MExGAN for semantic editing on segmentation maps, which employs a novel Multi-Expansion (MEx) loss that is implemented by adversarial losses on MEx regions. Each MEx region has the mask area of the generated image as the majority and the original context's boundary as the minority. To improve the convenience and stability of the MEx loss, we also suggest an Approximated MEx (A-MEx) loss. Additionally, unlike prior models that use only a portion of the whole image to create training data for semantic editing on segmentation maps, MExGAN employs the entire image to create the training data, resulting in better model performance. Our experiments on semantic editing on segmentation maps and natural image inpainting demonstrate competitive results on four datasets.",1
"This paper proposes a multi-grid method for learning energy-based generative ConvNet models of images. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We show that this multi-grid method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD.",0
"The aim of this paper is to present a multi-grid approach to develop energy-based generative ConvNet models for images. To achieve this, we establish an energy-based probabilistic model for each grid and define the energy function using a bottom-up ConvNet or CNN. To learn this model, we need to create synthesized examples. In every iteration of our learning algorithm, we create multiple synthesized images for each observed training image by initiating finite-step MCMC sampling from a 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by initiating finite-step MCMC from the synthesized image generated at the previous, coarser grid. After generating the synthesized examples, we update the parameters of the models at multiple grids separately and simultaneously, based on the differences between synthesized and observed examples. Our multi-grid method outperforms the original contrastive divergence (CD) and persistent CD by learning realistic energy-based generative ConvNet models.",1
"An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.",0
"The reduction or limitation of the information capacity of the latent representation is a crucial element in autoencoders. This study utilizes the fact that minimum-rank solutions are obtained through gradient descent learning in multi-layer linear networks to implicitly minimize the rank of the covariance matrix of the codes. Additional linear layers are incorporated between the encoder and decoder to facilitate the spontaneous learning of representations with a low effective dimension. The resulting model, named Implicit Rank-Minimizing Autoencoder (IRMAE), is uncomplicated, definite, and capable of learning condensed latent spaces. Numerous image generation and representation learning tasks were employed to demonstrate the effectiveness of this approach.",1
"It has been challenging to identify ferrograph images with a small dataset and various scales of wear particle. A novel model is proposed in this study to cope with these challenging problems. For the problem of insufficient samples, we first proposed a data augmentation algorithm based on the permutation of image patches. Then, an auxiliary loss function of image patch permutation recognition was proposed to identify the image generated by the data augmentation algorithm. Moreover, we designed a feature extraction loss function to force the proposed model to extract more abundant features and to reduce redundant representations. As for the challenge of large change range of wear particle size, we proposed a multi-scale feature extraction block to obtain the multi-scale representations of wear particles. We carried out experiments on a ferrograph image dataset and a mini-CIFAR-10 dataset. Experimental results show that the proposed model can improve the accuracy of the two datasets by 9% and 20% respectively compared with the baseline.",0
"The identification of ferrograph images has proven to be difficult due to a limited dataset and varying wear particle sizes. To address these challenges, we introduce a novel model in this study. To combat the issue of insufficient samples, we propose a data augmentation algorithm that involves permuting image patches. This is supplemented by an auxiliary loss function to identify images produced by the augmented data. We also design a feature extraction loss function to encourage the model to extract more diverse features and reduce redundancy. Additionally, we address the challenge of large changes in wear particle size by introducing a multi-scale feature extraction block for obtaining multi-scale representations of particles. Our experiments on a ferrograph image dataset and a mini-CIFAR-10 dataset demonstrate that our proposed model outperforms the baseline by 9% and 20%, respectively, in terms of accuracy.",1
"Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.",0
"Generative models have improved significantly in generating high-quality images and text. The evaluation metrics created by the community are insufficient in measuring data diversity. To tackle this issue, we propose a new diversity metric that can be applied to any type of data, whether synthetic or natural. Our approach uses random network distillation, which was originally introduced in reinforcement learning. We demonstrate the effectiveness of our metric by testing it on both images and text. Additionally, we investigate diversity in few-shot image generation, which was previously challenging to evaluate.",1
"This paper presents the evaluation methodology, datasets, and results of the BOP Challenge 2020, the third in a series of public competitions organized with the goal to capture the status quo in the field of 6D object pose estimation from an RGB-D image. In 2020, to reduce the domain gap between synthetic training and real test RGB images, the participants were provided 350K photorealistic training images generated by BlenderProc4BOP, a new open-source and light-weight physically-based renderer (PBR) and procedural data generator. Methods based on deep neural networks have finally caught up with methods based on point pair features, which were dominating previous editions of the challenge. Although the top-performing methods rely on RGB-D image channels, strong results were achieved when only RGB channels were used at both training and test time - out of the 26 evaluated methods, the third method was trained on RGB channels of PBR and real images, while the fifth on RGB channels of PBR images only. Strong data augmentation was identified as a key component of the top-performing CosyPose method, and the photorealism of PBR images was demonstrated effective despite the augmentation. The online evaluation system stays open and is available on the project website: bop.felk.cvut.cz.",0
"The BOP Challenge 2020's assessment methodology, data sets, and outcomes are presented in this paper. This challenge is the third in a series of public competitions that aim to reflect the current state of 6D object pose estimation from an RGB-D image. To decrease the gap between synthetic training and real test RGB images, contestants were given access to 350K photorealistic training images produced by BlenderProc4BOP, a new open-source and lightweight physically-based renderer (PBR) and procedural data generator. The dominance of methods based on point pair features in previous editions of the challenge has been matched by those based on deep neural networks. While the best performing methods rely on RGB-D image channels, satisfactory results were achieved when only RGB channels were utilized at both training and test times. The third and fifth methods were trained on RGB channels of PBR and real images, and RGB channels of PBR images only, respectively, out of the 26 methods evaluated. The top-performing CosyPose method was determined to rely heavily on strong data augmentation, and the efficiency of the photorealism of PBR images was demonstrated despite this augmentation. The online evaluation system is still available and can be found on the project website: bop.felk.cvut.cz.",1
"Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to all of the pixels of a real image. We find that a considerable amount of the perturbations on an image generated by some widely-used attacks may contribute little in attacking a classifier. However, they usually result in a more easily detectable adversarial image by both humans and adversarial attack detection algorithms. Therefore, it is important to impose the perturbations on the most vulnerable pixels of an image that can change the predictions of classifiers more readily. With the pixel vulnerability, given an existing attack, we can make its adversarial images more realistic and less detectable with fewer perturbations but keep its attack performance the same. Moreover, the discovered vulnerability assists to get a better understanding of the weakness of deep classifiers. Derived from the information-theoretic perspective, we propose a probabilistic approach for automatically finding the pixel vulnerability of an image, which is compatible with and improves over many existing adversarial attacks.",0
"Adversarial evasion attacks can deceive deep neural network image classifiers by using deliberately created images. Various types of adversarial attack methods have been introduced, primarily involving adding small perturbations to all pixels in a genuine image. However, some widely-used attacks generate an excessive amount of perturbations that make the adversarial image detectable by both humans and detection algorithms. It is important to focus on perturbing the most vulnerable pixels in an image that can more easily alter the classifier's predictions. By identifying pixel vulnerability, we can make adversarial images appear more realistic and less detectable with fewer perturbations while maintaining the attack performance. Additionally, discovering the vulnerability helps in comprehending the weaknesses of deep classifiers. We propose a probabilistic approach, based on information theory, to automatically determine the pixel vulnerability of an image, which is compatible with and enhances many existing adversarial attacks.",1
"An omni-directional image (ODI) is the image that has a field of view covering the entire sphere around the camera. The ODIs have begun to be used in a wide range of fields such as virtual reality (VR), robotics, and social network services. Although the contents using ODI have increased, the available images and videos are still limited, compared with widespread snapshot images. A large number of ODIs are desired not only for the VR contents, but also for training deep learning models for ODI. For these purposes, a novel computer vision task to generate ODI from a single snapshot image is proposed in this paper. To tackle this problem, the conditional generative adversarial network was applied in combination with class-conditioned convolution layers. With this novel task, VR images and videos will be easily created even with a smartphone camera.",0
"The term ""omni-directional image"" refers to an image that captures a 360-degree view around the camera. These images are becoming increasingly popular in various fields, including virtual reality, robotics, and social media. However, there are still limited options for ODIs compared to regular photos. To address this issue, a new computer vision task has been proposed to generate ODIs from a single snapshot image, using a conditional generative adversarial network and class-conditioned convolution layers. This method will enable the creation of VR images and videos using just a smartphone camera.",1
"The collection of high-resolution training data is crucial in building robust plant disease diagnosis systems, since such data have a significant impact on diagnostic performance. However, they are very difficult to obtain and are not always available in practice. Deep learning-based techniques, and particularly generative adversarial networks (GANs), can be applied to generate high-quality super-resolution images, but these methods often produce unexpected artifacts that can lower the diagnostic performance. In this paper, we propose a novel artifact-suppression super-resolution method that is specifically designed for diagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution (LASSR). Thanks to its own artifact removal module that detects and suppresses artifacts to a considerable extent, LASSR can generate much more pleasing, high-quality images compared to the state-of-the-art ESRGAN model. Experiments based on a five-class cucumber disease (including healthy) discrimination model show that training with data generated by LASSR significantly boosts the performance on an unseen test dataset by nearly 22% compared with the baseline, and that our approach is more than 2% better than a model trained with images generated by ESRGAN.",0
"Obtaining high-resolution training data is essential for developing reliable plant disease diagnosis systems as it significantly impacts diagnostic performance. However, acquiring such data can be challenging and impractical. Generative adversarial networks (GANs) can produce high-quality super-resolution images, but they often come with unexpected artifacts that can reduce diagnostic performance. This paper proposes a novel method called Leaf Artifact-Suppression Super Resolution (LASSR), specifically designed for diagnosing leaf disease. LASSR includes an artifact removal module that detects and suppresses artifacts, resulting in higher quality images compared to the state-of-the-art ESRGAN model. Experiments using a five-class cucumber disease discrimination model demonstrate that training with LASSR-generated data enhances performance on an unseen test dataset by almost 22% compared to the baseline. Furthermore, our approach outperforms a model trained with ESRGAN-generated images by over 2%.",1
"Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fr\'echet Inception Distance, a standard metric for generative models.   We show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10.",0
"Generative modeling has recently benefited from Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS). Initially, a neural network is trained to estimate the score of a distribution, followed by the use of Langevin dynamics to sample from the data distribution assumed by the score network. Although the samples produced appear visually convincing, the method performs worse than Generative Adversarial Networks (GANs) regarding the Fr\'echet Inception Distance, a standard metric for generative models. However, we demonstrate that the gap between the performance of DSM-ALS and GANs disappears when denoising the final Langevin samples using the score network. We also propose two enhancements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation employing both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we are able to elevate score matching methods and achieve results that are competitive with state-of-the-art image generation on CIFAR-10.",1
"In this work we introduce a new self-supervised, semi-parametric approach for synthesizing novel views of a vehicle starting from a single monocular image. Differently from parametric (i.e. entirely learning-based) methods, we show how a-priori geometric knowledge about the object and the 3D world can be successfully integrated into a deep learning based image generation framework. As this geometric component is not learnt, we call our approach semi-parametric. In particular, we exploit man-made object symmetry and piece-wise planarity to integrate rich a-priori visual information into the novel viewpoint synthesis process. An Image Completion Network (ICN) is then trained to generate a realistic image starting from this geometric guidance. This careful blend between parametric and non-parametric components allows us to i) operate in a real-world scenario, ii) preserve high-frequency visual information such as textures, iii) handle truly arbitrary 3D roto-translations of the input and iv) perform shape transfer to completely different 3D models. Eventually, we show that our approach can be easily complemented with synthetic data and extended to other rigid objects with completely different topology, even in presence of concave structures and holes (e.g. chairs). A comprehensive experimental analysis against state-of-the-art competitors shows the efficacy of our method both from a quantitative and a perceptive point of view. Supplementary material, animated results, code and data are available at: https://github.com/ndrplz/semiparametric",0
"A new method is presented in this study for generating novel views of a vehicle using a self-supervised, semi-parametric approach. Unlike entirely learning-based parametric methods, this approach integrates a priori geometric knowledge about the object and 3D world into a deep learning-based image generation framework. This geometric component is not learned, making the approach semi-parametric. The method leverages man-made object symmetry and piece-wise planarity to incorporate rich a priori visual information into the viewpoint synthesis process. An Image Completion Network (ICN) is trained to generate a realistic image using this geometric guidance. The combination of parametric and non-parametric components allows the method to handle arbitrary 3D roto-translations, preserve high-frequency visual information, and perform shape transfer to different 3D models. The method can also be extended to other rigid objects with different topology, including those with concave structures and holes. Experimental analysis demonstrates the efficacy of the method, with supplementary material, code, and data available at the provided link.",1
"We introduce the Generalized Energy Based Model (GEBM) for generative modelling. These models combine two trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. Both the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the ""generator""). GEBMs are trained by alternating between learning the energy and the base. We show that both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base. Samples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples. Empirically, the GEBM samples on image-generation tasks are of much better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. When using normalizing flows as base measures, GEBMs succeed on density modelling tasks, returning comparable performance to direct maximum likelihood of the same networks.",0
"Our aim is to present the Generalized Energy Based Model (GEBM), a method for generative modelling. These models incorporate two trained components: a base distribution, which is typically an implicit model and can learn the data support with low intrinsic dimension in a high dimensional space, and an energy function, which refines the probability mass on the learned support. Unlike GANs that rely solely on the base distribution, GEBMs use both the energy function and the base distribution to create the final model. The training of GEBMs involves alternating between learning the energy and the base, both of which have well-defined training stages. The energy is learned through maximizing a generalized likelihood, and the energy-based loss provides informative gradients for learning the base. The trained model's posterior samples in the latent space can be obtained via MCMC, allowing for the identification of regions in the space that generate higher quality samples. Empirical results demonstrate that GEBM samples in image-generation tasks are of higher quality than those produced by the learned generator alone, indicating that GEBMs will outperform GANs of the same complexity. Furthermore, when normalizing flows serve as base measures, GEBMs achieve comparable performance to direct maximum likelihood of the same networks in density modelling tasks.",1
"Nowadays, nonnegative matrix factorization (NMF) based methods have been widely applied to blind spectral unmixing. Introducing proper regularizers to NMF is crucial for mathematically constraining the solutions and physically exploiting spectral and spatial properties of images. Generally, properly handcrafting regularizers and solving the associated complex optimization problem are non-trivial tasks. In our work, we propose an NMF based unmixing framework which jointly uses a handcrafting regularizer and a learnt regularizer from data. we plug learnt priors of abundances where the associated subproblem can be addressed using various image denoisers, and we consider an l_2,1-norm regularizer to the abundance matrix to promote sparse unmixing results. The proposed framework is flexible and extendable. Both synthetic data and real airborne data are conducted to confirm the effectiveness of our method.",0
"Nonnegative matrix factorization (NMF) methods are commonly used for blind spectral unmixing. It is important to include appropriate regularizers in NMF to ensure that the solutions are mathematically constrained and to take advantage of the spectral and spatial properties of images. However, creating effective regularizers and solving the associated optimization problem can be challenging. Our study proposes an NMF-based unmixing approach that combines a handcrafted regularizer and a learned regularizer from data. We incorporate learned priors of abundances using various image denoisers and an l_2,1-norm regularizer for the abundance matrix to achieve sparse unmixing results. Our approach is flexible and can be extended. We validate our method using both synthetic and real airborne data.",1
"Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels.",0
"Advanced image generation technology has led to powerful tools for semantic image editing. However, current approaches can only work on a single image or require a significant amount of additional data. They are not capable of handling all editing operations, such as adding, manipulating, or removing semantic concepts. To solve these limitations, we propose SESAME, a new generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. Our system allows the user to provide the semantic labels of the areas to be edited, and the generator creates the corresponding pixels. Unlike previous methods that use a discriminator that simply concatenates semantics and image as input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We have evaluated our model on various datasets and achieved state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation based on semantic labels.",1
"Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \times$ to $50 \times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",0
"Denoising diffusion probabilistic models (DDPMs) have been successful in generating high-quality images without adversarial training. However, the generation process requires simulating a Markov chain for numerous steps, leading to slow sampling. To address this issue, we introduce denoising diffusion implicit models (DDIMs), which are more efficient iterative implicit probabilistic models that follow the same training procedure as DDPMs. While DDPMs use a reverse Markovian diffusion process for generation, we utilize a class of non-Markovian diffusion processes that achieve the same training objective but have a faster reverse process for sampling. Our experiments show that DDIMs can generate high-quality samples at a much faster speed, ranging from 10 to 50 times faster than DDPMs in terms of wall-clock time. Moreover, we can trade off computation for sample quality, and DDIMs allow for semantically meaningful image interpolation in the latent space.",1
"Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new proposed autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.",0
"The Relational Regularized Autoencoder (RAE) is a method that learns the data distribution by minimizing a reconstruction loss and incorporating relational regularization on the latent space. However, an issue with incorporating sliced fused Gromov-Wasserstein (SFG) to reduce the inner discrepancy between prior and posterior distributions is that it treats all slicing directions uniformly, despite several directions being irrelevant for the discriminative task. To address this, we propose a new relational discrepancy, called Spherical Sliced Fused Gromov Wasserstein (SSFG), that can identify important projection areas characterized by a von Mises-Fisher distribution. We introduce two variants of SSFG, the Mixture Spherical Sliced Fused Gromov Wasserstein (MSSFG), and the Power Spherical Sliced Fused Gromov Wasserstein (PSSFG), which capture multiple important direction areas and improve sampling time in high dimensions, respectively. We apply these discrepancies to the RAE framework to create new variants and conduct experiments to demonstrate their superior performance in learning latent manifold structure, image generation, and reconstruction.",1
"In game development, designing compelling visual assets that convey gameplay-relevant features requires time and experience. Recent image generation methods that create high-quality content could reduce development costs, but these approaches do not consider game mechanics. We propose a Convolutional Variational Autoencoder (CVAE) system to modify and generate new game visuals based on their gameplay relevance. We test this approach with Pok\'emon sprites and Pok\'emon type information, since types are one of the game's core mechanics and they directly impact the game's visuals. Our experimental results indicate that adopting a transfer learning approach can help to improve visual quality and stability over unseen data.",0
"Creating visually engaging assets that accurately convey relevant gameplay features is a time-consuming and skill-dependent task in game development. While newer methods for generating high-quality content exist, they often overlook game mechanics. To address this, we propose a Convolutional Variational Autoencoder (CVAE) system that can modify and generate game visuals with gameplay in mind. To demonstrate this, we use Pok\'emon sprites and types, which are crucial to the game's mechanics and visuals. Our experimental results show that using transfer learning can improve visual quality and consistency with new data.",1
"Estimating the 3D hand pose from a monocular RGB image is important but challenging. A solution is training on large-scale RGB hand images with accurate 3D hand keypoint annotations. However, it is too expensive in practice. Instead, we have developed a learning-based approach to synthesize realistic, diverse, and 3D pose-preserving hand images under the guidance of 3D pose information. We propose a 3D-aware multi-modal guided hand generative network (MM-Hand), together with a novel geometry-based curriculum learning strategy. Our extensive experimental results demonstrate that the 3D-annotated images generated by MM-Hand qualitatively and quantitatively outperform existing options. Moreover, the augmented data can consistently improve the quantitative performance of the state-of-the-art 3D hand pose estimators on two benchmark datasets. The code will be available at https://github.com/ScottHoang/mm-hand.",0
"It is challenging to estimate the 3D hand pose from a monocular RGB image, but it is essential. A potential solution is to train on large-scale RGB hand images with accurate 3D hand keypoint annotations, but it is not cost-effective. Instead, we created a learning-based method that generates realistic, diverse, and 3D pose-preserving hand images using 3D pose information. Our technique involves a 3D-aware multi-modal guided hand generative network (MM-Hand) and a novel geometry-based curriculum learning strategy. Our experiments demonstrate that the 3D-annotated images generated by MM-Hand outperform existing options both qualitatively and quantitatively. Additionally, the augmented data consistently enhances the performance of state-of-the-art 3D hand pose estimators on two benchmark datasets. The code is available at https://github.com/ScottHoang/mm-hand.",1
"In this paper, we investigate the problem of text-to-pedestrian synthesis, which has many potential applications in art, design, and video surveillance. Existing methods for text-to-bird/flower synthesis are still far from solving this fine-grained image generation problem, due to the complex structure and heterogeneous appearance that the pedestrians naturally take on. To this end, we propose the Multi-Grained Discrimination enhanced Generative Adversarial Network, that capitalizes a human-part-based Discriminator (HPD) and a self-cross-attended (SCA) global Discriminator in order to capture the coherence of the complex body structure. A fined-grained word-level attention mechanism is employed in the HPD module to enforce diversified appearance and vivid details. In addition, two pedestrian generation metrics, named Pose Score and Pose Variance, are devised to evaluate the generation quality and diversity, respectively. We conduct extensive experiments and ablation studies on the caption-annotated pedestrian dataset, CUHK Person Description Dataset. The substantial improvement over the various metrics demonstrates the efficacy of MGD-GAN on the text-to-pedestrian synthesis scenario.",0
"The focus of our study is the challenge of synthesizing pedestrian images from text, which has various potential uses in fields such as art, design, and video surveillance. Previous approaches to generating bird or flower images from text have not adequately addressed the complexities of creating detailed pedestrian images due to the diverse and intricate features of the human body. Therefore, we introduce the Multi-Grained Discrimination enhanced Generative Adversarial Network (MGD-GAN), which incorporates a human-part-based Discriminator (HPD) and a self-cross-attended (SCA) global Discriminator to capture the nuances of the complex body structure. The HPD module utilizes a fine-grained word-level attention mechanism to ensure diverse appearances and vivid details. Additionally, we developed two metrics, Pose Score and Pose Variance, to evaluate the quality and diversity of the generated pedestrian images. Our experiments and analyses on the CUHK Person Description Dataset demonstrate the superior performance of MGD-GAN in generating pedestrian images from text.",1
"GANs are well known for success in the realistic image generation. However, they can be applied in tabular data generation as well. We will review and examine some recent papers about tabular GANs in action. We will generate data to make train distribution bring closer to the test. Then compare model performance trained on the initial train dataset, with trained on the train with GAN generated data, also we train the model by sampling train by adversarial training. We show that using GAN might be an option in case of uneven data distribution between train and test data.",0
"Although GANs are often associated with their ability to generate realistic images, they can also be utilized for generating tabular data. Through analyzing recent research papers, we will explore the implementation of tabular GANs. Our objective is to generate data that can help to align the train distribution with the test distribution. We will compare the performance of models trained on the initial train dataset with those trained on the train dataset that includes GAN-generated data. Additionally, we will train the model through adversarial training by sampling the train. Our findings suggest that using GANs can potentially address the issue of uneven distribution of data between the train and test datasets.",1
"Generative Adversarial Networks (GANs) have become a powerful approach for generative image modeling. However, GANs are notorious for their training instability, especially on large-scale, complex datasets. While the recent work of BigGAN has significantly improved the quality of image generation on ImageNet, it requires a huge model, making it hard to deploy on resource-constrained devices. To reduce the model size, we propose a black-box knowledge distillation framework for compressing GANs, which highlights a stable and efficient training process. Given BigGAN as the teacher network, we manage to train a much smaller student network to mimic its functionality, achieving competitive performance on Inception and FID scores with the generator having $16\times$ fewer parameters.",0
"GANs have become a popular method for creating images, but they are known for their unstable training, especially on large and complex datasets. The BigGAN has shown significant improvement in generating high-quality images on ImageNet, but its size makes it difficult to use on devices with limited resources. To address this issue, we propose a black-box knowledge distillation framework that compresses GANs while maintaining stable and efficient training. Using the BigGAN as a teacher network, we train a smaller student network to mimic its functionality, achieving comparable results on Inception and FID scores with only 1/16th of the generator's parameters.",1
"Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",0
"Models that combine vision and language, such as ViLBERT, LXMERT, and UNITER, have achieved impressive results in various multimodal discriminative tasks, such as visual grounding and visual question answering. Recent research has adapted these models to generate image captions, leading to the question of whether they can generate images from text. Our analysis of LXMERT revealed that it was not capable of generating meaningful images with its current training setup. To address this issue, we developed X-LXMERT, an extension with improved training refinements, including visual representation discretization, uniform masking with a wide range of ratios, and alignment of pre-training datasets with objectives. X-LXMERT can produce high-quality images, while maintaining LXMERT's performance in question answering and captioning tasks. Furthermore, we applied the same training refinements to UNITER to create X-UNITER, demonstrating their general applicability.",1
"We propose a generative Causal Adversarial Network (CAN) for learning and sampling from conditional and interventional distributions. In contrast to the existing CausalGAN which requires the causal graph to be given, our proposed framework learns the causal relations from the data and generates samples accordingly. The proposed CAN comprises a two-fold process namely Label Generation Network (LGN) and Conditional Image Generation Network (CIGN). The LGN is a GAN-based architecture which learns and samples from the causal model over labels. The sampled labels are then fed to CIGN, a conditional GAN architecture, which learns the relationships amongst labels and pixels and pixels themselves and generates samples based on them. This framework is equipped with an intervention mechanism which enables. the model to generate samples from interventional distributions. We quantitatively and qualitatively assess the performance of CAN and empirically show that our model is able to generate both interventional and conditional samples without having access to the causal graph for the application of face generation on CelebA data.",0
"Our proposed framework, the Generative Causal Adversarial Network (CAN), is designed to learn and generate samples from conditional and interventional distributions. Unlike the existing CausalGAN, which requires a given causal graph, our approach learns causal relations from data and generates samples accordingly. The CAN consists of two components: the Label Generation Network (LGN) and the Conditional Image Generation Network (CIGN). The LGN is based on a GAN architecture that learns and samples from the causal model over labels. The resulting labels are then fed into CIGN, a conditional GAN architecture that learns relationships between labels, pixels, and generates samples based on them. Our framework also includes an intervention mechanism that enables the model to generate samples from interventional distributions. We evaluated the performance of CAN both quantitatively and qualitatively, demonstrating its ability to generate both interventional and conditional samples without requiring a causal graph for face generation using CelebA data.",1
"The novelty and creativity of DeepFake generation techniques have attracted worldwide media attention. Many researchers focus on detecting fake images produced by these GAN-based image generation methods with fruitful results, indicating that the GAN-based image generation methods are not yet perfect. Many studies show that the upsampling procedure used in the decoder of GAN-based image generation methods inevitably introduce artifact patterns into fake images. In order to further improve the fidelity of DeepFake images, in this work, we propose a simple yet powerful framework to reduce the artifact patterns of fake images without hurting image quality. The method is based on an important observation that adding noise to a fake image can successfully reduce the artifact patterns in both spatial and frequency domains. Thus we use a combination of additive noise and deep image filtering to reconstruct the fake images, and we name our method FakeRetouch. The deep image filtering provides a specialized filter for each pixel in the noisy image, taking full advantages of deep learning. The deeply filtered images retain very high fidelity to their DeepFake counterparts. Moreover, we use the semantic information of the image to generate an adversarial guidance map to add noise intelligently. Our method aims at improving the fidelity of DeepFake images and exposing the problems of existing DeepFake detection methods, and we hope that the found vulnerabilities can help improve the future generation DeepFake detection methods.",0
"DeepFake generation techniques have gained worldwide media attention due to their novelty and creativity. Researchers have focused on detecting fake images generated by GAN-based methods and have achieved fruitful results, indicating that these methods are not yet perfect. Studies have shown that the upsampling procedure used in the decoder of GAN-based methods introduces artifact patterns into fake images. To enhance the fidelity of DeepFake images, a simple and powerful framework called FakeRetouch is proposed in this work. The method is based on the observation that adding noise to a fake image can successfully reduce artifact patterns in both spatial and frequency domains. The combination of additive noise and deep image filtering is used to reconstruct fake images while retaining high fidelity to their DeepFake counterparts. Additionally, the semantic information of the image is utilized to generate an adversarial guidance map to add noise intelligently. The aim of the method is to improve the fidelity of DeepFake images and expose the vulnerabilities of existing detection methods, which can aid in the development of future detection methods.",1
"Generative Adversarial Networks (GANs) have long been used to understand the semantic relationship between the text and image. However, there are problems with mode collapsing in the image generation that causes some preferred output modes. Our aim is to improve the training of the network by using a specialized mode-seeking loss function to avoid this issue. In the text to image synthesis, our loss function differentiates two points in latent space for the generation of distinct images. We validate our model on the Caltech Birds (CUB) dataset and the Microsoft COCO dataset by changing the intensity of the loss function during the training. Experimental results demonstrate that our model works very well compared to some state-of-the-art approaches.",0
"For a while now, Generative Adversarial Networks (GANs) have been utilized to comprehend the semantic correlation between images and text. Nonetheless, there are problems with mode collapsing in image generation that lead to some favored output modes. Our objective is to enhance the network training by utilizing a customized mode-seeking loss function to evade this problem. In text to image synthesis, our loss function discriminates between two points in latent space to create distinct images. We tested our model on the Caltech Birds (CUB) dataset and the Microsoft COCO dataset by adjusting the intensity of the loss function while training. The experimental outcomes indicate that our model is highly effective compared to some of the most advanced techniques.",1
"This paper explores conditional image generation with a One-Vs-All classifier based on the Generative Adversarial Networks (GANs). Instead of the real/fake discriminator used in vanilla GANs, we propose to extend the discriminator to a One-Vs-All classifier (GAN-OVA) that can distinguish each input data to its category label. Specifically, we feed certain additional information as conditions to the generator and take the discriminator as a One-Vs-All classifier to identify each conditional category. Our model can be applied to different divergence or distances used to define the objective function, such as Jensen-Shannon divergence and Earth-Mover (or called Wasserstein-1) distance. We evaluate GAN-OVAs on MNIST and CelebA-HQ datasets, and the experimental results show that GAN-OVAs make progress toward stable training over regular conditional GANs. Furthermore, GAN-OVAs effectively accelerate the generation process of different classes and improves generation quality.",0
"The purpose of this paper is to investigate the concept of conditional image generation using a One-Vs-All classifier based on Generative Adversarial Networks (GANs). Instead of relying on the traditional real/fake discriminator utilized in vanilla GANs, we propose expanding the discriminator to a One-Vs-All classifier (GAN-OVA) that can distinguish each input data according to its category label. To achieve this, we introduce additional information as conditions to the generator, and the discriminator is used as a One-Vs-All classifier to identify each conditional category. Our model is versatile, as it can be applied to different divergence or distances that define the objective function, including Jensen-Shannon divergence and Earth-Mover (also known as Wasserstein-1) distance. We evaluated GAN-OVAs on MNIST and CelebA-HQ datasets, and the experimental results demonstrate that GAN-OVAs lead to improved stability during training compared to standard conditional GANs. Additionally, GAN-OVAs effectively speed up the generation process of multiple classes and enhance the quality of image generation.",1
"Text-to-Face (TTF) synthesis is a challenging task with great potential for diverse computer vision applications. Compared to Text-to-Image (TTI) synthesis tasks, the textual description of faces can be much more complicated and detailed due to the variety of facial attributes and the parsing of high dimensional abstract natural language. In this paper, we propose a Text-to-Face model that not only produces images in high resolution (1024x1024) with text-to-image consistency, but also outputs multiple diverse faces to cover a wide range of unspecified facial features in a natural way. By fine-tuning the multi-label classifier and image encoder, our model obtains the vectors and image embeddings which are used to transform the input noise vector sampled from the normal distribution. Afterwards, the transformed noise vector is fed into a pre-trained high-resolution image generator to produce a set of faces with the desired facial attributes. We refer to our model as TTF-HD. Experimental results show that TTF-HD generates high-quality faces with state-of-the-art performance.",0
"The task of Text-to-Face (TTF) synthesis has enormous potential for various computer vision applications, but it is a difficult undertaking. Compared to Text-to-Image (TTI) synthesis tasks, the textual depiction of faces can be more intricate and comprehensive, primarily because of the vast range of facial features and the parsing of dense and abstract natural language. This paper proposes a Text-to-Face model that produces high-resolution (1024x1024) images with text-to-image consistency and generates multiple diverse faces to cover a broad array of unspecified facial characteristics naturally. Our model fine-tunes the multi-label classifier and image encoder to obtain vectors and image embeddings, which are used to transform the input noise vector sampled from the normal distribution. The transformed noise vector is then fed into a pre-trained high-resolution image generator to create a set of faces with the desired facial attributes. Our model is called TTF-HD, and it generates high-quality faces with state-of-the-art performance, as demonstrated by our experimental results.",1
"Conditional image generation (CIG) is a widely studied problem in computer vision and machine learning. Given a class, CIG takes the name of this class as input and generates a set of images that belong to this class. In existing CIG works, for different classes, their corresponding images are generated independently, without considering the relationship among classes. In real-world applications, the classes are organized into a hierarchy and their hierarchical relationships are informative for generating high-fidelity images. In this paper, we aim to leverage the class hierarchy for conditional image generation. We propose two ways of incorporating class hierarchy: prior control and post constraint. In prior control, we first encode the class hierarchy, then feed it as a prior into the conditional generator to generate images. In post constraint, after the images are generated, we measure their consistency with the class hierarchy and use the consistency score to guide the training of the generator. Based on these two ideas, we propose a TreeGAN model which consists of three modules: (1) a class hierarchy encoder (CHE) which takes the hierarchical structure of classes and their textual names as inputs and learns an embedding for each class; the embedding captures the hierarchical relationship among classes; (2) a conditional image generator (CIG) which takes the CHE-generated embedding of a class as input and generates a set of images belonging to this class; (3) a consistency checker which performs hierarchical classification on the generated images and checks whether the generated images are compatible with the class hierarchy; the consistency score is used to guide the CIG to generate hierarchy-compatible images. Experiments on various datasets demonstrate the effectiveness of our method.",0
"The problem of generating images based on a given class, known as conditional image generation (CIG), has been widely studied in computer vision and machine learning. Typically, existing CIG methods generate images for each class independently, without considering the hierarchical relationships among classes that exist in real-world applications. In this paper, we propose two methods for leveraging the class hierarchy for CIG: prior control and post constraint. In prior control, we encode the class hierarchy and use it as a prior for the generator. In post constraint, we measure the consistency of generated images with the class hierarchy and use this to guide the generator. Our proposed TreeGAN model consists of three modules: a class hierarchy encoder, a conditional image generator, and a consistency checker. The encoder learns embeddings for each class based on their hierarchical relationships, the generator produces images based on these embeddings, and the consistency checker ensures that the generated images are compatible with the class hierarchy. Our experiments show that our method is effective on various datasets.",1
"An estimated half of the world's languages do not have a written form, making it impossible for these languages to benefit from any existing text-based technologies. In this paper, a speech-to-image generation (S2IG) framework is proposed which translates speech descriptions to photo-realistic images without using any text information, thus allowing unwritten languages to potentially benefit from this technology. The proposed S2IG framework, named S2IGAN, consists of a speech embedding network (SEN) and a relation-supervised densely-stacked generative model (RDG). SEN learns the speech embedding with the supervision of the corresponding visual information. Conditioned on the speech embedding produced by SEN, the proposed RDG synthesizes images that are semantically consistent with the corresponding speech descriptions. Extensive experiments on two public benchmark datasets CUB and Oxford-102 demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and a solid baseline for the S2IG task.",0
"Around half of the world's languages lack a written form, which means that they cannot benefit from text-based technologies. To address this issue, this paper introduces a framework called S2IG, or speech-to-image generation. By translating speech descriptions into realistic images without relying on text, this framework enables unwritten languages to potentially benefit from this technology. The S2IGAN framework comprises two components: a speech embedding network (SEN) and a relation-supervised densely-stacked generative model (RDG). SEN learns the speech embedding using corresponding visual information, while RDG produces images that are semantically consistent with the speech embedding. Experiments on two benchmark datasets demonstrate that S2IGAN is highly effective at generating high-quality and semantically-consistent images from speech signals, laying a solid foundation for the S2IG task.",1
"We propose a unified game-theoretical framework to perform classification and conditional image generation given limited supervision. It is formulated as a three-player minimax game consisting of a generator, a classifier and a discriminator, and therefore is referred to as Triple Generative Adversarial Network (Triple-GAN). The generator and the classifier characterize the conditional distributions between images and labels to perform conditional generation and classification, respectively. The discriminator solely focuses on identifying fake image-label pairs. Under a nonparametric assumption, we prove the unique equilibrium of the game is that the distributions characterized by the generator and the classifier converge to the data distribution. As a byproduct of the three-player mechanism, Triple-GAN is flexible to incorporate different semi-supervised classifiers and GAN architectures. We evaluate Triple-GAN in two challenging settings, namely, semi-supervised learning and the extreme low data regime. In both settings, Triple-GAN can achieve excellent classification results and generate meaningful samples in a specific class simultaneously. In particular, using a commonly adopted 13-layer CNN classifier, Triple-GAN outperforms extensive semi-supervised learning methods substantially on more than 10 benchmarks no matter data augmentation is applied or not.",0
"We propose a game-theoretical framework, called Triple-GAN, that can perform classification and generate images with limited supervision. The framework consists of a generator, a classifier, and a discriminator, and is formulated as a three-player minimax game. The generator and classifier characterize the conditional distributions between images and labels for conditional generation and classification, respectively, while the discriminator identifies fake image-label pairs. We prove that the unique equilibrium of the game is that the distributions characterized by the generator and classifier converge to the data distribution. Triple-GAN is flexible and can incorporate different semi-supervised classifiers and GAN architectures. We evaluate Triple-GAN in two challenging settings and find that it achieves excellent classification results and generates meaningful samples in a specific class simultaneously. Using a 13-layer CNN classifier, Triple-GAN outperforms extensive semi-supervised learning methods substantially on more than 10 benchmarks, regardless of whether data augmentation is applied.",1
"Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.",0
"The ability of deep neural networks to solve complex tasks over large data sets through hierarchical representations is well known. However, understanding these learned representations remains a challenge for humans. This study introduces network dissection, an analytic framework that systematically identifies the semantics of individual hidden units in image classification and generation networks. The framework is first used to analyze a convolutional neural network trained on scene classification, revealing units that correspond to a diverse set of object concepts. These object classes play crucial roles in classifying scene classes. Next, the framework is applied to a generative adversarial network model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, the study finds that objects can be added or removed from the output scenes while adapting to the context. Finally, the framework is used to understand adversarial attacks and semantic image editing.",1
"We present a simple yet effective general-purpose framework for modeling 3D shapes by leveraging recent advances in 2D image generation using CNNs. Using just a single depth image of the object, we can output a dense multi-view depth map representation of 3D objects. Our simple encoder-decoder framework, comprised of a novel identity encoder and class-conditional viewpoint generator, generates 3D consistent depth maps. Our experimental results demonstrate the two-fold advantage of our approach. First, we can directly borrow architectures that work well in the 2D image domain to 3D. Second, we can effectively generate high-resolution 3D shapes with low computational memory. Our quantitative evaluations show that our method is superior to existing depth map methods for reconstructing and synthesizing 3D objects and is competitive with other representations, such as point clouds, voxel grids, and implicit functions.",0
"In this article, we introduce a general-purpose framework for modeling 3D shapes, which is both simple and effective. We achieve this by utilizing recent developments in 2D image generation using CNNs. With only a single depth image of the object, we can produce a comprehensive multi-view depth map representation of 3D objects. Our framework consists of a straightforward encoder-decoder structure, which includes a unique identity encoder and a class-conditional viewpoint generator, to produce consistent 3D depth maps. Our approach has two major benefits: first, we can apply successful 2D image domain architectures to 3D, and second, we can generate high-resolution 3D shapes with minimal computational memory. Our quantitative assessments demonstrate that our method outperforms existing depth map methods in reconstructing and synthesizing 3D objects and is competitive with other representations, such as point clouds, voxel grids, and implicit functions.",1
"Deep neural networks (DNNs) have achieved great success in image classification, but they may be very vulnerable to adversarial attacks with small perturbations to images. Moreover, the adversarial training based on adversarial image samples has been shown to improve the robustness and generalization of DNNs. The aim of this paper is to develop a novel framework based on information-geometry sensitivity analysis and the particle swarm optimization to improve two aspects of adversarial image generation and training for DNNs. The first one is customized generation of adversarial examples. It can design adversarial attacks from options of the number of perturbed pixels, the misclassification probability, and the targeted incorrect class, and hence it is more flexible and effective to locate vulnerable pixels and also enjoys certain adversarial universality. The other is targeted adversarial training. DNN models can be improved in training with the adversarial information using a manifold-based influence measure effective in vulnerable image/pixel detection as well as allowing for targeted attacks, thereby exhibiting an enhanced adversarial defense in testing.",0
"Although deep neural networks (DNNs) have effectively classified images, they are susceptible to adversarial attacks resulting from minor image alterations. To increase DNNs' robustness and generalization, researchers have explored adversarial training using adversarial image samples. This paper introduces a new framework that employs information-geometry sensitivity analysis and particle swarm optimization to improve adversarial image generation and training for DNNs. The framework enhances adversarial attack generation by enabling customized creation of examples with options such as the number of perturbed pixels, misclassification probability, and targeted incorrect class. Additionally, it allows for targeted adversarial training by utilizing a manifold-based influence measure that detects vulnerable images/pixels and facilitates targeted attacks. These improvements result in increased defense against adversarial attacks during testing.",1
"We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity of non-existing people, expression, pose, and illumination. We embed 3D priors into adversarial learning and train the network to imitate the image formation of an analytic 3D face deformation and rendering process. To deal with the generation freedom induced by the domain gap between real and rendered faces, we further introduce contrastive learning to promote disentanglement by comparing pairs of generated images. Experiments show that through our imitative-contrastive learning, the factor variations are very well disentangled and the properties of a generated face can be precisely controlled. We also analyze the learned latent space and present several meaningful properties supporting factor disentanglement. Our method can also be used to embed real images into the disentangled latent space. We hope our method could provide new understandings of the relationship between physical properties and deep image synthesis.",0
"DiscoFaceGAN is our proposed method for creating face images of imaginary people with distinct and accurately adjustable latent representations for their identity, expression, pose, and illumination. We have incorporated 3D priors into our adversarial training to simulate the image creation process of a 3D face deformation and rendering process. To address the freedom of generation that arises from the gap between actual and rendered faces, we have introduced contrastive learning to increase disentanglement by evaluating pairs of created images. Our experiments show that our approach to imitative-contrastive learning has successfully disentangled the factor variations, allowing us to precisely control the properties of a generated face. We have also analyzed the learned latent space and identified several meaningful properties that support factor disentanglement. Additionally, our method can be used to embed real images into the disentangled latent space. We hope that our method can provide new insights into the connection between physical properties and deep image synthesis.",1
"Testing new, innovative technologies is a crucial task for safety and acceptance. But how can new systems be tested if no historical real-world data exist? Simulation provides an answer to this important question. Classical simulation tools such as event-based simulation are well accepted. But most of these established simulation models require the specification of many parameters. Furthermore, simulation runs, e.g., CFD simulations, are very time consuming. Generative Adversarial Networks (GANs) are powerful tools for generating new data for a variety of tasks. Currently, their most frequent application domain is image generation. This article investigates the applicability of GANs for imitating simulations. We are comparing the simulation output of a technical system with the output of a GAN. To exemplify this approach, a well-known multi-car elevator system simulator was chosen. Our study demonstrates the feasibility of this approach. It also discusses pitfalls and technical problems that occurred during the implementation. Although we were able to show that in principle, GANs can be used as substitutes for expensive simulation runs, we also show that they cannot be used ""out of the box"". Fine tuning is needed. We present a proof-of-concept, which can serve as a starting point for further research.",0
"The testing of new and innovative technologies is a crucial aspect in ensuring safety and acceptance. However, the lack of historical real-world data poses a challenge in testing new systems. Simulation provides a solution to this problem, with classical simulation tools such as event-based simulation being well-established. Nevertheless, these simulation models require the specification of many parameters, and simulation runs can be time-consuming. Generative Adversarial Networks (GANs) are powerful tools for generating new data for various tasks, with their primary application being image generation. In this article, we explore the feasibility of using GANs to imitate simulations. Specifically, we compare the simulation output of a technical system with the output of a GAN, using a well-known multi-car elevator system simulator as an example. Our study demonstrates that while GANs can be used as substitutes for expensive simulation runs, they require fine-tuning and cannot be used ""out of the box"". We provide a proof-of-concept that can serve as a starting point for further research, while also discussing the technical problems and pitfalls encountered during the implementation.",1
"We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps. The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fr\'echet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN.",0
"Our proposal introduces a unified Generative Adversarial Network (GAN) that enables controllable image-to-image translation, allowing the transfer of images from a source to a target domain while being guided by controllable structures. Our model can generate images conditioned on controllable structures such as class labels, object keypoints, human skeletons, and scene semantic maps, as well as reference images. Our model comprises a single generator and a discriminator that take a conditional image and the target controllable structure as input. The appearance information is provided by the conditional image, while the controllable structure provides structure information for generating the target result. Our model uses three novel losses, namely color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss, to learn the image-to-image mapping. We also present the Fr\'echet ResNet Distance (FRD) to evaluate the quality of the generated images. Our model outperforms other state-of-the-art methods in two challenging image translation tasks, namely hand gesture-to-gesture translation and cross-view image translation. Furthermore, our framework can be used to solve other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. Our proposal is the first to introduce a GAN framework that works on all such controllable structure guided image translation tasks. The code is available at https://github.com/Ha0Tang/GestureGAN.",1
"In this paper, we observe that most false positive images (i.e., different identities with query images) in the top ranking list usually have the similar color information with the query image in person re-identification (Re-ID). Meanwhile, when we use the greyscale images generated from RGB images to conduct the person Re-ID task, some hard query images can obtain better performance compared with using RGB images. Therefore, RGB and greyscale images seem to be complementary to each other for person Re-ID. In this paper, we aim to utilize both RGB and greyscale images to improve the person Re-ID performance. To this end, we propose a novel two-stream deep neural network with RGB-grey information, which can effectively fuse RGB and greyscale feature representations to enhance the generalization ability of Re-ID. Firstly, we convert RGB images to greyscale images in each training batch. Based on these RGB and greyscale images, we train the RGB and greyscale branches, respectively. Secondly, to build up connections between RGB and greyscale branches, we merge the RGB and greyscale branches into a new joint branch. Finally, we concatenate the features of all three branches as the final feature representation for Re-ID. Moreover, in the training process, we adopt the joint learning scheme to simultaneously train each branch by the independent loss function, which can enhance the generalization ability of each branch. Besides, a global loss function is utilized to further fine-tune the final concatenated feature. The extensive experiments on multiple benchmark datasets fully show that the proposed method can outperform the state-of-the-art person Re-ID methods. Furthermore, using greyscale images can indeed improve the person Re-ID performance.",0
The paper observes that false positive images in person re-identification often have similar color information to the query image. It also notes that using greyscale images instead of RGB images can improve performance in some cases. The authors propose a two-stream deep neural network that utilizes both RGB and greyscale images to enhance the generalization ability of Re-ID. The RGB and greyscale branches are trained separately and then merged into a joint branch. The final feature representation for Re-ID is obtained by concatenating the features of all three branches. The proposed method is shown to outperform state-of-the-art Re-ID methods on multiple benchmark datasets. The use of greyscale images is found to be beneficial for Re-ID performance.,1
"Pose-guided person image generation and animation aim to transform a source person image to target poses. These tasks require spatial manipulation of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. This framework first estimates global flow fields between sources and targets. Then, corresponding local source feature patches are sampled with content-aware local attention coefficients. We show that our framework can spatially transform the inputs in an efficient manner. Meanwhile, we further model the temporal consistency for the person image animation task to generate coherent videos. The experiment results of both image generation and animation tasks demonstrate the superiority of our model. Besides, additional results of novel view synthesis and face image animation show that our model is applicable to other tasks requiring spatial transformation. The source code of our project is available at https://github.com/RenYurui/Global-Flow-Local-Attention.",0
"The objective of pose-guided person image generation and animation is to convert a source image of a person to target poses, which involves spatial manipulation of the source data. However, Convolutional Neural Networks are limited in their ability to spatially transform inputs. This paper proposes a differentiable global-flow local-attention framework that reassembles the inputs at the feature level. The framework estimates global flow fields between sources and targets and samples corresponding local source feature patches with content-aware local attention coefficients. The proposed framework efficiently transforms inputs while also modeling temporal consistency to generate coherent videos in the person image animation task. The experiment results demonstrate the superiority of the proposed model for both image generation and animation tasks. Additionally, the model is applicable to other tasks that require spatial transformation, as demonstrated by the results of novel view synthesis and face image animation. The source code for the project is available at https://github.com/RenYurui/Global-Flow-Local-Attention.",1
"Recent approaches have achieved great success in image generation from structured inputs, e.g., semantic segmentation, scene graph or layout. Although these methods allow specification of objects and their locations at image-level, they lack the fidelity and semantic control to specify visual appearance of these objects at an instance-level. To address this limitation, we propose a new image generation method that enables instance-level attribute control. Specifically, the input to our attribute-guided generative model is a tuple that contains: (1) object bounding boxes, (2) object categories and (3) an (optional) set of attributes for each object. The output is a generated image where the requested objects are in the desired locations and have prescribed attributes. Several losses work collaboratively to encourage accurate, consistent and diverse image generation. Experiments on Visual Genome dataset demonstrate our model's capacity to control object-level attributes in generated images, and validate plausibility of disentangled object-attribute representation in the image generation from layout task. Also, the generated images from our model have higher resolution, object classification accuracy and consistency, as compared to the previous state-of-the-art.",0
"Image generation from structured inputs, such as semantic segmentation, scene graph or layout, has been successful in recent approaches. These methods allow for object specification and location at the image-level, but lack the ability to specify visual appearance at an instance-level. To overcome this limitation, we propose a new image generation method that enables instance-level attribute control. Our attribute-guided generative model takes in a tuple consisting of object bounding boxes, object categories and optional attributes for each object. The output is a generated image with requested objects in desired locations and prescribed attributes. Multiple losses encourage accurate, consistent and diverse image generation. Our experiments on the Visual Genome dataset demonstrate the model's capacity to control object-level attributes and validate the plausibility of disentangled object-attribute representation. Additionally, our model generates higher resolution images with improved object classification accuracy and consistency compared to the previous state-of-the-art.",1
"Cosplay has grown from its origins at fan conventions into a billion-dollar global dress phenomenon. To facilitate imagination and reinterpretation from animated images to real garments, this paper presents an automatic costume image generation method based on image-to-image translation. Cosplay items can be significantly diverse in their styles and shapes, and conventional methods cannot be directly applied to the wide variation in clothing images that are the focus of this study. To solve this problem, our method starts by collecting and preprocessing web images to prepare a cleaned, paired dataset of the anime and real domains. Then, we present a novel architecture for generative adversarial networks (GANs) to facilitate high-quality cosplay image generation. Our GAN consists of several effective techniques to fill the gap between the two domains and improve both the global and local consistency of generated images. Experiments demonstrated that, with two types of evaluation metrics, the proposed GAN achieves better performance than existing methods. We also showed that the images generated by the proposed method are more realistic than those generated by the conventional methods. Our codes and pretrained model are available on the web.",0
"The popularity of cosplay has expanded beyond fan conventions and become a lucrative global fashion trend. This study aims to aid in the creation of realistic costumes by presenting a method for generating costume images through image-to-image translation. Due to the vast variety of styles and shapes in cosplay garments, conventional methods are not suitable for this study. To overcome this, we collect and preprocess web images to create a paired dataset of anime and real-world images. Our novel generative adversarial network (GAN) architecture effectively bridges the gap between the two domains and enhances the global and local consistency of the generated images. Our experiments demonstrate that our GAN outperforms existing methods using two evaluation metrics and produces more realistic images. Our codes and pretrained model are available online.",1
"Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.",0
"While adversarial learning approaches have been recommended for a broad range of applications, it is widely known that training adversarial models can be quite unstable. It is crucial to effectively balance the performance of both the generator and discriminator since a discriminator that achieves high accuracy produces less informative gradients. In this study, we suggest a straightforward and universal technique for limiting information flow in the discriminator using an information bottleneck. By implementing a restriction on the mutual information between observations and the discriminator's internal representation, we can regulate the accuracy of the discriminator and maintain meaningful and informative gradients. Our proposed variational discriminator bottleneck (VDB) results in substantial improvements across three distinct application domains for adversarial learning algorithms. Our primary assessment examines the VDB's practicality in teaching dynamic continuous control abilities, such as running, through imitation learning from raw video demonstrations. We demonstrate that our approach outperforms previous adversarial imitation learning techniques by a significant margin. The VDB can also be used with adversarial inverse reinforcement learning to teach parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we show that VDB can more effectively train GANs for image generation, surpassing a variety of previous stabilization techniques.",1
"The quality of image generation and manipulation is reaching impressive levels, making it increasingly difficult for a human to distinguish between what is real and what is fake. However, deep networks can still pick up on the subtle artifacts in these doctored images. We seek to understand what properties of fake images make them detectable and identify what generalizes across different model architectures, datasets, and variations in training. We use a patch-based classifier with limited receptive fields to visualize which regions of fake images are more easily detectable. We further show a technique to exaggerate these detectable properties and demonstrate that, even when the image generator is adversarially finetuned against a fake image classifier, it is still imperfect and leaves detectable artifacts in certain image patches. Code is available at https://chail.github.io/patch-forensics/.",0
"As the quality of image generation and manipulation continues to improve, it is becoming more challenging for humans to differentiate between real and fake images. However, deep networks can still identify subtle artifacts present in doctored images. Our objective is to comprehend the distinguishing features of fake images that make them detectable and to determine what is generalizable across different model architectures, datasets, and variations in training. To visualize the regions of fake images that are more easily detectable, we use a patch-based classifier with limited receptive fields. Additionally, we demonstrate a method to magnify these detectable properties and show that even when the image generator is adversarially fine-tuned against a fake image classifier, it is still imperfect and leaves detectable artifacts in certain image patches. The code is accessible at https://chail.github.io/patch-forensics/.",1
"Generating realistic images of complex visual scenes becomes challenging when one wishes to control the structure of the generated images. Previous approaches showed that scenes with few entities can be controlled using scene graphs, but this approach struggles as the complexity of the graph (the number of objects and edges) increases. In this work, we show that one limitation of current methods is their inability to capture semantic equivalence in graphs. We present a novel model that addresses these issues by learning canonical graph representations from the data, resulting in improved image generation for complex visual scenes. Our model demonstrates improved empirical performance on large scene graphs, robustness to noise in the input scene graph, and generalization on semantically equivalent graphs. Finally, we show improved performance of the model on three different benchmarks: Visual Genome, COCO, and CLEVR.",0
"It is challenging to generate realistic images of intricate visual scenes while maintaining control over their structure. Earlier methods proved successful in controlling scenes with limited entities using scene graphs, but encountered difficulties when dealing with more complex graphs containing a greater number of objects and edges. Our research reveals how current techniques face limitations in capturing semantic equivalence in graphs, further exacerbating the issue. To address these problems, we present a new model that learns canonical graph representations from data, resulting in improved image generation for complex visual scenes. Our model exhibits superior empirical performance on large scene graphs, is resistant to noise in input scene graphs, and generalizes well on semantically equivalent graphs. Finally, our model outperforms existing methods on three different benchmarks: Visual Genome, COCO, and CLEVR.",1
"We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the challenging person image generation task. The proposed graph generator mainly consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed Bipartite Graph Reasoning (BGR) block aims to reason the crossing long-range relations between the source pose and the target pose in a bipartite graph, which mitigates some challenges caused by pose deformation. Moreover, we propose a new Interaction-and-Aggregation (IA) block to effectively update and enhance the feature representation capability of both person's shape and appearance in an interactive way. Experiments on two challenging and public datasets, i.e., Market-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN.",0
"We introduce a new approach called BiGraphGAN for generating person images, which is a difficult task. Our graph generator is made up of two unique blocks that model the relationships between poses and images. The BGR block is designed to reason about the long-range relations between poses in a bipartite graph, which helps to address issues caused by pose deformation. Additionally, we propose the IA block to update and enhance the representation capability of both person's shape and appearance in an interactive manner. We tested BiGraphGAN on two public and challenging datasets, Market-1501 and DeepFashion, and found that it performs well in terms of objective and subjective scores. Our source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN.",1
"Pose variation is one of the key factors which prevents the network from learning a robust person re-identification (Re-ID) model. To address this issue, we propose a novel person pose-guided image generation method, which is called the semantic attention network. The network consists of several semantic attention blocks, where each block attends to preserve and update the pose code and the clothing textures. The introduction of the binary segmentation mask and the semantic parsing is important for seamlessly stitching foreground and background in the pose-guided image generation. Compared with other methods, our network can characterize better body shape and keep clothing attributes, simultaneously. Our synthesized image can obtain better appearance and shape consistency related to the original image. Experimental results show that our approach is competitive with respect to both quantitative and qualitative results on Market-1501 and DeepFashion. Furthermore, we conduct extensive evaluations by using person re-identification (Re-ID) systems trained with the pose-transferred person based augmented data. The experiment shows that our approach can significantly enhance the person Re-ID accuracy.",0
"The lack of pose variation is a major obstacle in the development of a reliable person re-identification (Re-ID) model. Our solution to this problem is the semantic attention network, a unique method for generating images that are guided by a person's pose. This network is composed of multiple semantic attention blocks that focus on maintaining and updating the pose code and clothing textures. The inclusion of binary segmentation masks and semantic parsing is crucial for seamlessly combining the foreground and background in the pose-guided image generation process. Our network is superior to other methods in terms of accurately characterizing body shape and preserving clothing attributes. Our synthesized images exhibit better appearance and shape consistency compared to the original images. Our approach was experimentally evaluated using both quantitative and qualitative measures on Market-1501 and DeepFashion datasets, yielding competitive results. Additionally, we conducted extensive evaluations using Re-ID systems trained with pose-transferred person-based augmented data, which demonstrated a significant enhancement in the person Re-ID accuracy.",1
"At this moment, GAN-based image generation methods are still imperfect, whose upsampling design has limitations in leaving some certain artifact patterns in the synthesized image. Such artifact patterns can be easily exploited (by recent methods) for difference detection of real and GAN-synthesized images. However, the existing detection methods put much emphasis on the artifact patterns, which can become futile if such artifact patterns were reduced. Towards reducing the artifacts in the synthesized images, in this paper, we devise a simple yet powerful approach termed FakePolisher that performs shallow reconstruction of fake images through a learned linear dictionary, intending to effectively and efficiently reduce the artifacts introduced during image synthesis. The comprehensive evaluation on 3 state-of-the-art DeepFake detection methods and fake images generated by 16 popular GAN-based fake image generation techniques, demonstrates the effectiveness of our technique.Overall, through reducing artifact patterns, our technique significantly reduces the accuracy of the 3 state-of-the-art fake image detection methods, i.e., 47% on average and up to 93% in the worst case.",0
"GAN-based image generation methods are not yet perfect as their upsampling design has limitations that result in certain artifact patterns in the synthesized image. These patterns can be easily detected by recent methods to distinguish between real and GAN-synthesized images. However, existing detection methods focus too much on these artifact patterns, which could be ineffective if the patterns are reduced. This paper proposes a simple and powerful approach called FakePolisher to reduce artifacts in synthesized images by performing shallow reconstruction of fake images using a learned linear dictionary. The technique was evaluated using 3 state-of-the-art DeepFake detection methods and 16 popular GAN-based fake image generation techniques, which demonstrated its effectiveness in reducing artifact patterns. Overall, the technique significantly reduces the accuracy of the 3 state-of-the-art fake image detection methods, with an average reduction of 47% and up to 93% in the worst case, by reducing artifact patterns.",1
"Lane marker extraction is a basic yet necessary task for autonomous driving. Although past years have witnessed major advances in lane marker extraction with deep learning models, they all aim at ordinary RGB images generated by frame-based cameras, which limits their performance in extreme cases, like huge illumination change. To tackle this problem, we introduce Dynamic Vision Sensor (DVS), a type of event-based sensor to lane marker extraction task and build a high-resolution DVS dataset for lane marker extraction. We collect the raw event data and generate 5,424 DVS images with a resolution of 1280$\times$800 pixels, the highest one among all DVS datasets available now. All images are annotated with multi-class semantic segmentation format. We then propose a structure-aware network for lane marker extraction in DVS images. It can capture directional information comprehensively with multidirectional slice convolution. We evaluate our proposed network with other state-of-the-art lane marker extraction models on this dataset. Experimental results demonstrate that our method outperforms other competitors. The dataset is made publicly available, including the raw event data, accumulated images and labels.",0
"Autonomous driving requires the fundamental task of lane marker extraction, which has seen significant improvements with deep learning models in recent years. However, these models only work with ordinary RGB images captured by frame-based cameras, which limits their ability to perform in extreme scenarios such as significant changes in illumination. To overcome this challenge, we present the use of Dynamic Vision Sensor (DVS), an event-based sensor, for lane marker extraction and create a high-resolution DVS dataset. This dataset comprises 5,424 DVS images with a resolution of 1280$\times$800 pixels, the highest resolution among all DVS datasets currently available. All images are annotated using multi-class semantic segmentation format. We propose a structure-aware network that uses multidirectional slice convolution to capture directional information comprehensively for lane marker extraction in DVS images. Our method outperforms other state-of-the-art models for lane marker extraction, as demonstrated in our experimental results. We have made the dataset publicly available, including the raw event data, accumulated images, and labels.",1
"When designing a semantic segmentation module for a practical application, such as autonomous driving, it is crucial to understand the robustness of the module with respect to a wide range of image corruptions. While there are recent robustness studies for full-image classification, we are the first to present an exhaustive study for semantic segmentation, based on the state-of-the-art model DeepLabv3+. To increase the realism of our study, we utilize almost 400,000 images generated from Cityscapes, PASCAL VOC 2012, and ADE20K. Based on the benchmark study, we gain several new insights. Firstly, contrary to full-image classification, model robustness increases with model performance, in most cases. Secondly, some architecture properties affect robustness significantly, such as a Dense Prediction Cell, which was designed to maximize performance on clean data only.",0
"It is essential to comprehend the strength of a semantic segmentation module in various image corruptions when developing it for a practical use, like autonomous driving. Robustness studies on full-image classification have been conducted, but we present the first comprehensive study for semantic segmentation using the contemporary model, DeepLabv3+. Our study uses nearly 400,000 images from Cityscapes, PASCAL VOC 2012, and ADE20K to enhance realism. Our benchmark study provides new insights, including the fact that, in most cases, model robustness increases with model performance, unlike full-image classification. Additionally, some architecture properties, such as a Dense Prediction Cell, significantly affect robustness, which is designed to optimize performance on clean data.",1
"Road extraction in remote sensing images is of great importance for a wide range of applications. Because of the complex background, and high density, most of the existing methods fail to accurately extract a road network that appears correct and complete. Moreover, they suffer from either insufficient training data or high costs of manual annotation. To address these problems, we introduce a new model to apply structured domain adaption for synthetic image generation and road segmentation. We incorporate a feature pyramid network into generative adversarial networks to minimize the difference between the source and target domains. A generator is learned to produce quality synthetic images, and the discriminator attempts to distinguish them. We also propose a feature pyramid network that improves the performance of the proposed model by extracting effective features from all the layers of the network for describing different scales objects. Indeed, a novel scale-wise architecture is introduced to learn from the multi-level feature maps and improve the semantics of the features. For optimization, the model is trained by a joint reconstruction loss function, which minimizes the difference between the fake images and the real ones. A wide range of experiments on three datasets prove the superior performance of the proposed approach in terms of accuracy and efficiency. In particular, our model achieves state-of-the-art 78.86 IOU on the Massachusetts dataset with 14.89M parameters and 86.78B FLOPs, with 4x fewer FLOPs but higher accuracy (+3.47% IOU) than the top performer among state-of-the-art approaches used in the evaluation.",0
"The extraction of roads in remote sensing images has immense significance for various applications. However, due to the intricacy of the background and the high density, most existing methods are incapable of precisely extracting a complete and accurate road network. Furthermore, they face challenges such as inadequate training data and expensive manual annotation. To overcome these obstacles, we introduce a new model that leverages structured domain adaptation for synthetic image generation and road segmentation. Our approach incorporates a feature pyramid network into generative adversarial networks to minimize the gap between the source and target domains. We train a generator to produce high-quality synthetic images while a discriminator attempts to differentiate between them. Additionally, we propose a feature pyramid network that enhances the model's performance by extracting effective features from all layers of the network to describe objects of varying scales. The model is optimized with a joint reconstruction loss function that minimizes the difference between fake and real images. Our experiments on three datasets demonstrate the superior accuracy and efficiency of our proposed approach. Notably, our model achieves a state-of-the-art 78.86 IOU on the Massachusetts dataset, with 14.89M parameters and 86.78B FLOPs, using 4x fewer FLOPs but achieving higher accuracy (+3.47% IOU) than the top performer among state-of-the-art approaches.",1
"Creating fake images and videos such as ""Deepfake"" has become much easier these days due to the advancement in Generative Adversarial Networks (GANs). Moreover, recent research such as the few-shot learning can create highly realistic personalized fake images with only a few images. Therefore, the threat of Deepfake to be used for a variety of malicious intents such as propagating fake images and videos becomes prevalent. And detecting these machine-generated fake images has been quite challenging than ever. In this work, we propose a light-weight robust fine-tuning neural network-based classifier architecture called Fake Detection Fine-tuning Network (FDFtNet), which is capable of detecting many of the new fake face image generation models, and can be easily combined with existing image classification networks and finetuned on a few datasets. In contrast to many existing methods, our approach aims to reuse popular pre-trained models with only a few images for fine-tuning to effectively detect fake images. The core of our approach is to introduce an image-based self-attention module called Fine-Tune Transformer that uses only the attention module and the down-sampling layer. This module is added to the pre-trained model and fine-tuned on a few data to search for new sets of feature space to detect fake images. We experiment with our FDFtNet on the GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset (Deepfake and Face2Face) with a small input image resolution of 64x64 that complicates detection. Our FDFtNet achieves an overall accuracy of 90.29% in detecting fake images generated from the GANs-based dataset, outperforming the state-of-the-art.",0
"The use of Generative Adversarial Networks (GANs) has made it easier to create fake images and videos, including the highly realistic and personalized ""Deepfake"" technology. This poses a significant threat as these fake images can be used maliciously. Unfortunately, detecting these machine-generated fake images has become more challenging than ever before. To address this issue, we introduce a new approach called Fake Detection Fine-tuning Network (FDFtNet) that is a light-weight robust fine-tuning neural network-based classifier architecture. Our approach is capable of detecting many new fake face image generation models and can be easily combined with existing image classification networks. Unlike other methods, we aim to reuse popular pre-trained models, which can be fine-tuned with only a few images to effectively detect fake images. Our approach introduces an image-based self-attention module called Fine-Tune Transformer, which uses only the attention module and the down-sampling layer. This module is added to the pre-trained model and fine-tuned on a few data to search for new sets of feature space to detect fake images. We experiment with our FDFtNet on the GANs-based dataset (Progressive Growing GAN) and Deepfake-based dataset (Deepfake and Face2Face) with a small input image resolution of 64x64. Our FDFtNet achieves an overall accuracy of 90.29% in detecting fake images generated from the GANs-based dataset, which outperforms the state-of-the-art.",1
"Fast Style Transfer is a series of Neural Style Transfer algorithms that use feed-forward neural networks to render input images. Because of the high dimension of the output layer, these networks require much memory for computation. Therefore, for high-resolution images, most mobile devices and personal computers cannot stylize them, which greatly limits the application scenarios of Fast Style Transfer. At present, the two existing solutions are purchasing more memory and using the feathering-based method, but the former requires additional cost, and the latter has poor image quality. To solve this problem, we propose a novel image synthesis method named \emph{block shuffle}, which converts a single task with high memory consumption to multiple subtasks with low memory consumption. This method can act as a plug-in for Fast Style Transfer without any modification to the network architecture. We use the most popular Fast Style Transfer repository on GitHub as the baseline. Experiments show that the quality of high-resolution images generated by our method is better than that of the feathering-based method. Although our method is an order of magnitude slower than the baseline, it can stylize high-resolution images with limited memory, which is impossible with the baseline. The code and models will be made available on \url{https://github.com/czczup/block-shuffle}.",0
"The Fast Style Transfer comprises Neural Style Transfer algorithms that utilize feed-forward neural networks to create output images. However, due to the large size of the output layer, these networks demand substantial memory for computation. Consequently, mobile devices and personal computers cannot stylize high-resolution images, which limits the scope of Fast Style Transfer. Two current options are buying additional memory, which incurs extra costs, and using the feathering-based method, which results in poor image quality. To tackle this issue, we propose a new image synthesis approach called ""block shuffle,"" which transforms a high memory-consuming task into multiple low memory-consuming subtasks. This method can seamlessly integrate with Fast Style Transfer without any alteration to the network architecture. We employed the most popular Fast Style Transfer repository from GitHub as the benchmark and found that our technique generates better quality high-resolution images than the feathering-based method. Although our method is slower than the baseline by an order of magnitude, it can stylize high-resolution images within limited memory, which is infeasible with the baseline. Please find the code and models on \url{https://github.com/czczup/block-shuffle}.",1
"Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by humans) from real images. Most images so generated are not contained in the training dataset, suggesting potential for augmenting training sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to facilitate generation in a perceptually-distinct target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data.",0
"Recent research has revealed that generative adversarial networks (GANs) have the capability to produce highly realistic images that are often impossible to differentiate from real images by humans. This suggests that GAN-generated data could potentially be used to augment training sets, particularly when data is limited. However, the challenge remains of training the GAN with limited data. To overcome this challenge, we use existing GAN models that have been pre-trained on large-scale datasets, such as ImageNet, to introduce additional knowledge that may not exist within the limited data, utilizing the concept of transfer learning. Our experiments show that low-level filters of both the generator and discriminator of pre-trained GANs can be transferred to facilitate generation in a target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). We present an extensive set of experiments to demonstrate the effectiveness of our proposed techniques for generation with limited data.",1
"Generative adversarial networks (GAN) have shown remarkable results in image generation tasks. High fidelity class-conditional GAN methods often rely on stabilization techniques by constraining the global Lipschitz continuity. Such regularization leads to less expressive models and slower convergence speed; other techniques, such as the large batch training, require unconventional computing power and are not widely accessible. In this paper, we develop an efficient algorithm, namely FastGAN (Free AdverSarial Training), to improve the speed and quality of GAN training based on the adversarial training technique. We benchmark our method on CIFAR10, a subset of ImageNet, and the full ImageNet datasets. We choose strong baselines such as SNGAN and SAGAN; the results demonstrate that our training algorithm can achieve better generation quality (in terms of the Inception score and Frechet Inception distance) with less overall training time. Most notably, our training algorithm brings ImageNet training to the broader public by requiring 2-4 GPUs.",0
"The use of generative adversarial networks (GAN) has produced impressive outcomes in image generation tasks. However, the effectiveness of high fidelity class-conditional GAN methods has often relied on stabilization techniques that limit global Lipschitz continuity. While such regularization techniques result in less expressive models and slower convergence, other methods such as large batch training require unconventional computing resources that are not widely available. In this study, we introduce FastGAN (Free AdverSarial Training), an efficient algorithm that enhances the quality and speed of GAN training using adversarial training techniques. We evaluated FastGAN on CIFAR10, a subset of ImageNet, and the full ImageNet datasets, using strong baselines including SNGAN and SAGAN. Our results demonstrate that FastGAN achieves better generation quality, as measured by the Inception score and Frechet Inception distance, while requiring less overall training time. Notably, our algorithm makes ImageNet training accessible to a broader audience, requiring only 2-4 GPUs.",1
"Advances in Artificial Intelligence and Image Processing are changing the way people interacts with digital images and video. Widespread mobile apps like FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to produce extreme transformations on human face photos such gender swap, aging, etc. The results are utterly realistic and extremely easy to be exploited even for non-experienced users. This kind of media object took the name of Deepfake and raised a new challenge in the multimedia forensics field: the Deepfake detection challenge. Indeed, discriminating a Deepfake from a real image could be a difficult task even for human eyes but recent works are trying to apply the same technology used for generating images for discriminating them with preliminary good results but with many limitations: employed Convolutional Neural Networks are not so robust, demonstrate to be specific to the context and tend to extract semantics from images. In this paper, a new approach aimed to extract a Deepfake fingerprint from images is proposed. The method is based on the Expectation-Maximization algorithm trained to detect and extract a fingerprint that represents the Convolutional Traces (CT) left by GANs during image generation. The CT demonstrates to have high discriminative power achieving better results than state-of-the-art in the Deepfake detection task also proving to be robust to different attacks. Achieving an overall classification accuracy of over 98%, considering Deepfakes from 10 different GAN architectures not only involved in images of faces, the CT demonstrates to be reliable and without any dependence on image semantic. Finally, tests carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the fake detection task, demonstrated the effectiveness of the proposed technique on a real-case scenario.",0
"Digital images and video are being transformed by advancements in Artificial Intelligence and Image Processing. Tools like FACEAPP, which use advanced Generative Adversarial Networks (GAN), have made it easy for even novice users to produce realistic transformations on human face photos, including gender swaps and aging. This type of media is known as Deepfake and is a challenge for multimedia forensics as it can be difficult to distinguish from real images. While some attempts have been made to use Convolutional Neural Networks to discriminate between Deepfakes and real images, there are limitations to this approach. In this study, a new method is proposed using the Expectation-Maximization algorithm to detect and extract a fingerprint representing the Convolutional Traces (CT) left by GANs during image generation. Results show that CT has high discriminative power with an overall classification accuracy of over 98% and is robust to different attacks. Tests on Deepfakes generated by FACEAPP achieved 93% accuracy, demonstrating the effectiveness of the proposed technique in real-world scenarios.",1
"Drone racing is a recreational sport in which the goal is to pass through a sequence of gates in a minimum amount of time while avoiding collisions. In autonomous drone racing, one must accomplish this task by flying fully autonomously in an unknown environment by relying only on computer vision methods for detecting the target gates. Due to the challenges such as background objects and varying lighting conditions, traditional object detection algorithms based on colour or geometry tend to fail. Convolutional neural networks offer impressive advances in computer vision but require an immense amount of data to learn. Collecting this data is a tedious process because the drone has to be flown manually, and the data collected can suffer from sensor failures. In this work, a semi-synthetic dataset generation method is proposed, using a combination of real background images and randomised 3D renders of the gates, to provide a limitless amount of training samples that do not suffer from those drawbacks. Using the detection results, a line-of-sight guidance algorithm is used to cross the gates. In several experimental real-time tests, the proposed framework successfully demonstrates fast and reliable detection and navigation.",0
"Drone racing is an enjoyable pastime that involves maneuvering through a series of gates as quickly as possible while avoiding collisions. In autonomous drone racing, the aim is to accomplish this task by relying solely on computer vision techniques to detect the target gates and fly autonomously in an unfamiliar environment. However, conventional object detection algorithms based on color or geometry often fail due to challenges such as background objects and varying lighting conditions. Convolutional neural networks have made significant strides in computer vision, but they require vast amounts of data to learn. Gathering this data is a tedious process because the drone must be flown manually, and the data obtained may be affected by sensor failures. This study proposes a semi-synthetic dataset generation technique that combines real background images with random 3D renders of gates to provide an endless supply of training samples that are unaffected by these drawbacks. A line-of-sight guidance algorithm is then utilized to navigate through the gates using the detection results. The proposed framework demonstrates fast and reliable detection and navigation in multiple real-time experiments.",1
"In order to generate images for a given category, existing deep generative models generally rely on abundant training images. However, extensive data acquisition is expensive and fast learning ability from limited data is necessarily required in real-world applications. Also, these existing methods are not well-suited for fast adaptation to a new category.   Few-shot image generation, aiming to generate images from only a few images for a new category, has attracted some research interest. In this paper, we propose a Fusing-and-Filling Generative Adversarial Network (F2GAN) to generate realistic and diverse images for a new category with only a few images. In our F2GAN, a fusion generator is designed to fuse the high-level features of conditional images with random interpolation coefficients, and then fills in attended low-level details with non-local attention module to produce a new image. Moreover, our discriminator can ensure the diversity of generated images by a mode seeking loss and an interpolation regression loss. Extensive experiments on five datasets demonstrate the effectiveness of our proposed method for few-shot image generation.",0
"Typically, current deep generative models require a large amount of training images to produce images for a specific category. However, obtaining this data can be costly and real-world applications require the ability to learn quickly from limited data. Additionally, these methods do not adapt well to new categories. Therefore, there has been interest in few-shot image generation, which can generate images from just a few images for a new category. This paper presents the Fusing-and-Filling Generative Adversarial Network (F2GAN), which can create realistic and diverse images for a new category with only a few images. The F2GAN employs a fusion generator that combines high-level features with random interpolation coefficients, and a non-local attention module to add in low-level details. The discriminator ensures diversity with a mode seeking loss and an interpolation regression loss. Our experiments with five datasets show that our method is effective for few-shot image generation.",1
"Although Generative Adversarial Networks have shown remarkable performance in image generation, there are some challenges in image realism and convergence speed. The results of some models display the imbalances of quality within a generated image, in which some defective parts appear compared with other regions. Different from general single global optimization methods, we introduce an adaptive global and local bilevel optimization model(GL-GAN). The model achieves the generation of high-resolution images in a complementary and promoting way, where global optimization is to optimize the whole images and local is only to optimize the low-quality areas. With a simple network structure, GL-GAN is allowed to effectively avoid the nature of imbalance by local bilevel optimization, which is accomplished by first locating low-quality areas and then optimizing them. Moreover, by using feature map cues from discriminator output, we propose the adaptive local and global optimization method(Ada-OP) for specific implementation and find that it boosts the convergence speed. Compared with the current GAN methods, our model has shown impressive performance on CelebA, CelebA-HQ and LSUN datasets.",0
"Despite the impressive performance of Generative Adversarial Networks in generating images, challenges still remain in achieving image realism and convergence speed. Some models produce images with imbalanced quality, wherein certain regions may contain defective parts compared to others. In contrast to general single global optimization methods, we propose the use of an adaptive global and local bilevel optimization model (GL-GAN) that generates high-resolution images in a complementary and promoting manner. The GL-GAN model optimizes the entire image globally, while only optimizing low-quality areas locally. Its simple network structure effectively addresses the issue of imbalance through local bilevel optimization, which involves identifying and optimizing low-quality areas. We also introduce the adaptive local and global optimization method (Ada-OP), which uses feature map cues from the discriminator output to enhance convergence speed. Our model has demonstrated impressive performance in generating images on CelebA, CelebA-HQ, and LSUN datasets, surpassing current GAN methods.",1
"Most existing text-to-image synthesis tasks are static single-turn generation, based on pre-defined textual descriptions of images. To explore more practical and interactive real-life applications, we introduce a new task - Interactive Image Editing, where users can guide an agent to edit images via multi-turn textual commands on-the-fly. In each session, the agent takes a natural language description from the user as the input and modifies the image generated in the previous turn to a new design, following the user description. The main challenges in this sequential and interactive image generation task are two-fold: 1) contextual consistency between a generated image and the provided textual description; 2) step-by-step region-level modification to maintain visual consistency across the generated image sequence in each session. To address these challenges, we propose a novel Sequential Attention Generative Adversarial Net-work (SeqAttnGAN), which applies a neural state tracker to encode the previous image and the textual description in each turn of the sequence, and uses a GAN framework to generate a modified version of the image that is consistent with the preceding images and coherent with the description. To achieve better region-specific refinement, we also introduce a sequential attention mechanism into the model. To benchmark on the new task, we introduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain multi-turn sessions with image-description sequences in the fashion domain. Experiments on both datasets show that the proposed SeqAttnGANmodel outperforms state-of-the-art approaches on the interactive image editing task across all evaluation metrics including visual quality, image sequence coherence, and text-image consistency.",0
"Currently, most text-to-image synthesis tasks involve creating static images based on predetermined textual descriptions. In order to explore more practical and interactive real-life applications, we have introduced a new task called Interactive Image Editing. This task allows users to guide an agent in real-time to edit images through multi-turn textual commands. During each session, the agent receives a natural language description from the user and modifies the image generated in the previous turn accordingly. However, this sequential and interactive image generation task presents two main challenges: ensuring contextual consistency and maintaining visual consistency across the generated image sequence. To address these challenges, we propose a novel Sequential Attention Generative Adversarial Network (SeqAttnGAN) that includes a neural state tracker and a GAN framework to generate modified images that are consistent with preceding images and coherent with the textual description. We also introduce a sequential attention mechanism to achieve better region-specific refinement. To benchmark this new task, we have created two new datasets called Zap-Seq and DeepFashion-Seq that contain multi-turn sessions with image-description sequences in the fashion domain. Our experiments show that the proposed SeqAttnGAN model outperforms state-of-the-art approaches on the interactive image editing task in terms of visual quality, image sequence coherence, and text-image consistency.",1
"We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively ""de-freeze"" the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales.Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN. Source codes are available at https://github.com/duxingren14/BSD-GAN.",0
"BSD-GAN is a new approach to training Generative Adversarial Networks (GANs) that allows them to learn image representations at multiple scales, making them useful for a wide range of generation and editing tasks. The key feature of BSD-GAN is its use of multiple branches to train the network, covering both breadth and depth as resolutions of training images increase to reveal finer-scale features. Each noise vector is split into sub-vectors, each corresponding to a specific scale and trained to learn image representations at that scale. During training, sub-vectors are progressively ""de-freezed"" as higher-resolution images are used for training and more network layers are added. This explicit sub-vector designation allows for direct manipulation and combination of latent codes that model different feature scales. Extensive experiments have shown that BSD-GAN is effective in scale-disentangled learning of image representations and synthesis of novel image contents without any extra labels and without compromising the quality of synthesized high-resolution images. Several image generation and manipulation applications have been enabled or improved by BSD-GAN. The source codes for BSD-GAN are available at https://github.com/duxingren14/BSD-GAN.",1
"Single image deraining regards an input image as a fusion of a background image, a transmission map, rain streaks, and atmosphere light. While advanced models are proposed for image restoration (i.e., background image generation), they regard rain streaks with the same properties as background rather than transmission medium. As vapors (i.e., rain streaks accumulation or fog-like rain) are conveyed in the transmission map to model the veiling effect, the fusion of rain streaks and vapors do not naturally reflect the rain image formation. In this work, we reformulate rain streaks as transmission medium together with vapors to model rain imaging. We propose an encoder-decoder CNN named as SNet to learn the transmission map of rain streaks. As rain streaks appear with various shapes and directions, we use ShuffleNet units within SNet to capture their anisotropic representations. As vapors are brought by rain streaks, we propose a VNet containing spatial pyramid pooling (SSP) to predict the transmission map of vapors in multi-scales based on that of rain streaks. Meanwhile, we use an encoder CNN named ANet to estimate atmosphere light. The SNet, VNet, and ANet are jointly trained to predict transmission maps and atmosphere light for rain image restoration. Extensive experiments on the benchmark datasets demonstrate the effectiveness of the proposed visual model to predict rain streaks and vapors. The proposed deraining method performs favorably against state-of-the-art deraining approaches.",0
"The concept of single image deraining involves breaking down an input image into its constituent parts, which include a background image, transmission map, rain streaks, and atmosphere light. However, current models for image restoration do not accurately distinguish between rain streaks and the background, treating both as having the same properties. This results in an inaccurate representation of the veiling effect caused by the accumulation of rain streaks and vapors in the transmission map. To address this issue, we propose a new approach that treats rain streaks as part of the transmission medium, along with vapors, in order to better model the formation of rain images. Our method, called SNet, utilizes an encoder-decoder CNN to learn the transmission map of rain streaks, using ShuffleNet units to capture their anisotropic representations. We also introduce a VNet with spatial pyramid pooling (SSP) to predict the transmission map of vapors in multi-scales based on that of rain streaks, as well as an encoder CNN called ANet to estimate atmosphere light. By jointly training these networks, we are able to accurately predict transmission maps and atmosphere light for rain image restoration. Our experiments demonstrate that our model performs favorably compared to state-of-the-art deraining approaches on benchmark datasets.",1
"The well-known technique outlined in the paper of Leon A. Gatys et al., A Neural Algorithm of Artistic Style, has become a trending topic both in academic literature and industrial applications. Neural Style Transfer (NST) constitutes an essential tool for a wide range of applications, such as artistic stylization of 2D images, user-assisted creation tools and production tools for entertainment applications. The purpose of this study is to present a method for creating artistic maps from satellite images, based on the NST algorithm. This method includes three basic steps (i) application of semantic image segmentation on the original satellite image, dividing its content into classes (i.e. land, water), (ii) application of neural style transfer for each class and (iii) creation of a collage, i.e. an artistic image consisting of a combination of the two stylized image generated on the previous step.",0
"The technique described in Leon A. Gatys et al.'s paper, ""A Neural Algorithm of Artistic Style,"" has gained popularity in academic and industrial settings. Called Neural Style Transfer (NST), it is used for various purposes, such as creating stylized 2D images, user-assisted creation tools, and production tools for entertainment applications. This study aims to demonstrate a method for generating artistic maps from satellite images using the NST algorithm. The method involves three steps: (i) applying semantic image segmentation to the original satellite image to divide its content into classes (e.g. land, water), (ii) using neural style transfer for each class, and (iii) combining the two stylized images to create a collage or artistic image.",1
"There have been a fairly of research interests in exploring the disentanglement of appearance and shape from human images. Most existing endeavours pursuit this goal by either using training images with annotations or regulating the training process with external clues such as human skeleton, body segmentation or cloth patches etc. In this paper, we aim to address this challenge in a more unsupervised manner---we do not require any annotation nor any external task-specific clues. To this end, we formulate an encoder-decoder-like network to extract both the shape and appearance features from input images at the same time, and train the parameters by three losses: feature adversarial loss, color consistency loss and reconstruction loss. The feature adversarial loss mainly impose little to none mutual information between the extracted shape and appearance features, while the color consistency loss is to encourage the invariance of person appearance conditioned on different shapes. More importantly, our unsupervised (Unsupervised learning has many interpretations in different tasks. To be clear, in this paper, we refer unsupervised learning as learning without task-specific human annotations, pairs or any form of weak supervision.) framework utilizes learned shape features as masks which are applied to the input itself in order to obtain clean appearance features. Without using fixed input human skeleton, our network better preserves the conditional human posture while requiring less supervision. Experimental results on DeepFashion and Market1501 demonstrate that the proposed method achieves clean disentanglement and is able to synthesis novel images of comparable quality with state-of-the-art weakly-supervised or even supervised methods.",0
"There has been a significant amount of research focused on the disentanglement of appearance and shape in human images. Previous attempts to achieve this have involved using annotated training images or external cues such as body segmentation or clothing patches. However, our approach is more unsupervised in nature and does not require any annotations or external cues. We have developed an encoder-decoder network that can extract both shape and appearance features simultaneously, which we train using three different losses: feature adversarial, color consistency, and reconstruction. Our unsupervised framework uses learned shape features as masks to obtain clean appearance features from the input image, without the need for a fixed human skeleton. Our method achieves clean disentanglement and can produce high-quality images comparable to state-of-the-art weakly-supervised or even supervised methods, as demonstrated by our results on DeepFashion and Market1501 datasets.",1
"Deep learning is a hot research topic in the field of machine learning methods and applications. Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but both of them are difficult to train since they need to train the generator (or encoder) and the discriminator (or decoder) simultaneously, which is easy to cause unstable training. In order to solve or alleviate the synchronous training difficult problems of GANs and VAEs, recently, researchers propose Generative Scattering Networks (GSNs), which use wavelet scattering networks (ScatNets) as the encoder to obtain the features (or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder to generate the image. The advantage of GSNs is the parameters of ScatNets are not needed to learn, and the disadvantage of GSNs is that the expression ability of ScatNets is slightly weaker than CNNs and the dimensional reduction method of Principal Component Analysis (PCA) is easy to lead overfitting in the training of GSNs, and therefore affect the generated quality in the testing process. In order to further improve the quality of generated images while keep the advantages of GSNs, this paper proposes Generative Fractional Scattering Networks (GFRSNs), which use more expressive fractional wavelet scattering networks (FrScatNets) instead of ScatNets as the encoder to obtain the features (or FrScatNet embeddings) and use the similar CNNs of GSNs as the decoder to generate the image. Additionally, this paper develops a new dimensional reduction method named Feature-Map Fusion (FMF) instead of PCA for better keeping the information of FrScatNets and the effect of image fusion on the quality of image generation is also discussed.",0
"The field of machine learning methods and applications is currently focusing on deep learning. While Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) can create impressive image generations from Gaussian white noise, training them can be difficult because both the generator (or encoder) and discriminator (or decoder) need to be trained simultaneously, leading to unstable training. To address this issue, researchers have proposed Generative Scattering Networks (GSNs), which use wavelet scattering networks (ScatNets) as the encoder and convolutional neural networks (CNNs) as the decoder. While GSNs have the advantage of not needing to learn ScatNet parameters, their expression ability is weaker than that of CNNs, and the use of Principal Component Analysis (PCA) as a dimensional reduction method can lead to overfitting during training and affect the quality of generated images during testing. To improve image quality while retaining the benefits of GSNs, this paper proposes the use of more expressive fractional wavelet scattering networks (FrScatNets) as the encoder in Generative Fractional Scattering Networks (GFRSNs) and a new dimensional reduction method called Feature-Map Fusion (FMF). The paper also discusses the effect of image fusion on image generation quality.",1
"Neural style transfer (NST) is a powerful image generation technique that uses a convolutional neural network (CNN) to merge the content of one image with the style of another. Contemporary methods of NST use first or second order statistics of the CNN's features to achieve transfers with relatively little computational cost. However, these methods cannot fully extract the style from the CNN's features. We present a new algorithm for style transfer that fully extracts the style from the features by redefining the style loss as the Wasserstein distance between the distribution of features. Thus, we set a new standard in style transfer quality. In addition, we state two important interpretations of NST. The first is a re-emphasis from Li et al., which states that style is simply the distribution of features. The second states that NST is a type of generative adversarial network (GAN) problem.",0
"The technique known as neural style transfer (NST) merges the content of one image with the style of another using a convolutional neural network (CNN) and has become a powerful tool for image generation. While current methods of NST use first or second order statistics of the CNN's features to achieve transfers with minimal computational cost, they fall short in fully extracting the style from the features. To address this shortcoming, we introduce a new algorithm that redefines the style loss as the Wasserstein distance between the distribution of features, thereby fully extracting the style from the features and setting a new standard for style transfer quality. We also highlight two important interpretations of NST: the first emphasizes that style is represented by the distribution of features, while the second recognizes NST as a type of generative adversarial network (GAN) problem.",1
"Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work, we investigate object compositionality as an inductive bias for Generative Adversarial Networks (GANs). We present a minimal modification of a standard generator to incorporate this inductive bias and find that it reliably learns to generate images as compositions of objects. Using this general design as a backbone, we then propose two useful extensions to incorporate dependencies among objects and background. We extensively evaluate our approach on several multi-object image datasets and highlight the merits of incorporating structure for representation learning purposes. In particular, we find that our structured GANs are better at generating multi-object images that are more faithful to the reference distribution. More so, we demonstrate how, by leveraging the structure of the learned generative process, one can `invert' the learned generative model to perform unsupervised instance segmentation. On the challenging CLEVR dataset, it is shown how our approach is able to improve over other recent purely unsupervised object-centric approaches to image generation.",0
"The aim of deep generative models is to identify the process through which the observed data was created. Such models can be used for generating new data samples or to extract representations from the existing ones. When it comes to images, successful approaches rely on some core inductive biases, but they tend to overlook the bias for the way humans structure visual scenes in terms of objects. In this study, we explore object compositionality as an inductive bias for Generative Adversarial Networks (GANs), modifying a standard generator to incorporate this bias and finding that it can generate images composed of objects. Moreover, we propose two extensions to incorporate dependencies among objects and background. Our approach is thoroughly evaluated on several multi-object image datasets, highlighting the benefits of incorporating structure for representation learning. Our structured GANs perform better at generating multi-object images that are faithful to the reference distribution and can be used for unsupervised instance segmentation. We demonstrate the superiority of our approach over other recent unsupervised object-centric methods on the challenging CLEVR dataset.",1
"We address the problem of single photo age progression and regression-the prediction of how a person might look in the future, or how they looked in the past. Most existing aging methods are limited to changing the texture, overlooking transformations in head shape that occur during the human aging and growth process. This limits the applicability of previous methods to aging of adults to slightly older adults, and application of those methods to photos of children does not produce quality results. We propose a novel multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process. The network is trained on the FFHQ dataset, which we labeled for ages, gender, and semantic segmentation. Fixed age classes are used as anchors to approximate continuous age transformation. Our framework can predict a full head portrait for ages 0-70 from a single photo, modifying both texture and shape of the head. We demonstrate results on a wide variety of photos and datasets, and show significant improvement over the state of the art.",0
"Our focus is on solving the issue of predicting a person's appearance in the future or in the past using just one photo. Existing aging techniques typically only alter the texture of the photo, disregarding the changes in head shape that occur during growth and aging. This makes these methods unsuitable for predicting the appearance of children and less effective for older adults. To address this, we propose a new generative adversarial network architecture that can model a continuous aging process in both directions. We trained the network on the FFHQ dataset, which we annotated with gender, age, and semantic segmentation. Fixed age categories were utilized as reference points to approximate the continuous aging process. Our model can generate a complete head portrait for individuals aged 0-70, adjusting both texture and head shape. We present our results on various photo sets and demonstrate significant improvement compared to previous methods.",1
"This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves 46.4 $AP$ (and 65.1 $AP_{50}$), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in another task, namely, ""labels to photo"" image generation by integrating the voting module of HoughNet to two different GAN models and showing that the accuracy is significantly improved in both cases. Code is available at https://github.com/nerminsamet/houghnet.",0
"HoughNet, a new object detection approach, is presented in this paper. This method is a one-stage, anchor-free, and voting-based bottom-up model. HoughNet determines object presence at a given location by summing the votes cast on that location, inspired by the Generalized Hough Transform. The votes are collected from both near and far locations based on a log-polar vote field. This voting mechanism allows HoughNet to combine near and long-range, class-conditional evidence for visual recognition, which improves the current object detection methodology that relies on only local evidence. On the COCO dataset, HoughNet's best model achieves 46.4 AP and 65.1 AP50, which is on par with the state-of-the-art in bottom-up object detection and outperforms most major one-stage and two-stage methods. Moreover, the effectiveness of the proposal is validated by integrating the voting module of HoughNet into two different GAN models, which improves accuracy significantly in both cases. The code for HoughNet is available at https://github.com/nerminsamet/houghnet.",1
"Generative adversarial nets (GANs) have been successfully applied in many fields like image generation, inpainting, super-resolution and drug discovery, etc., by now, the inner process of GANs is far from been understood. To get deeper insight of the intrinsic mechanism of GANs, in this paper, a method for interpreting the latent space of GANs by analyzing the correlation between latent variables and the corresponding semantic contents in generated images is proposed. Unlike previous methods that focus on dissecting models via feature visualization, the emphasis of this work is put on the variables in latent space, i.e. how the latent variables affect the quantitative analysis of generated results. Given a pretrained GAN model with weights fixed, the latent variables are intervened to analyze their effect on the semantic content in generated images. A set of controlling latent variables can be derived for specific content generation, and the controllable semantic content manipulation be achieved. The proposed method is testified on the datasets Fashion-MNIST and UT Zappos50K, experiment results show its effectiveness.",0
"Although Generative adversarial nets (GANs) have been successfully implemented in fields such as image generation, drug discovery, super-resolution, and inpainting, their inner workings are not yet fully understood. This paper proposes a method for interpreting the latent space of GANs by analyzing the correlation between latent variables and the corresponding semantic contents in generated images. Unlike previous methods that focus on dissecting models via feature visualization, this work emphasizes variables in the latent space. The goal is to understand how latent variables affect the quantitative analysis of generated results. By intervening in the latent variables of a pretrained GAN model with fixed weights, their influence on the semantic content in generated images can be analyzed, and a set of controlling latent variables can be derived for specific content generation. The proposed method is demonstrated on the Fashion-MNIST and UT Zappos50K datasets, and the experimental results prove its effectiveness.",1
"Given an image, generating its natural language description (i.e., caption) is a well studied problem. Approaches proposed to address this problem usually rely on image features that are difficult to interpret. Particularly, these image features are subdivided into global and local features, where global features are extracted from the global representation of the image, while local features are extracted from the objects detected locally in an image. Although, local features extract rich visual information from the image, existing models generate captions in a blackbox manner and humans have difficulty interpreting which local objects the caption is aimed to represent. Hence in this paper, we propose a novel framework for the image captioning with an explicit object (e.g., knowledge graph entity) selection process while still maintaining its end-to-end training ability. The model first explicitly selects which local entities to include in the caption according to a human-interpretable mask, then generate proper captions by attending to selected entities. Experiments conducted on the MSCOCO dataset demonstrate that our method achieves good performance in terms of the caption quality and diversity with a more interpretable generating process than previous counterparts.",0
"The task of generating a natural language description (caption) for a given image has been extensively researched. The typical approach involves utilizing image features, which are often difficult to interpret, and can be categorized into global and local features. Global features represent the entire image, while local features are extracted from objects within an image. While local features provide rich visual information, existing models generate captions in a way that is difficult for humans to interpret, making it challenging to identify which local objects the caption refers to. To address this issue, we introduce a novel framework for image captioning that includes an explicit object selection process based on human-interpretable masks, while still maintaining end-to-end training ability. Our model selects which local entities to include in the caption, then generates appropriate captions by attending to the selected entities. Experiments on the MSCOCO dataset demonstrate that our method achieves excellent caption quality and diversity while offering a more interpretable generating process than previous approaches.",1
"As image generation techniques mature, there is a growing interest in explainable representations that are easy to understand and intuitive to manipulate. In this work, we turn to co-occurrence statistics, which have long been used for texture analysis, to learn a controllable texture synthesis model. We propose a fully convolutional generative adversarial network, conditioned locally on co-occurrence statistics, to generate arbitrarily large images while having local, interpretable control over the texture appearance. To encourage fidelity to the input condition, we introduce a novel differentiable co-occurrence loss that is integrated seamlessly into our framework in an end-to-end fashion. We demonstrate that our solution offers a stable, intuitive and interpretable latent representation for texture synthesis, which can be used to generate a smooth texture morph between different textures. We further show an interactive texture tool that allows a user to adjust local characteristics of the synthesized texture image using the co-occurrence values directly.",0
"As techniques for generating images become more advanced, there is a growing interest in creating representations that are easy to understand and manipulate. In this study, we explore the use of co-occurrence statistics, which have traditionally been used for analyzing textures, to develop a model for controllable texture synthesis. Our approach involves a fully convolutional generative adversarial network that is conditioned locally on co-occurrence statistics, which enables the generation of images of any size while maintaining local, interpretable control over texture appearance. To ensure that the input conditions are accurately represented, we introduce a novel differentiable co-occurrence loss that seamlessly integrates into our framework in an end-to-end fashion. Our results show that our solution provides a stable, intuitive, and interpretable latent representation for texture synthesis, which can be used to create a smooth texture morph between different textures. We also demonstrate the use of an interactive texture tool that allows users to adjust local characteristics of the synthesized texture image using co-occurrence values directly.",1
"Neural random fields (NRFs), referring to a class of generative models that use neural networks to implement potential functions in random fields (a.k.a. energy-based models), are not new but receive less attention with slow progress. Different from various directed graphical models such as generative adversarial networks (GANs), NRFs provide an interesting family of undirected graphical models for generative modeling. In this paper we propose a new approach, the inclusive-NRF approach, to learning NRFs for continuous data (e.g. images), by introducing inclusive-divergence minimized auxiliary generators and developing stochastic gradient sampling in an augmented space. Based on the new approach, specific inclusive-NRF models are developed and thoroughly evaluated in two important generative modeling applications - image generation and anomaly detection. The proposed models consistently improve over state-of-the-art results in both applications. Remarkably, in addition to superior sample generation, one additional benefit of our inclusive-NRF approach is that, unlike GANs, it can directly provide (unnormalized) density estimate for sample evaluation. With these contributions and results, this paper significantly advances the learning and applications of NRFs to a new level, both theoretically and empirically, which have never been obtained before.",0
"Neural random fields (NRFs) are a type of generative model that use neural networks to implement potential functions in random fields. Although they are not a new concept, they have received less attention due to slow progress. Unlike directed graphical models such as generative adversarial networks (GANs), NRFs offer undirected graphical models for generative modeling. This paper presents a new approach, called the inclusive-NRF approach, for learning NRFs for continuous data. This is accomplished by introducing inclusive-divergence minimized auxiliary generators and developing stochastic gradient sampling in an augmented space. The proposed models are evaluated in two important generative modeling applications - image generation and anomaly detection - and consistently improve over state-of-the-art results. Additionally, the inclusive-NRF approach provides a direct (unnormalized) density estimate for sample evaluation, which is not possible with GANs. Overall, this paper significantly advances the learning and applications of NRFs both theoretically and empirically.",1
"Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwritten words. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images.",0
"Despite the impressive quality levels of current image generation methods, generating plausible and diverse handwritten words remains a challenge. This is because handwritten words exhibit great variability across different writers and even within the same individual. To address this issue, we propose a novel method that uses calligraphic style features and textual content to produce realistic and varied artificially rendered handwritten words. Our generator is guided by three complementary learning objectives: to produce realistic images, imitate a certain handwriting style, and convey specific textual content. Our model is not constrained by any predefined vocabulary and can render any input word. Additionally, it can mimic a sample writer's calligraphic features in a few-shot setup. Through qualitative, quantitative, and human-based evaluations, we demonstrate significant advancements over prior art and the realistic aspect of our synthetically produced images.",1
"We propose a weakly-supervised approach for conditional image generation of complex scenes where a user has fine control over objects appearing in the scene. We exploit sparse semantic maps to control object shapes and classes, as well as textual descriptions or attributes to control both local and global style. In order to condition our model on textual descriptions, we introduce a semantic attention module whose computational cost is independent of the image resolution. To further augment the controllability of the scene, we propose a two-step generation scheme that decomposes background and foreground. The label maps used to train our model are produced by a large-vocabulary object detector, which enables access to unlabeled data and provides structured instance information. In such a setting, we report better FID scores compared to fully-supervised settings where the model is trained on ground-truth semantic maps. We also showcase the ability of our model to manipulate a scene on complex datasets such as COCO and Visual Genome.",0
"Our proposed method involves weakly-supervised conditional image generation for complex scenes, giving users precise control over the objects that appear. We utilize sparse semantic maps to manage object shapes and classes, and textual descriptions or attributes to handle both local and global style. To incorporate textual descriptions, we introduce a semantic attention module with a computational cost that is independent of image resolution. To enhance scene controllability, we suggest a two-step generation process that separates background and foreground. The label maps used to train our model are generated by a large-vocabulary object detector, allowing access to unlabeled data and providing structured instance information. We report improved FID scores in this setting compared to fully-supervised approaches where the model is trained on ground-truth semantic maps. Our model's capacity to manipulate complex datasets such as COCO and Visual Genome is also demonstrated.",1
"Autoregressive models recently achieved comparable results versus state-of-the-art Generative Adversarial Networks (GANs) with the help of Vector Quantized Variational AutoEncoders (VQ-VAE). However, autoregressive models have several limitations such as exposure bias and their training objective does not guarantee visual fidelity. To address these limitations, we propose to use Reinforced Adversarial Learning (RAL) based on policy gradient optimization for autoregressive models. By applying RAL, we enable a similar process for training and testing to address the exposure bias issue. In addition, visual fidelity has been further optimized with adversarial loss inspired by their strong counterparts: GANs. Due to the slow sampling speed of autoregressive models, we propose to use partial generation for faster training. RAL also empowers the collaboration between different modules of the VQ-VAE framework. To our best knowledge, the proposed method is first to enable adversarial learning in autoregressive models for image generation. Experiments on synthetic and real-world datasets show improvements over the MLE trained models. The proposed method improves both negative log-likelihood (NLL) and Fr\'echet Inception Distance (FID), which indicates improvements in terms of visual quality and diversity. The proposed method achieves state-of-the-art results on Celeba for 64 $\times$ 64 image resolution, showing promise for large scale image generation.",0
"Vector Quantized Variational AutoEncoders (VQ-VAE) have enabled autoregressive models to achieve comparable results to state-of-the-art Generative Adversarial Networks (GANs). However, autoregressive models still suffer from exposure bias and lack of visual fidelity in their training objective. To address these issues, we propose Reinforced Adversarial Learning (RAL) using policy gradient optimization for autoregressive models. This approach enables a similar process for training and testing, and improves visual fidelity with an adversarial loss inspired by GANs. Partial generation is used to speed up training, and RAL facilitates collaboration between different modules of the VQ-VAE framework. Our proposed method is the first to enable adversarial learning for image generation in autoregressive models. Experiments on synthetic and real-world datasets show improvements in negative log-likelihood (NLL) and Fr\'echet Inception Distance (FID), indicating improvements in visual quality and diversity. Our proposed method achieves state-of-the-art results on Celeba for 64 $\times$ 64 image resolution, demonstrating its potential for large scale image generation.",1
"We propose an end-to-end network for image generation from given structured-text that consists of the visual-relation layout module and the pyramid of GANs, namely stacking-GANs. Our visual-relation layout module uses relations among entities in the structured-text in two ways: comprehensive usage and individual usage. We comprehensively use all available relations together to localize initial bounding-boxes of all the entities. We also use individual relation separately to predict from the initial bounding-boxes relation-units for all the relations in the input text. We then unify all the relation-units to produce the visual-relation layout, i.e., bounding-boxes for all the entities so that each of them uniquely corresponds to each entity while keeping its involved relations. Our visual-relation layout reflects the scene structure given in the input text. The stacking-GANs is the stack of three GANs conditioned on the visual-relation layout and the output of previous GAN, consistently capturing the scene structure. Our network realistically renders entities' details in high resolution while keeping the scene structure. Experimental results on two public datasets show outperformances of our method against state-of-the-art methods.",0
"Our proposed approach involves an end-to-end network that generates images based on structured-text inputs. This network comprises two main components: the visual-relation layout module and the stacking-GANs pyramid. The visual-relation layout module utilizes relations among entities in the structured-text in two ways, namely comprehensive and individual usage. In the comprehensive approach, all available relations are used together to localize the initial bounding-boxes of all entities. In the individual approach, individual relations are used to predict relation-units for all relations in the input text. All relation-units are then unified to produce the visual-relation layout, which reflects the scene structure given in the input text. The stacking-GANs pyramid is a stack of three GANs conditioned on the visual-relation layout and the output of the previous GAN, which consistently captures the scene structure. Our approach realistically renders entities' details in high resolution while preserving the scene structure. Experimental results on two public datasets demonstrate that our method outperforms state-of-the-art methods.",1
"We propose a novel Generative Adversarial Network (XingGAN or CrossingGAN) for person image generation tasks, i.e., translating the pose of a given person to a desired one. The proposed Xing generator consists of two generation branches that model the person's appearance and shape information, respectively. Moreover, we propose two novel blocks to effectively transfer and update the person's shape and appearance embeddings in a crossing way to mutually improve each other, which has not been considered by any other existing GAN-based image generation work. Extensive experiments on two challenging datasets, i.e., Market-1501 and DeepFashion, demonstrate that the proposed XingGAN advances the state-of-the-art performance both in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/XingGAN.",0
"Our team has developed a unique Generative Adversarial Network, known as CrossingGAN or XingGAN, that can be used for generating person images. The network allows for the translation of a person's pose to a desired one. The Xing generator is made up of two branches that model the person's appearance and shape information separately. We have also introduced two new blocks that transfer and update the person's appearance and shape embeddings in a crossing way, to enhance each other. This approach has not been previously explored in any other GAN-based image generation work. Our experiments on two challenging datasets, Market-1501 and DeepFashion, have shown that the XingGAN significantly improves upon the state-of-the-art performance in both objective quantitative scores and subjective visual realness. The source code and trained models are available for download at https://github.com/Ha0Tang/XingGAN.",1
"In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.",0
"This paper presents a novel approach to generative adversarial network (GAN) architecture search using reinforcement learning (RL) and neural architecture search (NAS) techniques. The proposed methodology formulates the GAN architecture search problem as a Markov decision process (MDP) to facilitate smoother architecture sampling and improve the efficiency of the RL-based search algorithm. To further enhance efficiency, an off-policy GAN architecture search algorithm is employed, which efficiently utilizes samples generated by previous policies. Results from evaluating the proposed method on two standard benchmark datasets (CIFAR-10 and STL-10) demonstrate its ability to discover highly competitive architectures, resulting in better image generation results with a significantly reduced computational burden of 7 GPU hours. The code for this study is accessible at https://github.com/Yuantian013/E2GAN.",1
"Generation of high-quality person images is challenging, due to the sophisticated entanglements among image factors, e.g., appearance, pose, foreground, background, local details, global structures, etc. In this paper, we present a novel end-to-end framework to generate realistic person images based on given person poses and appearances. The core of our framework is a novel generator called Appearance-aware Pose Stylizer (APS) which generates human images by coupling the target pose with the conditioned person appearance progressively. The framework is highly flexible and controllable by effectively decoupling various complex person image factors in the encoding phase, followed by re-coupling them in the decoding phase. In addition, we present a new normalization method named adaptive patch normalization, which enables region-specific normalization and shows a good performance when adopted in person image generation model. Experiments on two benchmark datasets show that our method is capable of generating visually appealing and realistic-looking results using arbitrary image and pose inputs.",0
"Generating high-quality person images is a difficult task as there are intricate connections among various image factors such as appearance, pose, foreground, background, local details, and global structures. This paper introduces a new end-to-end framework that generates realistic person images based on given person poses and appearances. The framework consists of a novel generator called Appearance-aware Pose Stylizer (APS) that generates human images by progressively coupling the target pose with the conditioned person appearance. The encoding phase of the framework effectively decouples various complex person image factors, followed by re-coupling them in the decoding phase, making the framework highly flexible and controllable. Additionally, the paper presents a novel normalization method called adaptive patch normalization that enables region-specific normalization and performs well in person image generation models. Experiments on two benchmark datasets demonstrate that the proposed method generates visually appealing and realistic-looking results using arbitrary image and pose inputs.",1
"Flexible user controls are desirable for content creation and image editing. A semantic map is commonly used intermediate representation for conditional image generation. Compared to the operation on raw RGB pixels, the semantic map enables simpler user modification. In this work, we specifically target at generating semantic maps given a label-set consisting of desired categories. The proposed framework, SegVAE, synthesizes semantic maps in an iterative manner using conditional variational autoencoder. Quantitative and qualitative experiments demonstrate that the proposed model can generate realistic and diverse semantic maps. We also apply an off-the-shelf image-to-image translation model to generate realistic RGB images to better understand the quality of the synthesized semantic maps. Furthermore, we showcase several real-world image-editing applications including object removal, object insertion, and object replacement.",0
"The creation of content and editing of images can be made easier with flexible user controls. To simplify user modification, semantic maps are often used as an intermediate representation for conditional image generation. Our focus is on generating semantic maps using a label-set of desired categories. Our proposed framework, SegVAE, uses a conditional variational autoencoder to iteratively synthesize semantic maps, resulting in realistic and diverse outputs. We also use an off-the-shelf image-to-image translation model to generate realistic RGB images for comparison. Real-world image-editing applications such as object removal, insertion, and replacement are showcased using the synthesized semantic maps. Both quantitative and qualitative experiments show the effectiveness of our model.",1
"Image generation from scene description is a cornerstone technique for the controlled generation, which is beneficial to applications such as content creation and image editing. In this work, we aim to synthesize images from scene description with retrieved patches as reference. We propose a differentiable retrieval module. With the differentiable retrieval module, we can (1) make the entire pipeline end-to-end trainable, enabling the learning of better feature embedding for retrieval; (2) encourage the selection of mutually compatible patches with additional objective functions. We conduct extensive quantitative and qualitative experiments to demonstrate that the proposed method can generate realistic and diverse images, where the retrieved patches are reasonable and mutually compatible.",0
"The technique of generating images based on scene description is crucial for controlled generation, which has applications in content creation and image editing. Our goal in this study is to create images from scene description using retrieved patches as a reference. To achieve this, we have introduced a differentiable retrieval module. This module enables us to (1) make the entire process trainable end-to-end, allowing better feature embedding for retrieval to be learned, and (2) promote the selection of mutually compatible patches using additional objective functions. Through quantitative and qualitative experiments, we demonstrate that our method is capable of producing realistic and diverse images, where the retrieved patches are reasonable and compatible with each other.",1
"There exist many forms of deep latent variable models, such as the variational autoencoder and adversarial autoencoder. Regardless of the specific class of model, there exists an implicit consensus that the latent distribution should be regularized towards the prior, even in the case where the prior distribution is learned. Upon investigating the effect of latent regularization on image generation our results indicate that in the case where a sufficiently expressive prior is learned, latent regularization is not necessary and may in fact be harmful insofar as image quality is concerned. We additionally investigate the benefit of learned priors on two common problems in computer vision: latent variable disentanglement, and diversity in image-to-image translation.",0
"Various types of deep latent variable models exist, including the variational autoencoder and adversarial autoencoder. Although each model has its own unique characteristics, there is a general agreement that the latent distribution should be regulated towards the prior, even when the prior distribution is learned. Our research on the impact of latent regulation on image generation shows that if a sufficiently expressive prior is learned, latent regulation is unnecessary and may even be detrimental to image quality. Additionally, we explore the advantages of learned priors in addressing two common issues in computer vision: latent variable disentanglement and diversity in image-to-image translation.",1
"The instability in GAN training has been a long-standing problem despite remarkable research efforts. We identify that instability issues stem from difficulties of performing feature matching with mini-batch statistics, due to a fragile balance between the fixed target distribution and the progressively generated distribution. In this work, we propose Feature Quantization (FQ) for the discriminator, to embed both true and fake data samples into a shared discrete space. The quantized values of FQ are constructed as an evolving dictionary, which is consistent with feature statistics of the recent distribution history. Hence, FQ implicitly enables robust feature matching in a compact space. Our method can be easily plugged into existing GAN models, with little computational overhead in training. We apply FQ to 3 representative GAN models on 9 benchmarks: BigGAN for image generation, StyleGAN for face synthesis, and U-GAT-IT for unsupervised image-to-image translation. Extensive experimental results show that the proposed FQ-GAN can improve the FID scores of baseline methods by a large margin on a variety of tasks, achieving new state-of-the-art performance.",0
"Despite significant research efforts, the instability in GAN training has persisted as a longstanding issue. Our research has identified that the problem lies in the challenge of performing feature matching with mini-batch statistics, due to the delicate balance between the fixed target distribution and the progressively generated distribution. To address this issue, we propose Feature Quantization (FQ) for the discriminator, which embeds both real and fake data samples into a shared discrete space. FQ's quantized values are generated as an evolving dictionary, which remains consistent with the feature statistics of the recent distribution history, thereby enabling robust feature matching in a compact space. Our method can be easily integrated into existing GAN models, with minimal computational overhead during training. We have applied FQ to three representative GAN models on nine benchmarks, including BigGAN for image generation, StyleGAN for face synthesis, and U-GAT-IT for unsupervised image-to-image translation. Extensive experimental results demonstrate that FQ-GAN significantly improves the FID scores of baseline methods across a variety of tasks, achieving new state-of-the-art performance.",1
"People often create art by following an artistic workflow involving multiple stages that inform the overall design. If an artist wishes to modify an earlier decision, significant work may be required to propagate this new decision forward to the final artwork. Motivated by the above observations, we propose a generative model that follows a given artistic workflow, enabling both multi-stage image generation as well as multi-stage image editing of an existing piece of art. Furthermore, for the editing scenario, we introduce an optimization process along with learning-based regularization to ensure the edited image produced by the model closely aligns with the originally provided image. Qualitative and quantitative results on three different artistic datasets demonstrate the effectiveness of the proposed framework on both image generation and editing tasks.",0
"The process of creating art usually involves several stages that contribute to the overall design. If changes need to be made to an earlier decision, it can require a lot of work to apply those changes to the final artwork. Based on these observations, we suggest a generative model that adheres to an artistic workflow. This model allows for both creating images through multiple stages and editing existing artwork through multiple stages. Additionally, when editing artwork, we introduce an optimization process and learning-based regularization to ensure that the edited image closely matches the original. We have tested our framework on three different artistic datasets, and both qualitative and quantitative results demonstrate its effectiveness for both image generation and editing tasks.",1
"The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present B\'ezierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit B\'ezier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.",0
"A current modeling problem that is captivating is the exploration of neural generative models of human sketches, as there is a connection between sketch image creation and the human drawing process. Although the SketchRNN had a breakthrough in generating sketches by producing them sequentially as a sequence of waypoints, it fails to create high-resolution images and model long sketches. In our paper, we present B\'ezierSketch, an innovative generative model that produces fully scalable and high-resolution vector sketches. We accomplish this by introducing a new inverse graphics stroke embedding approach that trains an encoder to embed each stroke into a B\'ezier curve, enabling sketches to be treated as short sequences of parameterized strokes. This allows us to train a recurrent sketch generator with the capability to produce longer sketches and produce scalable, high-resolution results. Qualitative and quantitative results on the Quick, Draw! benchmark are reported.",1
"Deep encoder-decoder based CNNs have advanced image inpainting methods for hole filling. While existing methods recover structures and textures step-by-step in the hole regions, they typically use two encoder-decoders for separate recovery. The CNN features of each encoder are learned to capture either missing structures or textures without considering them as a whole. The insufficient utilization of these encoder features limit the performance of recovering both structures and textures. In this paper, we propose a mutual encoder-decoder CNN for joint recovery of both. We use CNN features from the deep and shallow layers of the encoder to represent structures and textures of an input image, respectively. The deep layer features are sent to a structure branch and the shallow layer features are sent to a texture branch. In each branch, we fill holes in multiple scales of the CNN features. The filled CNN features from both branches are concatenated and then equalized. During feature equalization, we reweigh channel attentions first and propose a bilateral propagation activation function to enable spatial equalization. To this end, the filled CNN features of structure and texture mutually benefit each other to represent image content at all feature levels. We use the equalized feature to supplement decoder features for output image generation through skip connections. Experiments on the benchmark datasets show the proposed method is effective to recover structures and textures and performs favorably against state-of-the-art approaches.",0
"Advanced image inpainting methods for hole filling have been developed using deep encoder-decoder based CNNs. However, existing techniques for recovering structures and textures in the hole regions typically use two separate encoder-decoders. Each encoder captures either missing structures or textures without considering them as a whole, resulting in insufficient utilization of these encoder features and limiting the performance of recovering both structures and textures. This paper proposes a mutual encoder-decoder CNN that jointly recovers both structures and textures. The deep and shallow layers of the encoder are used to represent structures and textures of an input image, respectively. We fill holes in multiple scales of the CNN features in each branch and concatenate and equalize the filled features from both branches. Feature equalization is achieved through channel reweighing and a bilateral propagation activation function for spatial equalization. The mutually beneficial filled CNN features of structure and texture represent image content at all feature levels and are used to supplement decoder features for output image generation through skip connections. Experimental results on benchmark datasets demonstrate that the proposed method is effective in recovering structures and textures and outperforms state-of-the-art approaches.",1
"We present a method for simultaneously learning, in an unsupervised manner, (i) a conditional image generator, (ii) foreground extraction and segmentation, (iii) clustering into a two-level class hierarchy, and (iv) object removal and background completion, all done without any use of annotation. The method combines a Generative Adversarial Network and a Variational Auto-Encoder, with multiple encoders, generators and discriminators, and benefits from solving all tasks at once. The input to the training scheme is a varied collection of unlabeled images from the same domain, as well as a set of background images without a foreground object. In addition, the image generator can mix the background from one image, with a foreground that is conditioned either on that of a second image or on the index of a desired cluster. The method obtains state of the art results in comparison to the literature methods, when compared to the current state of the art in each of the tasks.",0
"Our approach involves unsupervised learning of a conditional image generator, foreground extraction and segmentation, clustering into a two-level class hierarchy, and object removal and background completion. This is all accomplished without annotations and utilizing a Generative Adversarial Network and a Variational Auto-Encoder. Our method benefits from solving all of these tasks simultaneously, incorporating multiple encoders, generators, and discriminators. We used a collection of unlabeled images from the same domain and background images without a foreground object as input to the training scheme. Additionally, our image generator can mix backgrounds from one image with a foreground that is conditioned either on a second image or on the index of a desired cluster. Our results outperform existing literature methods in each of the tasks, achieving state of the art performance.",1
"Satellite imagery allows a plethora of applications ranging from weather forecasting to land surveying. The rapid development of computer vision systems could open new horizons to the utilization of satellite data due to the abundance of large volumes of data. However, current state-of-the-art computer vision systems mainly cater to applications that mainly involve natural images. While useful, those images exhibit a different distribution from satellite images in addition to having more spectral channels. This allows the use of pretrained deep learning models only in a subset of spectral channels that are equivalent to natural images thus discarding valuable information from other spectral channels. This calls for research effort to optimize deep learning models for satellite imagery to enable the assessment of their utility in the domain of remote sensing. Tensorflow tool allows for rapid prototyping and testing of deep learning models, however, its built-in image generator is designed to handle a maximum of four spectral channels. This manuscript introduces an open-source tool that allows the implementation of image augmentation for hyperspectral images in Tensorflow. Given how accessible and easy-to-use Tensorflow is, this tool would provide many researchers with the means to implement, test, and deploy deep learning models for remote sensing applications.",0
"The use of satellite imagery has a wide range of applications, from weather forecasting to land surveying. With the rapid advancements in computer vision technology, there is potential for new opportunities to utilize the vast amount of satellite data available. However, current computer vision systems are mainly focused on natural images, which differ from satellite images in terms of spectral channels and distribution. As a result, pretrained deep learning models can only be applied to certain spectral channels, leaving out valuable information from other channels. This highlights the need for research to optimize deep learning models for satellite imagery in remote sensing. Although Tensorflow is a useful tool for testing and prototyping deep learning models, its image generator can only handle up to four spectral channels. To address this limitation, an open-source tool has been introduced that allows for image augmentation of hyperspectral images in Tensorflow. This tool could provide researchers with a more accessible and user-friendly means of implementing, testing, and deploying deep learning models for remote sensing applications.",1
"Generating an image from a provided descriptive text is quite a challenging task because of the difficulty in incorporating perceptual information (object shapes, colors, and their interactions) along with providing high relevancy related to the provided text. Current methods first generate an initial low-resolution image, which typically has irregular object shapes, colors, and interaction between objects. This initial image is then improved by conditioning on the text. However, these methods mainly address the problem of using text representation efficiently in the refinement of the initially generated image, while the success of this refinement process depends heavily on the quality of the initially generated image, as pointed out in the DM-GAN paper. Hence, we propose a method to provide good initialized images by incorporating perceptual understanding in the discriminator module. We improve the perceptual information at the first stage itself, which results in significant improvement in the final generated image. In this paper, we have applied our approach to the novel StackGAN architecture. We then show that the perceptual information included in the initial image is improved while modeling image distribution at multiple stages. Finally, we generated realistic multi-colored images conditioned by text. These images have good quality along with containing improved basic perceptual information. More importantly, the proposed method can be integrated into the pipeline of other state-of-the-art text-based-image-generation models to generate initial low-resolution images. We also worked on improving the refinement process in StackGAN by augmenting the third stage of the generator-discriminator pair in the StackGAN architecture. Our experimental analysis and comparison with the state-of-the-art on a large but sparse dataset MS COCO further validate the usefulness of our proposed approach.",0
"It is difficult to create images from descriptive text as it requires the incorporation of perceptual information and high relevancy to the provided text. Current methods generate an initial low-resolution image which lacks regular object shapes, colors, and interactions between objects. Although these methods aim to refine the image by conditioning on the text, the quality of the initial image greatly impacts the success of the refinement process. To address this issue, we propose a method that incorporates perceptual understanding in the discriminator module to provide well-initialized images. By improving the perceptual information in the first stage, we significantly enhance the final generated image. Our approach is applied to the StackGAN architecture, and we demonstrate that the perceptual information in the initial image is improved while modeling image distribution at multiple stages. We generate realistic multi-colored images conditioned by text with improved basic perceptual information. Our proposed method can also be integrated into other text-based-image-generation models to generate initial low-resolution images. We augment the third stage of the generator-discriminator pair in the StackGAN architecture to improve the refinement process. Our experimental analysis and comparison with the state-of-the-art on MS COCO dataset validate the usefulness of our proposed approach.",1
"Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.",0
"Datasets in the real world frequently exhibit bias relating to significant demographic factors such as gender and race. Detecting and mitigating this bias using unsupervised machine learning can be especially difficult due to the concealed nature of the underlying factors. To address this issue, we propose a weakly supervised algorithm that can counter dataset bias for deep generative models. Our method utilizes an additional, small, unlabeled reference dataset as a form of supervision, circumventing the requirement for explicit labels on the hidden bias factors. By employing a density ratio technique to identify bias in existing datasets, we can learn generative models that achieve efficient data generation and data efficiency. Specifically, the generative models use training examples from both the biased and reference datasets to learn and produce data distributions that are similar to the reference dataset at test time. Our approach has been empirically evaluated, and results show that it reduces bias in latent factors by an average of up to 34.6% compared to baselines for image generation using generative adversarial networks.",1
"A laser scanner can easily acquire the geometric data of physical environments in the form of a point cloud. Recognizing objects from a point cloud is often required for industrial 3D reconstruction, which should include not only geometry information but also semantic information. However, recognition process is often a bottleneck in 3D reconstruction because it requires expertise on domain knowledge and intensive labor. To address this problem, various methods have been developed to recognize objects by retrieving the corresponding model in the database from an input geometry query. In recent years, the technique of converting geometric data into an image and applying view-based 3D shape retrieval has demonstrated high accuracy. Depth image which encodes depth value as intensity of pixel is frequently used for view-based 3D shape retrieval. However, geometric data collected from objects is often incomplete due to the occlusions and the limit of line of sight. Image generated by occluded point clouds lowers the performance of view-based 3D object retrieval due to loss of information. In this paper, we propose a method of viewpoint and image resolution estimation method for view-based 3D shape retrieval from point cloud query. Automatic selection of viewpoint and image resolution by calculating the data acquisition rate and density from the sampled viewpoints and image resolutions are proposed. The retrieval performance from the images generated by the proposed method is experimented and compared for various dataset. Additionally, view-based 3D shape retrieval performance with deep convolutional neural network has been experimented with the proposed method.",0
"The use of a laser scanner allows for easy acquisition of geometric data in the form of a point cloud, which is often required for industrial 3D reconstruction. However, recognizing objects from a point cloud can be a challenge due to the need for expertise in domain knowledge and intensive labor. To overcome this, various methods have been developed, including the conversion of geometric data into an image and applying view-based 3D shape retrieval. However, occlusions and limited line of sight often result in incomplete geometric data, leading to a loss of information in the generated images and a decrease in retrieval performance. In this paper, we propose a method for estimating viewpoint and image resolution to improve view-based 3D shape retrieval from point cloud queries. We suggest an automatic selection of viewpoint and image resolution by calculating the data acquisition rate and density from the sampled viewpoints and image resolutions. We experimentally compare the retrieval performance of images generated by the proposed method on various datasets and test the view-based 3D shape retrieval performance with deep convolutional neural networks.",1
"Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Fr\'echet Inception Distance (FID) score. Because it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics. Code: https://github.com/clovaai/generative-evaluation-prdc.",0
"Developing evaluation metrics to assess image generation remains an unresolved issue. While the Fr\'echet Inception Distance (FID) score has been the most commonly used metric to measure the similarity between real and produced images, it fails to distinguish between the fidelity and diversity of generated images. Recent studies have introduced precision and recall metrics to evaluate these properties separately, but our research reveals that even the latest versions of these metrics are not yet dependable. They are unable to identify identical distributions, are not resilient to outliers, and their evaluation hyperparameters are arbitrarily chosen. To address these limitations, we propose density and coverage metrics, which we demonstrate to be more reliable and interpretable than existing metrics through analytic and experimental evidence. Our code for this research is available at https://github.com/clovaai/generative-evaluation-prdc.",1
"Image synthesis is currently one of the most addressed image processing topic in computer vision and deep learning fields of study. Researchers have tackled this problem focusing their efforts on its several challenging problems, e.g. image quality and size, domain and pose changing, architecture of the networks, and so on. Above all, producing images belonging to different domains by using a single architecture is a very relevant goal for image generation. In fact, a single multi-domain network would allow greater flexibility and robustness in the image synthesis task than other approaches. This paper proposes a novel architecture and a training algorithm, which are able to produce multi-domain outputs using a single network. A small portion of a dataset is intentionally used, and there are no hard-coded labels (or classes). This is achieved by combining a conditional Generative Adversarial Network (cGAN) for image generation and a Meta-Learning algorithm for domain switch, and we called our approach MetalGAN. The approach has proved to be appropriate for solving the multi-domain problem and it is validated on facial attribute transfer, using CelebA dataset.",0
"The topic of image processing in computer vision and deep learning fields is currently focused on image synthesis. Researchers are working on addressing the challenging problems associated with this task, such as image quality and size, domain and pose changing, and network architecture. The ultimate goal is to generate images from different domains using a single architecture, which would offer greater flexibility and robustness. In this paper, we introduce a new architecture and training algorithm called MetalGAN that can produce multi-domain outputs using a single network. Our approach combines a conditional Generative Adversarial Network (cGAN) for image generation and a Meta-Learning algorithm for domain switching. The MetalGAN approach does not rely on hard-coded labels and uses only a small portion of the dataset. To validate our approach, we applied MetalGAN to facial attribute transfer using the CelebA dataset, and the results were promising.",1
"We show that high quality, diverse and realistic-looking diffusion-weighted magnetic resonance images can be synthesized using deep generative models. Based on professional neuroradiologists' evaluations and diverse metrics with respect to quality and diversity of the generated synthetic brain images, we present two networks, the Introspective Variational Autoencoder and the Style-Based GAN, that qualify for data augmentation in the medical field, where information is saved in a dispatched and inhomogeneous way and access to it is in many aspects restricted.",0
"Our study demonstrates that deep generative models have the ability to produce diffusion-weighted magnetic resonance images that are of superior quality, exhibit diversity, and appear realistic. We present two networks, the Introspective Variational Autoencoder and the Style-Based GAN, which have been evaluated by professional neuroradiologists and assessed using a range of metrics to determine the quality and diversity of the synthetic brain images. These networks are considered suitable for data augmentation in the medical field, where information is often distributed unevenly and access to it is restricted in many ways.",1
"In this paper, we investigate the application of deep convolutional GANs on car image generation. We improve upon the commonly used DCGAN architecture by implementing Wasserstein loss to decrease mode collapse and introducing dropout at the end of the discrimiantor to introduce stochasticity. Furthermore, we introduce convolutional layers at the end of the generator to improve expressiveness and smooth noise. All of these improvements upon the DCGAN architecture comprise our proposal of the novel BoolGAN architecture, which is able to decrease the FID from 195.922 (baseline) to 165.966.",0
"This paper explores the use of deep convolutional GANs for generating car images. To enhance the DCGAN architecture, we incorporate Wasserstein loss to mitigate mode collapse and integrate dropout at the discriminator's end to introduce randomness. Additionally, we introduce convolutional layers to the generator's end to improve smoothness and expressiveness. Our proposed BoolGAN architecture comprises all these enhancements and successfully reduces the FID from 195.922 (baseline) to 165.966.",1
"Recently, the task of image generation has attracted much attention. In particular, the recent empirical successes of the Markov Chain Monte Carlo (MCMC) technique of Langevin Dynamics have prompted a number of theoretical advances; despite this, several outstanding problems remain. First, the Langevin Dynamics is run in very high dimension on a nonconvex landscape; in the worst case, due to the NP-hardness of nonconvex optimization, it is thought that Langevin Dynamics mixes only in time exponential in the dimension. In this work, we demonstrate how the manifold hypothesis allows for the considerable reduction of mixing time, from exponential in the ambient dimension to depending only on the (much smaller) intrinsic dimension of the data. Second, the high dimension of the sampling space significantly hurts the performance of Langevin Dynamics; we leverage a multi-scale approach to help ameliorate this issue and observe that this multi-resolution algorithm allows for a trade-off between image quality and computational expense in generation.",0
"The process of generating images has recently become a prominent area of focus. Although the Markov Chain Monte Carlo (MCMC) technique of Langevin Dynamics has proven successful in practice, there are still unresolved issues. For example, Langevin Dynamics operates in a high-dimensional and nonconvex landscape, which can lead to exponential mixing times due to the NP-hardness of nonconvex optimization. Our research demonstrates how the manifold hypothesis can reduce mixing time by focusing on the smaller intrinsic dimension of the data. Additionally, Langevin Dynamics is hindered by the high dimensionality of the sampling space, but we address this through a multi-scale approach that balances image quality and computational cost.",1
"We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a soft-attention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1x to 4x magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",0
"The focus of our study is on image super-resolution (SR), which involves enhancing the texture of a low-resolution (LR) image. Recent advancements in this field have involved using high-resolution (HR) reference images to transfer relevant textures onto LR images. However, the existing methods have neglected to incorporate attention mechanisms for the transfer of HR textures, resulting in limitations when dealing with complex cases. In our paper, we present a new Texture Transformer Network for Image Super-Resolution (TTSR) that utilizes a transformer to treat LR and Ref images as queries and keys, respectively. TTSR comprises four interrelated modules optimized for image generation tasks, including a DNN-based texture extractor, a relevance embedding module, a hard-attention module for texture transfer, and a soft-attention module for texture synthesis. This framework promotes joint feature learning across LR and Ref images, enabling deep feature correspondences to be discovered by attention and facilitating precise texture feature transfer. The texture transformer can also be layered in a cross-scale manner, enabling texture recovery at different magnification levels. Our extensive experiments demonstrate that TTSR surpasses current state-of-the-art methods in both quantitative and qualitative evaluations.",1
"Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models have some limitations in different applications. For example, a VAE easily suffers from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve better disentangling and reconstruction quality than the existing methods. For language modelling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality of generated images compared to the original VAE.",0
"Variational Autoencoders (VAE) and their various forms have been extensively employed in diverse applications such as image and dialog generation, and disentangled representation learning. Despite this, existing VAE models exhibit limitations in different applications. For instance, language modeling is prone to KL vanishing, while disentangling may result in low reconstruction quality. To overcome these challenges, we present a novel framework, ControlVAE, which combines a controller inspired by automatic control theory with the basic VAE to enhance the performance of generative models. We introduce a non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically adjust the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. Our approach is evaluated using three applications, including language modeling, disentangled representation learning, and image generation. The results demonstrate that ControlVAE offers better disentangling and reconstruction quality than existing methods. In language modeling, it not only prevents KL-vanishing but also enhances the diversity of generated text. Finally, we show that ControlVAE enhances the reconstruction quality of generated images compared to the original VAE.",1
"This paper addresses two crucial problems of learning disentangled image representations, namely controlling the degree of disentanglement during image editing, and balancing the disentanglement strength and the reconstruction quality. To encourage disentanglement, we devise a distance covariance based decorrelation regularization. Further, for the reconstruction step, our model leverages a soft target representation combined with the latent image code. By exploring the real-valued space of the soft target representation, we are able to synthesize novel images with the designated properties. To improve the perceptual quality of images generated by autoencoder (AE)-based models, we extend the encoder-decoder architecture with the generative adversarial network (GAN) by collapsing the AE decoder and the GAN generator into one. We also design a classification based protocol to quantitatively evaluate the disentanglement strength of our model. Experimental results showcase the benefits of the proposed model.",0
"In this paper, we tackle two major issues related to disentangled image representations. Firstly, we address the challenge of controlling the extent of disentanglement while editing images and secondly, we focus on striking a balance between the strength of disentanglement and the quality of reconstruction. To promote disentanglement, we introduce a decorrelation regularization technique based on distance covariance. Additionally, for the reconstruction process, our model employs a combination of the latent image code and a soft target representation. By exploring the continuous space of the soft target representation, we can generate new images with specific characteristics. To enhance the visual appeal of images produced by autoencoder (AE)-based models, we merge the AE decoder and the GAN generator into a single entity using a generative adversarial network (GAN). Furthermore, we develop a classification-based approach for quantitatively assessing the disentanglement strength of our model. Our experimental outcomes demonstrate the effectiveness of our proposed model.",1
"We introduce a simple but effective unsupervised method for generating realistic and diverse images. We train a class-conditional GAN model without using manually annotated class labels. Instead, our model is conditional on labels automatically derived from clustering in the discriminator's feature space. Our clustering step automatically discovers diverse modes, and explicitly requires the generator to cover them. Experiments on standard mode collapse benchmarks show that our method outperforms several competing methods when addressing mode collapse. Our method also performs well on large-scale datasets such as ImageNet and Places365, improving both image diversity and standard quality metrics, compared to previous methods.",0
"Our approach presents a straightforward yet potent way of producing a range of authentic images without supervision. We utilize a class-conditional GAN framework that does not depend on class labels obtained through manual annotation. Instead, our model relies on labels automatically generated from the discriminator's feature space clustering process. By using this approach, our model can identify and cover different modes, resulting in diverse image output. Our experiments on commonly used mode collapse benchmarks indicate that our method surpasses other methods in tackling mode collapse. Additionally, our method also performs well on extensive datasets such as ImageNet and Places365. Compared to previous approaches, our method enhances both image variety and quality metrics.",1
"Generative adversarial networks (GANs) are widely used in image generation tasks, yet the generated images are usually lack of texture details. In this paper, we propose a general framework, called Progressively Unfreezing Perceptual GAN (PUPGAN), which can generate images with fine texture details. Particularly, we propose an adaptive perceptual discriminator with a pre-trained perceptual feature extractor, which can efficiently measure the discrepancy between multi-level features of the generated and real images. In addition, we propose a progressively unfreezing scheme for the adaptive perceptual discriminator, which ensures a smooth transfer process from a large scale classification task to a specified image generation task. The qualitative and quantitative experiments with comparison to the classical baselines on three image generation tasks, i.e. single image super-resolution, paired image-to-image translation and unpaired image-to-image translation demonstrate the superiority of PUPGAN over the compared approaches.",0
"Although Generative adversarial networks (GANs) are commonly used for creating images, they often lack texture details. Thus, we introduce a new framework called Progressively Unfreezing Perceptual GAN (PUPGAN) that generates images with fine texture details. The framework includes an adaptive perceptual discriminator with a pre-trained feature extractor that can measure the difference between multi-level features of the generated and real images. We also propose a progressively unfreezing method for the adaptive perceptual discriminator, which guarantees a smooth transition from a large-scale classification task to a specific image generation task. Our qualitative and quantitative experiments on three image generation tasks, such as single image super-resolution, paired image-to-image translation, and unpaired image-to-image translation, indicate that PUPGAN is superior to other conventional methods.",1
"Generative adversarial networks (GANs) have enjoyed much success in learning high-dimensional distributions. Learning objectives approximately minimize an $f$-divergence ($f$-GANs) or an integral probability metric (Wasserstein GANs) between the model and the data distribution using a discriminator. Wasserstein GANs enjoy superior empirical performance, but in $f$-GANs the discriminator can be interpreted as a density ratio estimator which is necessary in some GAN applications. In this paper, we bridge the gap between $f$-GANs and Wasserstein GANs (WGANs). First, we list two constraints over variational $f$-divergence estimation objectives that preserves the optimal solution. Next, we minimize over a Lagrangian relaxation of the constrained objective, and show that it generalizes critic objectives of both $f$-GAN and WGAN. Based on this generalization, we propose a novel practical objective, named KL-Wasserstein GAN (KL-WGAN). We demonstrate empirical success of KL-WGAN on synthetic datasets and real-world image generation benchmarks, and achieve state-of-the-art FID scores on CIFAR10 image generation.",0
"GANs have been highly successful in learning high-dimensional distributions by approximately minimizing an $f$-divergence or an integral probability metric using a discriminator. Wasserstein GANs have shown better empirical performance, while $f$-GANs use a discriminator as a density ratio estimator that's essential in some GAN applications. This paper bridges the gap between $f$-GANs and Wasserstein GANs by listing two constraints over variational $f$-divergence estimation objectives that maintain the optimal solution. It then minimizes a Lagrangian relaxation of the constrained objective and demonstrates that it generalizes critic objectives of both $f$-GAN and WGAN. Based on this generalization, the paper proposes a new practical objective called KL-Wasserstein GAN (KL-WGAN), which is shown to achieve state-of-the-art FID scores on CIFAR10 image generation, along with empirical success on synthetic datasets and real-world image generation benchmarks.",1
"Implicit Generative Models (IGMs) such as GANs have emerged as effective data-driven models for generating samples, particularly images. In this paper, we formulate the problem of learning an IGM as minimizing the expected distance between characteristic functions. Specifically, we minimize the distance between characteristic functions of the real and generated data distributions under a suitably-chosen weighting distribution. This distance metric, which we term as the characteristic function distance (CFD), can be (approximately) computed with linear time-complexity in the number of samples, in contrast with the quadratic-time Maximum Mean Discrepancy (MMD). By replacing the discrepancy measure in the critic of a GAN with the CFD, we obtain a model that is simple to implement and stable to train. The proposed metric enjoys desirable theoretical properties including continuity and differentiability with respect to generator parameters, and continuity in the weak topology. We further propose a variation of the CFD in which the weighting distribution parameters are also optimized during training; this obviates the need for manual tuning, and leads to an improvement in test power relative to CFD. We demonstrate experimentally that our proposed method outperforms WGAN and MMD-GAN variants on a variety of unsupervised image generation benchmarks.",0
"Effective data-driven models for generating samples, particularly images, have emerged in the form of Implicit Generative Models (IGMs) such as GANs. In this study, we present a formulation of the problem of learning an IGM by minimizing the expected distance between characteristic functions. Specifically, we aim to minimize the distance between the characteristic functions of the real and generated data distributions using a suitable weighting distribution. We introduce a new distance metric, called the characteristic function distance (CFD), which can be approximately computed with linear time-complexity in the number of samples, in contrast to the quadratic-time Maximum Mean Discrepancy (MMD). By using the CFD as the discrepancy measure in the critic of a GAN, we achieve a model that is both simple to implement and stable to train. The proposed metric has desirable theoretical properties, including continuity and differentiability with respect to generator parameters, and continuity in the weak topology. Furthermore, we propose a variation of the CFD where the weighting distribution parameters are also optimized during training, eliminating the need for manual tuning and leading to improved test power. Our experimental results demonstrate that our proposed method outperforms WGAN and MMD-GAN variants on a variety of unsupervised image generation benchmarks.",1
"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments - where compute and power are restricted by size/cost - for testing and prototyping.",0
"Our study focuses on using Deep Learning (DL) to segment fish in a small dataset of low-resolution images with noise generated by a multibeam echosounder (MBES). We utilize recent developments in Convolutional Neural Networks (CNNs) to accomplish semantic segmentation and create an end-to-end approach for predicting fish/non-fish probability for all range-azimuth positions in an imaging sonar. We train and test our model using self-collected data from the Danish Sound and the Faroe Islands, and have developed techniques to achieve satisfactory performance and generalization despite the limited dataset size. Our model effectively separates noise and non-targets from real targets by leveraging semantic context. Additionally, we present strategies for deploying models on low-cost embedded platforms, ideal for edge environments where compute and power are restricted by size and cost. This allows for higher performance during testing and prototyping.",1
"Generative Adversarial Networks (GANs) triggered an increased interest in problem of image generation due to their improved output image quality and versatility for expansion towards new methods. Numerous GAN-based works attempt to improve generation by architectural and loss-based extensions. We argue that one of the crucial points to improve the GAN performance in terms of realism and similarity to the original data distribution is to be able to provide the model with a capability to learn the spatial structure in data. To that end, we propose the DeshuffleGAN to enhance the learning of the discriminator and the generator, via a self-supervision approach. Specifically, we introduce a deshuffling task that solves a puzzle of randomly shuffled image tiles, which in turn helps the DeshuffleGAN learn to increase its expressive capacity for spatial structure and realistic appearance. We provide experimental evidence for the performance improvement in generated images, compared to the baseline methods, which is consistently observed over two different datasets.",0
"Due to their ability to produce high-quality images and adapt to new methods, Generative Adversarial Networks (GANs) have sparked more interest in image generation. To further improve GANs, many works have focused on architectural and loss-based extensions. However, we believe that one crucial aspect to enhance GAN performance and increase realism and similarity to the original data distribution is teaching the model to learn spatial structure in data. To achieve this, we suggest the DeshuffleGAN, which uses a self-supervision approach to improve the generator and discriminator's learning. The DeshuffleGAN introduces a deshuffling task by solving a puzzle of randomly shuffled image tiles, which helps increase the model's expressive capacity for spatial structure and realistic appearance. Our experiments show that the DeshuffleGAN outperforms baseline methods in generating images, with consistent results across two different datasets.",1
"This paper demonstrates how a Transformer Neural Network can be used to learn a Generative Model from a single path-based example image. We further show how a data set can be generated from the example image and how the model can be used to generate a large set of deviated images, which still represent the original image's style and concept.",0
"In this paper, it is illustrated how a Generative Model can be acquired from a solitary path-based image by employing a Transformer Neural Network. Additionally, we exhibit how a dataset can be produced from the exemplar image and how the acquired model can produce numerous variant images while preserving the original image's style and idea.",1
"While deep learning technologies are now capable of generating realistic images confusing humans, the research efforts are turning to the synthesis of images for more concrete and application-specific purposes. Facial image generation based on vocal characteristics from speech is one of such important yet challenging tasks. It is the key enabler to influential use cases of image generation, especially for business in public security and entertainment. Existing solutions to the problem of speech2face renders limited image quality and fails to preserve facial similarity due to the lack of quality dataset for training and appropriate integration of vocal features. In this paper, we investigate these key technical challenges and propose Speech Fusion to Face, or SF2F in short, attempting to address the issue of facial image quality and the poor connection between vocal feature domain and modern image generation models. By adopting new strategies on data model and training, we demonstrate dramatic performance boost over state-of-the-art solution, by doubling the recall of individual identity, and lifting the quality score from 15 to 19 based on the mutual information score with VGGFace classifier.",0
"Although deep learning technologies have the ability to generate images that can deceive humans, research efforts are shifting towards the creation of images that serve more specific applications. One of these crucial and challenging tasks is generating facial images based on vocal characteristics from speech. This is essential for image generation in fields such as business, public security, and entertainment. However, current solutions to the problem of speech2face have produced limited image quality and failed to maintain facial similarity due to inadequate training datasets and the improper integration of vocal features. In this article, we investigate these technical challenges and propose a solution called Speech Fusion to Face (SF2F) to address the issues of facial image quality and the weak connection between vocal feature domain and modern image generation models. By adopting new approaches to data modelling and training, we demonstrate a significant increase in performance compared to existing solutions. Our method doubles the recall of individual identity and improves the quality score from 15 to 19 based on the mutual information score with VGGFace classifier.",1
"Being an emerging class of in-memory computing architecture, brain-inspired hyperdimensional computing (HDC) mimics brain cognition and leverages random hypervectors (i.e., vectors with a dimensionality of thousands or even more) to represent features and to perform classification tasks. The unique hypervector representation enables HDC classifiers to exhibit high energy efficiency, low inference latency and strong robustness against hardware-induced bit errors. Consequently, they have been increasingly recognized as an appealing alternative to or even replacement of traditional deep neural networks (DNNs) for local on device classification, especially on low-power Internet of Things devices. Nonetheless, unlike their DNN counterparts, state-of-the-art designs for HDC classifiers are mostly security-oblivious, casting doubt on their safety and immunity to adversarial inputs. In this paper, we study for the first time adversarial attacks on HDC classifiers and highlight that HDC classifiers can be vulnerable to even minimally-perturbed adversarial samples. Concretely, using handwritten digit classification as an example, we construct a HDC classifier and formulate a grey-box attack problem, where an attacker's goal is to mislead the target HDC classifier to produce erroneous prediction labels while keeping the amount of added perturbation noise as little as possible. Then, we propose a modified genetic algorithm to generate adversarial samples within a reasonably small number of queries. Our results show that adversarial images generated by our algorithm can successfully mislead the HDC classifier to produce wrong prediction labels with a high probability (i.e., 78% when the HDC classifier uses a fixed majority rule for decision). Finally, we also present two defense strategies -- adversarial training and retraining-- to strengthen the security of HDC classifiers.",0
"Brain-inspired hyperdimensional computing (HDC) is a novel in-memory computing architecture that imitates brain cognition through the use of random hypervectors to represent features and perform classification tasks. HDC classifiers have numerous advantages over traditional deep neural networks, including high energy efficiency, low inference latency, and robustness against hardware-induced bit errors. However, security remains a concern, as state-of-the-art HDC designs are typically security-oblivious. This paper presents the first study of adversarial attacks on HDC classifiers, demonstrating that even minimally-perturbed adversarial samples can successfully mislead the classifier to produce incorrect predictions. Using handwritten digit classification as an example, we formulate a grey-box attack problem and propose a modified genetic algorithm to generate adversarial samples. Our results show a high probability of successful misclassification using our algorithm, prompting the development of two defense strategies: adversarial training and retraining. These strategies aim to enhance the security of HDC classifiers and protect against adversarial attacks.",1
"Black-Box Optimization (BBO) methods can find optimal policies for systems that interact with complex environments with no analytical representation. As such, they are of interest in many Artificial Intelligence (AI) domains. Yet classical BBO methods fall short in high-dimensional non-convex problems. They are thus often overlooked in real-world AI tasks. Here we present a BBO method, termed Explicit Gradient Learning (EGL), that is designed to optimize high-dimensional ill-behaved functions. We derive EGL by finding weak-spots in methods that fit the objective function with a parametric Neural Network (NN) model and obtain the gradient signal by calculating the parametric gradient. Instead of fitting the function, EGL trains a NN to estimate the objective gradient directly. We prove the convergence of EGL in convex optimization and its robustness in the optimization of integrable functions. We evaluate EGL and achieve state-of-the-art results in two challenging problems: (1) the COCO test suite against an assortment of standard BBO methods; and (2) in a high-dimensional non-convex image generation task.",0
"Many domains in Artificial Intelligence (AI) are interested in Black-Box Optimization (BBO) methods, which can find optimal policies for systems interacting with complex environments without analytical representation. However, classical BBO methods are inadequate for high-dimensional non-convex problems and are often disregarded in real-world AI tasks. This article presents a new BBO method, Explicit Gradient Learning (EGL), specifically designed to optimize high-dimensional ill-behaved functions. Unlike other methods that fit the objective function with a parametric Neural Network (NN) model to obtain the gradient signal, EGL trains a NN to estimate the objective gradient directly. The article proves the convergence of EGL in convex optimization and its robustness in the optimization of integrable functions. The EGL method achieves state-of-the-art results in two challenging problems: (1) the COCO test suite against an assortment of standard BBO methods; and (2) in a high-dimensional non-convex image generation task.",1
"Recognizing car license plates in natural scene images is an important yet still challenging task in realistic applications. Many existing approaches perform well for license plates collected under constrained conditions, eg, shooting in frontal and horizontal view-angles and under good lighting conditions. However, their performance drops significantly in an unconstrained environment that features rotation, distortion, occlusion, blurring, shading or extreme dark or bright conditions. In this work, we propose a robust framework for license plate recognition in the wild. It is composed of a tailored CycleGAN model for license plate image generation and an elaborate designed image-to-sequence network for plate recognition. On one hand, the CycleGAN based plate generation engine alleviates the exhausting human annotation work. Massive amount of training data can be obtained with a more balanced character distribution and various shooting conditions, which helps to boost the recognition accuracy to a large extent. On the other hand, the 2D attentional based license plate recognizer with an Xception-based CNN encoder is capable of recognizing license plates with different patterns under various scenarios accurately and robustly. Without using any heuristics rule or post-processing, our method achieves the state-of-the-art performance on four public datasets, which demonstrates the generality and robustness of our framework. Moreover, we released a new license plate dataset, named ""CLPD"", with 1200 images from all 31 provinces in mainland China. The dataset can be available from: https://github.com/wangpengnorman/CLPD_dataset.",0
"Recognizing license plates in natural scene images remains a challenging task in realistic applications. Although many existing approaches perform well for license plates captured under certain conditions, such as frontal and horizontal view-angles and good lighting, their performance significantly decreases in unconstrained environments featuring rotation, distortion, occlusion, blurring, shading, or extreme lighting conditions. To address this issue, we propose a robust framework for license plate recognition in the wild. Our framework comprises a tailored CycleGAN model for license plate image generation and an elaborately designed image-to-sequence network for plate recognition. The CycleGAN based plate generation engine reduces the need for human annotation work and enables the acquisition of a massive amount of training data with a more balanced character distribution and various shooting conditions, thereby significantly enhancing recognition accuracy. Additionally, our 2D attentional based license plate recognizer with an Xception-based CNN encoder accurately and robustly recognizes license plates with different patterns under various scenarios. Without using any heuristics rule or post-processing, our method achieves state-of-the-art performance on four public datasets, demonstrating the generality and robustness of our framework. Furthermore, we have released a new license plate dataset, ""CLPD,"" comprising 1200 images from all 31 provinces in mainland China, which is available at https://github.com/wangpengnorman/CLPD_dataset.",1
"We address the problem of reposing an image of a human into any desired novel pose. This conditional image-generation task requires reasoning about the 3D structure of the human, including self-occluded body parts. Most prior works are either based on 2D representations or require fitting and manipulating an explicit 3D body mesh. Based on the recent success in deep learning-based volumetric representations, we propose to implicitly learn a dense feature volume from human images, which lends itself to simple and intuitive manipulation through explicit geometric warping. Once the latent feature volume is warped according to the desired pose change, the volume is mapped back to RGB space by a convolutional decoder. Our state-of-the-art results on the DeepFashion and the iPER benchmarks indicate that dense volumetric human representations are worth investigating in more detail.",0
"Our focus is on the task of generating a new pose for an image of a human. To accomplish this, we must have an understanding of the human's 3D structure, including body parts that may be hidden from view. Previous methods have relied on either 2D representations or the manipulation of a 3D body mesh. However, we propose a new approach that utilizes deep learning-based volumetric representations. By learning a dense feature volume from human images, we can easily manipulate and warp the image to achieve the desired pose. Once the feature volume is transformed, it can be mapped back to RGB space through a convolutional decoder. Our results on the DeepFashion and iPER benchmarks demonstrate the potential of this approach and suggest further investigation into dense volumetric human representations.",1
"Deep neural networks for video-based eye tracking have demonstrated resilience to noisy environments, stray reflections, and low resolution. However, to train these networks, a large number of manually annotated images are required. To alleviate the cumbersome process of manual labeling, computer graphics rendering is employed to automatically generate a large corpus of annotated eye images under various conditions. In this work, we introduce a synthetic eye image generation platform that improves upon previous work by adding features such as an active deformable iris, an aspherical cornea, retinal retro-reflection, gaze-coordinated eye-lid deformations, and blinks. To demonstrate the utility of our platform, we render images reflecting the represented gaze distributions inherent in two publicly available datasets, NVGaze and OpenEDS. We also report on the performance of two semantic segmentation architectures (SegNet and RITnet) trained on rendered images and tested on the original datasets.",0
"Video-based eye tracking using deep neural networks has shown to be effective in challenging environments with low resolution and stray reflections. However, training these networks requires a large number of manually annotated images, which can be a tedious task. To address this, computer graphics rendering is used to generate annotated eye images automatically in various conditions. This study introduces a synthetic eye image generation platform that enhances previous work by incorporating an active deformable iris, aspherical cornea, retinal retro-reflection, gaze-coordinated eye-lid deformations, and blinks. The platform is tested by rendering images reflecting gaze distributions in two publicly available datasets, NVGaze and OpenEDS. The study also evaluates the performance of two semantic segmentation architectures, SegNet and RITnet, trained on rendered images and tested on the original datasets.",1
"Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g. whether an image generated from ""a car driving down the street"" contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.",0
"Generating realistic images using generative adversarial networks conditioned on textual image descriptions is feasible. However, despite their capability, current techniques still face obstacles in creating images based on intricate image captions from a diverse domain. Additionally, it is challenging to quantitatively evaluate these text-to-image models since most metrics only evaluate image quality and not the conformity between the image and its caption. To overcome these challenges, we present a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA), which evaluates images based on an image caption. The SOA uses a pre-trained object detector to determine if a generated image has objects mentioned in the image caption. For instance, it verifies whether an image created from ""a car driving down the street"" contains a car. We conducted a user study comparing several text-to-image models and found that our SOA metric ranks the models in the same way as humans, while other metrics such as the Inception Score do not. Our evaluation also demonstrates that models that explicitly model objects perform better than models that only model global image characteristics.",1
"In this paper, we propose a novel generative network (SegAttnGAN) that utilizes additional segmentation information for the text-to-image synthesis task. As the segmentation data introduced to the model provides useful guidance on the generator training, the proposed model can generate images with better realism quality and higher quantitative measures compared with the previous state-of-art methods. We achieved Inception Score of 4.84 on the CUB dataset and 3.52 on the Oxford-102 dataset. Besides, we tested the self-attention SegAttnGAN which uses generated segmentation data instead of masks from datasets for attention and achieved similar high-quality results, suggesting that our model can be adapted for the text-to-image synthesis task.",0
"Our paper introduces SegAttnGAN, a novel generative network that leverages segmentation information to enhance the text-to-image synthesis process. By incorporating this data into our model, we provide valuable guidance for the generator's training, resulting in images of superior realism and higher quantitative measures than previous state-of-the-art approaches. Our experiments on the CUB and Oxford-102 datasets yielded Inception Scores of 4.84 and 3.52, respectively. Furthermore, we tested a self-attention variant of SegAttnGAN that uses generated segmentation data instead of masks from the datasets, achieving similarly impressive results. These findings suggest that our model can be adapted effectively for text-to-image synthesis.",1
"Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter-conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.",0
"The ability to comprehend 3D shapes from 2D images without labeled information is a promising approach to understanding the real world without incurring annotation costs. We have developed a new generative model, RGBD-GAN, that uses unsupervised learning to create 3D representations from 2D images. Our approach enables the generation of camera parameter-conditional and depth images without any 3D annotations such as camera poses or depth. To achieve this, we use an explicit 3D consistency loss and the ordinal GAN objective, which is effective for any image generator architecture, including DCGAN and StyleGAN. Our experiments demonstrate that our method can learn 3D representations from 2D images using various generator architectures.",1
"Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.   Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep_image_prior .",0
"Image generation and restoration are commonly accomplished through deep convolutional networks. Typically, their impressive performance is attributed to their ability to acquire authentic image priors from numerous sample images. Nevertheless, our research demonstrates that the structure of a generator network alone can capture a considerable amount of low-level image statistics before any learning takes place. By utilizing a randomly-initialized neural network as a handcrafted prior, we exhibit outstanding outcomes in standard inverse problems such as denoising, super-resolution, and inpainting. Additionally, this same prior can be used to diagnose deep neural representations and restore images based on flash-no flash input pairs. Our approach emphasizes the inductive bias captured by standard generator network architectures and unites two popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors like self-similarity. Access to our code and supplementary material is provided at https://dmitryulyanov.github.io/deep_image_prior.",1
"Understanding a scene by decoding the visual relationships depicted in an image has been a long studied problem. While the recent advances in deep learning and the usage of deep neural networks have achieved near human accuracy on many tasks, there still exists a pretty big gap between human and machine level performance when it comes to various visual relationship detection tasks. Developing on earlier tasks like object recognition, segmentation and captioning which focused on a relatively coarser image understanding, newer tasks have been introduced recently to deal with a finer level of image understanding. A Scene Graph is one such technique to better represent a scene and the various relationships present in it. With its wide number of applications in various tasks like Visual Question Answering, Semantic Image Retrieval, Image Generation, among many others, it has proved to be a useful tool for deeper and better visual relationship understanding. In this paper, we present a detailed survey on the various techniques for scene graph generation, their efficacy to represent visual relationships and how it has been used to solve various downstream tasks. We also attempt to analyze the various future directions in which the field might advance in the future. Being one of the first papers to give a detailed survey on this topic, we also hope to give a succinct introduction to scene graphs, and guide practitioners while developing approaches for their applications.",0
"Decoding the visual relationships depicted in an image to understand a scene has been a challenging task that has been extensively studied. Despite recent advancements in deep learning and the use of deep neural networks, there remains a significant gap between human and machine performance in various visual relationship detection tasks. To address this, newer tasks have been introduced that focus on a finer level of image understanding, building on earlier tasks such as object recognition, segmentation, and captioning. One technique that has emerged is the Scene Graph, which better represents a scene and its relationships. This technique has numerous applications in tasks such as Visual Question Answering, Semantic Image Retrieval, and Image Generation, making it a valuable tool for better visual relationship understanding. In this paper, we provide a comprehensive survey of scene graph generation techniques, their effectiveness in representing visual relationships, and their ability to solve downstream tasks. Additionally, we analyze potential future directions for the field. As one of the first papers to offer a detailed survey of this topic, we aim to provide a concise introduction to scene graphs and guide practitioners in developing approaches for their applications.",1
"Adversarial examples have emerged as a significant threat to machine learning algorithms, especially to the convolutional neural networks (CNNs). In this paper, we propose two quantization-based defense mechanisms, Constant Quantization (CQ) and Trainable Quantization (TQ), to increase the robustness of CNNs against adversarial examples. CQ quantizes input pixel intensities based on a ""fixed"" number of quantization levels, while in TQ, the quantization levels are ""iteratively learned during the training phase"", thereby providing a stronger defense mechanism. We apply the proposed techniques on undefended CNNs against different state-of-the-art adversarial attacks from the open-source \textit{Cleverhans} library. The experimental results demonstrate 50%-96% and 10%-50% increase in the classification accuracy of the perturbed images generated from the MNIST and the CIFAR-10 datasets, respectively, on commonly used CNN (Conv2D(64, 8x8) - Conv2D(128, 6x6) - Conv2D(128, 5x5) - Dense(10) - Softmax()) available in \textit{Cleverhans} library.",0
"The threat of adversarial examples poses a significant challenge to machine learning algorithms, particularly convolutional neural networks (CNNs). This study suggests two defense mechanisms, Constant Quantization (CQ) and Trainable Quantization (TQ), that utilize quantization to enhance the robustness of CNNs against adversarial examples. CQ quantizes input pixel intensities using a fixed number of quantization levels, whereas TQ learns the quantization levels iteratively during the training process, resulting in a stronger defense. The proposed techniques were applied to undefended CNNs that were subjected to various state-of-the-art adversarial attacks from the open-source Cleverhans library. The findings indicate that both CQ and TQ led to a 50%-96% and 10%-50% improvement in the classification accuracy of perturbed images from the MNIST and CIFAR-10 datasets, respectively, using the commonly used CNN (Conv2D(64, 8x8) - Conv2D(128, 6x6) - Conv2D(128, 5x5) - Dense(10) - Softmax()) provided in the Cleverhans library.",1
"We introduce an open-source toolkit, i.e., the deep Self End-to-end Learning Framework (deepSELF), as a toolkit of deep self end-to-end learning framework for multi-modal signals. To the best of our knowledge, it is the first public toolkit assembling a series of state-of-the-art deep learning technologies. Highlights of the proposed deepSELF toolkit include: First, it can be used to analyse a variety of multi-modal signals, including images, audio, and single or multi-channel sensor data. Second, we provide multiple options for pre-processing, e.g., filtering, or spectrum image generation by Fourier or wavelet transformation. Third, plenty of topologies in terms of NN, 1D/2D/3D CNN, and RNN/LSTM/GRU can be customised and a series of pretrained 2D CNN models, e.g., AlexNet, VGGNet, ResNet can be used easily. Last but not least, above these features, deepSELF can be flexibly used not only as a single model but also as a fusion of such.",0
"We present deepSELF, an open-source toolkit for deep self end-to-end learning framework for multi-modal signals. It is the first publicly available toolkit that combines state-of-the-art deep learning technologies. The deepSELF toolkit can analyze various multi-modal signals, such as images, audio, and single or multi-channel sensor data. Moreover, we offer multiple pre-processing options, such as filtering or generating spectrum images using Fourier or wavelet transformations. The toolkit also allows for customization of various topologies, including NN, 1D/2D/3D CNN, and RNN/LSTM/GRU, and has a range of pretrained 2D CNN models, such as AlexNet, VGGNet, and ResNet. Additionally, deepSELF can be flexibly used as a single model or as a fusion of models.",1
"In recent years, Generative Adversarial Networks (GANs) have improved steadily towards generating increasingly impressive real-world images. It is useful to steer the image generation process for purposes such as content creation. This can be done by conditioning the model on additional information. However, when conditioning on additional information, there still exists a large set of images that agree with a particular conditioning. This makes it unlikely that the generated image is exactly as envisioned by a user, which is problematic for practical content creation scenarios such as generating facial composites or stock photos. To solve this problem, we propose a single pipeline for text-to-image generation and manipulation. In the first part of our pipeline we introduce textStyleGAN, a model that is conditioned on text. In the second part of our pipeline we make use of the pre-trained weights of textStyleGAN to perform semantic facial image manipulation. The approach works by finding semantic directions in latent space. We show that this method can be used to manipulate facial images for a wide range of attributes. Finally, we introduce the CelebTD-HQ dataset, an extension to CelebA-HQ, consisting of faces and corresponding textual descriptions.",0
"Over the years, Generative Adversarial Networks (GANs) have made significant progress in generating realistic images, which is beneficial for content creation. To achieve this, additional information can be used to guide the image generation process. However, even with added information, there may be numerous images that match a particular condition. This can lead to impractical situations where the generated image may not precisely align with the user's vision, such as in creating facial composites or stock photos. To address this issue, we propose a single pipeline that uses text-to-image generation and manipulation. Our pipeline contains two parts: the first part introduces a model called textStyleGAN, which is conditioned on text, while the second part utilizes the pre-trained weights of textStyleGAN to manipulate semantic facial images. This is done by identifying semantic directions in latent space, which can be used to manipulate facial attributes. We demonstrate the effectiveness of our approach in manipulating facial images for a wide range of attributes. Lastly, we introduce the CelebTD-HQ dataset, which is an extension of CelebA-HQ that includes faces and corresponding textual descriptions.",1
"This research study proposes using Generative Adversarial Networks (GAN) that incorporate a two-dimensional measure of human memorability to generate memorable or non-memorable images of scenes. The memorability of the generated images is evaluated by modelling Visual Memory Schemas (VMS), which correspond to mental representations that human observers use to encode an image into memory. The VMS model is based upon the results of memory experiments conducted on human observers, and provides a 2D map of memorability. We impose a memorability constraint upon the latent space of a GAN by employing a VMS map prediction model as an auxiliary loss. We assess the difference in memorability between images generated to be memorable or non-memorable through an independent computational measure of memorability, and additionally assess the effect of memorability on the realness of the generated images.",0
"The proposed research study suggests using Generative Adversarial Networks (GAN) that integrate a two-dimensional gauge of human memorability to create images that are either memorable or non-memorable. The memorability of the generated images is measured by a Visual Memory Schemas (VMS) model, which represents the mental formations that human observers use to encode an image into their memory. The VMS model is based on the results of memory experiments conducted on human observers and provides a 2D map of memorability. To enforce a memorability constraint on the latent space of a GAN, we use a VMS map prediction model as an auxiliary loss. We compare the memorability of images generated to be memorable or non-memorable through an independent computational measure of memorability and also evaluate the impact of memorability on the authenticity of the generated images.",1
"We explore different design choices for injecting noise into generative adversarial networks (GANs) with the goal of disentangling the latent space. Instead of traditional approaches, we propose feeding multiple noise codes through separate fully-connected layers respectively. The aim is restricting the influence of each noise code to specific parts of the generated image. We show that disentanglement in the first layer of the generator network leads to disentanglement in the generated image. Through a grid-based structure, we achieve several aspects of disentanglement without complicating the network architecture and without requiring labels. We achieve spatial disentanglement, scale-space disentanglement, and disentanglement of the foreground object from the background style allowing fine-grained control over the generated images. Examples include changing facial expressions in face images, changing beak length in bird images, and changing car dimensions in car images. This empirically leads to better disentanglement scores than state-of-the-art methods on the FFHQ dataset.",0
"Our study delves into various design options for introducing noise into generative adversarial networks (GANs) to disentangle the latent space. Instead of conventional techniques, we suggest utilizing separate fully-connected layers to feed multiple noise codes, which restricts the influence of each code to specific parts of the generated image. We demonstrate that disentangling the first layer of the generator network yields disentanglement in the generated image. We use a grid-based structure to achieve spatial disentanglement, scale-space disentanglement, and disentanglement of foreground objects from background styles, providing precise control over the generated images. Our examples include modifying facial expressions in face images, changing beak length in bird images, and altering car dimensions in car images. Our approach yields better disentanglement scores than state-of-the-art methods on the FFHQ dataset. We achieve this without complex network architectures or labels.",1
"Removing the effect of illumination variation in images has been proved to be beneficial in many computer vision applications such as object recognition and semantic segmentation. Although generating illumination-invariant images has been studied in the literature before, it has not been investigated on real 4-channel (4D) data. In this study, we examine the quality of illumination-invariant images generated from red, green, blue, and near-infrared (RGBN) data. Our experiments show that the near-infrared channel substantively contributes toward removing illumination. As shown in our numerical and visual results, the illumination-invariant image obtained by RGBN data is superior compared to that obtained by RGB alone.",0
"Numerous computer vision applications, including object recognition and semantic segmentation, have seen benefits from removing illumination variation in images. While previous literature has explored the generation of illumination-invariant images, this approach has not yet been examined on real 4D data. Our study addresses this gap by evaluating the quality of illumination-invariant images produced from RGBN data, which includes red, green, blue, and near-infrared channels. Our findings demonstrate that the near-infrared channel significantly contributes to reducing illumination effects. Our numerical and visual results further reveal that the illumination-invariant image generated from RGBN data surpasses that of RGB alone.",1
"The paucity of large curated hand-labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. More interestingly, such labeling functions are often easily generalizable, thus allowing our framework to be extended to different setups, including self-supervised labeled image generation, zero-shot text to labeled image generation, transfer learning, and multi-task learning.",0
"The lack of extensive hand-labeled training data is a major hindrance to the implementation of machine learning models in various fields, including computer vision. A recent study called Data Programming has demonstrated that labeling functions in the form of distant supervision signals can be leveraged to obtain labels for specific data at a near-constant rate. Our research, Adversarial Data Programming (ADP), introduces an adversarial approach to generating data and a well-curated aggregate label using weak labeling functions. Furthermore, these labeling functions are often easily generalizable, which enables our framework to be adapted to different scenarios, such as self-supervised labeled image generation, zero-shot text to labeled image generation, transfer learning, and multi-task learning.",1
"Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.",0
"Computer vision has been greatly enhanced by neural networks, which can learn powerful representations of input data. However, end-to-end training for maximal overall performance has resulted in black-box models with hidden representations that lack interpretability. This is due to the fact that distributed coding is optimal for latent layers to improve their robustness, making it difficult to attribute meaning to parts of a hidden feature vector or individual neurons. To address this, we propose a method of interpretation that translates hidden representations into semantic concepts that are comprehensible to the user. The mapping between both domains must be bijective to ensure that semantic modifications in the target domain accurately alter the original representation. Our proposed invertible interpretation network can be applied on top of existing architectures without the need for modification or retraining. This approach allows for the translation of an original representation to an equivalent yet interpretable one without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. We also present an efficient approach for defining semantic concepts using only two images and an unsupervised strategy. Our experimental evaluation demonstrates the wide applicability of this method for interpreting existing classification and image generation networks, as well as for semantically guided image manipulation.",1
"Binary image based classification and retrieval of documents of an intellectual nature is a very challenging problem. Variations in the binary image generation mechanisms which are subject to the document artisan designer including drawing style, view-point, inclusion of multiple image components are plausible causes for increasing the complexity of the problem. In this work, we propose a method suitable to binary images which bridges some of the successes of deep learning (DL) to alleviate the problems introduced by the aforementioned variations. The method consists on extracting the shape of interest from the binary image and applying a non-Euclidean geometric neural-net architecture to learn the local and global spatial relationships of the shape. Empirical results show that our method is in some sense invariant to the image generation mechanism variations and achieves results outperforming existing methods in a patent image dataset benchmark.",0
"The task of classifying and retrieving intellectual documents using binary image technology presents a significant challenge. The complexity of this problem is compounded by the fact that binary image generation methods are diverse and can be influenced by various factors such as drawing style, view-point, and inclusion of multiple image components. In this study, we propose a binary image method that incorporates some of the successful principles of deep learning (DL) to address the issues arising from these variations. Our approach involves extracting the relevant shape from the binary image and applying a non-Euclidean geometric neural-net design to learn the spatial relationships of the shape at both local and global levels. Through empirical results, we demonstrate that our method is resilient to image generation mechanism variations and outperforms other methods in a patent image dataset evaluation.",1
"Gastric endoscopy is a standard clinical process that enables medical practitioners to diagnose various lesions inside a patient's stomach. If any lesion is found, it is very important to perceive the location of the lesion relative to the global view of the stomach. Our previous research showed that this could be addressed by reconstructing the whole stomach shape from chromoendoscopic images using a structure-from-motion (SfM) pipeline, in which indigo carmine (IC) blue dye sprayed images were used to increase feature matches for SfM by enhancing stomach surface's textures. However, spraying the IC dye to the whole stomach requires additional time, labor, and cost, which is not desirable for patients and practitioners. In this paper, we propose an alternative way to achieve whole stomach 3D reconstruction without the need of the IC dye by generating virtual IC-sprayed (VIC) images based on image-to-image style translation trained on unpaired real no-IC and IC-sprayed images. We have specifically investigated the effect of input and output color channel selection for generating the VIC images and found that translating no-IC green-channel images to IC-sprayed red-channel images gives the best SfM reconstruction result.",0
"Gastric endoscopy is a common medical procedure used to identify lesions in a patient's stomach. Accurately determining the location of such lesions in relation to the entire stomach is crucial. A previous study utilized a structure-from-motion (SfM) pipeline to reconstruct the entire stomach shape from chromoendoscopic images. The process involved spraying indigo carmine (IC) blue dye on the stomach to enhance its textures for SfM. However, this method was deemed time-consuming, labor-intensive, and costly. Hence, this paper proposes an alternative approach that generates virtual IC-sprayed (VIC) images without the need for IC dye. The process involves using image-to-image style translation to transform unpaired real no-IC and IC-sprayed images into VIC images. The study examined the effect of input and output color channel selection on VIC image generation and found that translating no-IC green-channel images to IC-sprayed red-channel images yielded the best SfM reconstruction result.",1
"Lung cancer is the commonest cause of cancer deaths worldwide, and its mortality can be reduced significantly by performing early diagnosis and screening. Since the 1960s, driven by the pressing needs to accurately and effectively interpret the massive volume of chest images generated daily, computer-assisted diagnosis of pulmonary nodule has opened up new opportunities to relax the limitation from physicians' subjectivity, experiences and fatigue. And the fair access to the reliable and affordable computer-assisted diagnosis will fight the inequalities in incidence and mortality between populations. It has been witnessed that significant and remarkable advances have been achieved since the 1980s, and consistent endeavors have been exerted to deal with the grand challenges on how to accurately detect the pulmonary nodules with high sensitivity at low false-positives rate as well as on how to precisely differentiate between benign and malignant nodules. There is a lack of comprehensive examination of the techniques' development which is evolving the pulmonary nodules diagnosis from classical approaches to machine learning-assisted decision support. The main goal of this investigation is to provide a comprehensive state-of-the-art review of the computer-assisted nodules detection and benign-malignant classification techniques developed over 3 decades, which have evolved from the complicated ad hoc analysis pipeline of conventional approaches to the simplified seamlessly integrated deep learning techniques. This review also identifies challenges and highlights opportunities for future work in learning models, learning algorithms and enhancement schemes for bridging current state to future prospect and satisfying future demand.",0
"Early diagnosis and screening can significantly reduce the mortality of lung cancer, which is the most common cause of cancer deaths worldwide. To accurately interpret the massive volume of chest images generated daily, computer-assisted diagnosis of pulmonary nodules was developed in the 1960s, allowing for more objective and less fatigued diagnoses. Access to reliable and affordable computer-assisted diagnosis can help address inequalities in incidence and mortality between populations. Significant advances have been made since the 1980s, with ongoing efforts to improve detection sensitivity and accurately differentiate between benign and malignant nodules. This investigation aims to comprehensively review the development of computer-assisted nodules detection and benign-malignant classification techniques over three decades, from classical approaches to machine learning-assisted decision support. Future work should focus on learning models, algorithms, and enhancement schemes to meet future demand and bridge the current state to future prospects.",1
"Text-to-image synthesis is the task of generating images from text descriptions. Image generation, by itself, is a challenging task. When we combine image generation and text, we bring complexity to a new level: we need to combine data from two different modalities. Most of recent works in text-to-image synthesis follow a similar approach when it comes to neural architectures. Due to aforementioned difficulties, plus the inherent difficulty of training GANs at high resolutions, most methods have adopted a multi-stage training strategy. In this paper we shift the architectural paradigm currently used in text-to-image methods and show that an effective neural architecture can achieve state-of-the-art performance using a single stage training with a single generator and a single discriminator. We do so by applying deep residual networks along with a novel sentence interpolation strategy that enables learning a smooth conditional space. Finally, our work points a new direction for text-to-image research, which has not experimented with novel neural architectures recently.",0
"Synthesizing images from text descriptions is a challenging task that involves combining data from two different modalities. This complexity is further magnified due to the inherent difficulty of training GANs at high resolutions. As a result, most recent approaches in text-to-image synthesis employ a multi-stage training strategy using similar neural architectures. However, in this study, we deviate from this paradigm and demonstrate that a single-stage training approach with a single generator and discriminator using effective neural architecture can achieve state-of-the-art performance. We achieve this by using deep residual networks along with a novel sentence interpolation strategy that enables smooth conditional space learning. Our work opens up new directions for text-to-image research, which has not explored novel neural architectures recently.",1
"Domain adaptive person re-identification (re-ID) is a challenging task, especially when person identities in target domains are unknown. Existing methods attempt to address this challenge by transferring image styles or aligning feature distributions across domains, whereas the rich unlabeled samples in target domains are not sufficiently exploited. This paper presents a novel augmented discriminative clustering (AD-Cluster) technique that estimates and augments person clusters in target domains and enforces the discrimination ability of re-ID models with the augmented clusters. AD-Cluster is trained by iterative density-based clustering, adaptive sample augmentation, and discriminative feature learning. It learns an image generator and a feature encoder which aim to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space in an adversarial min-max manner. Finally, AD-Cluster increases the diversity of sample clusters and improves the discrimination capability of re-ID models greatly. Extensive experiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms the state-of-the-art with large margins.",0
"Person re-identification in domains that are adaptive is a complex undertaking, especially when the identities of individuals in the target domain are unknown. Current methods aim to tackle this issue by transferring image styles or aligning feature distributions across domains, however, they do not fully utilize the rich unlabeled samples available in the target domains. This study introduces a new technique called augmented discriminative clustering (AD-Cluster) that estimates and enhances person clusters in target domains and improves the discrimination ability of re-ID models through the augmented clusters. AD-Cluster is trained using density-based clustering, adaptive sample augmentation, and discriminative feature learning in an iterative manner. It creates an image generator and feature encoder that seek to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space through an adversarial min-max approach. AD-Cluster ultimately increases the diversity of sample clusters and significantly improves the discrimination capability of re-ID models. Extensive experiments conducted on Market-1501 and DukeMTMC-reID demonstrate that AD-Cluster outperforms existing methods by a large margin.",1
"The Deepfake phenomenon has become very popular nowadays thanks to the possibility to create incredibly realistic images using deep learning tools, based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we focus on the analysis of Deepfakes of human faces with the objective of creating a new detection method able to detect a forensics trace hidden in images: a sort of fingerprint left in the image generation process. The proposed technique, by means of an Expectation Maximization (EM) algorithm, extracts a set of local features specifically addressed to model the underlying convolutional generative process. Ad-hoc validation has been employed through experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as ground-truth for non-fakes. Results demonstrated the effectiveness of the technique in distinguishing the different architectures and the corresponding generation process.",0
"Nowadays, the Deepfake trend has gained immense popularity due to the ability to create highly realistic images using deep learning tools, primarily based on specific Generative Adversarial Networks (GANs). This study concentrates on examining Deepfakes of human faces to develop a novel detection approach capable of detecting a trace of forensics hidden in images, similar to a fingerprint left during image generation. The proposed technique employs an Expectation Maximization (EM) algorithm to extract a set of local features that specifically model the underlying convolutional generative process. To validate the technique, it underwent experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as the ground-truth for non-fakes. The results proved the effectiveness of the approach in distinguishing the various architectures and their corresponding generation processes.",1
"Humans are capable of learning new tasks without forgetting previous ones, while neural networks fail due to catastrophic forgetting between new and previously-learned tasks. We consider a class-incremental setting which means that the task-ID is unknown at inference time. The imbalance between old and new classes typically results in a bias of the network towards the newest ones. This imbalance problem can either be addressed by storing exemplars from previous tasks, or by using image replay methods. However, the latter can only be applied to toy datasets since image generation for complex datasets is a hard problem.   We propose a solution to the imbalance problem based on generative feature replay which does not require any exemplars. To do this, we split the network into two parts: a feature extractor and a classifier. To prevent forgetting, we combine generative feature replay in the classifier with feature distillation in the feature extractor. Through feature generation, our method reduces the complexity of generative replay and prevents the imbalance problem. Our approach is computationally efficient and scalable to large datasets. Experiments confirm that our approach achieves state-of-the-art results on CIFAR-100 and ImageNet, while requiring only a fraction of the storage needed for exemplar-based continual learning. Code available at \url{https://github.com/xialeiliu/GFR-IL}.",0
"In contrast to humans, neural networks suffer from catastrophic forgetting when learning new tasks, resulting in an imbalance between old and new classes. This problem can be mitigated by storing exemplars from previous tasks or using image replay methods. However, generating images for complex datasets is challenging, limiting the effectiveness of replay methods. Our proposed solution to this issue is generative feature replay, which splits the network into a feature extractor and a classifier. We combine generative feature replay in the classifier with feature distillation in the feature extractor to prevent forgetting and reduce complexity. Our approach is efficient and scalable, achieving state-of-the-art results on CIFAR-100 and ImageNet without requiring excessive storage. Code is available at \url{https://github.com/xialeiliu/GFR-IL}. We operate in a class-incremental setting, where the task-ID is unknown at inference time.",1
"Face verification aims at determining whether a pair of face images belongs to the same identity. Recent studies have revealed the negative impact of facial makeup on the verification performance. With the rapid development of deep generative models, this paper proposes a semanticaware makeup cleanser (SAMC) to remove facial makeup under different poses and expressions and achieve verification via generation. The intuition lies in the fact that makeup is a combined effect of multiple cosmetics and tailored treatments should be imposed on different cosmetic regions. To this end, we present both unsupervised and supervised semantic-aware learning strategies in SAMC. At image level, an unsupervised attention module is jointly learned with the generator to locate cosmetic regions and estimate the degree. At feature level, we resort to the effort of face parsing merely in training phase and design a localized texture loss to serve complements and pursue superior synthetic quality. The experimental results on four makeuprelated datasets verify that SAMC not only produces appealing de-makeup outputs at a resolution of 256*256, but also facilitates makeup-invariant face verification through image generation.",0
"The goal of face verification is to determine if two images of a face belong to the same person. Recent research has shown that facial makeup can have a negative impact on verification accuracy. This paper proposes a semantic-aware makeup cleanser (SAMC) that can remove makeup from faces in different poses and expressions, allowing for verification through image generation. The approach is based on the understanding that makeup is made up of multiple cosmetics, each requiring different treatments. The SAMC includes unsupervised and supervised semantic-aware learning strategies. At the image level, an unsupervised attention module is used to locate cosmetic regions and estimate the degree of makeup. At the feature level, a localized texture loss is employed to complement the face parsing and improve the quality of the synthetic image. The experiments on four makeup-related datasets show that SAMC can produce high-quality de-makeup images at 256*256 resolution and enable makeup-invariant face verification through image generation.",1
"In this paper we propose a novel image representation called face X-ray for detecting forgery in face images. The face X-ray of an input face image is a greyscale image that reveals whether the input image can be decomposed into the blending of two images from different sources. It does so by showing the blending boundary for a forged image and the absence of blending for a real image. We observe that most existing face manipulation methods share a common step: blending the altered face into an existing background image. For this reason, face X-ray provides an effective way for detecting forgery generated by most existing face manipulation algorithms. Face X-ray is general in the sense that it only assumes the existence of a blending step and does not rely on any knowledge of the artifacts associated with a specific face manipulation technique. Indeed, the algorithm for computing face X-ray can be trained without fake images generated by any of the state-of-the-art face manipulation methods. Extensive experiments show that face X-ray remains effective when applied to forgery generated by unseen face manipulation techniques, while most existing face forgery detection or deepfake detection algorithms experience a significant performance drop.",0
"The objective of our research is to introduce a new method of image representation termed as face X-ray, which can be used to identify fake images of faces. By generating a grey-scale image, face X-ray can help determine whether an input image is a result of blending two images from different sources. It can also indicate the blending boundary for a forged image and the absence of blending for a real one. We have observed that most existing face manipulation techniques involve blending the altered face into an existing background image. Therefore, face X-ray provides an effective way to detect forgery generated by most existing face manipulation algorithms. The algorithm for computing face X-ray is general and does not rely on any knowledge of the artifacts associated with a specific face manipulation technique. It can be trained without fake images generated by any of the state-of-the-art face manipulation methods. Our extensive experiments show that face X-ray is effective when applied to forgery generated by unseen face manipulation techniques, whereas most existing face forgery detection or deepfake detection algorithms experience a significant performance drop.",1
"We infer and generate three-dimensional (3D) scene information from a single input image and without supervision. This problem is under-explored, with most prior work relying on supervision from, e.g., 3D ground-truth, multiple images of a scene, image silhouettes or key-points. We propose Pix2Shape, an approach to solve this problem with four components: (i) an encoder that infers the latent 3D representation from an image, (ii) a decoder that generates an explicit 2.5D surfel-based reconstruction of a scene from the latent code (iii) a differentiable renderer that synthesizes a 2D image from the surfel representation, and (iv) a critic network trained to discriminate between images generated by the decoder-renderer and those from a training distribution. Pix2Shape can generate complex 3D scenes that scale with the view-dependent on-screen resolution, unlike representations that capture world-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a consistent scene representation in its encoded latent space and that the decoder can then be applied to this latent representation in order to synthesize the scene from a novel viewpoint. We evaluate Pix2Shape with experiments on the ShapeNet dataset as well as on a novel benchmark we developed, called 3D-IQTT, to evaluate models based on their ability to enable 3d spatial reasoning. Qualitative and quantitative evaluation demonstrate Pix2Shape's ability to solve scene reconstruction, generation, and understanding tasks.",0
"The task of inferring and generating three-dimensional (3D) scene information from a single input image, without supervision, is an under-explored problem. Previous approaches have relied on supervision from various sources, such as 3D ground-truth, multiple images, silhouettes or key-points. In this study, we propose an approach called Pix2Shape, consisting of four components: (i) an encoder that infers the latent 3D representation from an image, (ii) a decoder that generates a 2.5D surfel-based reconstruction of the scene from the latent code, (iii) a differentiable renderer that synthesizes a 2D image from the surfel representation, and (iv) a critic network trained to distinguish between images generated by the decoder-renderer and those from a training distribution. Unlike voxel or mesh representations, Pix2Shape can generate complex 3D scenes that scale with the view-dependent on-screen resolution. Our experiments on the ShapeNet dataset and a new benchmark, 3D-IQTT, demonstrate that Pix2Shape can consistently reconstruct, generate and understand scenes, making it a promising approach for 3D spatial reasoning.",1
"In this work, we propose a novel Cycle In Cycle Generative Adversarial Network (C$^2$GAN) for the task of keypoint-guided image generation. The proposed C$^2$GAN is a cross-modal framework exploring a joint exploitation of the keypoint and the image data in an interactive manner. C$^2$GAN contains two different types of generators, i.e., keypoint-oriented generator and image-oriented generator. Both of them are mutually connected in an end-to-end learnable fashion and explicitly form three cycled sub-networks, i.e., one image generation cycle and two keypoint generation cycles. Each cycle not only aims at reconstructing the input domain, and also produces useful output involving in the generation of another cycle. By so doing, the cycles constrain each other implicitly, which provides complementary information from the two different modalities and brings extra supervision across cycles, thus facilitating more robust optimization of the whole network. Extensive experimental results on two publicly available datasets, i.e., Radboud Faces and Market-1501, demonstrate that our approach is effective to generate more photo-realistic images compared with state-of-the-art models.",0
"Our work introduces a novel approach for keypoint-guided image generation called Cycle In Cycle Generative Adversarial Network (C$^2$GAN). This framework combines both keypoint and image data in an interactive manner through two types of generators: keypoint-oriented and image-oriented. These generators are connected in an end-to-end learnable fashion, forming three cycled sub-networks. The cycles reconstruct the input domain while providing useful output for another cycle. This implicit constraint between cycles provides complementary information from the two modalities and facilitates more robust optimization of the network. Our approach was tested on two publicly available datasets, Radboud Faces and Market-1501, and produced more photo-realistic images than state-of-the-art models.",1
"This paper addresses a major flaw of the cycle consistency loss when used to preserve the input appearance in the face-to-face synthesis domain. In particular, we show that the images generated by a network trained using this loss conceal a noise that hinders their use for further tasks. To overcome this limitation, we propose a ''recurrent cycle consistency loss"" which for different sequences of target attributes minimises the distance between the output images, independent of any intermediate step. We empirically validate not only that our loss enables the re-use of generated images, but that it also improves their quality. In addition, we propose the very first network that covers the task of unconstrained landmark-guided face-to-face synthesis. Contrary to previous works, our proposed approach enables the transfer of a particular set of input features to a large span of poses and expressions, whereby the target landmarks become the ground-truth points. We then evaluate the consistency of our proposed approach to synthesise faces at the target landmarks. To the best of our knowledge, we are the first to propose a loss to overcome the limitation of the cycle consistency loss, and the first to propose an ''in-the-wild'' landmark guided synthesis approach. Code and models for this paper can be found in https://github.com/ESanchezLozano/GANnotation",0
"The paper discusses a shortcoming of the cycle consistency loss in maintaining input appearance in face-to-face synthesis. The images generated by a network using this loss contain noise that hampers their usability for further tasks. To overcome this limitation, the authors propose a ""recurrent cycle consistency loss"" that minimizes the distance between output images for different target attribute sequences, without any intermediate step. The authors empirically prove that this loss enhances the quality of generated images and enables their re-use. Furthermore, they introduce a landmark-guided face-to-face synthesis network that can transfer specific input features to various poses and expressions. This approach uses target landmarks as ground-truth points and is evaluated for its consistency in synthesizing faces. The authors claim that theirs is the first study to propose a loss that addresses the limitation of the cycle consistency loss, and the first to propose an ""in-the-wild"" landmark-guided synthesis method. The code and models for this research are available at https://github.com/ESanchezLozano/GANnotation.",1
"We present MixNMatch, a conditional generative model that learns to disentangle and encode background, object pose, shape, and texture from real images with minimal supervision, for mix-and-match image generation. We build upon FineGAN, an unconditional generative model, to learn the desired disentanglement and image generator, and leverage adversarial joint image-code distribution matching to learn the latent factor encoders. MixNMatch requires bounding boxes during training to model background, but requires no other supervision. Through extensive experiments, we demonstrate MixNMatch's ability to accurately disentangle, encode, and combine multiple factors for mix-and-match image generation, including sketch2color, cartoon2img, and img2gif applications. Our code/models/demo can be found at https://github.com/Yuheng-Li/MixNMatch",0
"MixNMatch is a conditional generative model that can learn to disentangle and encode various features of real images, such as background, object pose, shape, and texture, with minimal supervision to create mix-and-match images. To accomplish this, we have built on the FineGAN model, which is an unconditional generative model, to obtain the desired disentanglement and image generator. Furthermore, we use adversarial joint image-code distribution matching to learn the latent factor encoders. MixNMatch requires bounding boxes during training to model the background, but no other supervision is necessary. We have conducted extensive experiments that demonstrate MixNMatch's ability to accurately disentangle, encode, and combine multiple factors for mix-and-match image generation, including applications like sketch2color, cartoon2img, and img2gif. Our code/models/demo can be accessed at https://github.com/Yuheng-Li/MixNMatch.",1
"This paper introduces a neural style transfer model to generate a stylized image conditioning on a set of examples describing the desired style. The proposed solution produces high-quality images even in the zero-shot setting and allows for more freedom in changes to the content geometry. This is made possible by introducing a novel Two-Stage Peer-Regularization Layer that recombines style and content in latent space by means of a custom graph convolutional layer. Contrary to the vast majority of existing solutions, our model does not depend on any pre-trained networks for computing perceptual losses and can be trained fully end-to-end thanks to a new set of cyclic losses that operate directly in latent space and not on the RGB images. An extensive ablation study confirms the usefulness of the proposed losses and of the Two-Stage Peer-Regularization Layer, with qualitative results that are competitive with respect to the current state of the art using a single model for all presented styles. This opens the door to more abstract and artistic neural image generation scenarios, along with simpler deployment of the model.",0
"In this paper, a neural style transfer model is introduced that can create stylized images based on a given set of style examples. This approach allows for greater flexibility in altering the content geometry, even in zero-shot situations. The model utilizes a unique Two-Stage Peer-Regularization Layer that combines style and content in latent space using a custom graph convolutional layer. Unlike most existing solutions, this model does not rely on pre-trained networks for computing perceptual losses and can be trained end-to-end using cyclic losses that operate directly in latent space rather than RGB images. An extensive study confirms the effectiveness of the proposed losses and the Two-Stage Peer-Regularization Layer, producing qualitative results that are competitive with the current state of the art. This new method offers possibilities for more abstract and artistic neural image generation scenarios and simpler deployment of the model.",1
"Neural Architecture Search (NAS) that aims to automate the procedure of architecture design has achieved promising results in many computer vision fields. In this paper, we propose an AdversarialNAS method specially tailored for Generative Adversarial Networks (GANs) to search for a superior generative model on the task of unconditional image generation. The AdversarialNAS is the first method that can search the architectures of generator and discriminator simultaneously in a differentiable manner. During searching, the designed adversarial search algorithm does not need to comput any extra metric to evaluate the performance of the searched architecture, and the search paradigm considers the relevance between the two network architectures and improves their mutual balance. Therefore, AdversarialNAS is very efficient and only takes 1 GPU day to search for a superior generative model in the proposed large search space ($10^{38}$). Experiments demonstrate the effectiveness and superiority of our method. The discovered generative model sets a new state-of-the-art FID score of $10.87$ and highly competitive Inception Score of $8.74$ on CIFAR-10. Its transferability is also proven by setting new state-of-the-art FID score of $26.98$ and Inception score of $9.63$ on STL-10. Code is at: \url{https://github.com/chengaopro/AdversarialNAS}.",0
"The goal of Neural Architecture Search (NAS) is to automate architecture design, and it has shown promise in various computer vision domains. This study introduces an AdversarialNAS method that is specifically tailored for Generative Adversarial Networks (GANs) and aims to find a superior generative model for unconditional image generation. AdversarialNAS is the first approach that can simultaneously search for the architectures of the generator and discriminator in a differentiable way. Throughout the search process, the adversarial search algorithm does not require any additional metrics to evaluate the performance of the architecture being searched, and the search approach considers the correlation between the two network architectures to enhance their mutual balance. Therefore, AdversarialNAS is highly efficient and only needs 1 GPU day to search for a superior generative model in a vast search space of $10^{38}$. The experiments demonstrate the effectiveness and superiority of this approach. The discovered generative model achieves a new state-of-the-art FID score of $10.87$ and an extremely competitive Inception Score of $8.74$ on CIFAR-10. The transferability of the model is also validated, as it achieves a new state-of-the-art FID score of $26.98$ and an Inception score of $9.63$ on STL-10. The code for this approach is accessible at \url{https://github.com/chengaopro/AdversarialNAS}.",1
"Conditional generative adversarial networks (cGANs) have been widely researched to generate class conditional images using a single generator. However, in the conventional cGANs techniques, it is still challenging for the generator to learn condition-specific features, since a standard convolutional layer with the same weights is used regardless of the condition. In this paper, we propose a novel convolution layer, called the conditional convolution layer, which directly generates different feature maps by employing the weights which are adjusted depending on the conditions. More specifically, in each conditional convolution layer, the weights are conditioned in a simple but effective way through filter-wise scaling and channel-wise shifting operations. In contrast to the conventional methods, the proposed method with a single generator can effectively handle condition-specific characteristics. The experimental results on CIFAR, LSUN and ImageNet datasets show that the generator with the proposed conditional convolution layer achieves a higher quality of conditional image generation than that with the standard convolution layer.",0
"Research on conditional generative adversarial networks (cGANs) has focused on generating class conditional images using a single generator. However, traditional cGAN techniques struggle with the generator learning condition-specific features due to the use of a standard convolutional layer with identical weights for all conditions. To address this issue, our paper introduces a novel convolution layer called the conditional convolution layer. This layer generates different feature maps by adjusting weights based on the conditions. The layer accomplishes this through filter-wise scaling and channel-wise shifting operations. Unlike traditional methods, our single generator can effectively handle condition-specific characteristics. Experimentation on CIFAR, LSUN and ImageNet datasets proves that the generator with the proposed conditional convolution layer can produce higher quality conditional images than the generator using the standard convolution layer.",1
"Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.",0
"Flow-based generative models are a type of exact inference model that allows for efficient inference and sampling when generating images. However, due to design constraints, such as split coupling flow layers where some pixels are not transformed, these models have limited ability to model long-range data dependencies compared to autoregressive models. To address this issue, we introduce multi-scale autoregressive priors (mAR) that create channel-wise dependencies in the latent space of flow-based models. Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data, resulting in state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Additionally, our mAR-SCF model improves image generation quality, as evidenced by higher FID and Inception scores compared to other flow-based models.",1
"Traditional convolution-based generative adversarial networks synthesize images based on hierarchical local operations, where long-range dependency relation is implicitly modeled with a Markov chain. It is still not sufficient for categories with complicated structures. In this paper, we characterize long-range dependence with attentive normalization (AN), which is an extension to traditional instance normalization. Specifically, the input feature map is softly divided into several regions based on its internal semantic similarity, which are respectively normalized. It enhances consistency between distant regions with semantic correspondence. Compared with self-attention GAN, our attentive normalization does not need to measure the correlation of all locations, and thus can be directly applied to large-size feature maps without much computational burden. Extensive experiments on class-conditional image generation and semantic inpainting verify the efficacy of our proposed module.",0
"Traditional generative adversarial networks that utilize convolution-based techniques for image synthesis rely on local operations that are hierarchical in nature. However, this approach falls short when dealing with complex structures. This paper proposes the use of attentive normalization (AN) as a means of characterizing long-range dependence. AN is an extension of traditional instance normalization and is used to softly divide the input feature map into regions based on internal semantic similarity. These regions are then normalized, thereby enhancing consistency between distant regions with semantic correspondence. Unlike self-attention GAN, AN does not measure the correlation of all locations, making it more efficient for use on large-size feature maps. Extensive experiments were conducted to validate the efficacy of the proposed module for class-conditional image generation and semantic inpainting.",1
"Image manipulation can be considered a special case of image generation where the image to be produced is a modification of an existing image. Image generation and manipulation have been, for the most part, tasks that operate on raw pixels. However, the remarkable progress in learning rich image and object representations has opened the way for tasks such as text-to-image or layout-to-image generation that are mainly driven by semantics. In our work, we address the novel problem of image manipulation from scene graphs, in which a user can edit images by merely applying changes in the nodes or edges of a semantic graph that is generated from the image. Our goal is to encode image information in a given constellation and from there on generate new constellations, such as replacing objects or even changing relationships between objects, while respecting the semantics and style from the original image. We introduce a spatio-semantic scene graph network that does not require direct supervision for constellation changes or image edits. This makes it possible to train the system from existing real-world datasets with no additional annotation effort.",0
"The act of manipulating an image can be seen as a particular instance of generating an image, where the resulting image is a modified version of an existing one. In the past, image generation and manipulation were primarily carried out on the raw pixels of an image. However, with advancements in image and object representation, new tasks such as generating images from text or layouts driven by semantics are now possible. Our work focuses on the innovative challenge of manipulating images using scene graphs. By making changes to the nodes or edges of a semantic graph generated from an image, users can edit the image. Our objective is to encode image details in a specific arrangement and then use that arrangement to produce new ones, such as replacing objects or altering relationships between them, while preserving the original image's style and meaning. We present a spatio-semantic scene graph network that doesn't require direct supervision for making changes to the arrangement or editing the image. This enables the system to be trained using real-world datasets without the need for additional annotation.",1
"We present AugurOne, a novel approach for training single image generative models. Our approach trains an upscaling neural network using non-affine augmentations of the (single) input image, particularly including non-rigid thin plate spline image warps. The extensive augmentations significantly increase the in-sample distribution for the upsampling network enabling the upscaling of highly variable inputs. A compact latent space is jointly learned allowing for controlled image synthesis. Differently from Single Image GAN, our approach does not require GAN training and takes place in an end-to-end fashion allowing fast and stable training. We experimentally evaluate our method and show that it obtains compelling novel animations of single-image, as well as, state-of-the-art performance on conditional generation tasks e.g. paint-to-image and edges-to-image.",0
"AugurOne is a new method for teaching single image generative models. It involves using a neural network for upscaling that is trained with non-affine augmentations of the input image. These augmentations include non-rigid thin plate spline image warps, which greatly increase the range of inputs that can be upscaled. A compact latent space is also learned, which allows for controlled image synthesis. Unlike Single Image GAN, our approach does not need GAN training and can be trained quickly and stably in an end-to-end fashion. We conducted experiments to evaluate our method and found that it produces impressive new animations of single images and achieves state-of-the-art performance on conditional generation tasks such as paint-to-image and edges-to-image.",1
"We introduce the first method for automatic image generation from scene-level freehand sketches. Our model allows for controllable image generation by specifying the synthesis goal via freehand sketches. The key contribution is an attribute vector bridged Generative Adversarial Network called EdgeGAN, which supports high visual-quality object-level image content generation without using freehand sketches as training data. We have built a large-scale composite dataset called SketchyCOCO to support and evaluate the solution. We validate our approach on the tasks of both object-level and scene-level image generation on SketchyCOCO. Through quantitative, qualitative results, human evaluation and ablation studies, we demonstrate the method's capacity to generate realistic complex scene-level images from various freehand sketches.",0
"Our novel approach involves generating images from freehand sketches at a scene-level automatically. With our model, users can control the image generation process by providing a synthesis goal through their sketches. A key feature of our approach is EdgeGAN, a Generative Adversarial Network that relies on an attribute vector to produce object-level image content of high visual quality without relying on freehand sketches as training data. To support and evaluate our solution, we created a large-scale composite dataset called SketchyCOCO. By using this dataset, we were able to validate our approach in generating both object-level and scene-level images. Through quantitative and qualitative results, human evaluation, and ablation studies, we were able to demonstrate that our method can produce realistic, complex scene-level images from a variety of freehand sketches.",1
"Generative Adversarial Networks (GANs) have shown great promise in modeling high dimensional data. The learning objective of GANs usually minimizes some measure discrepancy, \textit{e.g.}, $f$-divergence~($f$-GANs) or Integral Probability Metric~(Wasserstein GANs). With $f$-divergence as the objective function, the discriminator essentially estimates the density ratio, and the estimated ratio proves useful in further improving the sample quality of the generator. However, how to leverage the information contained in the discriminator of Wasserstein GANs (WGAN) is less explored. In this paper, we introduce the Discriminator Contrastive Divergence, which is well motivated by the property of WGAN's discriminator and the relationship between WGAN and energy-based model. Compared to standard GANs, where the generator is directly utilized to obtain new samples, our method proposes a semi-amortized generation procedure where the samples are produced with the generator's output as an initial state. Then several steps of Langevin dynamics are conducted using the gradient of the discriminator. We demonstrate the benefits of significant improved generation on both synthetic data and several real-world image generation benchmarks.",0
"GANs have demonstrated significant potential in modeling multidimensional data, and their learning objective typically involves minimizing a measure of discrepancy such as $f$-divergence or Integral Probability Metric. When using $f$-divergence, the discriminator estimates the density ratio to enhance sample quality. However, the usefulness of the WGAN discriminator is not well explored. This paper introduces Discriminator Contrastive Divergence, which leverages the WGAN discriminator's properties and relationship with energy-based models. Our method proposes a semi-amortized generation approach, where the generator's output is used as an initial state, and Langevin dynamics with the discriminator's gradient are applied. We demonstrate significant improvements in synthetic data and real-world image generation benchmarks.",1
"In this work we ask whether it is possible to create a ""universal"" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used. To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2). Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis. Code and pre-trained networks are available at https://peterwang512.github.io/CNNDetection/ .",0
"The objective of this study is to determine the feasibility of creating a universal detector that can differentiate real images from those generated by a CNN, irrespective of the architecture or dataset used. To accomplish this, we collected a dataset comprising of fake images created by 11 distinct CNN-based image generator models. These models were chosen to cover the range of commonly used architectures today, including ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, and seeing-in-the-dark. By implementing careful pre- and post-processing and data augmentation, we discovered that a standard image classifier trained on only one specific CNN generator (ProGAN) can generalize remarkably well to unseen architectures, datasets, and training methods, including the recently released StyleGAN2. This suggests that current CNN-generated images may share some systematic flaws, which limit their ability to achieve realistic image synthesis. The code and pre-trained networks are available at https://peterwang512.github.io/CNNDetection/.",1
"People spend a substantial part of their lives at rest in bed. 3D human pose and shape estimation for this activity would have numerous beneficial applications, yet line-of-sight perception is complicated by occlusion from bedding. Pressure sensing mats are a promising alternative, but training data is challenging to collect at scale. We describe a physics-based method that simulates human bodies at rest in a bed with a pressure sensing mat, and present PressurePose, a synthetic dataset with 206K pressure images with 3D human poses and shapes. We also present PressureNet, a deep learning model that estimates human pose and shape given a pressure image and gender. PressureNet incorporates a pressure map reconstruction (PMR) network that models pressure image generation to promote consistency between estimated 3D body models and pressure image input. In our evaluations, PressureNet performed well with real data from participants in diverse poses, even though it had only been trained with synthetic data. When we ablated the PMR network, performance dropped substantially.",0
"A significant amount of time is spent in bed at rest, making 3D human pose and shape estimation for this activity useful in various applications. However, the occlusion caused by bedding makes line-of-sight perception complicated. Although pressure sensing mats are a promising alternative, collecting training data at scale is challenging. To address this, we developed a physics-based method that simulates human bodies at rest in a bed with a pressure sensing mat. This led to the creation of PressurePose, a synthetic dataset containing 206K pressure images depicting 3D human poses and shapes. We also introduced PressureNet, a deep learning model that uses a pressure map reconstruction (PMR) network to estimate human pose and shape based on a pressure image and gender. Our evaluations showed that PressureNet performed well with real data from participants in various poses despite being trained solely on synthetic data. When we removed the PMR network, performance suffered significantly.",1
"We present a generative model of images that explicitly reasons over the set of objects they show. Our model learns a structured latent representation that separates objects from each other and from the background; unlike prior works, it explicitly represents the 2D position and depth of each object, as well as an embedding of its segmentation mask and appearance. The model can be trained from images alone in a purely unsupervised fashion without the need for object masks or depth information. Moreover, it always generates complete objects, even though a significant fraction of training images contain occlusions. Finally, we show that our model can infer decompositions of novel images into their constituent objects, including accurate prediction of depth ordering and segmentation of occluded parts.",0
"We introduce a model for generating images that takes into account the objects depicted in them. Our model uses a structured latent representation to distinguish objects from one another and the background. It uniquely represents each object's 2D position, depth, segmentation mask, and appearance. Unlike previous models, ours can be trained with unsupervised learning from images alone, without requiring object masks or depth information. Furthermore, our model generates complete objects, even when a significant portion of training images include occlusions. Lastly, our model can identify and separate novel images into their individual objects, accurately predicting depth ordering and segmenting occluded parts.",1
"It is now possible to synthesize highly realistic images of people who don't exist. Such content has, for example, been implicated in the creation of fraudulent social-media profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content.   We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near-0% accuracy. We develop five attack case studies on a state-of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.",0
"Advancements in technology now enable the creation of lifelike images of non-existent individuals, which have been used to fabricate social media profiles that disseminate misinformation. Consequently, efforts are underway to identify artificially-generated content. One approach involves training a neural network to differentiate between genuine and synthetic images. However, we demonstrate that such forensic classifiers are susceptible to various attacks that can drastically reduce their accuracy. We conducted five case studies on a cutting-edge classifier that achieves a high degree of accuracy when trained on a single generator, exposing its vulnerability to attack. By manipulating a small percentage of the image or adding a single noise pattern, we were able to reduce the classifier's accuracy to negligible levels. Moreover, we developed a black-box attack that can reduce the classifier's accuracy without any access to the target. These findings underscore the significant weaknesses of certain image-forensic classifiers.",1
"Generative adversarial networks (GANs) are neural networks that learn data distributions through adversarial training. In intensive studies, recent GANs have shown promising results for reproducing training images. However, in spite of noise, they reproduce images with fidelity. As an alternative, we propose a novel family of GANs called noise robust GANs (NR-GANs), which can learn a clean image generator even when training images are noisy. In particular, NR-GANs can solve this problem without having complete noise information (e.g., the noise distribution type, noise amount, or signal-noise relationship). To achieve this, we introduce a noise generator and train it along with a clean image generator. However, without any constraints, there is no incentive to generate an image and noise separately. Therefore, we propose distribution and transformation constraints that encourage the noise generator to capture only the noise-specific components. In particular, considering such constraints under different assumptions, we devise two variants of NR-GANs for signal-independent noise and three variants of NR-GANs for signal-dependent noise. On three benchmark datasets, we demonstrate the effectiveness of NR-GANs in noise robust image generation. Furthermore, we show the applicability of NR-GANs in image denoising. Our code is available at https://github.com/takuhirok/NR-GAN/.",0
"Adversarial training is used by generative adversarial networks (GANs) to learn data distributions. Although recent GANs have displayed excellence in reproducing training images, they fail to do so in the presence of noise. Our novel family of GANs, known as noise robust GANs (NR-GANs), can learn a clean image generator despite noisy training images, without complete noise information. We have introduced a noise generator and trained it alongside a clean image generator. However, to prevent the generator from producing image and noise together, we have set distribution and transformation constraints that encourage the noise generator to capture only noise-specific components. We have developed two variants of NR-GANs for signal-independent noise and three variants of NR-GANs for signal-dependent noise, considering different constraints. We have demonstrated the efficiency of NR-GANs in noise-robust image generation on three benchmark datasets and their applicability in image denoising. Our code is available at https://github.com/takuhirok/NR-GAN/.",1
"In this paper, we address the task of semantic-guided scene generation. One open challenge in scene generation is the difficulty of the generation of small objects and detailed local texture, which has been widely observed in global image-level generation methods. To tackle this issue, in this work we consider learning the scene generation in a local context, and correspondingly design a local class-specific generative network with semantic maps as a guidance, which separately constructs and learns sub-generators concentrating on the generation of different classes, and is able to provide more scene details. To learn more discriminative class-specific feature representations for the local generation, a novel classification module is also proposed. To combine the advantage of both the global image-level and the local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Extensive experiments on two scene image generation tasks show superior generation performance of the proposed model. The state-of-the-art results are established by large margins on both tasks and on challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.",0
"The focus of this paper is on semantic-guided scene generation. Generating small objects and detailed local texture is a challenge in scene generation, particularly with global image-level generation methods. To overcome this challenge, the authors propose a local class-specific generative network that uses semantic maps as guidance to generate scenes in a local context. The network is designed with sub-generators that concentrate on the generation of different classes, resulting in more detailed scenes. To learn more discriminative class-specific feature representations, a novel classification module is introduced. To combine the advantages of both global image-level and local class-specific generation, the authors design a joint generation network with an attention fusion module and a dual-discriminator structure. Extensive experiments show that the proposed model outperforms state-of-the-art models on two scene image generation tasks and on challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.",1
"We propose a novel Edge guided Generative Adversarial Network (EdgeGAN) for photo-realistic image synthesis from semantic layouts. Although considerable improvement has been achieved, the quality of synthesized images is far from satisfactory due to two largely unresolved challenges. First, the semantic labels do not provide detailed structural information, making it difficult to synthesize local details and structures. Second, the widely adopted CNN operations such as convolution, down-sampling and normalization usually cause spatial resolution loss and thus are unable to fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects). To tackle the first challenge, we propose to use the edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. Edge information is produced by a convolutional generator and introduces detailed structure information. Further, to preserve the semantic information, we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout. Extensive experiments on two challenging datasets show that the proposed EdgeGAN can generate significantly better results than state-of-the-art methods. The source code and trained models are available at https://github.com/Ha0Tang/EdgeGAN.",0
"Our novel Edge guided Generative Adversarial Network (EdgeGAN) addresses the challenge of synthesizing photo-realistic images from semantic layouts. However, despite significant progress, the quality of synthesized images remains unsatisfactory due to two unresolved challenges. Firstly, semantic labels lack detailed structural information, making it hard to synthesize local details and structures. Secondly, widely used CNN operations, such as convolution and down-sampling, cause spatial resolution loss, leading to semantically inconsistent results. To address these challenges, we propose using edge information as an intermediate representation to guide image generation, with an attention guided edge transfer module. The convolutional generator produces edge information that introduces detailed structure information. To preserve semantic information, we selectively highlight class-dependent feature maps based on the original semantic layout. Our EdgeGAN outperforms state-of-the-art methods on two challenging datasets. The source code and trained models are available at https://github.com/Ha0Tang/EdgeGAN.",1
"For a given image generation problem, the intrinsic image manifold is often low dimensional. We use the intuition that it is much better to train the GAN generator by minimizing the distributional distance between real and generated images in a small dimensional feature space representing such a manifold than on the original pixel-space. We use the feature space of the GAN discriminator for such a representation. For distributional distance, we employ one of two choices: the Fr\'{e}chet distance or direct optimal transport (OT); these respectively lead us to two new GAN methods: Fr\'{e}chet-GAN and OT-GAN. The idea of employing Fr\'{e}chet distance comes from the success of Fr\'{e}chet Inception Distance as a solid evaluation metric in image generation. Fr\'{e}chet-GAN is attractive in several ways. We propose an efficient, numerically stable approach to calculate the Fr\'{e}chet distance and its gradient. The Fr\'{e}chet distance estimation requires a significantly less computation time than OT; this allows Fr\'{e}chet-GAN to use much larger mini-batch size in training than OT. More importantly, we conduct experiments on a number of benchmark datasets and show that Fr\'{e}chet-GAN (in particular) and OT-GAN have significantly better image generation capabilities than the existing representative primal and dual GAN approaches based on the Wasserstein distance.",0
"When addressing image generation problems, the intrinsic image manifold is typically of low dimension. To optimize the training of the GAN generator, it is more effective to minimize the distributional distance between real and generated images in a smaller, feature space that represents this manifold, rather than in the original pixel-space. The feature space of the GAN discriminator is utilized for this representation. Two options for distributional distance are the Fr\'{e}chet distance or direct optimal transport (OT), which lead to two new GAN methods: Fr\'{e}chet-GAN and OT-GAN. The Fr\'{e}chet distance is inspired by the success of Fr\'{e}chet Inception Distance as an image generation evaluation metric. Fr\'{e}chet-GAN is advantageous due to an efficient, numerically stable approach to calculating the Fr\'{e}chet distance and its gradient, which requires significantly less computation time than OT. This allows for much larger mini-batch sizes during training. In experiments on various benchmark datasets, Fr\'{e}chet-GAN and OT-GAN outperformed existing GAN approaches based on the Wasserstein distance.",1
"Vehicle re-identification (Re-ID) has become a popular research topic owing to its practicability in intelligent transportation systems. Vehicle Re-ID suffers the numerous challenges caused by drastic variation in illumination, occlusions, background, resolutions, viewing angles, and so on. To address it, this paper formulates a multi-order deep cross-distance learning (\textbf{DCDLearn}) model for vehicle re-identification, where an efficient one-view CycleGAN model is developed to alleviate exhaustive and enumerative cross-camera matching problem in previous works and smooth the domain discrepancy of cross cameras. Specially, we treat the transferred images and the reconstructed images generated by one-view CycleGAN as multi-order augmented data for deep cross-distance learning, where the cross distances of multi-order image set with distinct identities are learned by optimizing an objective function with multi-order augmented triplet loss and center loss to achieve the camera-invariance and identity-consistency. Extensive experiments on three vehicle Re-ID datasets demonstrate that the proposed method achieves significant improvement over the state-of-the-arts, especially for the small scale dataset.",0
"The practicality of intelligent transportation systems has led to an increase in research on vehicle re-identification (Re-ID). However, the challenges of Re-ID are numerous, including variations in illumination, occlusions, background, resolutions, and viewing angles. To address these challenges, the paper proposes a multi-order deep cross-distance learning model called DCDLearn. This model incorporates an efficient one-view CycleGAN model to alleviate cross-camera matching issues and smooth the domain discrepancy of cross cameras. The transferred and reconstructed images generated by the one-view CycleGAN serve as augmented data for deep cross-distance learning, where the multi-order image set with distinct identities are learned through an objective function with multi-order augmented triplet loss and center loss. The proposed method achieves significant improvement over the state-of-the-arts, particularly for small scale datasets, as demonstrated by extensive experiments on three vehicle Re-ID datasets.",1
"We propose a new task towards more practical application for image generation - high-quality image synthesis from salient object layout. This new setting allows users to provide the layout of salient objects only (i.e., foreground bounding boxes and categories), and lets the model complete the drawing with an invented background and a matching foreground. Two main challenges spring from this new task: (i) how to generate fine-grained details and realistic textures without segmentation map input; and (ii) how to create a background and weave it seamlessly into standalone objects. To tackle this, we propose Background Hallucination Generative Adversarial Network (BachGAN), which first selects a set of segmentation maps from a large candidate pool via a background retrieval module, then encodes these candidate layouts via a background fusion module to hallucinate a suitable background for the given objects. By generating the hallucinated background representation dynamically, our model can synthesize high-resolution images with both photo-realistic foreground and integral background. Experiments on Cityscapes and ADE20K datasets demonstrate the advantage of BachGAN over existing methods, measured on both visual fidelity of generated images and visual alignment between output images and input layouts.",0
"Our proposition introduces a new practical application for image generation, which is high-quality image synthesis from salient object layout. This approach enables users to only provide the layout of the salient objects, such as foreground bounding boxes and categories, and allows the model to complete the drawing by adding an invented background that matches the foreground. The new task comes with two main challenges, which are generating fine-grained details and realistic textures without segmentation map input, and creating a seamless background that blends well with stand-alone objects. To overcome these challenges, we introduce Background Hallucination Generative Adversarial Network (BachGAN), which selects a set of segmentation maps from a large pool via a background retrieval module and encodes these candidate layouts through a background fusion module to hallucinate a suitable background for the given objects. Our model generates the hallucinated background representation dynamically, enabling it to synthesize high-resolution images with both photo-realistic foreground and integral background. Our experiments on Cityscapes and ADE20K datasets show the superiority of BachGAN over existing methods in terms of visual fidelity of generated images and visual alignment between output images and input layouts.",1
"A virtual try-on method takes a product image and an image of a model and produces an image of the model wearing the product. Most methods essentially compute warps from the product image to the model image and combine using image generation methods. However, obtaining a realistic image is challenging because the kinematics of garments is complex and because outline, texture, and shading cues in the image reveal errors to human viewers. The garment must have appropriate drapes; texture must be warped to be consistent with the shape of a draped garment; small details (buttons, collars, lapels, pockets, etc.) must be placed appropriately on the garment, and so on. Evaluation is particularly difficult and is usually qualitative.   This paper uses quantitative evaluation on a challenging, novel dataset to demonstrate that (a) for any warping method, one can choose target models automatically to improve results, and (b) learning multiple coordinated specialized warpers offers further improvements on results. Target models are chosen by a learned embedding procedure that predicts a representation of the products the model is wearing. This prediction is used to match products to models. Specialized warpers are trained by a method that encourages a second warper to perform well in locations where the first works poorly. The warps are then combined using a U-Net. Qualitative evaluation confirms that these improvements are wholesale over outline, texture shading, and garment details.",0
"To create an image of a model wearing a product, the virtual try-on method uses the product image and an image of a model. The approach involves computing warps from the product image to the model image and combining them using image generation techniques. However, achieving a realistic image is challenging due to the complexity of garment kinematics and the potential for errors in outline, texture, and shading cues that are noticeable to human viewers. To address this, the garment must have appropriate drapes, texture, and small details correctly placed. Evaluation is often subjective, but this study uses a quantitative approach on a new dataset to show that choosing target models automatically and using multiple coordinated specialized warpers can improve results. Target models are chosen using a learned embedding procedure, and specialized warpers are trained to perform well in areas where others do not. The warps are combined using a U-Net, and the improvements are confirmed qualitatively across multiple aspects of the image.",1
"We explore novel approaches to the task of image generation from their respective captions, building on state-of-the-art GAN architectures. Particularly, we baseline our models with the Attention-based GANs that learn attention mappings from words to image features. To better capture the features of the descriptions, we then built a novel cyclic design that learns an inverse function to maps the image back to original caption. Additionally, we incorporated recently developed BERT pretrained word embeddings as our initial text featurizer and observe a noticeable improvement in qualitative and quantitative performance compared to the Attention GAN baseline.",0
"In this study, we investigate innovative methods for generating images based on their corresponding captions, utilizing the latest GAN architectures. Our initial models are based on Attention-based GANs, which learn attention mappings between words and image attributes. To enhance the accuracy of the descriptions, we introduce a novel cyclic design that learns an inverse function, allowing the image to be mapped back to the original caption. Furthermore, we utilize BERT pretrained word embeddings as our initial text featurizer, resulting in significant improvements in both qualitative and quantitative performance when compared to the Attention GAN baseline.",1
"Deep Convolutional Neural Networks (DCNNs) is currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of DCNNs. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. $\Pi$-Nets can be implemented using special kind of skip connections and their parameters can be represented via high-order tensors. We empirically demonstrate that $\Pi$-Nets have better representation power than standard DCNNs and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in challenging tasks, such as image generation. Lastly, our framework elucidates why recent generative models, such as StyleGAN, improve upon their predecessors, e.g., ProGAN.",0
"In the field of computer vision and machine learning, Deep Convolutional Neural Networks (DCNNs) are currently the preferred method for both generative and discriminative learning. The success of DCNNs can be attributed to the careful selection of their building blocks, including residual blocks, rectifiers, and normalization schemes. In this study, a new class of DCNNs called $\Pi$-Nets is proposed. These polynomial neural networks use a special kind of skip connection and high-order tensors to represent their parameters. Empirical evidence suggests that $\Pi$-Nets have superior representation power compared to standard DCNNs, and can perform well without non-linear activation functions in tasks involving images, graphs, and audio. When combined with activation functions, $\Pi$-Nets perform exceptionally well in challenging tasks such as image generation. Lastly, the proposed framework provides insight into the improvements of recent generative models such as StyleGAN compared to their predecessors like ProGAN.",1
"In this paper, we focus on the semi-supervised person re-identification (Re-ID) case, which only has the intra-camera (within-camera) labels but not inter-camera (cross-camera) labels. In real-world applications, these intra-camera labels can be readily captured by tracking algorithms or few manual annotations, when compared with cross-camera labels. In this case, it is very difficult to explore the relationships between cross-camera persons in the training stage due to the lack of cross-camera label information. To deal with this issue, we propose a novel Progressive Cross-camera Soft-label Learning (PCSL) framework for the semi-supervised person Re-ID task, which can generate cross-camera soft-labels and utilize them to optimize the network. Concretely, we calculate an affinity matrix based on person-level features and adapt them to produce the similarities between cross-camera persons (i.e., cross-camera soft-labels). To exploit these soft-labels to train the network, we investigate the weighted cross-entropy loss and the weighted triplet loss from the classification and discrimination perspectives, respectively. Particularly, the proposed framework alternately generates progressive cross-camera soft-labels and gradually improves feature representations in the whole learning course. Extensive experiments on five large-scale benchmark datasets show that PCSL significantly outperforms the state-of-the-art unsupervised methods that employ labeled source domains or the images generated by the GAN-based models. Furthermore, the proposed method even has a competitive performance with respect to deep supervised Re-ID methods.",0
"The focus of this paper is on semi-supervised person re-identification (Re-ID) without inter-camera (cross-camera) labels, only intra-camera (within-camera) labels. In real-world scenarios, these intra-camera labels are easier to capture through tracking algorithms or manual annotations, compared to cross-camera labels. However, the lack of cross-camera label information makes it challenging to explore relationships between cross-camera persons during training. To address this issue, a novel Progressive Cross-camera Soft-label Learning (PCSL) framework is proposed, which generates cross-camera soft-labels to optimize the network. An affinity matrix is calculated based on person-level features to produce similarities between cross-camera persons, which are used as soft-labels for training. Weighted cross-entropy and triplet losses are investigated, and the proposed framework progressively generates cross-camera soft-labels and improves feature representations throughout the learning process. Extensive experiments on five benchmark datasets demonstrate that PCSL outperforms state-of-the-art unsupervised methods and even competes with deep supervised Re-ID methods.",1
"In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task.",0
"Generative Adversarial Networks have made remarkable advancements in generating photorealistic images, raising the possibility of replacing the traditional rendering pipeline with efficient models learned directly from images. However, current image synthesis models face challenges in disentangling 3D properties like camera perspective and object orientation in the 2D domain, and lack interpretability and control. Our hypothesis is that image generation should be modeled in 3D space, as the physical world is inherently three-dimensional. To address this, we propose the task of 3D controllable image synthesis and present an approach that reasons in both 3D and 2D domains. Our model can unsupervisedly disentangle latent 3D factors from raw images of simple multi-object scenes and synthesize consistent scenes with changes in viewpoint and object pose, outperforming pure 2D models. We also evaluate the usefulness of various 3D representations for this task.",1
"Pose-guided person image generation is to transform a source person image to a target pose. This task requires spatial manipulations of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. Specifically, our model first calculates the global correlations between sources and targets to predict flow fields. Then, the flowed local patch pairs are extracted from the feature maps to calculate the local attention coefficients. Finally, we warp the source features using a content-aware sampling method with the obtained local attention coefficients. The results of both subjective and objective experiments demonstrate the superiority of our model. Besides, additional results in video animation and view synthesis show that our model is applicable to other tasks requiring spatial transformation. Our source code is available at https://github.com/RenYurui/Global-Flow-Local-Attention.",0
"The objective of pose-guided person image generation is to change a given person image to a desired pose. This task involves manipulating the spatial arrangement of the original data. However, using Convolutional Neural Networks for this task is limited due to their inability to spatially transform the inputs. In this paper, we introduce a differentiable framework that combines global-flow and local-attention to reconstruct the inputs at the feature level. Our model first determines the global correlations between the source and target images to generate flow fields. Next, we extract local patch pairs from the feature maps to calculate the local attention coefficients. Finally, we apply a content-aware sampling method to warp the source features using the local attention coefficients. Our model outperforms other models in both subjective and objective experiments. Additionally, we demonstrate the applicability of our model for other tasks requiring spatial transformations such as video animation and view synthesis. Our source code is available at https://github.com/RenYurui/Global-Flow-Local-Attention.",1
"Though recent research has achieved remarkable progress in generating realistic images with generative adversarial networks (GANs), the lack of training stability is still a lingering concern of most GANs, especially on high-resolution inputs and complex datasets. Since the randomly generated distribution can hardly overlap with the real distribution, training GANs often suffers from the gradient vanishing problem. A number of approaches have been proposed to address this issue by constraining the discriminator's capabilities using empirical techniques, like weight clipping, gradient penalty, spectral normalization etc. In this paper, we provide a more principled approach as an alternative solution to this issue. Instead of training the discriminator to distinguish real and fake input samples, we investigate the relationship between paired samples by training the discriminator to separate paired samples from the same distribution and those from different distributions. To this end, we explore a relation network architecture for the discriminator and design a triplet loss which performs better generalization and stability. Extensive experiments on benchmark datasets show that the proposed relation discriminator and new loss can provide significant improvement on variable vision tasks including unconditional and conditional image generation and image translation.",0
"Despite remarkable progress in generating realistic images with GANs, the lack of training stability remains a concern, particularly on complex datasets and high-resolution inputs. The gradient vanishing problem often occurs during GAN training due to the difficulty in overlapping the randomly generated distribution with the real distribution. Several methods such as weight clipping, gradient penalty, and spectral normalization have been proposed to address this issue by constraining the discriminator's capabilities using empirical techniques. In this paper, we suggest a more principled approach to overcome this issue. Rather than training the discriminator to distinguish real and fake input samples, we explore the relationship between paired samples by training the discriminator to differentiate paired samples from the same distribution and those from different distributions. To achieve this, we use a relation network architecture for the discriminator and design a triplet loss that offers better generalization and stability. Extensive experiments on benchmark datasets demonstrate that the proposed relation discriminator and new loss provide significant improvements in variable vision tasks, including unconditional and conditional image generation and image translation.",1
"We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid -- a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled inpainting and compositing; all achieved with the same model, with no further training.",0
"Our proposed GAN-based model is unique in that it utilizes the deep features learned by a pre-trained classification model. We have developed the Semantic Generation Pyramid, which is inspired by classical image pyramid representations and is a hierarchical framework that utilizes the continuum of semantic information encapsulated in these deep features. This ranges from low-level information contained in fine features to high-level semantic information in deeper features. Our model generates diverse image samples with matching features at each semantic level of the classification model, given a set of features extracted from a reference image. We have demonstrated that this model is versatile and flexible and can be used for classic and novel image generation tasks. These include generating images with controllable semantic similarity to a reference image, semantically-controlled inpainting and compositing, all achieved with the same model without the need for further training.",1
"To generate new images for a given category, most deep generative models require abundant training images from this category, which are often too expensive to acquire. To achieve the goal of generation based on only a few images, we propose matching-based Generative Adversarial Network (GAN) for few-shot generation, which includes a matching generator and a matching discriminator. Matching generator can match random vectors with a few conditional images from the same category and generate new images for this category based on the fused features. The matching discriminator extends conventional GAN discriminator by matching the feature of generated image with the fused feature of conditional images. Extensive experiments on three datasets demonstrate the effectiveness of our proposed method.",0
"The majority of deep generative models require a large number of training images for a specific category, which can be costly to obtain. Our solution is a matching-based Generative Adversarial Network (GAN) that enables few-shot generation. This involves a matching generator and a matching discriminator. The generator matches random vectors with a small number of conditional images from the same category to produce new images based on fused features. The discriminator extends the conventional GAN discriminator by matching the generated image feature with the fused feature of the conditional images. We conducted extensive experiments on three datasets, which demonstrated the effectiveness of our proposed method.",1
"Visible-infrared person re-identification (VI-ReID) is an important task in night-time surveillance applications, since visible cameras are difficult to capture valid appearance information under poor illumination conditions. Compared to traditional person re-identification that handles only the intra-modality discrepancy, VI-ReID suffers from additional cross-modality discrepancy caused by different types of imaging systems. To reduce both intra- and cross-modality discrepancies, we propose a Hierarchical Cross-Modality Disentanglement (Hi-CMD) method, which automatically disentangles ID-discriminative factors and ID-excluded factors from visible-thermal images. We only use ID-discriminative factors for robust cross-modality matching without ID-excluded factors such as pose or illumination. To implement our approach, we introduce an ID-preserving person image generation network and a hierarchical feature learning module. Our generation network learns the disentangled representation by generating a new cross-modality image with different poses and illuminations while preserving a person's identity. At the same time, the feature learning module enables our model to explicitly extract the common ID-discriminative characteristic between visible-infrared images. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods on two VI-ReID datasets. The source code is available at: https://github.com/bismex/HiCMD.",0
"In night-time surveillance applications, visible-infrared person re-identification (VI-ReID) is a challenging task due to the difficulty in capturing accurate appearance information under poor illumination conditions. Additionally, VI-ReID involves cross-modality discrepancy, which traditional person re-identification does not address. To overcome these challenges, we propose the Hierarchical Cross-Modality Disentanglement (Hi-CMD) method, which disentangles ID-discriminative factors and ID-excluded factors from visible-thermal images to minimize discrepancies. Our approach uses only the ID-discriminative factors to ensure robust cross-modality matching. We achieve this through an ID-preserving person image generation network and a hierarchical feature learning module, which extracts the common ID-discriminative characteristic among visible-infrared images. Our method outperforms state-of-the-art methods on two VI-ReID datasets, and the source code is available on https://github.com/bismex/HiCMD.",1
"In the visual decoding domain, visually reconstructing presented images given the corresponding human brain activity monitored by functional magnetic resonance imaging (fMRI) is difficult, especially when reconstructing viewed natural images. Visual reconstruction is a conditional image generation on fMRI data and thus generative adversarial network (GAN) for natural image generation is recently introduced for this task. Although GAN-based methods have greatly improved, the fidelity and naturalness of reconstruction are still unsatisfactory due to the small number of fMRI data samples and the instability of GAN training. In this study, we proposed a new GAN-based Bayesian visual reconstruction method (GAN-BVRM) that includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluator to evaluate generated images. GAN-BVRM employs the pre-trained generator of the prevailing BigGAN to generate masses of natural images, and selects the images that best matches with the corresponding brain activity through the encoding models as the reconstruction of the image stimuli. In this process, the semantic and detailed contents of reconstruction are controlled by decoded categories and encoding models, respectively. GAN-BVRM used the Bayesian manner to avoid contradiction between naturalness and fidelity from current GAN-based methods and thus can improve the advantages of GAN. Experimental results revealed that GAN-BVRM improves the fidelity and naturalness, that is, the reconstruction is natural and similar to the presented image stimuli.",0
"It can be challenging to visually reconstruct presented images given the corresponding human brain activity monitored by fMRI, particularly when it comes to natural images. This is due to the fact that visual reconstruction is a conditional image generation on fMRI data, which is why a GAN for natural image generation has been introduced to address the issue. However, the fidelity and naturalness of the reconstruction are still inadequate because of the limited number of fMRI data samples and the instability of GAN training. To overcome these limitations, we propose a new GAN-based Bayesian visual reconstruction method called GAN-BVRM. This approach includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluators to assess the generated images. GAN-BVRM leverages the pre-trained generator of the prevailing BigGAN to produce numerous natural images and selects the ones that best match with the corresponding brain activity through the encoding models to reconstruct the image stimuli. The semantic and detailed contents of the reconstruction are controlled by decoded categories and encoding models, respectively. By using the Bayesian approach, GAN-BVRM avoids contradictions between naturalness and fidelity that are present in current GAN-based methods, thus improving upon the advantages of GAN. Our experimental results demonstrate that GAN-BVRM enhances the fidelity and naturalness of the reconstruction, resulting in a natural and similar image stimulus.",1
"Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on \emph{Deep Domain-Adversarial Image Generation} (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach.",0
"The problem of domain shift is common in machine learning models, where they are trained on a dataset, but when evaluated on a different distribution target dataset, they tend to fail. To mitigate this issue, domain generalisation (DG) methods are used, which utilize data from multiple source domains to train a model that can generalize to unseen domains. This paper proposes a new DG approach known as Deep Domain-Adversarial Image Generation (DDAIG), which comprises three components: a label classifier, a domain classifier, and a domain transformation network (DoTNet). The main objective of DoTNet is to map the source training data to unseen domains by generating data that can be correctly classified by the label classifier while deceiving the domain classifier. This approach enhances the label classifier's ability to handle unknown domain changes by supplementing the source training data with the generated unseen domain data. The proposed technique's effectiveness is demonstrated through extensive experiments on four DG datasets.",1
"Purpose: Accurate estimation of the position and orientation (pose) of surgical instruments is crucial for delicate minimally invasive temporal bone surgery. Current techniques lack in accuracy and/or line-of-sight constraints (conventional tracking systems) or expose the patient to prohibitive ionizing radiation (intra-operative CT). A possible solution is to capture the instrument with a c-arm at irregular intervals and recover the pose from the image.   Methods: i3PosNet infers the position and orientation of instruments from images using a pose estimation network. Said framework considers localized patches and outputs pseudo-landmarks. The pose is reconstructed from pseudo-landmarks by geometric considerations.   Results: We show i3PosNet reaches errors less than 0.05mm. It outperforms conventional image registration-based approaches reducing average and maximum errors by at least two thirds. i3PosNet trained on synthetic images generalizes to real x-rays without any further adaptation.   Conclusion: The translation of Deep Learning based methods to surgical applications is difficult, because large representative datasets for training and testing are not available. This work empirically shows sub-millimeter pose estimation trained solely based on synthetic training data.",0
"The accurate determination of the position and orientation of surgical instruments is essential for minimally invasive temporal bone surgery. However, current methods lack accuracy or have line-of-sight constraints or require the use of harmful ionizing radiation. To address this issue, one potential solution involves using a c-arm to capture images of the instrument at random intervals and using a pose estimation network to infer its position and orientation. The i3PosNet framework accomplishes this by analyzing localized patches and generating pseudo-landmarks, which are then used to reconstruct the pose through geometric calculations. Results demonstrate that i3PosNet achieves errors below 0.05mm and outperforms conventional image registration-based methods by reducing average and maximum errors by at least two-thirds. Moreover, i3PosNet can be trained on synthetic images and generalized to real x-rays without any further modifications. This study highlights the challenges of applying Deep Learning methods to surgical practices due to the lack of representative datasets, but it also shows that sub-millimeter pose estimation can be achieved through the use of synthetic training data.",1
"Despite Generative Adversarial Networks (GANs) have been widely used in various image-to-image translation tasks, they can be hardly applied on mobile devices due to their heavy computation and storage cost. Traditional network compression methods focus on visually recognition tasks, but never deal with generation tasks. Inspired by knowledge distillation, a student generator of fewer parameters is trained by inheriting the low-level and high-level information from the original heavy teacher generator. To promote the capability of student generator, we include a student discriminator to measure the distances between real images, and images generated by student and teacher generators. An adversarial learning process is therefore established to optimize student generator and student discriminator. Qualitative and quantitative analysis by conducting experiments on benchmark datasets demonstrate that the proposed method can learn portable generative models with strong performance.",0
"Although Generative Adversarial Networks (GANs) have found widespread use in various image-to-image translation tasks, their high computational and storage costs make them impractical to use on mobile devices. Traditional compression methods for neural networks have focused on visual recognition tasks, neglecting generation tasks. Inspired by knowledge distillation, we propose training a student generator with fewer parameters by inheriting low-level and high-level information from the original teacher generator. To improve the student generator's performance, we introduce a student discriminator to measure the similarity between real images and those generated by both the student and teacher generators. This establishes an adversarial learning process that optimizes both the student generator and discriminator. Our experiments on benchmark datasets demonstrate that our approach can learn portable generative models with strong performance, as shown by qualitative and quantitative analysis.",1
"To synthesize high-quality person images with arbitrary poses is challenging. In this paper, we propose a novel Multi-scale Conditional Generative Adversarial Networks (MsCGAN), aiming to convert the input conditional person image to a synthetic image of any given target pose, whose appearance and the texture are consistent with the input image. MsCGAN is a multi-scale adversarial network consisting of two generators and two discriminators. One generator transforms the conditional person image into a coarse image of the target pose globally, and the other is to enhance the detailed quality of the synthetic person image through a local reinforcement network. The outputs of the two generators are then merged into a synthetic, discriminant and high-resolution image. On the other hand, the synthetic image is downsampled to multiple resolutions as the input to multi-scale discriminator networks. The proposed multi-scale generators and discriminators handling different levels of visual features can benefit to synthesizing high-resolution person images with realistic appearance and texture. Experiments are conducted on the Market-1501 and DeepFashion datasets to evaluate the proposed model, and both qualitative and quantitative results demonstrate the superior performance of the proposed MsCGAN.",0
"Generating person images with arbitrary poses of high quality is a difficult task. This paper introduces a new approach called Multi-scale Conditional Generative Adversarial Networks (MsCGAN), which aims to produce a synthetic image of a given target pose that is consistent with the input image's appearance and texture. MsCGAN uses two generators and two discriminators in a multi-scale adversarial network. One generator transforms the input image into a coarse image of the target pose globally, while the other enhances the synthetic person image's detailed quality through a local reinforcement network. The outputs of the two generators are merged into a high-resolution, synthetic, and discriminant image. Additionally, the synthetic image is downsampled into multiple resolutions to serve as input to the multi-scale discriminator networks. The proposed approach utilizes multi-scale generators and discriminators to handle various levels of visual features, leading to the synthesis of high-resolution person images with realistic appearance and texture. The Market-1501 and DeepFashion datasets were used to evaluate the proposed model, and both qualitative and quantitative results demonstrate the superior performance of MsCGAN.",1
"In conditional Generative Adversarial Networks (cGANs), when two different initial noises are concatenated with the same conditional information, the distance between their outputs is relatively smaller, which makes minor modes likely to collapse into large modes. To prevent this happen, we proposed a hierarchical mode exploring method to alleviate mode collapse in cGANs by introducing a diversity measurement into the objective function as the regularization term. We also introduced the Expected Ratios of Expansion (ERE) into the regularization term, by minimizing the sum of differences between the real change of distance and ERE, we can control the diversity of generated images w.r.t specific-level features. We validated the proposed algorithm on four conditional image synthesis tasks including categorical generation, paired and un-paired image translation and text-to-image generation. Both qualitative and quantitative results show that the proposed method is effective in alleviating the mode collapse problem in cGANs, and can control the diversity of output images w.r.t specific-level features.",0
"The output distance between two different initial noises concatenated with the same conditional information is relatively small in conditional Generative Adversarial Networks (cGANs), resulting in minor modes collapsing into larger ones. To prevent this, we proposed a hierarchical mode exploring method that introduces a diversity measurement into the objective function as a regularization term. The Expected Ratios of Expansion (ERE) is also incorporated into the regularization term, allowing us to control the diversity of generated images with respect to specific-level features by minimizing the difference between the real change of distance and ERE. We conducted four conditional image synthesis tasks, including categorical generation, paired and unpaired image translation, and text-to-image generation, to validate our algorithm. The proposed method effectively alleviates the mode collapse problem in cGANs and can control the diversity of output images concerning specific-level features, as shown by both qualitative and quantitative results.",1
"Due to its potential wide applications in video surveillance and other computer vision tasks like tracking, person re-identification (ReID) has become popular and been widely investigated. However, conventional person re-identification can only handle RGB color images, which will fail at dark conditions. Thus RGB-infrared ReID (also known as Infrared-Visible ReID or Visible-Thermal ReID) is proposed. Apart from appearance discrepancy in traditional ReID caused by illumination, pose variations and viewpoint changes, modality discrepancy produced by cameras of the different spectrum also exists, which makes RGB-infrared ReID more difficult. To address this problem, we focus on extracting the shared cross-spectrum features of different modalities. In this paper, a novel multi-spectrum image generation method is proposed and the generated samples are utilized to help the network to find discriminative information for re-identifying the same person across modalities. Another challenge of RGB-infrared ReID is that the intra-person (images from the same person) discrepancy is often larger than the inter-person (images from different persons) discrepancy, so a dual-subspace pairing strategy is proposed to alleviate this problem. Combining those two parts together, we also design a one-stream neural network combining the aforementioned methods to extract compact representations of person images, called Cross-spectrum Dual-subspace Pairing (CDP) model. Furthermore, during the training process, we also propose a Dynamic Hard Spectrum Mining method to automatically mine more hard samples from hard spectrum based on the current model state to further boost the performance. Extensive experimental results on two public datasets, SYSU-MM01 with RGB + near-infrared images and RegDB with RGB + far-infrared images, have demonstrated the efficiency and generality of our proposed method.",0
"Person re-identification (ReID) has gained popularity and extensive research due to its potential applications in video surveillance and other computer vision tasks such as tracking. However, conventional RGB color image-based ReID fails in dark conditions. To address this issue, RGB-infrared ReID has been proposed. However, this method faces the challenge of modality discrepancy produced by cameras of different spectrums. To overcome this challenge, we propose a novel multi-spectrum image generation method to extract shared cross-spectrum features of different modalities. Additionally, we propose a dual-subspace pairing strategy to alleviate the problem of intra-person discrepancy being larger than inter-person discrepancy. We propose a one-stream neural network, named Cross-spectrum Dual-subspace Pairing (CDP) model, to extract compact representations of person images. Furthermore, we propose a Dynamic Hard Spectrum Mining method to automatically mine more hard samples from hard spectrum during the training process based on the current model state to further boost performance. Our proposed method has been demonstrated to be efficient and effective on two public datasets, SYSU-MM01 with RGB + near-infrared images and RegDB with RGB + far-infrared images.",1
"Existing domain adaptation methods aim at learning features that can be generalized among domains. These methods commonly require to update source classifier to adapt to the target domain and do not properly handle the trade off between the source domain and the target domain. In this work, instead of training a classifier to adapt to the target domain, we use a separable component called data calibrator to help the fixed source classifier recover discrimination power in the target domain, while preserving the source domain's performance. When the difference between two domains is small, the source classifier's representation is sufficient to perform well in the target domain and outperforms GAN-based methods in digits. Otherwise, the proposed method can leverage synthetic images generated by GANs to boost performance and achieve state-of-the-art performance in digits datasets and driving scene semantic segmentation. Our method empirically reveals that certain intriguing hints, which can be mitigated by adversarial attack to domain discriminators, are one of the sources for performance degradation under the domain shift.",0
"Current domain adaptation techniques aim to acquire generalizable features across domains. However, such methods often require updating the source classifier to fit the target domain and fail to balance the source and target domains effectively. This study proposes an alternative approach, utilizing a separable component known as a data calibrator to assist the fixed source classifier in maintaining its discriminatory power in the target domain while preserving performance in the source domain. When the difference between domains is minimal, the source classifier's representation suffices to perform well in the target domain, surpassing GAN-based methods in digits. If the difference is significant, the proposed approach can utilize synthetic images generated by GANs to enhance performance and achieve state-of-the-art outcomes in digits and driving scene semantic segmentation datasets. The study empirically demonstrates that certain intriguing hints, which can be mitigated by adversarial attacks on domain discriminators, contribute to performance deterioration during domain shifts.",1
"There have been many work in the literature on generation of various kinds of images such as Hand-Written characters (MNIST dataset), scene images (CIFAR-10 dataset), various objects images (ImageNet dataset), road signboard images (SVHN dataset) etc. Unfortunately, there have been very limited amount of work done in the domain of document image processing. Automatic image generation can lead to the enormous increase of labeled datasets with the help of only limited amount of labeled data. Various kinds of Deep generative models can be primarily divided into two categories. First category is auto-encoder (AE) and the second one is Generative Adversarial Networks (GANs). In this paper, we have evaluated various kinds of AE as well as GANs and have compared their performances on hand-written digits dataset (MNIST) and also on historical hand-written character dataset of Indonesian BALI language. Moreover, these generated characters are recognized by using character recognition tool for calculating the statistical performance of these generated characters with respect to original character images.",0
"Numerous studies have been conducted on the generation of diverse image types, including hand-written characters (MNIST dataset), scene images (CIFAR-10 dataset), object images (ImageNet dataset), and road signboard images (SVHN dataset), among others. However, research on document image processing has been limited. Automatic image generation has the potential to substantially increase labeled datasets with minimal labeled data. Deep generative models are typically divided into two categories: auto-encoder (AE) and Generative Adversarial Networks (GANs). This study evaluates various types of AE and GANs, comparing their performance on the MNIST dataset and a historical hand-written character dataset in Balinese script. The generated characters are recognized using a character recognition tool to calculate the statistical performance of the generated characters with respect to the original images.",1
"We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.",0
"Our novel approach, Sparse Sinkhorn Attention, offers an efficient and sparse strategy for acquiring the skill of attending. The technique is grounded in the differentiable sorting of internal representations, where we introduce a meta sorting network to generate latent permutations over sequences. Once we have sorted sequences, we can calculate quasi-global attention with only local windows, boosting the memory efficiency of the attention module. We have introduced algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for fitting Sinkhorn Attention for encoding and/or decoding purposes. Through extensive experimentation on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification, and natural language inference, we prove that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently surpasses recently proposed efficient Transformer models like Sparse Transformers.",1
"While generative adversarial networks (GANs) have revolutionized machine learning, a number of open questions remain to fully understand them and exploit their power. One of these questions is how to efficiently achieve proper diversity and sampling of the multi-mode data space. To address this, we introduce BasisGAN, a stochastic conditional multi-mode image generator. By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we learn a plug-and-played basis generator to stochastically generate basis elements, with just a few hundred of parameters, to fully embed stochasticity into convolutional filters. By sampling basis elements instead of filters, we dramatically reduce the cost of modeling the parameter space with no sacrifice on either image diversity or fidelity. To illustrate this proposed plug-and-play framework, we construct variants of BasisGAN based on state-of-the-art conditional image generation networks, and train the networks by simply plugging in a basis generator, without additional auxiliary components, hyperparameters, or training objectives. The experimental success is complemented with theoretical results indicating how the perturbations introduced by the proposed sampling of basis elements can propagate to the appearance of generated images.",0
"Although generative adversarial networks (GANs) have had a significant impact on machine learning, a number of unanswered questions still exist regarding their full potential and understanding. One such question pertains to achieving efficient diversity and sampling of the multi-mode data space. Therefore, we present BasisGAN, a stochastic conditional multi-mode image generator, which addresses this issue. We utilize the observation that a convolutional filter can be approximated as a linear combination of a small set of basis elements, allowing us to learn a basis generator to stochastically produce basis elements with only a few hundred parameters. By sampling basis elements instead of filters, we decrease the cost of parameter space modeling while maintaining image diversity and fidelity. We demonstrate this plug-and-play approach by constructing BasisGAN variants based on state-of-the-art conditional image generation networks, and training them by plugging in the basis generator without the need for auxiliary components, hyperparameters, or training objectives. Along with experimental success, we also provide theoretical results on how sampling basis elements can impact the appearance of generated images.",1
"Convolutional Neural Networks (CNNs) have emerged as highly successful tools for image generation, recovery, and restoration. A major contributing factor to this success is that convolutional networks impose strong prior assumptions about natural images. A surprising experiment that highlights this architectural bias towards natural images is that one can remove noise and corruptions from a natural image without using any training data, by simply fitting (via gradient descent) a randomly initialized, over-parameterized convolutional generator to the corrupted image. While this over-parameterized network can fit the corrupted image perfectly, surprisingly after a few iterations of gradient descent it generates an almost uncorrupted image. This intriguing phenomenon enables state-of-the-art CNN-based denoising and regularization of other inverse problems. In this paper, we attribute this effect to a particular architectural choice of convolutional networks, namely convolutions with fixed interpolating filters. We then formally characterize the dynamics of fitting a two-layer convolutional generator to a noisy signal and prove that early-stopped gradient descent denoises/regularizes. Our proof relies on showing that convolutional generators fit the structured part of an image significantly faster than the corrupted portion.",0
"The use of Convolutional Neural Networks (CNNs) has been highly successful in generating, recovering, and restoring images. A major factor contributing to this success is that these networks make strong assumptions about natural images. An interesting experiment shows that it is possible to remove noise and corruptions from a natural image without using any training data. This is achieved by fitting an over-parameterized convolutional generator to the corrupted image using gradient descent. Although this network can fit the corrupted image perfectly, after a few iterations of gradient descent, it generates an almost uncorrupted image. This phenomenon has enabled state-of-the-art CNN-based denoising and regularization of other inverse problems. In this paper, we attribute this effect to a particular architectural choice of convolutional networks and formally characterize the dynamics of fitting a two-layer convolutional generator to a noisy signal. We prove that early-stopped gradient descent denoises/regularizes by showing that convolutional generators fit the structured part of an image significantly faster than the corrupted portion.",1
"Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",0
"Despite extensive research efforts, it is widely recognized that training Generative Adversarial Networks (GANs) is a challenging task. Existing techniques for stabilizing training, such as regularization methods, are known to have limitations and can lead to increased computational overheads. In this study, we propose a novel training stabilizer based on consistency regularization, a popular approach in semi-supervised learning. Specifically, we introduce modifications to the data passed into the GAN discriminator and penalize the discriminator's sensitivity to these changes. Our experiments demonstrate that our approach works effectively with various GAN architectures, loss functions, and optimizer settings, including spectral normalization. Our proposed method outperforms other regularization techniques, achieving the best FID scores for unconditional image generation on CIFAR-10 and CelebA datasets. Additionally, our consistency regularized GAN (CR-GAN) improves the state-of-the-art FID scores for conditional image generation on CIFAR-10 and ImageNet-2012, reducing the scores from 14.73 to 11.48 and from 8.73 to 6.66, respectively.",1
"RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. The key solution is to learn aligned features to the bridge RGB and IR modalities. However, due to the lack of correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing the distance between the entire RGB and IR sets. However, this set-level alignment may lead to misalignment of some instances, which limits the performance for RGB-IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross-modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged images. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Extensive experimental results on two standard benchmarks demonstrate that the proposed model favourably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve a gain of 9.2% and 7.7% in terms of Rank-1 and mAP. Code is available at https://github.com/wangguanan/JSIA-ReID.",0
"The task of RGB-Infrared (IR) person re-identification poses a significant challenge due to the significant differences between RGB and IR images. To overcome this challenge, it is essential to learn features that align the two modalities. However, because there are no correspondence labels between every pair of RGB and IR images, most methods rely on set-level alignment, which reduces the distance between the entire RGB and IR sets. Unfortunately, this approach may result in some instances being misaligned, which hinders the performance of RGB-IR Re-ID. In this paper, we propose a new method that generates cross-modality paired-images and uses global set-level and fine-grained instance-level alignments. Our method has several advantages. Firstly, it can perform set-level alignment by separating modality-specific and modality-invariant features, which can more effectively reduce modality variation. Secondly, it can generate cross-modality paired images from unpaired images, enabling direct instance-level alignment by minimizing distances between each pair of images. Our proposed model outperforms state-of-the-art methods on two standard benchmarks, achieving a 9.2% and 7.7% gain in Rank-1 and mAP on the SYSU-MM01 dataset. The code is available at https://github.com/wangguanan/JSIA-ReID.",1
"In this paper, we introduce Random Path Generative Adversarial Network (RPGAN) -- an alternative design of GANs that can serve as a tool for generative model analysis. While the latent space of a typical GAN consists of input vectors, randomly sampled from the standard Gaussian distribution, the latent space of RPGAN consists of random paths in a generator network. As we show, this design allows to understand factors of variation, captured by different generator layers, providing their natural interpretability. With experiments on standard benchmarks, we demonstrate that RPGAN reveals several interesting insights about the roles that different layers play in the image generation process. Aside from interpretability, the RPGAN model also provides competitive generation quality and allows efficient incremental learning on new data.",0
"The article introduces RPGAN, a novel design of GANs that can be used to analyze generative models. Unlike traditional GANs that use input vectors randomly sampled from a Gaussian distribution, RPGAN uses random paths in a generator network, which allows for natural interpretability and understanding of the variation factors captured by different generator layers. Through experiments on standard benchmarks, we demonstrate that RPGAN provides insights into the roles that different layers play in image generation while maintaining competitive generation quality and enabling efficient incremental learning on new data.",1
"Object detection in thermal images is an important computer vision task and has many applications such as unmanned vehicles, robotics, surveillance and night vision. Deep learning based detectors have achieved major progress, which usually need large amount of labelled training data. However, labelled data for object detection in thermal images is scarce and expensive to collect. How to take advantage of the large number labelled visible images and adapt them into thermal image domain, is expected to solve. This paper proposes an unsupervised image-generation enhanced adaptation method for object detection in thermal images. To reduce the gap between visible domain and thermal domain, the proposed method manages to generate simulated fake thermal images that are similar to the target images, and preserves the annotation information of the visible source domain. The image generation includes a CycleGAN based image-to-image translation and an intensity inversion transformation. Generated fake thermal images are used as renewed source domain. And then the off-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between generated intermediate domain and the thermal target domain. Experiments demonstrate the effectiveness and superiority of the proposed method.",0
"The detection of objects in thermal images is a crucial task in computer vision and has numerous applications, such as unmanned vehicles, robotics, surveillance, and night vision. Although deep learning-based detectors have made significant progress, they require a vast amount of labelled training data. Unfortunately, obtaining labelled data for object detection in thermal images is difficult and expensive. Therefore, this study proposes an unsupervised image-generation enhanced adaptation method to overcome this limitation. The proposed method generates simulated fake thermal images, which are similar to the target images, using a CycleGAN-based image-to-image translation and intensity inversion transformation. These generated fake thermal images preserve the annotation information of the visible source domain. Subsequently, the off-the-shelf Domain Adaptive Faster RCNN is employed to reduce the gap between the generated intermediate domain and the thermal target domain. Experiments confirm the effectiveness and superiority of the proposed method.",1
"Although the inherently ambiguous task of predicting what resides beyond all four edges of an image has rarely been explored before, we demonstrate that GANs hold powerful potential in producing reasonable extrapolations. Two outpainting methods are proposed that aim to instigate this line of research: the first approach uses a context encoder inspired by common inpainting architectures and paradigms, while the second approach adds an extra post-processing step using a single-image generative model. This way, the hallucinated details are integrated with the style of the original image, in an attempt to further boost the quality of the result and possibly allow for arbitrary output resolutions to be supported.",0
"Although predicting what lies beyond the edges of an image is a difficult task that has not been explored much before, we have found that GANs have the potential to produce plausible extrapolations. In order to investigate this further, we have proposed two methods for outpainting. The first method involves the use of a context encoder that is inspired by common inpainting techniques. The second method involves adding an extra post-processing step that utilizes a single-image generative model. By integrating the hallucinated details with the original image's style, we hope to improve the quality of the result and possibly enable support for arbitrary output resolutions.",1
"Cross-view image generation has been recently proposed to generate images of one view from another dramatically different view. In this paper, we investigate exocentric (third-person) view to egocentric (first-person) view image generation. This is a challenging task since egocentric view sometimes is remarkably different from exocentric view. Thus, transforming the appearances across the two views is a non-trivial task. To this end, we propose a novel Parallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss to learn the shared information for generating egocentric images from exocentric view. We also incorporate a novel contextual feature loss in the learning procedure to capture the contextual information in images. Extensive experiments on the Exo-Ego datasets show that our model outperforms the state-of-the-art approaches.",0
"Recently, there has been a proposal for cross-view image generation which generates images of one view from another completely different view. This paper focuses on investigating the generation of egocentric (first-person) view images from exocentric (third-person) view images. Since the egocentric view can be drastically different from the exocentric view, this task is challenging and transforming the appearance across these two views is not an easy feat. To address this, we introduce a novel Parallel Generative Adversarial Network (P-GAN) with a cross-cycle loss to learn shared information for generating egocentric images from exocentric view. Additionally, we incorporate a contextual feature loss to capture contextual information in images. Our model outperforms the state-of-the-art approaches, as demonstrated by extensive experiments on the Exo-Ego datasets.",1
"Convolutional neural networks (CNNs) have become a key asset to most of fields in AI. Despite their successful performance, CNNs suffer from a major drawback. They fail to capture the hierarchy of spatial relation among different parts of an entity. As a remedy to this problem, the idea of capsules was proposed by Hinton. In this paper, we propose the SubSpace Capsule Network (SCN) that exploits the idea of capsule networks to model possible variations in the appearance or implicitly defined properties of an entity through a group of capsule subspaces instead of simply grouping neurons to create capsules. A capsule is created by projecting an input feature vector from a lower layer onto the capsule subspace using a learnable transformation. This transformation finds the degree of alignment of the input with the properties modeled by the capsule subspace. We show that SCN is a general capsule network that can successfully be applied to both discriminative and generative models without incurring computational overhead compared to CNN during test time. Effectiveness of SCN is evaluated through a comprehensive set of experiments on supervised image classification, semi-supervised image classification and high-resolution image generation tasks using the generative adversarial network (GAN) framework. SCN significantly improves the performance of the baseline models in all 3 tasks.",0
"Most AI fields rely on Convolutional neural networks (CNNs) as a powerful tool. However, CNNs have a major issue; they fail to capture the spatial hierarchy of an entity's various parts. To address this, Hinton created the concept of capsules. In this study, we introduce the SubSpace Capsule Network (SCN), which utilizes capsule networks to model entity appearance variations through a series of capsule subspaces instead of grouping neurons to form capsules. Capsules are formed by projecting an input feature vector from a lower layer onto the capsule subspace using a learnable transformation. This transformation determines the input's alignment with the subspace's properties. Our experiments demonstrate that SCN is a versatile capsule network that can be used in both discriminative and generative models without incurring additional computational costs compared to CNN during testing. We evaluate SCN's effectiveness in supervised image classification, semi-supervised image classification, and high-resolution image generation tasks using the generative adversarial network (GAN) framework. SCN outperforms baseline models in all three tasks.",1
"Objective and interpretable metrics to evaluate current artificial intelligent systems are of great importance, not only to analyze the current state of such systems but also to objectively measure progress in the future. In this work, we focus on the evaluation of image generation tasks. We propose a novel approach, called Fuzzy Topology Impact (FTI), that determines both the quality and diversity of an image set using topology representations combined with fuzzy logic. When compared to current evaluation methods, FTI shows better and more stable performance on multiple experiments evaluating the sensitivity to noise, mode dropping and mode inventing.",0
"It is crucial to have objective and understandable measures to assess present artificial intelligence systems. These metrics not only help in examining the current state of such systems but also aid in measuring progress in the future. This research concentrates on appraising image generation tasks. An innovative approach known as Fuzzy Topology Impact (FTI) is presented, which gauges the quality and variety of an image set through topology representations and fuzzy logic. Compared to conventional evaluation techniques, FTI exhibits superior and consistent results on numerous experiments assessing sensitivity to noise, mode dropping, and mode inventing.",1
"We introduce the ""adversarial code learning"" (ACL) module that improves overall image generation performance to several types of deep models. Instead of performing a posterior distribution modeling in the pixel spaces of generators, ACLs aim to jointly learn a latent code with another image encoder/inference net, with a prior noise as its input. We conduct the learning in an adversarial learning process, which bears a close resemblance to the original GAN but again shifts the learning from image spaces to prior and latent code spaces. ACL is a portable module that brings up much more flexibility and possibilities in generative model designs. First, it allows flexibility to convert non-generative models like Autoencoders and standard classification models to decent generative models. Second, it enhances existing GANs' performance by generating meaningful codes and images from any part of the prior. We have incorporated our ACL module with the aforementioned frameworks and have performed experiments on synthetic, MNIST, CIFAR-10, and CelebA datasets. Our models have achieved significant improvements which demonstrated the generality for image generation tasks.",0
"The ""adversarial code learning"" (ACL) module is presented as a tool to enhance image generation performance across various deep models. Instead of focusing on posterior distribution modeling within generator pixel spaces, ACLs work to jointly learn a latent code with an image encoder/inference net, using prior noise as input. This is achieved through an adversarial learning process that shares similarities with the original GAN, but prioritizes learning in prior and latent code spaces rather than image spaces. ACL is a portable module that offers greater flexibility and potential in generative model design, allowing for non-generative models like Autoencoders and standard classification models to be converted into decent generative models. Additionally, ACL enhances existing GANs by generating meaningful codes and images from any part of the prior. We have incorporated ACL into various frameworks and conducted experiments on synthetic, MNIST, CIFAR-10, and CelebA datasets, achieving significant improvements that demonstrate the module's generality in image generation tasks.",1
"Generative adversarial networks (GANs) have shown great success in applications such as image generation and inpainting. However, they typically require large datasets, which are often not available, especially in the context of prediction tasks such as image segmentation that require labels. Therefore, methods such as the CycleGAN use more easily available unlabelled data, but do not offer a way to leverage additional labelled data for improved performance. To address this shortcoming, we show how to factorise the joint data distribution into a set of lower-dimensional distributions along with their dependencies. This allows splitting the discriminator in a GAN into multiple ""sub-discriminators"" that can be independently trained from incomplete observations. Their outputs can be combined to estimate the density ratio between the joint real and the generator distribution, which enables training generators as in the original GAN framework. We apply our method to image generation, image segmentation and audio source separation, and obtain improved performance over a standard GAN when additional incomplete training examples are available. For the Cityscapes segmentation task in particular, our method also improves accuracy by an absolute 14.9% over CycleGAN while using only 25 additional paired examples.",0
"GANs have been successful in creating images and filling in missing parts, but they require large datasets that are often unavailable for prediction tasks like image segmentation. The CycleGAN method uses available unlabeled data, but cannot utilize labeled data for better performance. We address this issue by breaking down the joint data distribution into lower-dimensional distributions and their dependencies, allowing for multiple sub-discriminators to be trained independently from incomplete observations. These sub-discriminators' outputs can be combined to estimate the density ratio between the joint real and generator distribution, enabling generator training like in the original GAN framework. Our approach yields improved performance in image generation, segmentation, and audio source separation, particularly for the Cityscapes segmentation task where we see a 14.9% increase in accuracy with just 25 additional paired examples.",1
"With the emergence of high-resolution fingerprint sensors, there has been a lot of focus on level-3 fingerprint features, especially the pores, for the next generation automated fingerprint recognition systems (AFRS). Following the success of deep learning in various computer vision tasks, researchers have developed learning-based approaches for detection of pores in high-resolution fingerprint images. Generally, learning-based approaches provide better performance than handcrafted feature-based approaches. However, domain adaptability of the existing learning-based pore detection methods has never been studied. In this paper, we study this aspect and propose an approach for pore detection in cross-sensor scenarios. For this purpose, we have generated an in-house 1000 dpi fingerprint dataset with ground truth pore coordinates (referred to as IITI-HRFP-GT), and evaluated the performance of the existing learning-based pore detection approaches. The core of the proposed approach for detection of pores in cross-sensor scenarios is DeepDomainPore, which is a residual learning-based convolutional neural network(CNN) trained for pore detection. The domain adaptability in DeepDomainPore is achieved by embedding a gradient reversal layer between the CNN and a domain classifier network. The proposed approach achieves state-of-the-art performance in a cross-sensor scenario involving public high-resolution fingerprint datasets with 88.12% true detection rate and 83.82% F-score.",0
"The focus of the next generation automated fingerprint recognition systems (AFRS) is on level-3 fingerprint features, particularly the pores, due to the emergence of high-resolution fingerprint sensors. Researchers have developed learning-based methods for detecting pores in high-resolution fingerprint images, which outperform handcrafted feature-based methods. However, the domain adaptability of these methods has not been studied. This paper proposes an approach for detecting pores in cross-sensor scenarios using a residual learning-based convolutional neural network called DeepDomainPore. The approach achieves state-of-the-art performance with an 88.12% true detection rate and 83.82% F-score on public high-resolution fingerprint datasets. To evaluate the approach, an in-house 1000 dpi fingerprint dataset with ground truth pore coordinates (IITI-HRFP-GT) was generated and used. The domain adaptability of DeepDomainPore is achieved by embedding a gradient reversal layer between the CNN and a domain classifier network.",1
"Single Image Super-Resolution (SISR) task refers to learn a mapping from low-resolution images to the corresponding high-resolution ones. This task is known to be extremely difficult since it is an ill-posed problem. Recently, Convolutional Neural Networks (CNNs) have achieved state of the art performance on SISR. However, the images produced by CNNs do not contain fine details of the images. Generative Adversarial Networks (GANs) aim to solve this issue and recover sharp details. Nevertheless, GANs are notoriously difficult to train. Besides that, they generate artifacts in the high-resolution images. In this paper, we have proposed a method in which CNNs try to align images in different spaces rather than only the pixel space. Such a space is designed using convex optimization techniques. CNNs are encouraged to learn high-frequency components of the images as well as low-frequency components. We have shown that the proposed method can recover fine details of the images and it is stable in the training process.",0
"The task of Single Image Super-Resolution (SISR) involves learning a mapping from low-resolution images to their high-resolution counterparts, which is known to be challenging due to it being an ill-posed problem. While Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in SISR, the resulting images lack fine details. Generative Adversarial Networks (GANs) aim to address this issue, but they are challenging to train and may introduce artifacts. To overcome these challenges, we propose a method in which CNNs align images in different spaces using convex optimization techniques, enabling them to learn both high- and low-frequency components. Our approach successfully recovers fine details and is stable during training.",1
"Conditional image modeling based on textual descriptions is a relatively new domain in unsupervised learning. Previous approaches use a latent variable model and generative adversarial networks. While the formers are approximated by using variational auto-encoders and rely on the intractable inference that can hamper their performance, the latter is unstable to train due to Nash equilibrium based objective function. We develop a tractable and stable caption-based image generation model. The model uses an attention-based encoder to learn word-to-pixel dependencies. A conditional autoregressive based decoder is used for learning pixel-to-pixel dependencies and generating images. Experimentations are performed on Microsoft COCO, and MNIST-with-captions datasets and performance is evaluated by using the Structural Similarity Index. Results show that the proposed model performs better than contemporary approaches and generate better quality images. Keywords: Generative image modeling, autoregressive image modeling, caption-based image generation, neural attention, recurrent neural networks.",0
"The domain of unsupervised learning has recently seen the emergence of conditional image modeling based on textual descriptions. Prior methods have relied on latent variable models and generative adversarial networks, but these have faced issues such as intractable inference and instability in training. To address these challenges, we present a new caption-based image generation model that leverages an attention-based encoder to learn word-to-pixel dependencies and a conditional autoregressive decoder to learn pixel-to-pixel dependencies. We evaluated our model on Microsoft COCO and MNIST-with-captions datasets and found it to outperform existing approaches. Our results demonstrate the effectiveness of our proposed method in generating high-quality images and highlight the importance of generative image modeling, autoregressive image modeling, caption-based image generation, neural attention, and recurrent neural networks.",1
"Image-to-image transformation is a kind of problem, where the input image from one visual representation is transformed into the output image of another visual representation. Since 2014, Generative Adversarial Networks (GANs) have facilitated a new direction to tackle this problem by introducing the generator and the discriminator networks in its architecture. Many recent works, like Pix2Pix, CycleGAN, DualGAN, PS2MAN and CSGAN handled this problem with the required generator and discriminator networks and choice of the different losses that are used in the objective functions. In spite of these works, still there is a gap to fill in terms of both the quality of the images generated that should look more realistic and as much as close to the ground truth images. In this work, we introduce a new Image-to-Image Transformation network named Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills the above mentioned gaps. The proposed CDGAN generates high quality and more realistic images by incorporating the additional discriminator networks for cycled images in addition to the original architecture of the CycleGAN. To demonstrate the performance of the proposed CDGAN, it is tested over three different baseline image-to-image transformation datasets. The quantitative metrics such as pixel-wise similarity, structural level similarity and perceptual level similarity are used to judge the performance. Moreover, the qualitative results are also analyzed and compared with the state-of-the-art methods. The proposed CDGAN method clearly outperformed all the state-of-the-art methods when compared over the three baseline Image-to-Image transformation datasets.",0
"The problem of image-to-image transformation involves converting an input image from one visual representation to an output image of another visual representation. Starting in 2014, Generative Adversarial Networks (GANs) have enabled a new approach to solving this problem by using generator and discriminator networks in their architecture. Various recent works, such as Pix2Pix, CycleGAN, DualGAN, PS2MAN, and CSGAN, have addressed this problem using the necessary generator and discriminator networks and selecting different losses for the objective functions. Despite these efforts, there is still a need to improve the quality of the generated images to make them more realistic and closer to the ground truth images. To address this gap, we introduce a new Image-to-Image Transformation network called Cyclic Discriminative Generative Adversarial Networks (CDGAN). The CDGAN incorporates additional discriminator networks for cycled images in addition to the original architecture of the CycleGAN, resulting in high-quality and more realistic images. We tested the proposed CDGAN on three different baseline image-to-image transformation datasets using quantitative metrics such as pixel-wise similarity, structural level similarity, and perceptual level similarity, as well as qualitative analysis compared to state-of-the-art methods. The proposed CDGAN method significantly outperformed all state-of-the-art methods when evaluated on the three baseline Image-to-Image transformation datasets.",1
"Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.",0
"While continual learning is gaining interest, most studies have focused on a limited setting with clearly defined tasks and boundaries during training. However, if our goal is to create an algorithm that learns like humans do, this approach is unrealistic. Therefore, it is crucial to develop a methodology that can work without specific tasks. Among various approaches to continual learning, expansion-based methods are advantageous as they can allocate new resources to learn new data without catastrophic forgetting. This paper proposes an expansion-based model called Continual Neural Dirichlet Process Mixture (CN-DPM) for task-free continual learning. It consists of a group of neural network experts responsible for a subset of data, expanding the number of experts through Bayesian nonparametric framework. Through extensive experiments, the model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and generation.",1
"Presenting context images to a viewer's peripheral vision is one of the most effective techniques to enhance immersive visual experiences. However, most images only present a narrow view, since the field-of-view (FoV) of standard cameras is small. To overcome this limitation, we propose a deep learning approach that learns to predict a 180{\deg} panoramic image from a narrow-view image. Specifically, we design a foveated framework that applies different strategies on near-periphery and mid-periphery regions. Two networks are trained separately, and then are employed jointly to sequentially perform narrow-to-90{\deg} generation and 90{\deg}-to-180{\deg} generation. The generated outputs are then fused with their aligned inputs to produce expanded equirectangular images for viewing. Our experimental results show that single-view-to-panoramic image generation using deep learning is both feasible and promising.",0
"One of the most effective ways to enhance immersive visual experiences is to present context images to a viewer's peripheral vision. However, this technique is limited by the narrow view provided by standard cameras due to their small field-of-view (FoV). To overcome this limitation, we propose a deep learning approach that uses a foveated framework to predict a 180{\deg} panoramic image from a narrow-view image. We train two networks separately for near-periphery and mid-periphery regions, and then use them jointly to generate narrow-to-90{\deg} and 90{\deg}-to-180{\deg} images. The resulting images are then fused with their aligned inputs to create expanded equirectangular images. Our experimental results indicate that deep learning-based single-view-to-panoramic image generation is both feasible and promising.",1
"Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.",0
"The interpretability of generative models can be improved by learning disentangled representation of data without supervision. However, existing approaches for disentangled representation learning usually face a trade-off between representation learning and generation performance. In this study, we propose a simple and versatile framework called Information-Distillation Generative Adversarial Network (ID-GAN) that integrates state-of-the-art models for disentanglement learning and high-fidelity synthesis. Our approach employs VAE-based models to learn disentangled representation and distills the learned representation with an additional nuisance variable to the GAN-based generator for high-fidelity synthesis. We also constrain the GAN generator to maximize the mutual information between the learned latent code and the output to ensure both generative models are aligned. Despite its simplicity, our method achieves comparable image generation quality to state-of-the-art methods using disentangled representation, while also leading to an efficient and stable model design. Notably, we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) using disentangled representations for the first time.",1
"For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic. We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model. In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions. The resulting benchmarks cannot be ""won"" by training set memorization, while still being perceptually correlated and computable only from samples. We survey past work on using NNDs for evaluation and implement an example black-box metric based on these ideas. Through experimental validation we show that it can effectively measure diversity, sample quality, and generalization.",0
"It is problematic that for many commonly used evaluation metrics in unconditional image generation, models considered state-of-the-art do not perform as well as trivially memorizing the training set. To address this issue, we believe it is necessary for an evaluation metric to require a large sample from the model. To find such a metric, we turn to neural network divergences (NNDs), which utilize a neural network trained to distinguish between distributions. NNDs provide benchmarks that cannot be won by memorizing the training set, yet are still perceptually correlated and computable only from samples. We review previous research on using NNDs for evaluation and create a black-box metric based on these concepts. Our experimental validation demonstrates that it effectively measures diversity, sample quality, and generalization.",1
"Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.",0
"The goal of semantic image synthesis is to produce realistic images based on semantic layouts. Previous methods using conditional generative adversarial networks have shown excellent performance by either inputting semantic label maps into the generator or adjusting activations via affine transformations. However, we propose that the generator's convolutional kernels should be aware of the distinct semantic labels at different positions to better utilize the semantic layout. To achieve this, we suggest predicting convolutional kernels conditioned on the semantic label map to generate intermediate feature maps and ultimately create images. Additionally, we introduce a feature pyramid semantics-embedding discriminator that enhances fine details and semantic alignment between the generated images and input semantic layouts better than previous multi-scale discriminators. Our approach achieves outstanding results on various semantic segmentation datasets, both in quantitative metrics and subjective assessments, demonstrating its effectiveness.",1
"Spherical images taken in all directions (360 degrees) allow representing the surroundings of the subject and the space itself, providing an immersive experience to the viewers. Generating a spherical image from a single normal-field-of-view (NFOV) image is convenient and considerably expands the usage scenarios because there is no need to use a specific panoramic camera or take images from multiple directions; however, it is still a challenging and unsolved problem. The primary challenge is controlling the high degree of freedom involved in generating a wide area that includes the all directions of the desired plausible spherical image. On the other hand, scene symmetry is a basic property of the global structure of the spherical images, such as rotation symmetry, plane symmetry and asymmetry. We propose a method to generate spherical image from a single NFOV image, and control the degree of freedom of the generated regions using scene symmetry. We incorporate scene-symmetry parameters as latent variables into conditional variational autoencoders, following which we learn the conditional probability of spherical images for NFOV images and scene symmetry. Furthermore, the probability density functions are represented using neural networks, and scene symmetry is implemented using both circular shift and flip of the hidden variables. Our experiments show that the proposed method can generate various plausible spherical images, controlled from symmetric to asymmetric.",0
"Capturing spherical images in all directions (360 degrees) provides an immersive experience for viewers, allowing them to explore the subject's surroundings and the space itself. While generating a spherical image from a single normal-field-of-view (NFOV) image is convenient and expands usage scenarios, it poses a challenge due to the high degree of freedom involved in creating a wide area that includes all directions. Additionally, scene symmetry is a fundamental property of spherical images, such as rotation symmetry, plane symmetry, and asymmetry. To address these challenges, we propose a method that uses scene symmetry to control the degree of freedom in generated regions. We incorporate scene-symmetry parameters as latent variables into conditional variational autoencoders and learn the conditional probability of spherical images for NFOV images and scene symmetry. Neural networks represent probability density functions, and scene symmetry is implemented using circular shift and flip of the hidden variables. Our experiments demonstrate that our method can generate a range of plausible spherical images, from symmetric to asymmetric.",1
"For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN), a versatile deep generative model that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a deep topic model, a ladder-structured image encoder, and StackGAN++, into VHE-GAN, which already achieves competitive performance. This further motivates the development of VHE-raster-scan-GAN that generates photo-realistic images in not only a multi-scale low-to-high-resolution manner, but also a hierarchical-semantic coarse-to-fine fashion. By capturing and relating hierarchical semantic and visual concepts with end-to-end training, VHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of image-text multi-modality learning and generation tasks.",0
"A versatile deep generative model called the Variational Hetero-Encoder (VHE) Randomized Generative Adversarial Network (GAN) has been developed for bidirectional joint image-text modeling. This model integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multi-modality learning framework. The VHE-GAN encodes an image to decode its corresponding text, and utilizes the variational posterior as the source of randomness into the GAN image generator. Three off-the-shelf modules, such as a deep topic model, a ladder-structured image encoder, and StackGAN++, are plugged into the VHE-GAN to achieve competitive performance. This leads to the development of the VHE-raster-scan-GAN, which generates photo-realistic images in a hierarchical-semantic coarse-to-fine fashion and a multi-scale low-to-high-resolution manner. By capturing and relating hierarchical semantic and visual concepts with end-to-end training, the VHE-raster-scan-GAN produces state-of-the-art performance in a wide range of image-text multi-modality learning and generation tasks.",1
"This article studies the domain adaptation problem in person re-identification (re-ID) under a ""learning via translation"" framework, consisting of two components, 1) translating the labeled images from the source to the target domain in an unsupervised manner, 2) learning a re-ID model using the translated images. The objective is to preserve the underlying human identity information after image translation, so that translated images with labels are effective for feature learning on the target domain. To this end, we propose a similarity preserving generative adversarial network (SPGAN) and its end-to-end trainable version, eSPGAN. Both aiming at similarity preserving, SPGAN enforces this property by heuristic constraints, while eSPGAN does so by optimally facilitating the re-ID model learning. More specifically, SPGAN separately undertakes the two components in the ""learning via translation"" framework. It first preserves two types of unsupervised similarity, namely, self-similarity of an image before and after translation, and domain-dissimilarity of a translated source image and a target image. It then learns a re-ID model using existing networks. In comparison, eSPGAN seamlessly integrates image translation and re-ID model learning. During the end-to-end training of eSPGAN, re-ID learning guides image translation to preserve the underlying identity information of an image. Meanwhile, image translation improves re-ID learning by providing identity-preserving training samples of the target domain style. In the experiment, we show that identities of the fake images generated by SPGAN and eSPGAN are well preserved. Based on this, we report the new state-of-the-art domain adaptation results on two large-scale person re-ID datasets.",0
"This article explores how to solve the domain adaptation problem in person re-identification (re-ID) by using a ""learning via translation"" approach, which involves two main components. The first component translates labeled images from the source to the target domain in an unsupervised manner, while the second component learns a re-ID model using the translated images. The goal is to maintain the underlying human identity information after image translation, so that the translated images with labels can effectively be used for feature learning on the target domain. To achieve this, the authors propose a similarity preserving generative adversarial network (SPGAN) and its end-to-end trainable version, eSPGAN, both of which aim to preserve similarity. SPGAN enforces this property through heuristic constraints, while eSPGAN optimally facilitates the re-ID model learning process. In the experiment, the authors demonstrate that both SPGAN and eSPGAN successfully preserve the identities of the fake images generated, leading to new state-of-the-art domain adaptation results on two large-scale person re-ID datasets.",1
"We treat the problem of color enhancement as an image translation task, which we tackle using both supervised and unsupervised learning. Unlike traditional image to image generators, our translation is performed using a global parameterized color transformation instead of learning to directly map image information. In the supervised case, every training image is paired with a desired target image and a convolutional neural network (CNN) learns from the expert retouched images the parameters of the transformation. In the unpaired case, we employ two-way generative adversarial networks (GANs) to learn these parameters and apply a circularity constraint. We achieve state-of-the-art results compared to both supervised (paired data) and unsupervised (unpaired data) image enhancement methods on the MIT-Adobe FiveK benchmark. Moreover, we show the generalization capability of our method, by applying it on photos from the early 20th century and to dark video frames.",0
"Our approach to color enhancement involves treating it as an image translation task. To accomplish this goal, we utilize both supervised and unsupervised learning techniques. Unlike conventional image generators, we use a global parameterized color transformation to perform the translation instead of directly mapping image information. In the supervised approach, we pair each training image with a target image and train a convolutional neural network (CNN) to learn the transformation parameters from expert retouched images. In the unsupervised approach, we use two-way generative adversarial networks (GANs) to learn the parameters and apply a circularity constraint. Our method yields state-of-the-art results compared to both supervised and unsupervised image enhancement techniques on the MIT-Adobe FiveK benchmark. Furthermore, we demonstrate the generalization ability of our approach by applying it to photos from the early 20th century and dark video frames.",1
"High dynamic range (HDR) image generation from a single exposure low dynamic range (LDR) image has been made possible due to the recent advances in Deep Learning. Various feed-forward Convolutional Neural Networks (CNNs) have been proposed for learning LDR to HDR representations. To better utilize the power of CNNs, we exploit the idea of feedback, where the initial low level features are guided by the high level features using a hidden state of a Recurrent Neural Network. Unlike a single forward pass in a conventional feed-forward network, the reconstruction from LDR to HDR in a feedback network is learned over multiple iterations. This enables us to create a coarse-to-fine representation, leading to an improved reconstruction at every iteration. Various advantages over standard feed-forward networks include early reconstruction ability and better reconstruction quality with fewer network parameters. We design a dense feedback block and propose an end-to-end feedback network- FHDR for HDR image generation from a single exposure LDR image. Qualitative and quantitative evaluations show the superiority of our approach over the state-of-the-art methods.",0
"Recent advancements in Deep Learning have made it possible to generate High Dynamic Range (HDR) images from a single exposure Low Dynamic Range (LDR) image. Various Convolutional Neural Networks (CNNs) have been proposed for learning LDR to HDR representations. However, to better utilize the power of CNNs, the concept of feedback has been exploited, where the initial low level features are guided by the high level features using a hidden state of a Recurrent Neural Network. Unlike a conventional feed-forward network, the reconstruction from LDR to HDR in a feedback network is learned over multiple iterations, enabling a coarse-to-fine representation and improved reconstruction at every iteration. Our proposed end-to-end feedback network- FHDR, which includes a dense feedback block, offers early reconstruction ability and better reconstruction quality with fewer network parameters. Our approach was found to be superior to the state-of-the-art methods in both qualitative and quantitative evaluations.",1
"In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at https://github.com/mrlibw/ControlGAN.",0
"The proposed ControlGAN in this study is a new type of text-to-image generative adversarial network that has the ability to produce high-quality images and control the image generation process based on natural language descriptions. This is achieved by introducing a word-level spatial and channel-wise attention-driven generator that can separate various visual attributes and focus on generating and manipulating subregions related to the most significant words. Additionally, a word-level discriminator is suggested to provide detailed feedback by correlating words with image regions, enabling the generator to manipulate specific visual attributes without affecting other content. The use of perceptual loss helps to minimize randomness in image generation and encourages the generator to manipulate specific attributes required in the modified text. The experimental results on benchmark datasets demonstrate that this method outperforms existing methods and can efficiently manipulate synthetic images using natural language descriptions. The code for this study is available at https://github.com/mrlibw/ControlGAN.",1
"In this work, we propose a modeling technique for jointly training image and video generation models by simultaneously learning to map latent variables with a fixed prior onto real images and interpolate over images to generate videos. The proposed approach models the variations in representations using residual vectors encoding the change at each time step over a summary vector for the entire video. We utilize the technique to jointly train an image generation model with a fixed prior along with a video generation model lacking constraints such as disentanglement. The joint training enables the image generator to exploit temporal information while the video generation model learns to flexibly share information across frames. Moreover, experimental results verify our approach's compatibility with pre-training on videos or images and training on datasets containing a mixture of both. A comprehensive set of quantitative and qualitative evaluations reveal the improvements in sample quality and diversity over both video generation and image generation baselines. We further demonstrate the technique's capabilities of exploiting similarity in features across frames by applying it to a model based on decomposing the video into motion and content. The proposed model allows minor variations in content across frames while maintaining the temporal dependence through latent vectors encoding the pose or motion features.",0
"Our work introduces a modeling technique that trains image and video generation models together. We achieve this by simultaneously learning to map latent variables with a fixed prior to real images and using interpolation to generate videos. We use residual vectors to encode changes at each time step, creating a summary vector for the entire video. Our approach allows us to jointly train an image generation model with a fixed prior, and a video generation model that lacks constraints like disentanglement. This joint training allows the image generator to use temporal information, while the video generation model can share information across frames. Our evaluations show that our approach improves sample quality and diversity over both video generation and image generation baselines. We also show that our technique can exploit the similarity in features across frames by applying it to a model based on decomposing videos into motion and content. This proposed model allows for minor variations in content across frames while maintaining temporal dependence through latent vectors encoding pose or motion features. Our experimental results confirm our approach's compatibility with pre-training on videos or images and training on datasets containing a mixture of both.",1
"Over the past few years many research efforts have been devoted to the field of affect analysis. Various approaches have been proposed for: i) discrete emotion recognition in terms of the primary facial expressions; ii) emotion analysis in terms of facial Action Units (AUs), assuming a fixed expression intensity; iii) dimensional emotion analysis, in terms of valence and arousal (VA). These approaches can only be effective, if they are developed using large, appropriately annotated databases, showing behaviors of people in-the-wild, i.e., in uncontrolled environments. Aff-Wild has been the first, large-scale, in-the-wild database (including around 1,200,000 frames of 300 videos), annotated in terms of VA. In the vast majority of existing emotion databases, their annotation is limited to either primary expressions, or valence-arousal, or action units. In this paper, we first annotate a part (around $234,000$ frames) of the Aff-Wild database in terms of $8$ AUs and another part (around $288,000$ frames) in terms of the $7$ basic emotion categories, so that parts of this database are annotated in terms of VA, as well as AUs, or primary expressions. Then, we set up and tackle multi-task learning for emotion recognition, as well as for facial image generation. Multi-task learning is performed using: i) a deep neural network with shared hidden layers, which learns emotional attributes by exploiting their inter-dependencies; ii) a discriminator of a generative adversarial network (GAN). On the other hand, image generation is implemented through the generator of the GAN. For these two tasks, we carefully design loss functions that fit the examined set-up. Experiments are presented which illustrate the good performance of the proposed approach when applied to the new annotated parts of the Aff-Wild database.",0
"In recent years, much research has been conducted on affect analysis. Different approaches have been suggested for identifying emotions, including discrete emotion recognition based on facial expressions, emotion analysis based on fixed expression intensity, and dimensional emotion analysis based on valence and arousal. However, these approaches can only be effective if they are developed using large, appropriately annotated databases of behaviors in uncontrolled environments. The Aff-Wild database is the first large-scale, in-the-wild database with annotations in terms of valence and arousal. In this paper, we annotate parts of the Aff-Wild database in terms of action units and basic emotion categories. We then use multi-task learning with a deep neural network and a generative adversarial network to perform emotion recognition and image generation. We also design loss functions for the two tasks and present experimental results demonstrating the effectiveness of our approach.",1
"Generative Adversarial Networks (GANs) were proposed in 2014 by Goodfellow et al., and have since been extended into multiple computer vision applications. This report provides a thorough survey of recent GAN research, outlining the various architectures and applications, as well as methods for training GANs and dealing with latent space. This is followed by a discussion of potential areas for future GAN research, including: evaluating GANs, better understanding GANs, and techniques for training GANs. The second part of this report outlines the compilation of a dataset of images `in the wild' representing each of the 7 basic human emotions, and analyses experiments done when training a StarGAN on this dataset combined with the FER2013 dataset.",0
"In 2014, Goodfellow et al. introduced Generative Adversarial Networks (GANs) which have now been applied to various computer vision tasks. A comprehensive overview of recent GAN research is presented in this report, including discussions on different architectures, applications, and training methods for GANs. Additionally, potential areas for future research such as evaluating and understanding GANs, as well as techniques for training them, are outlined. The second part of the report describes the creation of a dataset of images depicting the seven basic human emotions and the analysis of experiments conducted using a StarGAN trained on this dataset and the FER2013 dataset.",1
"Deep generative neural networks (DGNNs) have achieved realistic and high-quality data generation. In particular, the adversarial training scheme has been applied to many DGNNs and has exhibited powerful performance. Despite of recent advances in generative networks, identifying the image generation mechanism still remains challenging. In this paper, we present an explorative sampling algorithm to analyze generation mechanism of DGNNs. Our method efficiently obtains samples with identical attributes from a query image in a perspective of the trained model. We define generative boundaries which determine the activation of nodes in the internal layer and probe inside the model with this information. To handle a large number of boundaries, we obtain the essential set of boundaries using optimization. By gathering samples within the region surrounded by generative boundaries, we can empirically reveal the characteristics of the internal layers of DGNNs. We also demonstrate that our algorithm can find more homogeneous, the model specific samples compared to the variations of {\epsilon}-based sampling method.",0
"Realistic and high-quality data generation has been achieved by deep generative neural networks (DGNNs), with the adversarial training scheme exhibiting powerful performance when applied to many DGNNs. Despite recent progress, identifying the image generation mechanism remains a challenge. This paper presents an explorative sampling algorithm for analyzing the generation mechanism of DGNNs. The method efficiently obtains samples with identical attributes as a query image from the perspective of the trained model. Generative boundaries are defined to determine node activation in the internal layer, and optimization is used to obtain the essential set of boundaries to handle a large number. By gathering samples within the region surrounded by generative boundaries, the characteristics of the internal layers of DGNNs can be empirically revealed. The algorithm is also shown to find more homogeneous, model-specific samples compared to variations of the epsilon-based sampling method.",1
"Continuous Normalizing Flows (CNFs) have emerged as promising deep generative models for a wide range of tasks thanks to their invertibility and exact likelihood estimation. However, conditioning CNFs on signals of interest for conditional image generation and downstream predictive tasks is inefficient due to the high-dimensional latent code generated by the model, which needs to be of the same size as the input data. In this paper, we propose InfoCNF, an efficient conditional CNF that partitions the latent space into a class-specific supervised code and an unsupervised code that shared among all classes for efficient use of labeled information. Since the partitioning strategy (slightly) increases the number of function evaluations (NFEs), InfoCNF also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. We show empirically that InfoCNF improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs on CIFAR10. Furthermore, applying the same partitioning strategy in InfoCNF on time-series data helps improve extrapolation performance.",0
"Continuous Normalizing Flows (CNFs) are a type of deep generative model that have gained popularity due to their exact likelihood estimation and invertibility, making them useful for various tasks. However, when it comes to conditional image generation and downstream predictive tasks, conditioning CNFs on signals of interest can be inefficient due to the high-dimensional latent code that needs to be the same size as the input data. To address this issue, we introduce InfoCNF, a conditional CNF that divides the latent space into a supervised code specific to each class and an unsupervised code shared among all classes. This strategy improves the utilization of labeled information while also employing gating networks to learn the error tolerances of its ODE solvers for better speed and performance. Our experiments on CIFAR10 demonstrate that InfoCNF enhances test accuracy, reduces NFEs, and maintains comparable likelihood scores compared to the baseline. Moreover, we apply the same partitioning strategy to time-series data, resulting in improved extrapolation performance.",1
"In this paper, we investigate the problem of learning disentangled representations. Given a pair of images sharing some attributes, we aim to create a low-dimensional representation which is split into two parts: a shared representation that captures the common information between the images and an exclusive representation that contains the specific information of each image. To address this issue, we propose a model based on mutual information estimation without relying on image reconstruction or image generation. Mutual information maximization is performed to capture the attributes of data in the shared and exclusive representations while we minimize the mutual information between the shared and exclusive representation to enforce representation disentanglement. We show that these representations are useful to perform downstream tasks such as image classification and image retrieval based on the shared or exclusive component. Moreover, classification results show that our model outperforms the state-of-the-art model based on VAE/GAN approaches in representation disentanglement.",0
"The focus of this research paper is the exploration of disentangled representations. Our goal is to create a low-dimensional representation consisting of two parts: a shared representation that captures common information and an exclusive representation that contains unique information for each image pair. We have developed a model that relies on mutual information estimation instead of image reconstruction or generation. By maximizing mutual information, we can capture attributes in shared and exclusive representations, while minimizing mutual information between the two to ensure representation disentanglement. Our findings demonstrate that these representations are effective for downstream tasks such as image classification and retrieval based on the shared or exclusive component. Furthermore, our model surpassed the state-of-the-art VAE/GAN-based model in terms of representation disentanglement, as demonstrated by classification results.",1
"Supervised training of an automated medical image analysis system often requires a large amount of expert annotations that are hard to collect. Moreover, the proportions of data available across different classes may be highly imbalanced for rare diseases. To mitigate these issues, we investigate a novel data augmentation pipeline that selectively adds new synthetic images generated by conditional Adversarial Networks (cGANs), rather than extending directly the training set with synthetic images. The selection mechanisms that we introduce to the synthetic augmentation pipeline are motivated by the observation that, although cGAN-generated images can be visually appealing, they are not guaranteed to contain essential features for classification performance improvement. By selecting synthetic images based on the confidence of their assigned labels and their feature similarity to real labeled images, our framework provides quality assurance to synthetic augmentation by ensuring that adding the selected synthetic images to the training set will improve performance. We evaluate our model on a medical histopathology dataset, and two natural image classification benchmarks, CIFAR10 and SVHN. Results on these datasets show significant and consistent improvements in classification performance (with 6.8%, 3.9%, 1.6% higher accuracy, respectively) by leveraging cGAN generated images with selective augmentation.",0
"Collecting a large number of expert annotations for supervised training of an automated medical image analysis system can be challenging. Additionally, rare diseases may have imbalanced data availability across different classes. To address these issues, we propose a unique data augmentation pipeline that selectively adds new synthetic images generated by conditional Adversarial Networks (cGANs) instead of directly expanding the training set with synthetic images. Our synthetic augmentation pipeline incorporates selection mechanisms based on the confidence of the assigned labels and feature similarity to real labeled images, as visually appealing cGAN-generated images may not necessarily contain essential features for classification performance improvement. Our framework ensures quality assurance of synthetic augmentation and guarantees performance improvement by adding selected synthetic images to the training set. We evaluate our model on a medical histopathology dataset, CIFAR10, and SVHN. Our results show significant and consistent improvements in classification accuracy (6.8%, 3.9%, 1.6% higher accuracy, respectively) by leveraging cGAN-generated images with selective augmentation.",1
"We propose an unsupervised multi-conditional image generation pipeline: cFineGAN, that can generate an image conditioned on two input images such that the generated image preserves the texture of one and the shape of the other input. To achieve this goal, we extend upon the recently proposed work of FineGAN \citep{singh2018finegan} and make use of standard as well as shape-biased pre-trained ImageNet models. We demonstrate both qualitatively as well as quantitatively the benefit of using the shape-biased network. We present our image generation result across three benchmark datasets- CUB-200-2011, Stanford Dogs and UT Zappos50k.",0
"Our proposal is cFineGAN, an unsupervised image generation pipeline that can create images based on two input images. This approach preserves the texture of one image and the shape of the other. We build upon FineGAN \citep{singh2018finegan} and incorporate standard and shape-biased pre-trained ImageNet models to achieve our goal. We demonstrate the advantages of using the shape-biased network through both qualitative and quantitative analysis. Our image generation results are tested on three benchmark datasets: CUB-200-2011, Stanford Dogs, and UT Zappos50k.",1
"Cutting and pasting image segments feels intuitive: the choice of source templates gives artists flexibility in recombining existing source material. Formally, this process takes an image set as input and outputs a collage of the set elements. Such selection from sets of source templates does not fit easily in classical convolutional neural models requiring inputs of fixed size. Inspired by advances in attention and set-input machine learning, we present a novel architecture that can generate in one forward pass image collages of source templates using set-structured representations. This paper has the following contributions: (i) a novel framework for image generation called Memory Attentive Generation of Image Collages (MAGIC) which gives artists new ways to create digital collages; (ii) from the machine-learning perspective, we show a novel Generative Adversarial Networks (GAN) architecture that uses Set-Transformer layers and set-pooling to blend sets of random image samples - a hybrid non-parametric approach.",0
"The process of cutting and pasting image segments is easy to understand and allows artists to combine different source materials in a flexible manner. However, this process cannot be easily integrated into traditional convolutional neural models which require fixed-sized inputs. To address this issue, our paper presents a new architecture inspired by recent advancements in attention and set-input machine learning. This new framework, called Memory Attentive Generation of Image Collages (MAGIC), allows for the generation of image collages using set-structured representations. Our paper introduces two main contributions: (i) MAGIC, a novel framework for image generation which provides artists with new ways to create digital collages, and (ii) a new Generative Adversarial Networks (GAN) architecture that utilizes Set-Transformer layers and set-pooling to blend random image samples - a hybrid non-parametric approach.",1
"Despite some exciting progress on high-quality image generation from structured(scene graphs) or free-form(sentences) descriptions, most of them only guarantee the image-level semantical consistency, i.e. the generated image matching the semantic meaning of the description. They still lack the investigations on synthesizing the images in a more controllable way, like finely manipulating the visual appearance of every object. Therefore, to generate the images with preferred objects and rich interactions, we propose a semi-parametric method, PasteGAN, for generating the image from the scene graph and the image crops, where spatial arrangements of the objects and their pair-wise relationships are defined by the scene graph and the object appearances are determined by the given object crops. To enhance the interactions of the objects in the output, we design a Crop Refining Network and an Object-Image Fuser to embed the objects as well as their relationships into one map. Multiple losses work collaboratively to guarantee the generated images highly respecting the crops and complying with the scene graphs while maintaining excellent image quality. A crop selector is also proposed to pick the most-compatible crops from our external object tank by encoding the interactions around the objects in the scene graph if the crops are not provided. Evaluated on Visual Genome and COCO-Stuff dataset, our proposed method significantly outperforms the SOTA methods on Inception Score, Diversity Score and Fr\'echet Inception Distance. Extensive experiments also demonstrate our method's ability to generate complex and diverse images with given objects.",0
"Although progress has been made in generating high-quality images from structured (scene graphs) or free-form (sentences) descriptions, most methods only ensure semantic consistency at the image level. They lack the ability to precisely manipulate the visual appearance of individual objects, limiting their ability to generate images with preferred objects and rich interactions. To address this issue, we propose PasteGAN, a semi-parametric method that generates images from scene graphs and image crops, allowing for fine control over object appearance and spatial arrangement. We also introduce a Crop Refining Network and Object-Image Fuser to enhance object interactions and ensure image quality. Our method outperforms state-of-the-art approaches on Inception Score, Diversity Score, and Fr\'echet Inception Distance, demonstrating its ability to generate complex and diverse images with given objects.",1
"Here, we explore the low-level statistics of images generated by state-of-the-art deep generative models. First, Variational auto-encoder (VAE~\cite{kingma2013auto}), Wasserstein generative adversarial network (WGAN~\cite{arjovsky2017wasserstein}) and deep convolutional generative adversarial network (DCGAN~\cite{radford2015unsupervised}) are trained on the ImageNet dataset and a large set of cartoon frames from animations. Then, for images generated by these models as well as natural scenes and cartoons, statistics including mean power spectrum, the number of connected components in a given image area, distribution of random filter responses, and contrast distribution are computed. Our analyses on training images support current findings on scale invariance, non-Gaussianity, and Weibull contrast distribution of natural scenes. We find that although similar results hold over cartoon images, there is still a significant difference between statistics of natural scenes and images generated by VAE, DCGAN and WGAN models. In particular, generated images do not have scale invariant mean power spectrum magnitude, which indicates existence of extra structures in these images. Inspecting how well the statistics of deep generated images match the known statistical properties of natural images, such as scale invariance, non-Gaussianity, and Weibull contrast distribution, can a) reveal the degree to which deep learning models capture the essence of the natural scenes, b) provide a new dimension to evaluate models, and c) allow possible improvement of image generative models (e.g., via defining new loss functions).",0
"The aim of this study is to investigate the basic characteristics of images produced by advanced deep generative models. The research involves the training of three models, namely Variational auto-encoder (VAE~\cite{kingma2013auto}), Wasserstein generative adversarial network (WGAN~\cite{arjovsky2017wasserstein}), and deep convolutional generative adversarial network (DCGAN~\cite{radford2015unsupervised}), using the ImageNet dataset and a vast collection of animated cartoon frames. The generated images, as well as natural and cartoon scenes, are analyzed for various statistical features such as mean power spectrum, number of connected components, distribution of random filter responses, and contrast distribution. Our investigation of training images confirms existing research on the size invariance, non-Gaussianity, and Weibull contrast distribution of natural scenes. However, while cartoon images exhibit similar results, the statistics of the images produced by VAE, DCGAN and WGAN models differ significantly from natural scenes. Notably, the generated images lack scale invariant mean power spectrum magnitude, indicating the presence of additional structures in these images. By comparing the statistical properties of the deep generated images to those of natural images, we can assess the extent to which deep learning models capture the essence of natural scenes, evaluate the models from a new perspective, and potentially enhance image generative models by introducing new loss functions.",1
"This work addresses a new problem that learns generative adversarial networks (GANs) from multiple data collections that are each i) owned separately by different clients and ii) drawn from a non-identical distribution that comprises different classes. Given such non-iid data as input, we aim to learn a distribution involving all the classes input data can belong to, while keeping the data decentralized in each client storage. Our key contribution to this end is a new decentralized approach for learning GANs from non-iid data called Forgiver-First Update (F2U), which a) asks clients to train an individual discriminator with their own data and b) updates a generator to fool the most `forgiving' discriminators who deem generated samples as the most real. Our theoretical analysis proves that this updating strategy allows the decentralized GAN to achieve a generator's distribution with all the input classes as its global optimum based on f-divergence minimization. Moreover, we propose a relaxed version of F2U called Forgiver-First Aggregation (F2A) that performs well in practice, which adaptively aggregates the discriminators while emphasizing forgiving ones. Our empirical evaluations with image generation tasks demonstrated the effectiveness of our approach over state-of-the-art decentralized learning methods.",0
"This study focuses on the challenge of training generative adversarial networks (GANs) using non-identical data collections owned by different clients. Our objective is to create a distribution that includes all the classes of input data while maintaining decentralization in each client's storage. To achieve this, we introduce a new decentralized approach called Forgiver-First Update (F2U), which involves training individual discriminators with each client's data and updating the generator to deceive the most forgiving discriminators. We prove that this approach allows the decentralized GAN to achieve a generator's distribution with all input classes as its global optimum. We also propose Forgiver-First Aggregation (F2A), a relaxed version of F2U that performs well in practice by adaptively aggregating discriminators while emphasizing forgiving ones. Our empirical results demonstrate the effectiveness of our approach over state-of-the-art decentralized learning methods in image generation tasks.",1
"Most existing dehazing algorithms often use hand-crafted features or Convolutional Neural Networks (CNN)-based methods to generate clear images using pixel-level Mean Square Error (MSE) loss. The generated images generally have better visual appeal, but not always have better performance for high-level vision tasks, e.g. image classification. In this paper, we investigate a new point of view in addressing this problem. Instead of focusing only on achieving good quantitative performance on pixel-based metrics such as Peak Signal to Noise Ratio (PSNR), we also ensure that the dehazed image itself does not degrade the performance of the high-level vision tasks such as image classification. To this end, we present an unified CNN architecture that includes three parts: a dehazing sub-network (DNet), a classification-driven Conditional Generative Adversarial Networks sub-network (CCGAN) and a classification sub-network (CNet) related to image classification, which has better performance both on visual appeal and image classification. We conduct comprehensive experiments on two challenging benchmark datasets for fine-grained and object classification: CUB-200-2011 and Caltech-256. Experimental results demonstrate that the proposed method outperforms many recent state-of-the-art single image dehazing methods in terms of image dehazing metrics and classification accuracy.",0
"A lot of current dehazing algorithms rely on either hand-crafted features or Convolutional Neural Networks (CNN) to produce clear images with a low pixel-level Mean Square Error (MSE) loss. While the images created with these methods look better, they don't always perform better for high-level vision tasks like image classification. In this study, we take a new approach to this issue. Instead of only worrying about achieving good quantitative results on pixel-based metrics such as Peak Signal to Noise Ratio (PSNR), we also make sure that the dehazed image doesn't diminish the performance of high-level vision tasks like image classification. We propose a unified CNN architecture that has three components: a dehazing sub-network (DNet), a classification-driven Conditional Generative Adversarial Networks sub-network (CCGAN), and a classification sub-network (CNet) related to image classification. This architecture performs better both visually and in terms of image classification. We conduct comprehensive experiments on two challenging benchmark datasets for fine-grained and object classification: CUB-200-2011 and Caltech-256. Our experimental results show that our method outperforms many recent state-of-the-art single image dehazing methods in terms of image dehazing metrics and classification accuracy.",1
"Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.",0
"The effectiveness and efficiency of deep hashing methods for large-scale Web media search have been established. However, the success of these data-driven methods is often limited by the availability of labeled data. To overcome this issue, current solutions utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generation and hashing learning as separate processes, leading to generation ineffectiveness. Additionally, most methods fail to utilize the semantic information in unlabeled data. In this paper, we propose a novel method called Semi-supervised Self-pace Adversarial Hashing (SSAH) to address these issues in a single framework. Our method includes an adversarial network (A-Net) and a hashing network (H-Net). The A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. We also introduce a self-paced hard generation policy to gradually increase the hashing difficulty of generated samples and a semi-supervised consistent loss to utilize the semantic information in unlabeled data. Our experimental results show that our method outperforms state-of-the-art models on widely-used hashing datasets and fine-grained datasets.",1
"Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is a primary cause of blindness in working-age people and it is estimated that 3 to 4 million people with diabetes are blinded by DR every year worldwide. Early diagnosis have been considered an effective way to mitigate such problem. The ultimate goal of our research is to develop novel machine learning techniques to analyze the DR images generated by the fundus camera for automatically DR diagnosis. In this paper, we focus on identifying small lesions on DR fundus images. The results from our analysis, which include the lesion category and their exact locations in the image, can be used to facilitate the determination of DR severity (indicated by DR stages). Different from traditional object detection for natural images, lesion detection for fundus images have unique challenges. Specifically, the size of a lesion instance is usually very small, compared with the original resolution of the fundus images, making them diffcult to be detected. We analyze the lesion-vs-image scale carefully and propose a large-size feature pyramid network (LFPN) to preserve more image details for mini lesion instance detection. Our method includes an effective region proposal strategy to increase the sensitivity. The experimental results show that our proposed method is superior to the original feature pyramid network (FPN) method and Faster RCNN.",0
"DR is a diabetes-related complication that impacts the eyes and is a leading cause of blindness in working-age individuals. Globally, it is estimated that DR causes 3 to 4 million people with diabetes to go blind annually. Early detection is crucial in mitigating this issue. Our research aims to create innovative machine learning techniques that can automatically diagnose DR by analyzing the DR images taken by the fundus camera. We focus on identifying small lesions on DR fundus images, which can facilitate the determination of DR severity. Lesion detection for fundus images presents unique challenges as the size of the lesion instance is typically very small compared to the original resolution of the fundus image, making them difficult to detect. Our method analyzes the lesion-to-image scale and proposes a large-size feature pyramid network (LFPN) to preserve more image details for mini lesion instance detection. We also incorporate an effective region proposal strategy to increase sensitivity. Experimental results reveal that our proposed method outperforms the original feature pyramid network (FPN) method and Faster RCNN.",1
"One-shot fine-grained visual recognition often suffers from the problem of training data scarcity for new fine-grained classes. To alleviate this problem, an off-the-shelf image generator can be applied to synthesize additional training images, but these synthesized images are often not helpful for actually improving the accuracy of one-shot fine-grained recognition. This paper proposes a meta-learning framework to combine generated images with original images, so that the resulting ``hybrid'' training images can improve one-shot learning. Specifically, the generic image generator is updated by a few training instances of novel classes, and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner, and our experiments demonstrate consistent improvement over baselines on one-shot fine-grained image classification benchmarks.",0
"The problem with one-shot fine-grained visual recognition is the limited availability of training data for new fine-grained classes. Although an off-the-shelf image generator can be used to create additional training images, these images do not necessarily enhance the accuracy of one-shot fine-grained recognition. This study suggests a meta-learning framework that combines original and generated images to create hybrid training images that can improve one-shot learning. The model uses a Meta Image Reinforcing Network (MetaIRNet) to update the generic image generator with a few training instances of novel classes. The model is trained end-to-end and consistently outperforms baselines on one-shot fine-grained image classification benchmarks.",1
"While Visual Question Answering (VQA) models continue to push the state-of-the-art forward, they largely remain black-boxes - failing to provide insight into how or why an answer is generated. In this ongoing work, we propose addressing this shortcoming by learning to generate counterfactual images for a VQA model - i.e. given a question-image pair, we wish to generate a new image such that i) the VQA model outputs a different answer, ii) the new image is minimally different from the original, and iii) the new image is realistic. Our hope is that providing such counterfactual examples allows users to investigate and understand the VQA model's internal mechanisms.",0
"Despite the advancements of Visual Question Answering (VQA) models, they still lack transparency and fail to explain the rationale behind their generated answers. To overcome this limitation, we propose an ongoing approach to address this issue by teaching the VQA model to produce counterfactual images. Essentially, we aim to generate a new image from a given question-image pair that satisfies three conditions: a different VQA answer, minimal difference from the original image, and realism. Our goal with this approach is to provide users with counterfactual examples that enable them to explore and comprehend the internal workings of the VQA model.",1
"The significant progress on Generative Adversarial Networks (GANs) have made it possible to generate surprisingly realistic images for single object based on natural language descriptions. However, controlled generation of images for multiple entities with explicit interactions is still difficult to achieve due to the scene layout generation heavily suffer from the diversity object scaling and spatial locations. In this paper, we proposed a novel framework for generating realistic image layout from textual scene graphs. In our framework, a spatial constraint module is designed to fit reasonable scaling and spatial layout of object pairs with considering relationship between them. Moreover, a contextual fusion module is introduced for fusing pair-wise spatial information in terms of object dependency in scene graph. By using these two modules, our proposed framework tends to generate more commonsense layout which is helpful for realistic image generation. Experimental results including quantitative results, qualitative results and user studies on two different scene graph datasets demonstrate our proposed framework's ability to generate complex and logical layout with multiple objects from scene graph.",0
"Generative Adversarial Networks (GANs) have made remarkable advancements in generating surprisingly realistic images for a single object based on natural language descriptions. However, generating images for multiple entities with explicit interactions is still challenging due to the diversity of object scaling and spatial locations in scene layout generation. In this paper, we introduce a novel framework for generating realistic image layout from textual scene graphs. Our framework includes a spatial constraint module designed to fit object pairs' reasonable scaling and spatial layout while considering their relationship. Additionally, a contextual fusion module is introduced to fuse pair-wise spatial information based on object dependency in the scene graph. With these modules, our proposed framework generates more commonsense layouts, enhancing realistic image generation. Our experimental results, including quantitative and qualitative results and user studies on two different scene graph datasets, demonstrate our framework's ability to generate complex and logical layouts with multiple objects from a scene graph.",1
"Generating a virtual try-on image from in-shop clothing images and a model person's snapshot is a challenging task because the human body and clothes have high flexibility in their shapes. In this paper, we develop a Virtual Try-on Generative Adversarial Network (VITON-GAN), that generates virtual try-on images using images of in-shop clothing and a model person. This method enhances the quality of the generated image when occlusion is present in a model person's image (e.g., arms crossed in front of the clothes) by adding an adversarial mechanism in the training pipeline.",0
"The creation of a virtual try-on image using in-store clothing images and a model's picture is a complex undertaking due to the flexibility of both the human form and clothing shapes. To address this challenge, we present the Virtual Try-on Generative Adversarial Network (VITON-GAN) in this study. Our approach employs in-store clothing images and a model's picture to generate virtual try-on images. Additionally, we incorporate an adversarial mechanism into the training process to improve the quality of the generated image when the model's picture contains occlusions such as crossed arms in front of the clothes.",1
"Generative Adversarial Networks (GANs) have become a very popular tool for implicitly learning high-dimensional probability distributions. Several improvements have been made to the original GAN formulation to address some of its shortcomings like mode collapse, convergence issues, entanglement, poor visual quality etc. While a significant effort has been directed towards improving the visual quality of images generated by GANs, it is rather surprising that objective image quality metrics have neither been employed as cost functions nor as regularizers in GAN objective functions. In this work, we show how a distance metric that is a variant of the Structural SIMilarity (SSIM) index (a popular full-reference image quality assessment algorithm), and a novel quality aware discriminator gradient penalty function that is inspired by the Natural Image Quality Evaluator (NIQE, a popular no-reference image quality assessment algorithm) can each be used as excellent regularizers for GAN objective functions. Specifically, we demonstrate state-of-the-art performance using the Wasserstein GAN gradient penalty (WGAN-GP) framework over CIFAR-10, STL10 and CelebA datasets.",0
"GANs have gained popularity as a tool for implicitly learning high-dimensional probability distributions, but have been criticized for shortcomings such as mode collapse, convergence issues, entanglement, and poor visual quality. Despite efforts to improve visual quality, objective image quality metrics have not been utilized as cost functions or regularizers in GAN objective functions. In this study, we propose using a distance metric based on the Structural SIMilarity index and a quality-aware discriminator gradient penalty function inspired by the Natural Image Quality Evaluator as regularizers. We demonstrate the effectiveness of these regularizers in improving the performance of the Wasserstein GAN gradient penalty framework on CIFAR-10, STL10, and CelebA datasets.",1
"This work introduces a novel system for the generation of images that contain multiple classes of objects. Recent work in Generative Adversarial Networks have produced high quality images, but many focus on generating images of a single object or set of objects. Our system addresses the task of image generation conditioned on a list of desired classes to be included in a single image. This enables our system to generate images with any given combination of objects, all composed into a visually realistic natural image. The system learns the interrelationships of all classes represented in a dataset, and can generate diverse samples including a set of these classes. It displays the ability to arrange these objects together, accounting for occlusions and inter-object spatial relations that characterize complex natural images. To accomplish this, we introduce a novel architecture based on Conditional Deep Convolutional GANs that is stabilized against collapse relative to both mode and condition. The system learns to rectify mode collapse during training, self-correcting to avoid suboptimal generation modes.",0
"This article presents a new approach to creating images that feature multiple categories of objects. While previous studies on Generative Adversarial Networks have produced high-quality images, their focus has been on generating images of single or specific objects. Our system, however, tackles the challenge of generating images that are conditioned on a list of desired categories to be included in a single image. As a result, our system can generate visually realistic images that feature any combination of objects. The system is designed to learn the relationships between all categories represented in a dataset and can generate diverse samples that include a selection of these categories. It can also arrange the objects in a way that accounts for occlusions and spatial relations that are typical of complex natural images. To achieve this, we introduce a new architecture based on Conditional Deep Convolutional GANs that is stable against collapse relative to both mode and condition. Our system is capable of rectifying mode collapse during training, ensuring that it avoids generating suboptimal modes.",1
"We study the problem of multimodal generative modelling of images based on generative adversarial networks (GANs). Despite the success of existing methods, they often ignore the underlying structure of vision data or its multimodal generation characteristics. To address this problem, we introduce the Dirichlet prior for multimodal image generation, which leads to a new Latent Dirichlet Allocation based GAN (LDAGAN). In detail, for the generative process modelling, LDAGAN defines a generative mode for each sample, determining which generative sub-process it belongs to. For the adversarial training, LDAGAN derives a variational expectation-maximization (VEM) algorithm to estimate model parameters. Experimental results on real-world datasets have demonstrated the outstanding performance of LDAGAN over other existing GANs.",0
"Our focus is on multimodal generative modelling of images using generative adversarial networks (GANs). However, current methods often neglect the underlying structure of vision data and its ability to generate multiple modes. To address this, we propose the Dirichlet prior, which forms the basis of our new Latent Dirichlet Allocation based GAN (LDAGAN). LDAGAN assigns a generative mode to each sample, indicating its sub-process. We use a variational expectation-maximization (VEM) algorithm to estimate model parameters during adversarial training. Our experiments on real-world datasets demonstrate LDAGAN's superior performance compared to other existing GANs.",1
"Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key to effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA), an algorithm for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",0
"The emergence of neural backdoor attacks is a significant security threat to deep learning, and current defense methods have limited capabilities, especially for complex backdoor triggers. To address this issue, we explore the space formed by the pixel values of all possible backdoor triggers. An attacker's original trigger used to create the backdoored model represents only one point in the space, which is then generalized into a distribution of valid triggers that can influence the backdoored model. Previous methods that model only one point of the trigger distribution are insufficient. Therefore, obtaining the entire trigger distribution, such as through generative modeling, is crucial for effective defense. However, existing generative modeling techniques for image generation are not applicable in the backdoor scenario because the trigger distribution is completely unknown. To solve this problem, we introduce the max-entropy staircase approximator (MESA), an algorithm for high-dimensional sampling-free generative modeling, and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on the Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",1
"Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",0
"Recently, adversarially trained generative models (GANs) have produced impressive results in image synthesis. However, despite initial success in using GANs for unsupervised representation learning, they have been surpassed by self-supervision-based approaches. Our study highlights that advancements in image generation quality lead to significant improvements in representation learning performance. Our method, BigBiGAN, builds on the cutting-edge BigGAN model by incorporating an encoder and modifying the discriminator for representation learning. We evaluate the BigBiGAN models extensively and find that these generation-based models surpass the state of the art in unsupervised representation learning on ImageNet and unconditional image generation. Pretrained BigBiGAN models, including image generators and encoders, are accessible on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",1
"Conditional generative models enjoy remarkable progress over the past few years. One of the popular conditional models is Auxiliary Classifier GAN (AC-GAN), which generates highly discriminative images by extending the loss function of GAN with an auxiliary classifier. However, the diversity of the generated samples by AC-GAN tends to decrease as the number of classes increases, hence limiting its power on large-scale data. In this paper, we identify the source of the low diversity issue theoretically and propose a practical solution to solve the problem. We show that the auxiliary classifier in AC-GAN imposes perfect separability, which is disadvantageous when the supports of the class distributions have significant overlap. To address the issue, we propose Twin Auxiliary Classifiers Generative Adversarial Net (TAC-GAN) that further benefits from a new player that interacts with other players (the generator and the discriminator) in GAN. Theoretically, we demonstrate that TAC-GAN can effectively minimize the divergence between the generated and real-data distributions. Extensive experimental results show that our TAC-GAN can successfully replicate the true data distributions on simulated data, and significantly improves the diversity of class-conditional image generation on real datasets.",0
"Over the past few years, there has been remarkable progress in conditional generative models. One of the most popular models is the Auxiliary Classifier GAN (AC-GAN), which generates highly discriminative images by extending the loss function of GAN with an auxiliary classifier. However, as the number of classes increases, the diversity of the generated samples tends to decrease, limiting its effectiveness on large-scale data. In this paper, we address the low diversity issue theoretically and propose a practical solution. We found that the auxiliary classifier in AC-GAN imposes perfect separability, which is disadvantageous when the class distributions have significant overlap. To solve this issue, we propose the Twin Auxiliary Classifiers Generative Adversarial Net (TAC-GAN), which benefits from a new player that interacts with the generator and the discriminator in GAN. Theoretically, we show that TAC-GAN can effectively minimize the divergence between the generated and real-data distributions. Through extensive experiments, we demonstrate that our TAC-GAN can successfully replicate the true data distributions on simulated data and significantly improve the diversity of class-conditional image generation on real datasets.",1
"Neural network based models for collaborative filtering have started to gain attention recently. One branch of research is based on using deep generative models to model user preferences where variational autoencoders were shown to produce state-of-the-art results. However, there are some potentially problematic characteristics of the current variational autoencoder for CF. The first is the too simplistic prior that VAEs incorporate for learning the latent representations of user preference. The other is the model's inability to learn deeper representations with more than one hidden layer for each network. Our goal is to incorporate appropriate techniques to mitigate the aforementioned problems of variational autoencoder CF and further improve the recommendation performance. Our work is the first to apply flexible priors to collaborative filtering and show that simple priors (in original VAEs) may be too restrictive to fully model user preferences and setting a more flexible prior gives significant gains. We experiment with the VampPrior, originally proposed for image generation, to examine the effect of flexible priors in CF. We also show that VampPriors coupled with gating mechanisms outperform SOTA results including the Variational Autoencoder for Collaborative Filtering by meaningful margins on 2 popular benchmark datasets (MovieLens & Netflix).",0
"Recently, there has been increasing interest in neural network-based models for collaborative filtering. One area of research focuses on utilizing deep generative models, such as variational autoencoders, to capture user preferences and achieve superior results. However, there are some potential issues with the current variational autoencoder for collaborative filtering. Firstly, the model's prior for learning latent representations of user preference is too simplistic. Additionally, the model cannot learn deeper representations with more than one hidden layer per network. Our aim is to address these issues and enhance recommendation performance through appropriate techniques. Our work demonstrates the first application of flexible priors to collaborative filtering, revealing that simple priors are too limiting for modeling user preferences. We investigate the effectiveness of flexible VampPriors, originally designed for image generation, and demonstrate that they, coupled with gating mechanisms, outperform the state-of-the-art Variational Autoencoder for Collaborative Filtering by a significant margin on two popular benchmark datasets (MovieLens and Netflix).",1
"Generative Adversarial Networks (GANs) have proven successful for unsupervised image generation. Several works extended GANs to image inpainting by conditioning the generation with parts of the image one wants to reconstruct. However, these methods have limitations in settings where only a small subset of the image pixels is known beforehand. In this paper, we study the effectiveness of conditioning GANs by adding an explicit regularization term to enforce pixel-wise conditions when very few pixel values are provided. In addition, we also investigate the influence of this regularization term on the quality of the generated images and the satisfaction of the conditions. Conducted experiments on MNIST and FashionMNIST show evidence that this regularization term allows for controlling the trade-off between quality of the generated images and constraint satisfaction.",0
"GANs have been successful in generating images in an unsupervised manner. Some studies have expanded GANs to image inpainting by conditioning the generation with specific parts of the image for reconstruction. However, these methods are limited when only a small subset of the image pixels is known beforehand. This research examines the effectiveness of conditioning GANs by incorporating a regularization term to enforce pixel-wise conditions when only a few pixel values are provided. The study also explores how this regularization term affects the quality of the generated images and the satisfaction of the conditions. Results from experiments conducted on MNIST and FashionMNIST suggest that this regularization term enables control over the trade-off between the quality of the generated images and the satisfaction of the constraints.",1
"Image generation task has received increasing attention because of its wide application in security and entertainment. Sketch-based face generation brings more fun and better quality of image generation due to supervised interaction. However, When a sketch poorly aligned with the true face is given as input, existing supervised image-to-image translation methods often cannot generate acceptable photo-realistic face images. To address this problem, in this paper we propose Cali-Sketch, a poorly-drawn-sketch to photo-realistic-image generation method. Cali-Sketch explicitly models stroke calibration and image generation using two constituent networks: a Stroke Calibration Network (SCN), which calibrates strokes of facial features and enriches facial details while preserving the original intent features; and an Image Synthesis Network (ISN), which translates the calibrated and enriched sketches to photo-realistic face images. In this way, we manage to decouple a difficult cross-domain translation problem into two easier steps. Extensive experiments verify that the face photos generated by Cali-Sketch are both photo-realistic and faithful to the input sketches, compared with state-of-the-art methods",0
"Due to its extensive application in security and entertainment, the task of image generation has gained more attention. Supervised interaction in sketch-based face generation has resulted in better quality and more enjoyable image generation. However, when a poorly-aligned sketch is used as input, existing supervised image-to-image translation methods fail to generate realistic face images. To overcome this issue, we present Cali-Sketch, a method for generating photo-realistic images from poorly-drawn sketches. Cali-Sketch involves two constituent networks: a Stroke Calibration Network (SCN) that enriches facial details while maintaining the original intent features, and an Image Synthesis Network (ISN) that translates the calibrated and enriched sketches into photo-realistic face images. This approach simplifies a complex cross-domain translation problem into two manageable steps. Our experiments demonstrate that Cali-Sketch generates face photos that are both realistic and accurate compared to state-of-the-art techniques.",1
"Generative models often use human evaluations to measure the perceived quality of their outputs. Automated metrics are noisy indirect proxies, because they rely on heuristics or pretrained embeddings. However, up until now, direct human evaluation strategies have been ad-hoc, neither standardized nor validated. Our work establishes a gold standard human benchmark for generative realism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) able to produce separable model performances, and (4) efficient in cost and time. We introduce two variants: one that measures visual perception under adaptive time constraints to determine the threshold at which a model's outputs appear real (e.g. 250ms), and the other a less expensive variant that measures human error rate on fake and real images sans time constraints. We test HYPE across six state-of-the-art generative adversarial networks and two sampling techniques on conditional and unconditional image generation using four datasets: CelebA, FFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements across training epochs, and we confirm via bootstrap sampling that HYPE rankings are consistent and replicable.",0
"Human evaluations are often used by generative models to assess their output quality, as automated metrics are unreliable since they rely on heuristics or pre-existing embeddings. However, the direct evaluation strategies used so far have been ad-hoc and not standardized. Our study establishes a human benchmark for generative realism using Human eYe Perceptual Evaluation (HYPE), which is grounded in perception research, reliable across different model outputs, provides separable model performances, and is cost and time-efficient. HYPE has two variants: one that measures visual perception under adaptive time constraints and the other that measures human error rates without time constraints. We tested HYPE on six generative adversarial networks and two sampling techniques using four datasets. HYPE can track model improvements and its rankings are consistent and replicable, confirmed by bootstrap sampling.",1
"The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations.",0
"A crucial aspect of algorithm research is the ability to automatically assess the quality and coverage of samples generated by a generative model. Our study introduces an evaluation metric that can accurately measure both of these factors in image generation tasks by creating explicit, non-parametric representations of the manifolds of real and generated data. Through the analysis of various design variants of StyleGAN, we identify new methods that improve the current state-of-the-art. Additionally, we conduct the first principled examination of truncation techniques and discover an enhanced method. We also extend our metric to evaluate the perceptual quality of individual samples, allowing us to study latent space interpolations. Our metric's effectiveness is demonstrated in StyleGAN and BigGAN, as we provide examples where existing metrics produce uninformative or conflicting results.",1
"Histopathology images; microscopy images of stained tissue biopsies contain fundamental prognostic information that forms the foundation of pathological analysis and diagnostic medicine. However, diagnostics from histopathology images generally rely on a visual cognitive assessment of tissue slides which implies an inherent element of interpretation and hence subjectivity. Access to digitized histopathology images enabled the development of computational systems aiming at reducing manual intervention and automating parts of pathologists' workflow. Specifically, applications of deep learning to histopathology image analysis now offer opportunities for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However digitized histopathology tissue slides are unique in a variety of ways and come with their own set of computational challenges. In this survey, we summarize the different challenges facing computational systems for digital pathology and provide a review of state-of-the-art works that developed deep learning-based solutions for the predictive modeling of histopathology images from a detection, stain normalization, segmentation, and tissue classification perspective. We then discuss the challenges facing the validation and integration of such deep learning-based computational systems in clinical workflow and reflect on future opportunities for histopathology derived image measurements and better predictive modeling.",0
"Microscopy images of stained tissue biopsies, known as histopathology images, are crucial for pathological analysis and diagnostic medicine, as they provide essential prognostic information. However, the interpretation of histopathology images is subjective and relies on visual cognitive assessment. With the advent of digitized histopathology images, computational systems have been developed to automate parts of pathologists' workflow and reduce manual intervention. Deep learning applications have shown promise in improving disease prediction and patient outcome by providing better quantitative modeling of disease appearance. Nevertheless, digitized histopathology tissue slides come with their own set of computational challenges. In this survey, we review the state-of-the-art works that address these challenges from a detection, stain normalization, segmentation, and tissue classification perspective. We also examine the challenges of validating and integrating deep learning-based computational systems into clinical workflow and explore future opportunities for histopathology-derived image measurements and better predictive modeling.",1
"Stochastic-sampling-based Generative Neural Networks, such as Restricted Boltzmann Machines and Generative Adversarial Networks, are now used for applications such as denoising, image occlusion removal, pattern completion, and motion synthesis. In scenarios which involve performing such inference tasks with these models, it is critical to determine metrics that allow for model selection and/or maintenance of requisite generative performance under pre-specified implementation constraints. In this paper, we propose a new metric for evaluating generative model performance based on $p$-values derived from the combined use of Maximum Mean Discrepancy (MMD) and permutation-based (PT-based) resampling, which we refer to as PT-MMD. We demonstrate the effectiveness of this metric for two cases: (1) Selection of bitwidth and activation function complexity to achieve minimum power-at-performance for Restricted Boltzmann Machines; (2) Quantitative comparison of images generated by two types of Generative Adversarial Networks (PGAN and WGAN) to facilitate model selection in order to maximize the fidelity of generated images. For these applications, our results are shown using Euclidean and Haar-based kernels for the PT-MMD two sample hypothesis test. This demonstrates the critical role of distance functions in comparing generated images against their corresponding ground truth counterparts as what would be perceived by human users.",0
"Generative Neural Networks that use stochastic sampling, like Restricted Boltzmann Machines and Generative Adversarial Networks, are being employed in various applications ranging from image occlusion removal and denoising to pattern completion and motion synthesis. However, determining metrics that can select and maintain generative performance within specific implementation constraints is crucial when performing inference tasks using such models. This study presents a new metric called PT-MMD, which uses Maximum Mean Discrepancy (MMD) and permutation-based (PT-based) resampling to calculate $p$-values to evaluate generative model performance. The PT-MMD metric's effectiveness is demonstrated in two cases: choosing bitwidth and activation function complexity to achieve minimum power-at-performance for Restricted Boltzmann Machines, and comparing images generated by two Generative Adversarial Networks (PGAN and WGAN) to select a model that produces high-fidelity images. The results of the study are presented using Euclidean and Haar-based kernels for the PT-MMD two sample hypothesis test. The study highlights the importance of using distance functions to compare generated images against their corresponding ground truth counterparts to ensure they are perceived as accurately as possible by human users.",1
"Human pose and shape are two important components of 2D human body. However, how to efficiently represent both of them in images is still an open question. In this paper, we propose the Triplet Representation for Body (TRB) -- a compact 2D human body representation, with skeleton keypoints capturing human pose information and contour keypoints containing human shape information. TRB not only preserves the flexibility of skeleton keypoint representation, but also contains rich pose and human shape information. Therefore, it promises broader application areas, such as human shape editing and conditional image generation. We further introduce the challenging problem of TRB estimation, where joint learning of human pose and shape is required. We construct several large-scale TRB estimation datasets, based on popular 2D pose datasets: LSP, MPII, COCO. To effectively solve TRB estimation, we propose a two-branch network (TRB-net) with three novel techniques, namely X-structure (Xs), Directional Convolution (DC) and Pairwise Mapping (PM), to enforce multi-level message passing for joint feature learning. We evaluate our proposed TRB-net and several leading approaches on our proposed TRB datasets, and demonstrate the superiority of our method through extensive evaluations.",0
"The representation of human pose and shape in 2D images is an important aspect, but it remains an open question on how to do so efficiently. This paper introduces the Triplet Representation for Body (TRB), which is a compact 2D human body representation that captures both skeleton keypoints for pose information and contour keypoints for shape information. TRB offers flexibility in skeleton keypoint representation and contains rich information on human shape and pose, thereby allowing for broader application areas such as human shape editing and conditional image generation. The paper also introduces the challenging problem of TRB estimation, requiring joint learning of human pose and shape. To address this, the authors construct several large-scale TRB estimation datasets based on popular 2D pose datasets and propose a two-branch network (TRB-net) with three novel techniques to enforce multi-level message passing for joint feature learning. Extensive evaluations show the superiority of TRB-net over several leading approaches on the proposed TRB datasets.",1
"Generative models based on generative adversarial networks (GANs) and variational autoencoders (VAEs) have been widely studied in the fields of image generation, speech generation, and drug discovery, but, only a few studies have focused on the generation of inorganic materials. Such studies use the crystal structures of materials, but material researchers rarely store this information. Thus, we generate chemical compositions without using crystal information. We use a conditional VAE (CondVAE) and a conditional GAN (CondGAN) and show that CondGAN using the bag-of-atom representation with physical descriptors generates better compositions than other generative models. Also, we evaluate the effectiveness of the Metropolis-Hastings-based atomic valency modification and the extrapolation performance, which is important to material discovery.",0
"The use of generative models, specifically generative adversarial networks (GANs) and variational autoencoders (VAEs), has been extensively researched in image and speech generation as well as drug discovery. However, there has been limited research on generating inorganic materials using these models, as material researchers do not typically store crystal information. To address this, we generated chemical compositions without crystal information using a conditional VAE (CondVAE) and conditional GAN (CondGAN). Our results show that using the bag-of-atom representation with physical descriptors, CondGAN produces superior compositions compared to other generative models. We also evaluated the effectiveness of Metropolis-Hastings-based atomic valency modification and extrapolation performance, both crucial to material discovery.",1
"Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.",0
"Recent advances in deep generative models have allowed for the production of high-quality images that exhibit both fidelity and diversity. However, training these models necessitates a vast dataset. To mitigate this requirement, we present a novel approach for transferring prior knowledge from a pre-trained generator, trained with a large dataset, to a smaller dataset in a distinct domain. By incorporating common sense gleaned from the larger dataset, our model can generate images with higher quality than previous methods. We achieve this by focusing on the parameters for batch statistics, scale, and shift in the generator's hidden layers, training them in a supervised manner for stable generator training. Our results indicate that diversity in the filters obtained from the pre-trained generator is crucial for target domain performance. Our method allows for the addition of new classes or domains to a pre-trained generator without affecting its original domain performance.",1
"Despite the significant advances in recent years, Generative Adversarial Networks (GANs) are still notoriously hard to train. In this paper, we propose three novel curriculum learning strategies for training GANs. All strategies are first based on ranking the training images by their difficulty scores, which are estimated by a state-of-the-art image difficulty predictor. Our first strategy is to divide images into gradually more difficult batches. Our second strategy introduces a novel curriculum loss function for the discriminator that takes into account the difficulty scores of the real images. Our third strategy is based on sampling from an evolving distribution, which favors the easier images during the initial training stages and gradually converges to a uniform distribution, in which samples are equally likely, regardless of difficulty. We compare our curriculum learning strategies with the classic training procedure on two tasks: image generation and image translation. Our experiments indicate that all strategies provide faster convergence and superior results. For example, our best curriculum learning strategy applied on spectrally normalized GANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like images are real in 25.0% of the presented cases, while the SNGANs trained using the classic procedure fooled the annotators in only 18.4% cases. Similarly, in image translation, the human annotators preferred the images produced by the Cycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5% cases and those produced by CycleGAN based on classic training in only 19.8% cases, 39.7% cases being labeled as ties.",0
"GANs remain notoriously difficult to train, despite recent advances. This paper proposes three novel curriculum learning strategies for GAN training. These strategies rank training images by difficulty score, estimated by an advanced image difficulty predictor. The first strategy involves dividing images into gradually more difficult batches; the second introduces a novel curriculum loss function for the discriminator, considering the difficulty of real images; the third samples from an evolving distribution favoring easier images initially, gradually converging to a uniform distribution. These strategies outperform the classic training procedure on image generation and translation tasks, providing faster convergence and superior results. For example, the best curriculum strategy on SNGANs fooled human annotators in 25.0% of cases, while classic training only fooled them in 18.4%. Similarly, the CycleGAN trained with curriculum learning was preferred by annotators in 40.5% of cases, compared to 19.8% of cases for classic training, with 39.7% of cases being ties.",1
"Recent studies on the adversarial vulnerability of neural networks have shown that models trained with the objective of minimizing an upper bound on the worst-case loss over all possible adversarial perturbations improve robustness against adversarial attacks. Beside exploiting adversarial training framework, we show that by enforcing a Deep Neural Network (DNN) to be linear in transformed input and feature space improves robustness significantly. We also demonstrate that by augmenting the objective function with Local Lipschitz regularizer boost robustness of the model further. Our method outperforms most sophisticated adversarial training methods and achieves state of the art adversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also propose a novel adversarial image generation method by leveraging Inverse Representation Learning and Linearity aspect of an adversarially trained deep neural network classifier.",0
"Recent research has revealed that neural networks can be vulnerable to adversarial attacks. However, models trained to minimize the worst-case loss over all possible perturbations can increase robustness. Additionally, enforcing linearity in both the input and feature space of a Deep Neural Network (DNN) can also improve robustness. By incorporating a Local Lipschitz regularizer into the objective function, we can further enhance the model's robustness. Our proposed method surpasses existing adversarial training techniques and achieves state-of-the-art accuracy on MNIST, CIFAR10, and SVHN datasets. Furthermore, we introduce a new method for generating adversarial images that utilizes Inverse Representation Learning and the linearity aspect of an adversarially trained DNN classifier.",1
"Mammographic mass detection and segmentation are usually performed as serial and separate tasks, with segmentation often only performed on manually confirmed true positive detections in previous studies. We propose a fully-integrated computer-aided detection (CAD) system for simultaneous mammographic mass detection and segmentation without user intervention. The proposed CAD only consists of a pseudo-color image generation and a mass detection-segmentation stage based on Mask R-CNN. Grayscale mammograms are transformed into pseudo-color images based on multi-scale morphological sifting where mass-like patterns are enhanced to improve the performance of Mask R-CNN. Transfer learning with the Mask R-CNN is then adopted to simultaneously detect and segment masses on the pseudo-color images. Evaluated on the public dataset INbreast, the method outperforms the state-of-the-art methods by achieving an average true positive rate of 0.90 at 0.9 false positive per image and an average Dice similarity index of 0.88 for mass segmentation.",0
"Typically, mammographic mass detection and segmentation are done separately, with segmentation only being performed on previously confirmed true positive detections. We propose a computer-aided detection (CAD) system that integrates mass detection and segmentation. Our CAD system does not require user intervention and consists of a pseudo-color image generation and a mass detection-segmentation stage based on Mask R-CNN. We use multi-scale morphological sifting to enhance mass-like patterns in grayscale mammograms, which are then transformed into pseudo-color images to improve the performance of Mask R-CNN. We also use transfer learning with the Mask R-CNN to detect and segment masses simultaneously. Our method was evaluated on the INbreast dataset and outperformed state-of-the-art methods by achieving an average true positive rate of 0.90 at 0.9 false positive per image and an average Dice similarity index of 0.88 for mass segmentation.",1
"In this work we present an adversarial training algorithm that exploits correlations in video to learn --without supervision-- an image generator model with a disentangled latent space. The proposed methodology requires only a few modifications to the standard algorithm of Generative Adversarial Networks (GAN) and involves training with sets of frames taken from short videos. We train our model over two datasets of face-centered videos which present different people speaking or moving the head: VidTIMIT and YouTube Faces datasets. We found that our proposal allows us to split the generator latent space into two subspaces. One of them controls content attributes, those that do not change along short video sequences. For the considered datasets, this is the identity of the generated face. The other subspace controls motion attributes, those attributes that are observed to change along short videos. We observed that these motion attributes are face expressions, head orientation, lips and eyes movement. The presented experiments provide quantitative and qualitative evidence supporting that the proposed methodology induces a disentangling of this two kinds of attributes in the latent space.",0
"Our work introduces an adversarial training algorithm that takes advantage of video correlations to acquire an unsupervised image generator model featuring a disentangled latent space. Our suggested approach involves minimal modifications to the commonly used Generative Adversarial Networks (GAN) algorithm and entails training with sets of frames extracted from brief videos. We carried out our training with two different datasets, namely VidTIMIT and YouTube Faces, both comprising videos of individuals talking or moving their heads. Our findings indicate that our algorithm allows us to divide the generator latent space into two subspaces. The first subspace governs content attributes that remain constant throughout short video sequences, such as the identity of the generated face. The second subspace controls motion attributes, which include facial expressions, head orientation, and movement of the lips and eyes. Our experiments furnish both quantitative and qualitative evidence that our suggested method induces the disentanglement of these two types of attributes within the latent space.",1
"To detect GAN generated images, conventional supervised machine learning algorithms require collection of a number of real and fake images from the targeted GAN model. However, the specific model used by the attacker is often unavailable. To address this, we propose a GAN simulator, AutoGAN, which can simulate the artifacts produced by the common pipeline shared by several popular GAN models. Additionally, we identify a unique artifact caused by the up-sampling component included in the common GAN pipeline. We show theoretically such artifacts are manifested as replications of spectra in the frequency domain and thus propose a classifier model based on the spectrum input, rather than the pixel input. By using the simulated images to train a spectrum based classifier, even without seeing the fake images produced by the targeted GAN model during training, our approach achieves state-of-the-art performances on detecting fake images generated by popular GAN models such as CycleGAN.",0
"Conventional supervised machine learning algorithms require a collection of real and fake images from a targeted GAN model to detect GAN generated images. However, the specific model used by the attacker is often unavailable. To solve this problem, we suggest using a GAN simulator called AutoGAN, which can replicate the artifacts produced by the common pipeline shared by several popular GAN models. In addition, we have identified a unique artifact caused by the up-sampling component included in the common GAN pipeline. Theoretically, such artifacts are manifested as replications of spectra in the frequency domain. Therefore, we propose a classifier model based on the spectrum input rather than the pixel input. Our approach achieves state-of-the-art performances on detecting fake images generated by popular GAN models such as CycleGAN, even without seeing the fake images produced by the targeted GAN model during training, by using the simulated images to train a spectrum based classifier.",1
"Despite significant recent progress on generative models, controlled generation of images depicting multiple and complex object layouts is still a difficult problem. Among the core challenges are the diversity of appearance a given object may possess and, as a result, exponential set of images consistent with a specified layout. To address these challenges, we propose a novel approach for layout-based image generation; we call it Layout2Im. Given the coarse spatial layout (bounding boxes + object categories), our model can generate a set of realistic images which have the correct objects in the desired locations. The representation of each object is disentangled into a specified/certain part (category) and an unspecified/uncertain part (appearance). The category is encoded using a word embedding and the appearance is distilled into a low-dimensional vector sampled from a normal distribution. Individual object representations are composed together using convolutional LSTM, to obtain an encoding of the complete layout, and then decoded to an image. Several loss terms are introduced to encourage accurate and diverse generation. The proposed Layout2Im model significantly outperforms the previous state of the art, boosting the best reported inception score by 24.66% and 28.57% on the very challenging COCO-Stuff and Visual Genome datasets, respectively. Extensive experiments also demonstrate our method's ability to generate complex and diverse images with multiple objects.",0
"Generating images that depict complex object layouts remains challenging, despite progress in generative models. The difficulty lies in the vast range of appearances an object may have, resulting in an exponential number of images matching a layout. To overcome these challenges, we developed Layout2Im, a novel approach to layout-based image generation. Our model can generate realistic images based on a coarse spatial layout of object categories and bounding boxes. Each object is represented by a category and an appearance, which is encoded using a word embedding and low-dimensional vector, respectively. The object representations are combined using convolutional LSTM to create a layout encoding that is decoded into an image. We introduced several loss terms to encourage accurate and diverse generation, and our Layout2Im model outperforms the previous state of the art on the COCO-Stuff and Visual Genome datasets. Our method can generate complex and diverse images with multiple objects.",1
"In this paper, we address the problem of generating person images conditioned on both pose and appearance information. Specifically, given an image xa of a person and a target pose P(xb), extracted from a different image xb, we synthesize a new image of that person in pose P(xb), while preserving the visual details in xa. In order to deal with pixel-to-pixel misalignments caused by the pose differences between P(xa) and P(xb), we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. Quantitative and qualitative results, using common datasets and protocols recently proposed for this task, show that our approach is competitive with respect to the state of the art. Moreover, we conduct an extensive evaluation using off-the-shell person re-identification (Re-ID) systems trained with person-generation based augmented data, which is one of the main important applications for this task. Our experiments show that our Deformable GANs can significantly boost the Re-ID accuracy and are even better than data-augmentation methods specifically trained using Re-ID losses.",0
"The focus of this paper is on the production of person images through the consideration of pose and appearance information. Specifically, the aim is to generate an image of a person in a particular pose, whilst retaining the visual details of an original image of that same person. To overcome pixel-to-pixel discrepancies caused by differences in pose, the generator of our Generative Adversarial Network employs deformable skip connections. In addition, we propose a nearest-neighbour loss to better match the generated image with the target image. Our approach competes well with current state-of-the-art methods, as demonstrated through quantitative and qualitative results using standard datasets and protocols. Furthermore, we evaluate the efficacy of our Deformable GANs in enhancing person re-identification (Re-ID) accuracy through the use of person-generation based augmented data. Our results show that our approach outperforms data-augmentation methods specifically trained using Re-ID losses.",1
"This paper presents an approach to address data scarcity problems in underwater image datasets for visual detection of marine debris. The proposed approach relies on a two-stage variational autoencoder (VAE) and a binary classifier to evaluate the generated imagery for quality and realism. From the images generated by the two-stage VAE, the binary classifier selects ""good quality"" images and augments the given dataset with them. Lastly, a multi-class classifier is used to evaluate the impact of the augmentation process by measuring the accuracy of an object detector trained on combinations of real and generated trash images. Our results show that the classifier trained with the augmented data outperforms the one trained only with the real data. This approach will not only be valid for the underwater trash classification problem presented in this paper, but it will also be useful for any data-dependent task for which collecting more images is challenging or infeasible.",0
"In this paper, a solution is proposed to overcome the challenge of insufficient data in underwater image datasets used for identifying marine debris. The approach involves utilizing a two-stage variational autoencoder (VAE) and a binary classifier to assess the quality and authenticity of the generated images. The binary classifier selects high-quality images generated by the VAE and adds them to the existing dataset. The effectiveness of the augmentation process is evaluated using a multi-class classifier that measures the accuracy of an object detector trained on real and generated images of marine debris. The findings demonstrate that the classifier trained on augmented data performs better than the one trained solely on real data. This approach can be applied to any data-reliant task that faces challenges in collecting more images.",1
"Current developments in computer vision and deep learning allow to automatically generate hyper-realistic images, hardly distinguishable from real ones. In particular, human face generation achieved a stunning level of realism, opening new opportunities for the creative industry but, at the same time, new scary scenarios where such content can be maliciously misused. Therefore, it is essential to develop innovative methodologies to automatically tell apart real from computer generated multimedia, possibly able to follow the evolution and continuous improvement of data in terms of quality and realism. In the last few years, several deep learning-based solutions have been proposed for this problem, mostly based on Convolutional Neural Networks (CNNs). Although results are good in controlled conditions, it is not clear how such proposals can adapt to real-world scenarios, where learning needs to continuously evolve as new types of generated data appear. In this work, we tackle this problem by proposing an approach based on incremental learning for the detection and classification of GAN-generated images. Experiments on a dataset comprising images generated by several GAN-based architectures show that the proposed method is able to correctly perform discrimination when new GANs are presented to the network",0
"Recent advancements in computer vision and deep learning have led to the creation of hyper-realistic images that are nearly indistinguishable from real ones. This has resulted in exciting opportunities for the creative industry, but also raises concern about potential misuse of such content. To address this issue, it is crucial to develop innovative methodologies that can automatically differentiate between real and computer-generated multimedia and keep up with the continuous improvement of data quality and realism. While several deep learning-based solutions have been proposed for this problem, they are mostly limited to controlled conditions and may not be adaptable to real-world scenarios. In this study, we propose an incremental learning approach for detecting and classifying GAN-generated images. Our experiments on a dataset containing images generated by various GAN-based architectures demonstrate the effectiveness of our method in accurately discriminating between real and generated images, even when new GANs are introduced to the network.",1
"Sparse representation-based classification (SRC) has been shown to achieve a high level of accuracy in face recognition (FR). However, matching faces captured in unconstrained video against a gallery with a single reference facial still per individual typically yields low accuracy. For improved robustness to intra-class variations, SRC techniques for FR have recently been extended to incorporate variational information from an external generic set into an auxiliary dictionary. Despite their success in handling linear variations, non-linear variations (e.g., pose and expressions) between probe and reference facial images cannot be accurately reconstructed with a linear combination of images in the gallery and auxiliary dictionaries because they do not share the same type of variations. In order to account for non-linear variations due to pose, a paired sparse representation model is introduced allowing for joint use of variational information and synthetic face images. The proposed model, called synthetic plus variational model, reconstructs a probe image by jointly using (1) a variational dictionary and (2) a gallery dictionary augmented with a set of synthetic images generated over a wide diversity of pose angles. The augmented gallery dictionary is then encouraged to pair the same sparsity pattern with the variational dictionary for similar pose angles by solving a newly formulated simultaneous sparsity-based optimization problem. Experimental results obtained on Chokepoint and COX-S2V datasets, using different face representations, indicate that the proposed approach can outperform state-of-the-art SRC-based methods for still-to-video FR with a single sample per person.",0
"Face recognition accuracy using sparse representation-based classification (SRC) is high, but matching unconstrained video faces against a single reference face per individual results in low accuracy. To improve robustness, SRC techniques have incorporated variational information from an external generic set into an auxiliary dictionary. However, linear variations cannot accurately reconstruct non-linear variations (such as pose and expressions) between reference and probe facial images. To account for non-linear pose variations, a new model called synthetic plus variational model is introduced, which uses a paired sparse representation model that jointly uses variational information and synthetic face images. This model reconstructs a probe image using a variational dictionary and a gallery dictionary augmented with synthetic images generated over a wide diversity of pose angles. The proposed approach outperforms state-of-the-art SRC-based methods for still-to-video FR with a single sample per person, as shown in experimental results obtained on Chokepoint and COX-S2V datasets using different face representations.",1
"Generative adversarial networks (GANs) have demonstrated great success in generating various visual content. However, images generated by existing GANs are often of attributes (e.g., smiling expression) learned from one image domain. As a result, generating images of multiple attributes requires many real samples possessing multiple attributes which are very resource expensive to be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to learn multiple attributes from different image domains through an intersecting architecture. For example, given two image domains $X_1$ and $X_2$ with certain attributes, the intersection $X_1 \cap X_2$ denotes a new domain where images possess the attributes from both $X_1$ and $X_2$ domains. The proposed IntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish between generated and real samples of different domains, and three generators where the intersection generator is trained against both discriminators. And an overall adversarial loss function is defined over three generators. As a result, our proposed IntersectGAN can be trained on multiple domains of which each presents one specific attribute, and eventually eliminates the need of real sample images simultaneously possessing multiple attributes. By using the CelebFaces Attributes dataset, our proposed IntersectGAN is able to produce high quality face images possessing multiple attributes (e.g., a face with black hair and a smiling expression). Both qualitative and quantitative evaluations are conducted to compare our proposed IntersectGAN with other baseline methods. Besides, several different applications of IntersectGAN have been explored with promising results.",0
"GANs have been successful in generating various images, but they often only learn attributes from one image domain, making it difficult to generate images with multiple attributes. This requires many real samples with multiple attributes, which is expensive and time-consuming. To address this issue, we introduce a new GAN called IntersectGAN, which employs an intersecting architecture to learn multiple attributes from different image domains. By using two discriminators and three generators, including an intersection generator trained against both discriminators, IntersectGAN can be trained on multiple domains, each presenting a specific attribute, without the need for real samples with multiple attributes. Our experiments on the CelebFaces Attributes dataset demonstrate that IntersectGAN can produce high-quality face images with multiple attributes, such as black hair and a smiling expression. We also explore several different applications of IntersectGAN with promising results.",1
"In this paper we present, to the best of our knowledge, the first method to learn a generative model of 3D shapes from natural images in a fully unsupervised way. For example, we do not use any ground truth 3D or 2D annotations, stereo video, and ego-motion during the training. Our approach follows the general strategy of Generative Adversarial Networks, where an image generator network learns to create image samples that are realistic enough to fool a discriminator network into believing that they are natural images. In contrast, in our approach the image generation is split into 2 stages. In the first stage a generator network outputs 3D objects. In the second, a differentiable renderer produces an image of the 3D objects from random viewpoints. The key observation is that a realistic 3D object should yield a realistic rendering from any plausible viewpoint. Thus, by randomizing the choice of the viewpoint our proposed training forces the generator network to learn an interpretable 3D representation disentangled from the viewpoint. In this work, a 3D representation consists of a triangle mesh and a texture map that is used to color the triangle surface by using the UV-mapping technique. We provide analysis of our learning approach, expose its ambiguities and show how to overcome them. Experimentally, we demonstrate that our method can learn realistic 3D shapes of faces by using only the natural images of the FFHQ dataset.",0
"The paper introduces the first unsupervised method for learning a generative model of 3D shapes from natural images. This approach does not rely on any form of annotations or additional training data such as stereo video or ego-motion. The method follows the strategy of Generative Adversarial Networks where an image generator network is trained to create realistic image samples that can fool a discriminator network. However, this approach splits the image generation into two stages. In the first stage, a generator network produces 3D objects, while in the second stage, a differentiable renderer creates images of these objects from random viewpoints. The training process randomizes the viewpoint choice to force the generator network to learn an interpretable 3D representation that is disentangled from the viewpoint. The 3D representation is composed of a texture map and a triangle mesh. The paper provides an analysis of the learning approach, identifies its ambiguities, and demonstrates its effectiveness in generating realistic 3D shapes of faces using only natural images from the FFHQ dataset.",1
"We focus on explicitly learning disentangled representation for natural image generation, where the underlying spatial structure and the rendering on the structure can be independently controlled respectively, yet using no tuple supervision. The setting is significant since tuple supervision is costly and sometimes even unavailable. However, the task is highly unconstrained and thus ill-posed. To address this problem, we propose to introduce an auxiliary domain which shares a common underlying-structure space with the target domain, and we make a partially shared latent space assumption. The key idea is to encourage the partially shared latent variable to represent the similar underlying spatial structures in both domains, while the two domain-specific latent variables will be unavoidably arranged to present renderings of two domains respectively. This is achieved by designing two parallel generative networks with a common Progressive Rendering Architecture (PRA), which constrains both generative networks' behaviors to model shared underlying structure and to model spatially dependent relation between rendering and underlying structure. Thus, we propose DSRGAN (GANs for Disentangling Underlying Structure and Rendering) to instantiate our method. We also propose a quantitative criterion (the Normalized Disentanglability) to quantify disentanglability. Comparison to the state-of-the-art methods shows that DSRGAN can significantly outperform them in disentanglability.",0
"Our focus is on learning disentangled representation for the purpose of generating natural images. We aim to control the underlying spatial structure and its rendering independently, without requiring any tuple supervision. This approach is significant because tuple supervision can be costly or unavailable. However, the task is highly unconstrained and ill-posed. To address this issue, we propose introducing an auxiliary domain that shares a common underlying-structure space with the target domain, while assuming a partially shared latent space. Our key idea is to encourage the partially shared latent variable to represent similar underlying spatial structures in both domains, while the two domain-specific latent variables represent renderings of their respective domains. To achieve this, we design two parallel generative networks with a common Progressive Rendering Architecture (PRA). This constrains the generative networks to model the shared underlying structure and the spatially dependent relation between rendering and underlying structure. We refer to our method as DSRGAN (GANs for Disentangling Underlying Structure and Rendering) and propose a quantitative criterion (the Normalized Disentanglability) to assess disentanglability. Our comparison with state-of-the-art methods shows that DSRGAN outperforms them significantly in terms of disentanglability.",1
"We present a novel dataset for training and benchmarking semantic SLAM methods. The dataset consists of 200 long sequences, each one containing 3000-5000 data frames. We generate the sequences using realistic home layouts. For that we sample trajectories that simulate motions of a simple home robot, and then render the frames along the trajectories. Each data frame contains a) RGB images generated using physically-based rendering, b) simulated depth measurements, c) simulated IMU readings and d) ground truth occupancy grid of a house. Our dataset serves a wider range of purposes compared to existing datasets and is the first large-scale benchmark focused on the mapping component of SLAM. The dataset is split into train/validation/test parts sampled from different sets of virtual houses. We present benchmarking results forboth classical geometry-based and recent learning-based SLAM algorithms, a baseline mapping method, semantic segmentation and panoptic segmentation.",0
"Introducing a new dataset that is ideal for training and evaluating semantic SLAM techniques. This dataset comprises of 200 lengthy sequences, each containing between 3000 to 5000 data frames, that were created using practical home designs. We created these sequences by simulating the movements of a basic home robot and then rendering the frames along the trajectories. Each data frame contains four components: a) RGB images that have been generated using physically-based rendering, b) simulated depth measurements, c) simulated IMU readings, and d) an occupancy grid of a house's ground truth. Compared to other datasets, ours can serve a wider variety of purposes and is the first large-scale benchmark to concentrate on the mapping component of SLAM. We divided the dataset into train/validation/test portions, which were extracted from different sets of virtual houses. We provide benchmarking results for classical geometry-based and recent learning-based SLAM algorithms, a baseline mapping approach, semantic segmentation, and panoptic segmentation.",1
"Acquisition of data in task-specific applications of machine learning like plant disease recognition is a costly endeavor owing to the requirements of professional human diligence and time constraints. In this paper, we present a simple pipeline that uses GANs in an unsupervised image translation environment to improve learning with respect to the data distribution in a plant disease dataset, reducing the partiality introduced by acute class imbalance and hence shifting the classification decision boundary towards better performance. The empirical analysis of our method is demonstrated on a limited dataset of 2789 tomato plant disease images, highly corrupted with an imbalance in the 9 disease categories. First, we extend the state of the art for the GAN-based image-to-image translation method by enhancing the perceptual quality of the generated images and preserving the semantics. We introduce AR-GAN, where in addition to the adversarial loss, our synthetic image generator optimizes on Activation Reconstruction loss (ARL) function that optimizes feature activations against the natural image. We present visually more compelling synthetic images in comparison to most prominent existing models and evaluate the performance of our GAN framework in terms of various datasets and metrics. Second, we evaluate the performance of a baseline convolutional neural network classifier for improved recognition using the resulting synthetic samples to augment our training set and compare it with the classical data augmentation scheme. We observe a significant improvement in classification accuracy (+5.2%) using generated synthetic samples as compared to (+0.8%) increase using classic augmentation in an equal class distribution environment.",0
"The acquisition of data in specialized machine learning applications, such as plant disease recognition, is expensive due to the need for professional human labor and time constraints. This paper proposes a straightforward pipeline that utilizes GANs in an unsupervised image translation setting to enhance learning by improving the data distribution in a plant disease dataset. This reduces the bias introduced by severe class imbalance, thereby shifting the classification decision boundary towards better performance. The method is evaluated on a small dataset of 2789 tomato plant disease images with an imbalance in the 9 disease categories. The study improves the state-of-the-art GAN-based image-to-image translation technique by enhancing the perceptual quality of the generated images and preserving the semantics. The proposed AR-GAN method optimizes on the Activation Reconstruction loss (ARL) function in addition to the adversarial loss, resulting in visually more compelling synthetic images and improved performance on various datasets and metrics. The study also evaluates the performance of a baseline convolutional neural network classifier for improved recognition using the generated synthetic samples to augment the training set, showing a significant increase in classification accuracy (+5.2%) compared to the classic augmentation scheme (+0.8%) in an equal class distribution environment.",1
"Generative adversarial network (GAN) has greatly improved the quality of unsupervised image generation. Previous GAN-based methods often require a large amount of high-quality training data while producing a small number (e.g., tens) of classes. This work aims to scale up GANs to thousands of classes meanwhile reducing the use of high-quality data in training. We propose an image generation method based on conditional transferring features, which can capture pixel-level semantic changes when transforming low-quality images into high-quality ones. Moreover, self-supervision learning is integrated into our GAN architecture to provide more label-free semantic supervisory information observed from the training data. As such, training our GAN architecture requires much fewer high-quality images with a small number of additional low-quality images. The experiments on CIFAR-10 and STL-10 show that even removing 30% high-quality images from the training set, our method can still outperform previous ones. The scalability on object classes has been experimentally validated: our method with 30% fewer high-quality images obtains the best quality in generating 1,000 ImageNet classes, as well as generating all 3,755 classes of CASIA-HWDB1.0 Chinese handwriting characters.",0
"The quality of unsupervised image generation has greatly improved with the use of Generative Adversarial Networks (GANs). However, previous GAN-based methods required large amounts of high-quality training data to generate only a small number of classes. To address this issue, we propose a method that can scale up GANs to thousands of classes while reducing the need for high-quality data. Our approach is based on conditional transferring features, which capture pixel-level semantic changes when transforming low-quality images into high-quality ones. Additionally, we integrate self-supervision learning into our GAN architecture to provide more label-free semantic supervisory information from the training data. As a result, our GAN architecture requires far fewer high-quality images and only a small number of additional low-quality images for training. Our experiments on CIFAR-10 and STL-10 demonstrate that our method outperforms previous ones, even when 30% of high-quality images are removed from the training set. We have also experimentally validated the scalability of our method on object classes, showing that it generates the best quality in generating 1,000 ImageNet classes and all 3,755 classes of CASIA-HWDB1.0 Chinese handwriting characters, even with 30% fewer high-quality images.",1
"The availability of large image data sets has been a crucial factor in the success of deep learning-based classification and detection methods. While data sets for everyday objects are widely available, data for specific industrial use-cases (e.g. identifying packaged products in a warehouse) remains scarce. In such cases, the data sets have to be created from scratch, placing a crucial bottleneck on the deployment of deep learning techniques in industrial applications.   We present work carried out in collaboration with a leading UK online supermarket, with the aim of creating a computer vision system capable of detecting and identifying unique supermarket products in a warehouse setting. To this end, we demonstrate a framework for using synthetic data to create an end-to-end deep learning pipeline, beginning with real-world objects and culminating in a trained model.   Our method is based on the generation of a synthetic dataset from 3D models obtained by applying photogrammetry techniques to real-world objects. Using 100k synthetic images generated from 60 real images per class, an InceptionV3 convolutional neural network (CNN) was trained, which achieved classification accuracy of 95.8% on a separately acquired test set of real supermarket product images. The image generation process supports automatic pixel annotation. This eliminates the prohibitively expensive manual annotation typically required for detection tasks. Based on this readily available data, a one-stage RetinaNet detector was trained on the synthetic, annotated images to produce a detector that can accurately localize and classify the specimen products in real-time.",0
"Deep learning-based classification and detection methods have been successful due to the availability of large image data sets. Although data sets for everyday objects are abundant, there is a scarcity of data for specific industrial use-cases such as identifying packaged products in warehouses. This results in a bottleneck for deploying deep learning techniques in industrial applications as data sets have to be created from scratch. We collaborated with a leading UK online supermarket to create a computer vision system that can detect and identify unique supermarket products in a warehouse setting. We developed a framework that uses synthetic data to create an end-to-end deep learning pipeline. Our method involves generating a synthetic dataset from 3D models obtained using photogrammetry techniques. We trained an InceptionV3 convolutional neural network on 100k synthetic images and achieved a classification accuracy of 95.8% on a real supermarket product test set. The image generation process supports automatic pixel annotation, eliminating the need for expensive manual annotation. Based on this data, we trained a one-stage RetinaNet detector that accurately localizes and classifies specimen products in real-time.",1
"Precise calibration is a must for high reliance 3D computer vision algorithms. A challenging case is when the camera is behind a protective glass or transparent object: due to refraction, the image is heavily distorted; the pinhole camera model alone can not be used and a distortion correction step is required. By directly modeling the geometry of the refractive media, we build the image generation process by tracing individual light rays from the camera to a target. Comparing the generated images to their distorted - observed - counterparts, we estimate the geometry parameters of the refractive surface via model inversion by employing an RBF neural network. We present an image collection methodology that produces data suited for finding the distortion parameters and test our algorithm on synthetic and real-world data. We analyze the results of the algorithm.",0
"Accurate calibration is essential for 3D computer vision algorithms to function reliably. A particularly challenging situation arises when the camera is positioned behind a transparent object or protective glass, causing severe image distortion due to refraction. In such cases, the pinhole camera model alone is insufficient, and a separate distortion correction process becomes necessary. To address this issue, we utilize a direct modeling approach that traces individual light rays from the camera to the target, taking into account the geometry of the refractive medium. By comparing the generated images to their distorted counterparts, we employ an RBF neural network to estimate the parameters of the refractive surface through model inversion. To gather suitable data for finding the distortion parameters, we propose an image collection methodology and test our algorithm on both synthetic and real-world data, analyzing the results in detail.",1
"Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step. One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/ .",0
"The research on conditional text-to-image generation is a dynamic field with several potential uses. The current focus of research has been on generating a single image from available conditioning information in one go. However, an iterative system that generates images based on ongoing linguistic input or feedback is a practical extension that poses a significant challenge. Such a system must comprehend the contents of its generated images in terms of feedback history, current feedback, and the interplay among concepts present in the feedback history. This study introduces a recurrent image generation model that considers both the generated output up to the current step and all past instructions for generation. The model can create backgrounds, add new objects, and make minor modifications to existing objects. It is a crucial step towards interactive image generation. The code and data can be found at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/.",1
"In this paper, we propose a novel variational generator framework for conditional GANs to catch semantic details for improving the generation quality and diversity. Traditional generators in conditional GANs simply concatenate the conditional vector with the noise as the input representation, which is directly employed for upsampling operations. However, the hidden condition information is not fully exploited, especially when the input is a class label. Therefore, we introduce a variational inference into the generator to infer the posterior of latent variable only from the conditional input, which helps achieve a variable augmented representation for image generation. Qualitative and quantitative experimental results show that the proposed method outperforms the state-of-the-art approaches and achieves the realistic controllable images.",0
"This paper introduces a new framework for conditional GANs that utilizes a variational generator to enhance the quality and diversity of generated images by capturing semantic details. Unlike traditional generators that simply concatenate the conditional vector with noise, our approach incorporates variational inference to fully exploit hidden condition information, especially when the input is a class label. By inferring the posterior of latent variable only from the conditional input, we achieve a variable augmented representation for image generation. Our experimental results demonstrate that our method surpasses existing approaches and produces realistic and controllable images.",1
"Three-dimensional medical image segmentation is one of the most important problems in medical image analysis and plays a key role in downstream diagnosis and treatment. Recent years, deep neural networks have made groundbreaking success in medical image segmentation problem. However, due to the high variance in instrumental parameters, experimental protocols, and subject appearances, the generalization of deep learning models is often hindered by the inconsistency in medical images generated by different machines and hospitals. In this work, we present StyleSegor, an efficient and easy-to-use strategy to alleviate this inconsistency issue. Specifically, neural style transfer algorithm is applied to unlabeled data in order to minimize the differences in image properties including brightness, contrast, texture, etc. between the labeled and unlabeled data. We also apply probabilistic adjustment on the network output and integrate multiple predictions through ensemble learning. On a publicly available whole heart segmentation benchmarking dataset from MICCAI HVSMR 2016 challenge, we have demonstrated an elevated dice accuracy surpassing current state-of-the-art method and notably, an improvement of the total score by 29.91\%. StyleSegor is thus corroborated to be an accurate tool for 3D whole heart segmentation especially on highly inconsistent data, and is available at https://github.com/horsepurve/StyleSegor.",0
"Medical image segmentation in three dimensions is a crucial aspect of medical image analysis as it plays a pivotal role in downstream diagnosis and treatment. Deep neural networks have recently achieved groundbreaking success in this field. However, the inconsistency in medical images generated by different machines and hospitals often hinders the generalization of deep learning models due to high variance in instrumental parameters, experimental protocols, and subject appearances. To address this issue, we present StyleSegor, an effective and user-friendly method that employs the neural style transfer algorithm to minimize differences in image properties between labeled and unlabeled data, including contrast, texture, and brightness. Additionally, we apply probabilistic adjustment and ensemble learning to integrate multiple predictions. In a publicly available dataset from MICCAI HVSMR 2016 challenge, StyleSegor demonstrated an elevated dice accuracy surpassing the current state-of-the-art method, resulting in a total score improvement of 29.91\%. Therefore, StyleSegor is an accurate tool for 3D whole heart segmentation, especially on highly inconsistent data and is available at https://github.com/horsepurve/StyleSegor.",1
"Scene graphs have become an important form of structured knowledge for tasks such as for image generation, visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with different forms of supervision, specifically introducing triplet super-vision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%),after the addition of triplet supervision and data augmentation. To understand how these different methods affect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with the performance of the layout prediction model.",0
"Structured knowledge in the form of scene graphs is crucial for various tasks, including image generation, visual relation detection, visual question answering, and image retrieval. Although word embeddings visualization and interpretation is well-established, scene graph embeddings are yet to be fully explored. This study focuses on training scene graph embeddings for layout generation with different forms of supervision, namely triplet supervision and data augmentation. The results show significant improvement in performance metrics, such as mean intersection-over-union (mIoU) and relation score, after implementing triplet supervision and data augmentation. To further understand the impact of these methods on the scene graph representation, new visualization and evaluation techniques are utilized. The study reveals that triplet supervision enhances embedding separability, which is highly correlated with layout prediction model performance.",1
