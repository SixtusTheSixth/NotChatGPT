"360$^{\circ}$ panoramas are a rich medium, yet notoriously difficult to visualize in the 2D image plane. We explore how intelligent rotations of a spherical image may enable content-aware projection with fewer perceptible distortions. Whereas existing approaches assume the viewpoint is fixed, intuitively some viewing angles within the sphere preserve high-level objects better than others. To discover the relationship between these optimal snap angles and the spherical panorama's content, we develop a reinforcement learning approach for the cubemap projection model. Implemented as a deep recurrent neural network, our method selects a sequence of rotation actions and receives reward for avoiding cube boundaries that overlap with important foreground objects. We show our approach creates more visually pleasing panoramas while using 5x less computation than the baseline.",0
"Although 360$^{\circ}$ panoramas are a valuable tool, they can be challenging to visualize in two dimensions. However, we investigate the potential of intelligent rotations of a spherical image to produce content-aware projection with fewer visible distortions. Unlike existing methods that assume a fixed viewpoint, some viewing angles within the sphere may preserve high-level objects more effectively. To identify the optimal snap angles and their relationship with the content of the spherical panorama, we use a reinforcement learning approach for the cubemap projection model. Our method, which is implemented as a deep recurrent neural network, selects a sequence of rotation actions and receives rewards for avoiding cube boundaries that overlap with important foreground objects. Our findings demonstrate that our approach can produce more aesthetically pleasing panoramas while consuming 5 times less computation than the baseline.",1
"Policy optimization is a core component of reinforcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its surrogate. Though often achieving encouraging empirical success, its underlying mathematical principle on {\em policy-distribution} optimization is unclear. We place policy optimization into the space of probability measures, and interpret it as Wasserstein gradient flows. On the probability-measure space, under specified circumstances, policy optimization becomes a convex problem in terms of distribution optimization. To make optimization feasible, we develop efficient algorithms by numerically solving the corresponding discrete gradient flows. Our technique is applicable to several RL settings, and is related to many state-of-the-art policy-optimization algorithms. Empirical results verify the effectiveness of our framework, often obtaining better performance compared to related algorithms.",0
"Reinforcement learning (RL) relies heavily on policy optimization, where existing methods optimize the parameters of a policy to maximize the expected total reward or its surrogate. Although these methods have shown promising results, the underlying mathematical principle of policy-distribution optimization remains unclear. In this study, we introduce a new approach that places policy optimization in the probability-measure space and interprets it as Wasserstein gradient flows. By doing so, policy optimization becomes a convex problem in terms of distribution optimization under certain circumstances. To enable optimization, we have developed efficient algorithms that solve the corresponding discrete gradient flows. Our technique is versatile and can be applied to various RL settings, and is related to many state-of-the-art policy-optimization algorithms. Our empirical results demonstrate the effectiveness of our framework, which often outperforms related algorithms.",1
"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",0
"A variety of challenging decision making and control tasks have been tackled successfully using model-free deep reinforcement learning (RL) algorithms. However, these algorithms are typically plagued by two major issues: high sample complexity and brittle convergence properties, which require careful hyperparameter tuning. As a result, their applicability to complex, real-world domains is severely limited. This paper introduces the soft actor-critic, an off-policy actor-critic deep RL algorithm built on the maximum entropy reinforcement learning framework. In this framework, the actor seeks to maximize both the expected reward and entropy, aiming to perform the task while acting randomly. Unlike previous deep RL methods that relied on Q-learning methods, our approach combines off-policy updates with a stable stochastic actor-critic formulation, achieving state-of-the-art performance on various continuous control benchmark tasks, surpassing on-policy and off-policy methods. Furthermore, we demonstrate that our approach is remarkably stable, producing similar outcomes across different random seeds, unlike other off-policy methods.",1
"Reinforcement learning studies how to balance exploration and exploitation in real-world systems, optimizing interactions with the world while simultaneously learning how the world operates. One general class of algorithms for such learning is the multi-armed bandit setting. Randomized probability matching, based upon the Thompson sampling approach introduced in the 1930s, has recently been shown to perform well and to enjoy provable optimality properties. It permits generative, interpretable modeling in a Bayesian setting, where prior knowledge is incorporated, and the computed posteriors naturally capture the full state of knowledge. In this work, we harness the information contained in the Bayesian posterior and estimate its sufficient statistics via sampling. In several application domains, for example in health and medicine, each interaction with the world can be expensive and invasive, whereas drawing samples from the model is relatively inexpensive. Exploiting this viewpoint, we develop a double sampling technique driven by the uncertainty in the learning process: it favors exploitation when certain about the properties of each arm, exploring otherwise. The proposed algorithm does not make any distributional assumption and it is applicable to complex reward distributions, as long as Bayesian posterior updates are computable. Utilizing the estimated posterior sufficient statistics, double sampling autonomously balances the exploration-exploitation tradeoff to make better informed decisions. We empirically show its reduced cumulative regret when compared to state-of-the-art alternatives in representative bandit settings.",0
"Reinforcement learning involves finding a balance between exploration and exploitation in real-world systems. This is achieved by optimizing interactions with the world while simultaneously learning how the world operates. One way to approach this is through multi-armed bandit algorithms, which have been shown to be effective and have provable optimality properties. These algorithms are based on the Thompson sampling approach, which is a randomized probability matching method introduced in the 1930s. This approach allows for generative, interpretable modeling in a Bayesian setting that incorporates prior knowledge, with computed posteriors capturing the full state of knowledge. In this study, we utilize the Bayesian posterior and estimate its sufficient statistics through sampling. This is particularly useful in domains where each interaction with the world is expensive and invasive, while drawing samples from the model is relatively inexpensive. By exploiting this viewpoint, we develop a double sampling technique that balances the exploration-exploitation tradeoff by favoring exploitation when certain about each arm's properties and exploring otherwise. The algorithm is not dependent on any distributional assumptions and is applicable to complex reward distributions, as long as Bayesian posterior updates are computable. The double sampling technique autonomously balances the exploration-exploitation tradeoff to make better informed decisions. Our empirical results show reduced cumulative regret when compared to state-of-the-art alternatives in representative bandit settings.",1
"The recently proposed distributional approach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a deep generative model of the value distribution, driven by the discrepancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to propose a GAN-based approach to DiRL, which leverages the strengths of GANs in learning distributions of high-dimensional data. In particular, we show that our GAN approach can be used for DiRL with multivariate rewards, an important setting which cannot be tackled with prior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exploit this idea to devise a novel exploration method that is driven by the discrepancy in estimating both values and states.",0
"The latest technique in reinforcement learning, Distributional approach to reinforcement learning (DiRL), focuses on acquiring knowledge about the value distribution, which is the distribution of the reward-to-go. Our research demonstrates that the distributional Bellman equation, which governs DiRL methods, is tantamount to a generative adversarial network (GAN) model. This means that DiRL is essentially a deep generative model of the value distribution, driven by the difference between the current value distribution and the distribution of the sum of the current reward and the next value. We utilize this understanding to propose a GAN-based approach to DiRL, which benefits from GANs' ability to learn high-dimensional data distributions. Our GAN model is well-suited for DiRL with multivariate rewards, which was not possible with previous methods. Besides, we introduce a novel exploration approach that relies on estimating both values and states' discrepancies, which is possible in the multivariate setting.",1
"We propose a reinforcement learning approach for real-time exposure control of a mobile camera that is personalizable. Our approach is based on Markov Decision Process (MDP). In the camera viewfinder or live preview mode, given the current frame, our system predicts the change in exposure so as to optimize the trade-off among image quality, fast convergence, and minimal temporal oscillation. We model the exposure prediction function as a fully convolutional neural network that can be trained through Gaussian policy gradient in an end-to-end fashion. As a result, our system can associate scene semantics with exposure values; it can also be extended to personalize the exposure adjustments for a user and device. We improve the learning performance by incorporating an adaptive metering module that links semantics with exposure. This adaptive metering module generalizes the conventional spot or matrix metering techniques. We validate our system using the MIT FiveK and our own datasets captured using iPhone 7 and Google Pixel. Experimental results show that our system exhibits stable real-time behavior while improving visual quality compared to what is achieved through native camera control.",0
"A personalized reinforcement learning method for real-time exposure control of mobile cameras is proposed using Markov Decision Process. Our system predicts the change in exposure based on the current frame in the camera viewfinder or live preview mode to optimize image quality, fast convergence, and minimal temporal oscillation. The exposure prediction function is modeled as a fully convolutional neural network trained through Gaussian policy gradient. Our system associates scene semantics with exposure values and can be customized for a user and device. The learning performance is enhanced by an adaptive metering module that links semantics with exposure, which generalizes conventional spot or matrix metering techniques. The system is validated using MIT FiveK and our own datasets captured using iPhone 7 and Google Pixel. Experimental results demonstrate stable real-time behavior and improved visual quality compared to native camera control.",1
"Within the context of autonomous driving a model-based reinforcement learning algorithm is proposed for the design of neural network-parameterized controllers. Classical model-based control methods, which include sampling- and lattice-based algorithms and model predictive control, suffer from the trade-off between model complexity and computational burden required for the online solution of expensive optimization or search problems at every short sampling time. To circumvent this trade-off, a 2-step procedure is motivated: first learning of a controller during offline training based on an arbitrarily complicated mathematical system model, before online fast feedforward evaluation of the trained controller. The contribution of this paper is the proposition of a simple gradient-free and model-based algorithm for deep reinforcement learning using task separation with hill climbing (TSHC). In particular, (i) simultaneous training on separate deterministic tasks with the purpose of encoding many motion primitives in a neural network, and (ii) the employment of maximally sparse rewards in combination with virtual velocity constraints (VVCs) in setpoint proximity are advocated.",0
"A reinforcement learning algorithm based on models is suggested for developing neural network-controlled autonomous driving. The traditional model-based control methods, like lattice-based algorithms, model predictive control, and sampling-based algorithms, have a drawback of balancing between the complexity of the model and computational burden required for solving expensive optimization or search problems at every short sampling time. To overcome this limitation, a two-step process is introduced: first, the controller is learned during offline training on a highly complex mathematical system model, followed by the fast feedforward evaluation of the trained controller online. This paper proposes a straightforward and model-based algorithm for deep reinforcement learning, which uses task separation with hill climbing (TSHC). It advocates for simultaneous training on separate deterministic tasks to encode many motion primitives in a neural network and the utilization of maximally sparse rewards in combination with virtual velocity constraints (VVCs) for proximity with setpoints.",1
"Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.",0
"The computation of uncertainty is crucial for developing dependable and strong deep learning systems. Although Variational Inference (VI) is a promising technique for such computations, compared to maximum-likelihood methods, it demands more implementation and execution efforts. The objective of this paper is to introduce new natural-gradient algorithms, which will decrease the exertion needed for Gaussian mean-field VI. Our algorithms can be merged within the Adam optimizer, and uncertainty estimates can be obtained at a low cost by utilizing the learning rate-adapting vector, obtained through perturbing the network weights during gradient evaluations. This method requires less memory, computation, and implementation effort than the existing VI methods while providing similar quality uncertainty estimates. Our research findings confirm this and indicate that the weight-perturbation in our algorithm could be beneficial for stochastic optimization and exploration in reinforcement learning.",1
"One of the challenges in model-based control of stochastic dynamical systems is that the state transition dynamics are involved, and it is not easy or efficient to make good-quality predictions of the states. Moreover, there are not many representational models for the majority of autonomous systems, as it is not easy to build a compact model that captures the entire dynamical subtleties and uncertainties. In this work, we present a hierarchical Bayesian linear regression model with local features to learn the dynamics of a micro-robotic system as well as two simpler examples, consisting of a stochastic mass-spring damper and a stochastic double inverted pendulum on a cart. The model is hierarchical since we assume non-stationary priors for the model parameters. These non-stationary priors make the model more flexible by imposing priors on the priors of the model. To solve the maximum likelihood (ML) problem for this hierarchical model, we use the variational expectation maximization (EM) algorithm, and enhance the procedure by introducing hidden target variables. The algorithm yields parsimonious model structures, and consistently provides fast and accurate predictions for all our examples involving large training and test sets. This demonstrates the effectiveness of the method in learning stochastic dynamics, which makes it suitable for future use in a paradigm, such as model-based reinforcement learning, to compute optimal control policies in real time.",0
"The modeling of stochastic dynamical systems presents a challenge due to the involvement of state transition dynamics, which can make it difficult and inefficient to generate high-quality predictions of the states. Additionally, there is a lack of representational models for autonomous systems, as creating a compact model that captures the full range of dynamical intricacies and uncertainties is challenging. To address these issues, the authors propose a hierarchical Bayesian linear regression model that utilizes local features to learn the dynamics of a micro-robotic system, as well as two simpler examples. The model is hierarchical, with non-stationary priors for the model parameters that increase flexibility by imposing priors on the priors. To solve the maximum likelihood problem, the authors employ the variational expectation maximization algorithm, which is enhanced by the incorporation of hidden target variables. The resulting model structures are parsimonious and consistently provide accurate predictions for large training and test sets. This approach holds promise for future applications in model-based reinforcement learning and real-time control policy computation.",1
"Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection---even choosing the number of nodes---remains an open question. Recent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. In this work, we propose several modeling and inference advances that consistently improve the compactness of the model learned while maintaining predictive performance, especially in smaller-sample settings including reinforcement learning.",0
"The capability of Bayesian Neural Networks (BNNs) to provide accurately calibrated posterior uncertainties has garnered significant interest lately. Nevertheless, there is still no clear solution for model selection, including the determination of the number of nodes. A new approach has suggested utilizing a horseshoe prior on node pre-activations in a BNN to disable nodes that do not contribute to data explanation. This study presents various modeling and inference developments that consistently enhance the efficiency of the model learned while retaining predictive performance, particularly in smaller-sample scenarios like reinforcement learning.",1
"Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage ""motion prior"" in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.",0
"While there have been significant advancements in deep-learning based semantic segmentation, real-world performance often suffers due to distribution mismatch. To address this issue, some domain adaptation and active learning methods have been proposed, but they have not explored the potential of leveraging information from videos, which are commonly captured by cameras. This study suggests utilizing the ""motion prior"" in videos to enhance human segmentation in a weakly-supervised active learning environment. By using optical flow to extract motion information, candidate foreground motion segments can be extracted and used to finetune the model. A memory-network-based policy model is learned to select strong candidate segments via reinforcement learning. This method improves human segmentation performance across multiple scenes and modalities, and can be combined with domain adaptation approaches to achieve additional performance gains.",1
"Self-supervised learning of convolutional neural networks can harness large amounts of cheap unlabeled data to train powerful feature representations. As surrogate task, we jointly address ordering of visual data in the spatial and temporal domain. The permutations of training samples, which are at the core of self-supervision by ordering, have so far been sampled randomly from a fixed preselected set. Based on deep reinforcement learning we propose a sampling policy that adapts to the state of the network, which is being trained. Therefore, new permutations are sampled according to their expected utility for updating the convolutional feature representation. Experimental evaluation on unsupervised and transfer learning tasks demonstrates competitive performance on standard benchmarks for image and video classification and nearest neighbor retrieval.",0
"Convolutional neural networks can use self-supervised learning to create powerful feature representations by utilizing abundant, low-cost unlabeled data. One surrogate task involves organizing visual data in both temporal and spatial domains. However, the current method of randomly selecting permutations from a predetermined set is limited. To address this, we propose a sampling policy that adapts to the current state of the network being trained. Using deep reinforcement learning, we can sample new permutations based on their expected utility in updating the feature representation. Our experimental evaluation for unsupervised and transfer learning tasks proves competitive with standard image and video classification and nearest neighbor retrieval benchmarks.",1
We examine the impact of learning Lipschitz continuous models in the context of model-based reinforcement learning. We provide a novel bound on multi-step prediction error of Lipschitz models where we quantify the error using the Wasserstein metric. We go on to prove an error bound for the value-function estimate arising from Lipschitz models and show that the estimated value function is itself Lipschitz. We conclude with empirical results that show the benefits of controlling the Lipschitz constant of neural-network models.,0
"The focus of our study is the influence of acquiring knowledge on Lipschitz continuous models in the model-based reinforcement learning domain. We introduce a fresh constraint on the multi-step prediction error of Lipschitz models, which is determined by utilizing the Wasserstein metric. Additionally, we demonstrate the existence of an error limit for the value-function estimate produced by Lipschitz models, and we verify that the estimated value function is Lipschitz. Finally, we present empirical evidence that highlights the advantages of managing the Lipschitz constant of neural network models.",1
"Today, the optimal performance of existing noise-suppression algorithms, both data-driven and those based on classic statistical methods, is range bound to specific levels of instantaneous input signal-to-noise ratios. In this paper, we present a new approach to improve the adaptivity of such algorithms enabling them to perform robustly across a wide range of input signal and noise types. Our methodology is based on the dynamic control of algorithmic parameters via reinforcement learning. Specifically, we model the noise-suppression module as a black box, requiring no knowledge of the algorithmic mechanics except a simple feedback from the output. We utilize this feedback as the reward signal for a reinforcement-learning agent that learns a policy to adapt the algorithmic parameters for every incoming audio frame (16 ms of data). Our preliminary results show that such a control mechanism can substantially increase the overall performance of the underlying noise-suppression algorithm; 42% and 16% improvements in output SNR and MSE, respectively, when compared to no adaptivity.",0
"Currently, noise-suppression algorithms have limited performance capabilities and are only effective within certain signal-to-noise ratio ranges. This paper introduces a new methodology that enhances the adaptivity of these algorithms, allowing them to perform reliably across a wider range of signal and noise types. The approach involves using reinforcement learning to dynamically control algorithmic parameters. By modeling the noise-suppression module as a black box, with feedback from the output as the reward signal, the reinforcement-learning agent can learn to adapt the algorithmic parameters for every incoming audio frame. Our preliminary results show that this approach can significantly improve the overall performance of the noise-suppression algorithm, with 42% and 16% increases in output SNR and MSE, respectively, compared to no adaptivity.",1
"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",0
"Our new method for learning the structure of convolutional neural networks (CNNs) is more effective than the most recent state-of-the-art methods, which rely on reinforcement learning and evolutionary algorithms. Our approach involves using a sequential model-based optimization (SMBO) strategy, where we search for structures in ascending order of complexity, while simultaneously creating a surrogate model to aid the search through structure space. When compared under the same search space, our method is up to five times more efficient than Zoph et al.'s (2018) RL method in terms of the number of model evaluations, and eight times quicker in terms of total compute. The structures we discover using this method achieve top-tier classification accuracies on CIFAR-10 and ImageNet.",1
"Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments.",0
"Previous investigations into the connection between vision and language for robot navigation have focused on enhancing model-free deep reinforcement learning (DRL) models in artificial settings. However, these models disregard the dynamics of actual environments and frequently struggle to generalize to new scenarios. This study takes a groundbreaking approach to bridging the gap between artificial research and real-world applications. We introduce a new hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to tackle a vision-language navigation task in the real world. Our look-ahead module tightly fuses a policy model with an environment model, which predicts the next state and reward. The experimental results demonstrate that our novel method surpasses the baseline models and performs the best on the Room-to-Room dataset in the real world. Furthermore, our adaptable approach is more transferable when applied to unfamiliar environments.",1
"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",0
"Algorithms that utilize imitation learning have the ability to learn a policy from expert demonstrations without requiring a reward signal. Nevertheless, most current techniques are not suitable for multi-agent settings because of the presence of multiple (Nash) equilibria and non-stationary environments. To address this, we have developed a new framework for multi-agent imitation learning in general Markov games, which is based on a generalized concept of inverse reinforcement learning. Additionally, we have introduced a practical multi-agent actor-critic algorithm that exhibits impressive empirical performance. With our approach, it is possible to imitate intricate behaviors in high-dimensional environments that involve multiple cooperative or competing agents.",1
"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.",0
"Q-values, which are state-action value functions, are commonly used in reinforcement learning (RL) and are the basis for popular algorithms like SARSA and Q-learning. Our proposal introduces a new idea of action value, which involves a Gaussian smoothed version of the expected Q-value. We demonstrate that these smoothed Q-values still satisfy the Bellman equation and can be learned from experience obtained from an environment. Furthermore, we can recover the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy from the gradient and Hessian of the smoothed Q-value function. Using these connections, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. This approach can also easily accommodate proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We observe that the ability to learn both the mean and covariance during training leads to significantly better results on standard continuous control benchmarks.",1
"We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure (referred to as ontology in this paper) that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers. OONP supports a rich family of operations (both symbolic and differentiable) for composing the ontology, and a big variety of forms (both symbolic and differentiable) for representing the state and the document. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL) , reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.",0
"The proposed framework, Object-oriented Neural Programming (OONP), is designed to semantically parse documents within specific domains. Essentially, OONP reads through a document and uses a pre-designed object-oriented data structure (known as ontology) to reflect the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process by utilizing a neural net-based Reader, which sequentially reads through the document and updates an intermediate ontology to summarize its partial understanding of the text. OONP offers a variety of different operations for composing the ontology, as well as various forms for representing the state and document. OONP can be trained with different forms and strengths of supervision, including supervised learning, reinforcement learning, and hybrid methods. Our experiments on both synthetic and real-world document parsing tasks have demonstrated that OONP can effectively handle complex ontologies with modest training data.",1
"While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30-90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.",0
"Although deeper convolutional networks are necessary for achieving maximum accuracy in visual perception tasks, shallower networks are adequate for many inputs. To take advantage of this observation, we developed SkipNet, a modified residual network that uses a gating network to selectively skip convolutional blocks based on the previous layer's activations. We approached the dynamic skipping issue in the context of sequential decision-making and proposed a hybrid learning algorithm combining supervised and reinforcement learning to overcome non-differentiable skipping decisions. On four benchmark datasets, SkipNet reduces computation by 30-90% while maintaining the accuracy of the original model and surpassing state-of-the-art dynamic networks and static compression techniques. We also conducted a qualitative evaluation of the gating policy, revealing a connection between image scale, saliency, and the number of skipped layers.",1
"In this article, we sketch an algorithm that extends the Q-learning algorithms to the continuous action space domain. Our method is based on the discretization of the action space. Despite the commonly used discretization methods, our method does not increase the discretized problem dimensionality exponentially. We will show that our proposed method is linear in complexity when the discretization is employed. The variant of the Q-learning algorithm presented in this work, labeled as Finite Step Q-Learning (FSQ), can be deployed to both shallow and deep neural network architectures.",0
"Our article presents an algorithm that broadens the Q-learning algorithm to include the continuous action space domain by discretizing the action space. Unlike other discretization methods, our approach does not exponentially increase the dimensionality of the problem. Our method has a linear complexity when discretization is used, as we demonstrate. We introduce the Finite Step Q-Learning (FSQ) algorithm, which can be used in shallow and deep neural network architectures.",1
"Machine Learning models become increasingly proficient in complex tasks. However, even for experts in the field, it can be difficult to understand what the model learned. This hampers trust and acceptance, and it obstructs the possibility to correct the model. There is therefore a need for transparency of machine learning models. The development of transparent classification models has received much attention, but there are few developments for achieving transparent Reinforcement Learning (RL) models. In this study we propose a method that enables a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we define a translation of states and actions to a description that is easier to understand for human users. Second, we developed a procedure that enables the agent to obtain the consequences of a single action, as well as its entire policy. The method calculates contrasts between the consequences of a policy derived from a user query, and of the learned policy of the agent. Third, a format for generating explanations was constructed. A pilot survey study was conducted to explore preferences of users for different explanation properties. Results indicate that human users tend to favor explanations about policy rather than about single actions.",0
"As Machine Learning models become more advanced, it can be challenging for experts to comprehend what the models have learned. This lack of understanding can impede trust and acceptance, and prevent correcting the model. Thus, there is a need for transparency in machine learning models. Although there has been much attention given to developing transparent classification models, there have been few advances in achieving transparency in Reinforcement Learning (RL) models. To address this issue, we propose a method that allows a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we translate states and actions into a language more easily understood by humans. Second, we developed a procedure that allows the agent to obtain the consequences of a single action or its entire policy. The method then calculates the differences between the consequences of the policy derived from a user query and the learned policy of the agent. Lastly, we created a format for generating explanations. We conducted a pilot survey study to explore user preferences for different explanations, which indicated that users preferred explanations about the policy rather than single actions.",1
"Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.",0
"Although reinforcement learning is a potent technique for training an agent to carry out a particular task, it has limitations. Specifically, an agent trained using this method is only proficient in accomplishing the single task assigned to it via its reward function. This approach is not ideal for situations that require an agent to perform diverse tasks, like navigating to different positions in a room or moving objects to various spots. To address this challenge, we introduce a novel approach that enables an agent to identify the range of tasks it can perform automatically. We leverage a generator network that recommends tasks for the agent to attempt, which are expressed as goal states. We optimize the generator network using adversarial training to create tasks that are suitably challenging for the agent continuously. Thus, our method generates a curriculum of tasks that the agent can master. Our approach demonstrates that an agent can learn to carry out an extensive range of tasks effectively and automatically, without any prior knowledge of its environment. Furthermore, our method can master tasks with sparse rewards, a difficulty that has traditionally been a significant obstacle.",1
"Activities in reinforcement learning (RL) revolve around learning the Markov decision process (MDP) model, in particular, the following parameters: state values, V; state-action values, Q; and policy, pi. These parameters are commonly implemented as an array. Scaling up the problem means scaling up the size of the array and this will quickly lead to a computational bottleneck. To get around this, the RL problem is commonly formulated to learn a specific task using hand-crafted input features to curb the size of the array. In this report, we discuss an alternative end-to-end Deep Reinforcement Learning (DRL) approach where the DRL attempts to learn general task representations which in our context refers to learning to play the Pong game from a sequence of screen snapshots without game-specific hand-crafted features. We apply artificial neural networks (ANN) to approximate a policy of the RL model. The policy network, via Policy Gradients (PG) method, learns to play the Pong game from a sequence of frames without any extra semantics apart from the pixel information and the score. In contrast to the traditional tabular RL approach where the contents in the array have clear interpretations such as V or Q, the interpretation of knowledge content from the weights of the policy network is more illusive. In this work, we experiment with various Deep ANN architectures i.e., Feed forward ANN (FFNN), Convolution ANN (CNN) and Asynchronous Advantage Actor-Critic (A3C). We also examine the activation of hidden nodes and the weights between the input and the hidden layers, before and after the DRL has successfully learnt to play the Pong game. Insights into the internal learning mechanisms and future research directions are then discussed.",0
"The main focus of reinforcement learning (RL) involves understanding the Markov decision process (MDP) model, specifically the state values (V), state-action values (Q), and policy (pi), which are usually comprised of an array. However, when scaling up the problem, the array size can become computationally limiting. To combat this issue, RL is often formulated to learn a particular task using pre-determined input features. This report explores an alternative approach using Deep Reinforcement Learning (DRL) to learn generalized task representations, specifically playing the Pong game from screen snapshots without hand-crafted features. Artificial neural networks (ANN) are used to approximate the RL model's policy, with the Policy Gradients (PG) method used to teach the network to play Pong using only pixel data and the score. Unlike traditional tabular RL, where content in the array has clear interpretations, the knowledge content from the policy network's weights is more difficult to interpret. The report experiments with various Deep ANN architectures (FFNN, CNN, and A3C) and examines the hidden node activation and input-to-hidden weight changes before and after successful learning of the Pong game. The study provides insights into internal learning mechanisms and potential research directions.",1
"We present NAVREN-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning RL. A suitable reward function is designed keeping in mind the cost and weight constraints for micro drone with minimum number of sensing modalities. Collection of small number of expert data and knowledge based data aggregation is integrated into the RL process to aid convergence. Experimentation is carried out on a Parrot AR drone in different indoor arenas and the results are compared with other baseline technologies. We demonstrate how the drone successfully avoids obstacles and navigates across different arenas.",0
"Introducing NAVREN-RL, a technique that employs end-to-end reinforcement learning (RL) to guide an unmanned aerial vehicle through an indoor Real Environment. The reward function is designed with the limitations in mind, such as the cost and weight of micro drones, and the minimum number of sensing modalities. A small amount of professional and knowledge-based data aggregation is combined with the RL process to assist with convergence. The Parrot AR drone is tested in various indoor arenas, and the findings are compared to other baseline technologies. We show how the drone effectively avoids obstacles and travels through different areas.",1
"Distributional reinforcement learning (distributional RL) has seen empirical success in complex Markov Decision Processes (MDPs) in the setting of nonlinear function approximation. However, there are many different ways in which one can leverage the distributional approach to reinforcement learning. In this paper, we propose GAN Q-learning, a novel distributional RL method based on generative adversarial networks (GANs) and analyze its performance in simple tabular environments, as well as OpenAI Gym. We empirically show that our algorithm leverages the flexibility and blackbox approach of deep learning models while providing a viable alternative to traditional methods.",0
"Empirical success has been observed in complex Markov Decision Processes (MDPs) through distributional reinforcement learning (distributional RL) when nonlinear function approximation is applied. However, there are several ways to utilize the distributional approach to reinforcement learning. In this study, we introduce GAN Q-learning, a novel distributional RL technique that utilizes generative adversarial networks (GANs), and assess its effectiveness in both simple tabular environments and OpenAI Gym. Our empirical findings demonstrate that our algorithm employs the adaptability and blackbox approach of deep learning models while presenting a feasible alternative to conventional methods.",1
"Deep neural network architectures have traditionally been designed and explored with human expertise in a long-lasting trial-and-error process. This process requires huge amount of time, expertise, and resources. To address this tedious problem, we propose a novel algorithm to optimally find hyperparameters of a deep network architecture automatically. We specifically focus on designing neural architectures for medical image segmentation task. Our proposed method is based on a policy gradient reinforcement learning for which the reward function is assigned a segmentation evaluation utility (i.e., dice index). We show the efficacy of the proposed method with its low computational cost in comparison with the state-of-the-art medical image segmentation networks. We also present a new architecture design, a densely connected encoder-decoder CNN, as a strong baseline architecture to apply the proposed hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As an application, we train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from a baseline segmentation architecture, the resulting network architecture obtains the state-of-the-art results in accuracy without performing any trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.",0
"Traditionally, deep neural network architectures have been developed and tested through a lengthy process of trial-and-error, requiring significant amounts of time, expertise, and resources. To address this issue, we propose a new algorithm that can automatically optimize the hyperparameters of a deep network architecture. Our focus is on the design of neural architectures for medical image segmentation tasks. Our method is based on a policy gradient reinforcement learning approach, with the reward function assigned a segmentation evaluation utility (i.e., dice index). We demonstrate the effectiveness of our method, which has low computational cost in comparison to state-of-the-art medical image segmentation networks. In addition, we present a new architecture design, consisting of a densely connected encoder-decoder CNN, as a strong baseline architecture to apply our hyperparameter search algorithm. We apply our algorithm to each layer of the baseline architectures and train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. As a result, the resulting network architecture achieves state-of-the-art accuracy without requiring trial-and-error based architecture design approaches or close supervision of hyperparameter changes.",1
"The idea of reusing information from previously learned tasks (source tasks) for the learning of new tasks (target tasks) has the potential to significantly improve the sample efficiency reinforcement learning agents. In this work, we describe an approach to concisely store and represent learned task knowledge, and reuse it by allowing it to guide the exploration of an agent while it learns new tasks. In order to do so, we use a measure of similarity that is defined directly in the space of parameterized representations of the value functions. This similarity measure is also used as a basis for a variant of the growing self-organizing map algorithm, which is simultaneously used to enable the storage of previously acquired task knowledge in an adaptive and scalable manner.We empirically validate our approach in a simulated navigation environment and discuss possible extensions to this approach along with potential applications where it could be particularly useful.",0
"The concept of taking information from previously learned tasks (known as source tasks) and applying it to new tasks (known as target tasks) has the potential to greatly enhance the efficiency of reinforcement learning agents. Our study outlines a method for compactly storing and representing learned task knowledge, which can be utilized to guide an agent's exploration while learning new tasks. To accomplish this, we use a similarity measure that is defined in the parameterized representations of the value functions. This similarity measure is also the basis for a version of the growing self-organizing map algorithm, which enables the storage of previously acquired task knowledge in a flexible and adaptable manner. We have tested our method in a simulated navigation environment and discuss potential extensions and applications where it could prove especially valuable.",1
"In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent's representations during training or via use as part of an explicit planning mechanism. However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment. Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.",0
"The use of generative and temporal models in model-based reinforcement learning can enhance an agent's performance through representation tuning during training or explicit planning. However, these models have only been applied in simple environments due to their difficulty in training for larger, potentially partially-observed and 3D environments. To address this issue, we introduce a novel action-conditioned generative model that includes a non-parametric spatial memory system storing learned, disentangled representations of the environment. The model uses a state-space model for low-dimensional spatial updates and a Variational Auto-Encoder for high-dimensional visual observations. This scalable architecture can make coherent predictions over hundreds of time steps in a variety of partially observed 2D and 3D environments.",1
"The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.",0
"The use of deep reinforcement learning (DRL) in cyber-physical systems is becoming more common, but there are concerns about the safety and reliability of autonomous agents. Researchers have discovered that it is possible for malicious actors to trick DRL policies into behaving improperly by generating adversarial attacks. While some studies have addressed certain types of attacks, most only focus on optimizing data space offline. This paper introduces the Meta-Learned Advantage Hierarchy (MLAH) framework, which is attack model-agnostic and better suited for reinforcement learning. The MLAH framework handles attacks in the decision space, directly reducing the learned bias introduced by the adversary. The algorithm involves learning separate sub-policies (nominal and adversarial) in an online manner, guided by a supervisory master agent that detects the presence of the adversary using the advantage function for the sub-policies. The proposed algorithm reduces bias in policy learning even in the presence of heavy state information attacks. The paper presents algorithm analysis and simulation results using popular OpenAI Gym environments.",1
"Deep Reinforcement Learning (DRL) has achieved impressive success in many applications. A key component of many DRL models is a neural network representing a Q function, to estimate the expected cumulative reward following a state-action pair. The Q function neural network contains a lot of implicit knowledge about the RL problems, but often remains unexamined and uninterpreted. To our knowledge, this work develops the first mimic learning framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than five baseline methods. The transparent tree structure of an LMUT facilitates understanding the network's learned knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.",0
"In numerous applications, Deep Reinforcement Learning (DRL) has demonstrated remarkable achievements. One of the vital elements of numerous DRL models is a neural network that represents a Q function, which approximates the expected cumulative reward after a state-action pair. The neural network that comprises the Q function includes a great deal of implicit knowledge about RL problems; however, it often remains unexplored and unexplained. As far as we know, this study presents the first mimic learning framework for Q functions in DRL. We propose Linear Model U-trees (LMUTs) to approximate the neural network's predictions. We teach an LMUT using a novel online algorithm that is well-matched for an active play environment, where the mimic learner monitors an ongoing interaction between the neural network and the environment. Empirical evidence reveals that an LMUT outperforms five benchmark methods in mimicking a Q function. The LMUT's clear tree structure enables us to understand the network's acquired knowledge by inspecting feature influence, extracting rules, and highlighting the super-pixels in image inputs.",1
"A variety of machine learning models have been proposed to assess the performance of players in professional sports. However, they have only a limited ability to model how player performance depends on the game context. This paper proposes a new approach to capturing game context: we apply Deep Reinforcement Learning (DRL) to learn an action-value Q function from 3M play-by-play events in the National Hockey League (NHL). The neural network representation integrates both continuous context signals and game history, using a possession-based LSTM. The learned Q-function is used to value players' actions under different game contexts. To assess a player's overall performance, we introduce a novel Game Impact Metric (GIM) that aggregates the values of the player's actions. Empirical Evaluation shows GIM is consistent throughout a play season, and correlates highly with standard success measures and future salary.",0
"Several machine learning models have been proposed to evaluate the performance of professional sports players. However, they have limitations in modeling how player performance relates to the context of the game. This paper suggests a new approach to incorporate game context by utilizing Deep Reinforcement Learning (DRL) to learn an action-value Q function from 3M play-by-play events in the National Hockey League (NHL). The neural network representation integrates continuous context signals and game history through a possession-based LSTM. The learned Q-function is utilized to determine players' actions' worth in different game contexts. To gauge a player's overall performance, a unique Game Impact Metric (GIM) is introduced, which sums up the values of the player's actions. Empirical analysis demonstrates that GIM is consistent throughout the season and highly correlates with standard success measures and future salary.",1
"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",0
"The problem of Multi-task Inverse Reinforcement Learning (IRL) involves inferring multiple reward functions from expert demonstrations. However, prior research that relied on Bayesian IRL has computational limitations, which restricts its scalability for complex environments. This paper introduces a new approach to multi-task IRL using the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Our experiments show that our approach can achieve one-shot imitation learning in a gridworld environment, while single-task IRL algorithms require hundreds of demonstrations to solve the same problem. We also discuss our preliminary work on extending our method to the function approximator setting of modern MCE IRL algorithms using meta-learning. Additionally, we evaluate our method on multi-task variants of common simulated robotics benchmarks, which reveal serious limitations of existing IRL algorithms. Finally, we provide suggestions for future research.",1
"Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.",0
"Although deep reinforcement learning has demonstrated impressive successes recently, its lack of data efficiency has hindered its application to real-world problems. To address this challenge, we propose the Bottleneck Simulator, a model-based reinforcement learning method that combines a factorized transition model of the environment with rollout simulations to learn an effective policy using few examples. The Bottleneck Simulator employs an abstract, discrete state to increase sample efficiency by reducing the number of model parameters and exploiting the environment's structural properties. Our mathematical analysis of the Bottleneck Simulator reveals that its performance is influenced by four sources of error: abstract space structure, transition model estimation variance, transition model estimation bias, and transition model class bias. To evaluate the Bottleneck Simulator, we applied it to two natural language processing tasks: a text adventure game and a complex dialogue response selection task. The Bottleneck Simulator outperformed competing approaches on both tasks, delivering excellent results.",1
"The present work extends the randomized shortest-paths framework (RSP), interpolating between shortest-path and random-walk routing in a network, in three directions. First, it shows how to deal with equality constraints on a subset of transition probabilities and develops a generic algorithm for solving this constrained RSP problem using Lagrangian duality. Second, it derives a surprisingly simple iterative procedure to compute the optimal, randomized, routing policy generalizing the previously developed ""soft"" Bellman-Ford algorithm. The resulting algorithm allows balancing exploitation and exploration in an optimal way by interpolating between a pure random behavior and the deterministic, optimal, policy (least-cost paths) while satisfying the constraints. Finally, the two algorithms are applied to Markov decision problems by considering the process as a constrained RSP on a bipartite state-action graph. In this context, the derived ""soft"" value iteration algorithm appears to be closely related to dynamic policy programming as well as Kullback-Leibler and path integral control, and similar to a recently introduced reinforcement learning exploration strategy. This shows that this strategy is optimal in the RSP sense - it minimizes expected path cost subject to relative entropy constraint. Simulation results on illustrative examples show that the model behaves as expected.",0
"This work expands upon the randomized shortest-paths framework (RSP) by exploring three different directions. Firstly, it addresses how to handle equality constraints on a subset of transition probabilities, presenting a general algorithm to solve this problem using Lagrangian duality. Secondly, it introduces an iterative process to compute the optimal randomized routing policy, which balances exploitation and exploration while adhering to the constraints. This algorithm is an extension of the previously developed ""soft"" Bellman-Ford algorithm. Finally, the two algorithms are applied to Markov decision problems by treating the process as a constrained RSP on a bipartite state-action graph. The ""soft"" value iteration algorithm derived from this approach is shown to be closely related to dynamic policy programming, Kullback-Leibler and path integral control, and a recently introduced reinforcement learning exploration strategy. Simulation results demonstrate that the model performs as expected.",1
"An important property for lifelong-learning agents is the ability to combine existing skills to solve unseen tasks. In general, however, it is unclear how to compose skills in a principled way. We provide a ""recipe"" for optimal value function composition in entropy-regularised reinforcement learning (RL) and then extend this to the standard RL setting. Composition is demonstrated in a video game environment, where an agent with an existing library of policies is able to solve new tasks without the need for further learning.",0
"The capacity to merge existing skills to tackle novel challenges is a crucial feature that lifelong-learning agents must possess. Nonetheless, the systematic approach to combining skills remains uncertain. We offer a ""formula"" for optimal value function combination in entropy-regularised reinforcement learning (RL) and subsequently expand it to the conventional RL context. We demonstrate this composition in a video game setting, where an agent equipped with a pre-existing set of policies can address fresh tasks without requiring additional learning.",1
"We introduce a deep generative model for functions. Our model provides a joint distribution p(f, z) over functions f and latent variables z which lets us efficiently sample from the marginal p(f) and maximize a variational lower bound on the entropy H(f). We can thus maximize objectives of the form E_{f~p(f)}[R(f)] + c*H(f), where R(f) denotes, e.g., a data log-likelihood term or an expected reward. Such objectives encompass Bayesian deep learning in function space, rather than parameter space, and Bayesian deep RL with representations of uncertainty that offer benefits over bootstrapping and parameter noise. In this short paper we describe our model, situate it in the context of prior work, and present proof-of-concept experiments for regression and RL.",0
"Our research presents a deep generative model that focuses on functions. By providing a joint distribution p(f, z) over functions f and latent variables z, we can efficiently sample from the marginal p(f) and maximize the entropy H(f) through a variational lower bound. This enables us to maximize objectives of the form E_{f~p(f)}[R(f)] + c*H(f), where R(f) represents a data log-likelihood term or an expected reward. Such objectives offer advantages over bootstrapping and parameter noise, and encompass Bayesian deep learning in function space instead of parameter space, as well as Bayesian deep RL with representations of uncertainty. In this paper, we detail our model, contextualize it among prior research, and provide proof-of-concept experiments for regression and RL.",1
"Autonomous urban driving navigation with complex multi-agent dynamics is under-explored due to the difficulty of learning an optimal driving policy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-fidelity car simulator. To alleviate the low exploration efficiency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the first successful case of the learned driving policy through reinforcement learning in the high-fidelity simulator, which performs better-than supervised imitation learning.",0
"The difficulty of learning an optimal driving policy has hindered the exploration of autonomous urban driving navigation with complex multi-agent dynamics. The traditional modular pipeline relies heavily on hand-designed rules and pre-processing perception systems, while supervised learning-based models are limited by the availability of extensive human experience. We introduce a Controllable Imitative Reinforcement Learning (CIRL) approach that enables a driving agent to achieve higher success rates using only vision inputs in a high-fidelity car simulator. To address the low exploration efficiency for large continuous action space, which often makes the use of classical RL challenging, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Additionally, we propose specialized adaptive policies and steering-angle reward designs for different control signals, based on shared representations, to enhance the model's ability to handle diverse cases. CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on various goal-directed driving tasks in the CARLA driving benchmark, and it exhibits superior generalization capability in unseen environments. This is the first successful case of the learned driving policy through reinforcement learning in a high-fidelity simulator, surpassing supervised imitation learning.",1
"Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The theoretical question of ""whether model-free algorithms can be made sample efficient"" is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions.   We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tilde{O}(\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single $\sqrt{H}$ factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes $\sqrt{T}$ regret without requiring access to a ""simulator.""",0
"Reinforcement learning (RL) algorithms that are model-free, such as Q-learning, update value functions or policies directly without explicitly modeling the environment. These algorithms are simpler and more flexible, making them more widespread in modern deep RL than model-based approaches. However, studies have suggested that model-free algorithms may require more samples to learn. The fundamental question of whether model-free algorithms can be made sample efficient remains unsolved, even in scenarios with finitely many states and actions. In this study, we prove that in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tilde{O}(\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single $\sqrt{H}$ factor. This is the first analysis in the model-free setting that establishes $\sqrt{T}$ regret without requiring access to a simulator.",1
"Model-based compression is an effective, facilitating, and expanded model of neural network models with limited computing and low power. However, conventional models of compression techniques utilize crafted features [2,3,12] and explore specialized areas for exploration and design of large spaces in terms of size, speed, and accuracy, which usually have returns Less and time is up. This paper will effectively analyze deep auto compression (ADC) and reinforcement learning strength in an effective sample and space design, and improve the compression quality of the model. The results of compression of the advanced model are obtained without any human effort and in a completely automated way. With a 4- fold reduction in FLOP, the accuracy of 2.8% is higher than the manual compression model for VGG-16 in ImageNet.",0
"Model-based compression presents an expanded neural network model that is effective and facilitates limited computing and low power. However, conventional compression techniques rely on crafted features and specialized areas for exploration and design, which often yield minimal returns and require significant time investment. This study examines the effectiveness of deep auto compression and reinforcement learning in sample and space design to enhance model compression quality. The results demonstrate that the advanced model achieves a 4-fold reduction in FLOP and 2.8% higher accuracy than manual compression methods for VGG-16 in ImageNet, without requiring any human effort and through a completely automated process.",1
"Learning a generative model is a key component of model-based reinforcement learning. Though learning a good model in the tabular setting is a simple task, learning a useful model in the approximate setting is challenging. In this context, an important question is the loss function used for model learning as varying the loss function can have a remarkable impact on effectiveness of planning. Recently Farahmand et al. (2017) proposed a value-aware model learning (VAML) objective that captures the structure of value function during model learning. Using tools from Asadi et al. (2018), we show that minimizing the VAML objective is in fact equivalent to minimizing the Wasserstein metric. This equivalence improves our understanding of value-aware models, and also creates a theoretical foundation for applications of Wasserstein in model-based reinforcement~learning.",0
"In model-based reinforcement learning, acquiring a generative model is crucial. While it's relatively easy to learn a model in the tabular setting, doing so in the approximate setting poses a challenge. One key aspect to consider is the loss function employed during model learning, as this can greatly impact planning effectiveness. Recently, Farahmand et al. (2017) introduced the value-aware model learning (VAML) objective, which incorporates the structure of the value function. By utilizing techniques from Asadi et al. (2018), we demonstrate that minimizing the VAML objective is equivalent to minimizing the Wasserstein metric. This finding advances our comprehension of value-aware models and establishes a theoretical basis for employing Wasserstein in model-based reinforcement learning applications.",1
"Existing person re-identification (re-id) methods assume the provision of accurately cropped person bounding boxes with minimum background noise, mostly by manually cropping. This is significantly breached in practice when person bounding boxes must be detected automatically given a very large number of images and/or videos processed. Compared to carefully cropped manually, auto-detected bounding boxes are far less accurate with random amount of background clutter which can degrade notably person re-id matching accuracy. In this work, we develop a joint learning deep model that optimises person re-id attention selection within any auto-detected person bounding boxes by reinforcement learning of background clutter minimisation subject to re-id label pairwise constraints. Specifically, we formulate a novel unified re-id architecture called Identity DiscriminativE Attention reinforcement Learning (IDEAL) to accurately select re-id attention in auto-detected bounding boxes for optimising re-id performance. Our model can improve re-id accuracy comparable to that from exhaustive human manual cropping of bounding boxes with additional advantages from identity discriminative attention selection that specially benefits re-id tasks beyond human knowledge. Extensive comparative evaluations demonstrate the re-id advantages of the proposed IDEAL model over a wide range of state-of-the-art re-id methods on two auto-detected re-id benchmarks CUHK03 and Market-1501.",0
"The current methods for person re-identification (re-id) assume that there are accurately cropped person bounding boxes with minimal background noise, which are usually achieved through manual cropping. However, this is not always feasible in practice when a large number of images and/or videos must be processed and person bounding boxes must be automatically detected. The accuracy of automatically detected bounding boxes is significantly lower than that of manually cropped ones, and the random amount of background clutter can have a negative impact on person re-id matching accuracy. To address this issue, we propose a joint learning deep model that optimizes person re-id attention selection within auto-detected person bounding boxes by minimizing background clutter through reinforcement learning. Our model, called Identity DiscriminativE Attention reinforcement Learning (IDEAL), accurately selects re-id attention in auto-detected bounding boxes to optimize re-id performance. The IDEAL model improves re-id accuracy comparable to that achieved through human manual cropping of bounding boxes, with additional benefits from identity discriminative attention selection that benefit re-id tasks beyond human knowledge. Extensive comparative evaluations demonstrate the advantages of the proposed IDEAL model over a wide range of state-of-the-art re-id methods on two auto-detected re-id benchmarks: CUHK03 and Market-1501.",1
"Learning from small data sets is critical in many practical applications where data collection is time consuming or expensive, e.g., robotics, animal experiments or drug design. Meta learning is one way to increase the data efficiency of learning algorithms by generalizing learned concepts from a set of training tasks to unseen, but related, tasks. Often, this relationship between tasks is hard coded or relies in some other way on human expertise. In this paper, we frame meta learning as a hierarchical latent variable model and infer the relationship between tasks automatically from data. We apply our framework in a model-based reinforcement learning setting and show that our meta-learning model effectively generalizes to novel tasks by identifying how new tasks relate to prior ones from minimal data. This results in up to a 60% reduction in the average interaction time needed to solve tasks compared to strong baselines.",0
"In many practical applications, such as robotics, animal experiments, or drug design, learning from small data sets is crucial due to the high cost or time-consuming nature of data collection. Meta learning is a technique that can improve the data efficiency of learning algorithms by expanding learned concepts from a set of training tasks to similar, unseen tasks. Typically, this connection between tasks involves some degree of human expertise or predetermined coding. However, in this study, we present meta learning as a hierarchical latent variable model, allowing for an automatic inference of the relationship between tasks from the data. Our approach is applied to a model-based reinforcement learning environment, demonstrating that our meta-learning model can effectively generalize to new tasks by identifying how they relate to prior ones based on minimal data. This leads to a 60% decrease in the average interaction time required to solve tasks compared to strong baselines.",1
"We introduce SCAL, an algorithm designed to perform efficient exploration-exploitation in any unknown weakly-communicating Markov decision process (MDP) for which an upper bound $c$ on the span of the optimal bias function is known. For an MDP with $S$ states, $A$ actions and $\Gamma \leq S$ possible next states, we prove a regret bound of $\widetilde{O}(c\sqrt{\Gamma SAT})$, which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the optimal bias span is finite and often much smaller than $D$ (e.g., $D=\infty$ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.",0
"SCAL is an algorithm that efficiently explores and exploits unknown weakly-communicating Markov decision processes (MDPs) with a known upper bound $c$ on the optimal bias function span. Our regret bound for an MDP with $S$ states, $A$ actions, and $\Gamma \leq S$ possible next states is $\widetilde{O}(c\sqrt{\Gamma SAT})$, which is a significant improvement over existing algorithms like UCRL and PSRL, whose regret scales linearly with the MDP diameter $D$. The optimal bias span is often much smaller than $D$, and our algorithm provides the first computationally efficient solution to the optimization problem at the core of REGAL.C, which was originally derived by Bartlett and Tewari (2009) but lacks a tractable algorithm. We present numerical simulations that demonstrate SCAL's superior performance over UCRL in MDPs with a large diameter and a small span, supporting our theoretical analysis.",1
"Reinforcement learning (RL) has advanced greatly in the past few years with the employment of effective deep neural networks (DNNs) on the policy networks. With the great effectiveness came serious vulnerability issues with DNNs that small adversarial perturbations on the input can change the output of the network. Several works have pointed out that learned agents with a DNN policy network can be manipulated against achieving the original task through a sequence of small perturbations on the input states. In this paper, we demonstrate furthermore that it is also possible to impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks. Our method involves the latest adversarial attack technique, Adversarial Transformer Network (ATN), that learns to generate the attack and is easy to integrate into the policy network. As a result of our attack, the victim agent is misguided to optimise for the adversarial reward over time. Our results expose serious security threats for RL applications in safety-critical systems including drones, medical analysis, and self-driving cars.",0
"The use of effective deep neural networks (DNNs) on policy networks has greatly advanced reinforcement learning (RL) in recent years. However, these advancements have also resulted in serious vulnerabilities as small adversarial perturbations on the input can alter the network's output. Previous research has shown that learned agents with DNN policy networks can be manipulated to prevent them from achieving their original task through a sequence of small input state perturbations. In this study, we take it a step further and demonstrate that we can also impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks using the Adversarial Transformer Network (ATN) technique. This method is easy to integrate into the policy network and results in the victim agent optimizing for the adversarial reward over time, posing a significant threat to RL applications in safety-critical systems, such as drones, medical analysis, and self-driving cars.",1
"Exploration is a difficult challenge in reinforcement learning and even recent state-of-the art curiosity-based methods rely on the simple epsilon-greedy strategy to generate novelty. We argue that pure random walks do not succeed to properly expand the exploration area in most environments and propose to replace single random action choices by random goals selection followed by several steps in their direction. This approach is compatible with any curiosity-based exploration and off-policy reinforcement learning agents and generates longer and safer trajectories than individual random actions. To illustrate this, we present a task-independent agent that learns to reach coordinates in screen frames and demonstrate its ability to explore with the game Super Mario Bros. improving significantly the score of a baseline DQN agent.",0
"Reinforcement learning poses a tough challenge when it comes to exploration, and even the most recent curiosity-based methods depend on the basic epsilon-greedy strategy to generate new experiences. However, we believe that pure random walks are not effective in expanding the exploration area in most environments. To address this issue, we suggest replacing single random actions with random goal selection followed by several steps in that direction. This approach can work with any curiosity-based exploration and off-policy reinforcement learning agents, and it produces longer and safer trajectories than individual random actions. We present an agent that can learn to reach coordinates on screen frames, which is not task-specific, and demonstrate its efficacy in exploring the game Super Mario Bros. Our agent significantly enhances the score of a baseline DQN agent.",1
"In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.",0
"Reinforcement learning typically involves allowing an agent to interact with its environment for a set amount of time, resetting it, and repeating the process in multiple episodes. The agent's objective can be either to maximize its performance over a fixed period or an indefinite period with time limits used only during training to diversify experience. This paper presents a formal account of how time limits can be handled effectively in each case. Not doing so can lead to suboptimal policies, training instability, state-aliasing, and invalidation of experience replay. For case (i), we argue that the remaining time should be included as part of the agent's input to avoid violating the Markov property. For case (ii), we suggest bootstrapping from the value of the state at the end of each episode. We demonstrate empirically the importance of our proposals in improving the performance and stability of existing reinforcement learning algorithms, achieving state-of-the-art results on several control tasks.",1
"In a voice-controlled smart-home, a controller must respond not only to user's requests but also according to the interaction context. This paper describes Arcades, a system which uses deep reinforcement learning to extract context from a graphical representation of home automation system and to update continuously its behavior to the user's one. This system is robust to changes in the environment (sensor breakdown or addition) through its graphical representation (scale well) and the reinforcement mechanism (adapt well). The experiments on realistic data demonstrate that this method promises to reach long life context-aware control of smart-home.",0
"Arcades is a system that employs deep reinforcement learning to extract context from a graphical representation of a home automation system and adjust its behavior to the user's requests. The controller in a voice-controlled smart-home must consider the interaction context, and Arcades is designed to be robust to changes in the environment through its graphical representation and reinforcement mechanism. Realistic data experiments demonstrate that this method is capable of achieving long-term context-aware control of a smart-home.",1
"Recent developments in deep reinforcement learning have enabled the creation of agents for solving a large variety of games given a visual input. These methods have been proven successful for 2D games, like the Atari games, or for simple tasks, like navigating in mazes. It is still an open question, how to address more complex environments, in which the reward is sparse and the state space is huge. In this paper we propose a divide and conquer deep reinforcement learning solution and we test our agent in the first person shooter (FPS) game of Doom. Our work is based on previous works in deep reinforcement learning and in Doom agents. We also present how our agent is able to perform better in unknown environments compared to a state of the art reinforcement learning algorithm.",0
"Advanced progress in deep reinforcement learning has made it possible to develop agents that can tackle a wide range of games, based on visual input. While these techniques have shown success in 2D games like Atari games, and simple tasks such as mazes, it remains unclear how to approach more complex environments that have sparse rewards and an enormous state space. To address this challenge, we propose a divide and conquer deep reinforcement learning approach and test our agent in the first-person shooter game Doom. Our work builds upon prior research in deep reinforcement learning and Doom agents. Additionally, we demonstrate how our agent outperforms a state-of-the-art reinforcement learning algorithm in unfamiliar environments.",1
"In recent years, reinforcement learning (RL) methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go, and Poker. However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding different players' diverse behaviors. In this paper, we present a novel method called Multi-Motivation Behavior Modeling (MMBM) that takes the multifaceted human motivations into consideration and models the underlying value structure of the players using inverse RL. Our approach does not require the access to the dynamic of the system, making it feasible to model complex interactive environments such as massively multiplayer online games. MMBM is tested on the World of Warcraft Avatar History dataset, which recorded over 70,000 users' gameplay spanning three years period. Our model reveals the significant difference of value structures among different player groups. Using the results of motivation modeling, we also predict and explain their diverse gameplay behaviors and provide a quantitative assessment of how the redesign of the game environment impacts players' behaviors.",0
"Reinforcement learning (RL) methods have been effectively employed to simulate gameplay, resulting in super-human performance in various games including Atari, Go, and Poker. However, these studies have primarily focused on winning the game, overlooking the intricate and diverse human motivations that are crucial in understanding players' behaviors. This paper introduces a new approach, Multi-Motivation Behavior Modeling (MMBM), which incorporates multifaceted human motivations and models players' underlying value structure using inverse RL. Our method does not necessitate access to the system's dynamics, making it possible to simulate complex interactive environments like massively multiplayer online games. We tested MMBM on the World of Warcraft Avatar History dataset, which documents over 70,000 users' gameplay over three years. Our model identifies significant disparities in value structures among different player groups, and we use the results of motivation modeling to predict and explain their varied gameplay behaviors. We also provide a quantitative evaluation of how game environment redesign affects players' behaviors.",1
"Consistently checking the statistical significance of experimental results is one of the mandatory methodological steps to address the so-called ""reproducibility crisis"" in deep reinforcement learning. In this tutorial paper, we explain how the number of random seeds relates to the probabilities of statistical errors. For both the t-test and the bootstrap confidence interval test, we recall theoretical guidelines to determine the number of random seeds one should use to provide a statistically significant comparison of the performance of two algorithms. Finally, we discuss the influence of deviations from the assumptions usually made by statistical tests. We show that they can lead to inaccurate evaluations of statistical errors and provide guidelines to counter these negative effects. We make our code available to perform the tests.",0
"To address the ""reproducibility crisis"" in deep reinforcement learning, it is crucial to consistently check the statistical significance of experimental results. This tutorial paper provides a comprehensive explanation of how the number of random seeds impacts the probabilities of statistical errors. The t-test and bootstrap confidence interval test are both discussed, and we provide theoretical guidelines for determining the appropriate number of random seeds required to achieve a statistically significant comparison of two algorithms. Additionally, we examine how deviations from standard statistical test assumptions can result in inaccurate evaluations of statistical errors and suggest guidelines to mitigate these issues. Our code is available to perform these tests.",1
"Multi-step temporal difference (TD) learning is an important approach in reinforcement learning, as it unifies one-step TD learning with Monte Carlo methods in a way where intermediate algorithms can outperform either extreme. They address a bias-variance trade off between reliance on current estimates, which could be poor, and incorporating longer sampled reward sequences into the updates. Especially in the off-policy setting, where the agent aims to learn about a policy different from the one generating its behaviour, the variance in the updates can cause learning to diverge as the number of sampled rewards used in the estimates increases. In this paper, we introduce per-decision control variates for multi-step TD algorithms, and compare them to existing methods. Our results show that including the control variates can greatly improve performance on both on and off-policy multi-step temporal difference learning tasks.",0
"Temporal difference (TD) learning using multiple steps is a significant method in reinforcement learning since it combines one-step TD learning and Monte Carlo approaches, resulting in intermediate algorithms that can surpass both extremes. These algorithms tackle the trade-off between bias and variance by balancing the reliance on current estimates, which may not be accurate, and including longer sequences of sampled rewards in the updates. When learning about a policy different from the one that generates its behavior, the variance in the updates can cause learning to diverge as the number of sampled rewards used in the estimates increases, particularly in the off-policy setting. In this study, we present per-decision control variates for multi-step TD algorithms and compare them to existing approaches. Our findings demonstrate that incorporating control variates can significantly enhance the performance of on and off-policy multi-step temporal difference learning tasks.",1
"Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.",0
"Due to their lengthy training times, modern deep learning methods pose a challenge for vanilla Bayesian hyperparameter optimization, which becomes computationally infeasible. Meanwhile, bandit-based configuration evaluation approaches based on random search lack direction and fail to converge to the best configurations in a timely manner. To overcome these shortcomings, we propose a hybrid approach that combines the best of both worlds: the strengths of Bayesian optimization and bandit-based methods. Our new state-of-the-art hyperparameter optimization method delivers strong anytime performance and achieves fast convergence to optimal configurations. It consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our approach is versatile and robust, yet conceptually simple and easy to implement.",1
"A key question in Reinforcement Learning is which representation an agent can learn to efficiently reuse knowledge between different tasks. Recently the Successor Representation was shown to have empirical benefits for transferring knowledge between tasks with shared transition dynamics. This paper presents Model Features: a feature representation that clusters behaviourally equivalent states and that is equivalent to a Model-Reduction. Further, we present a Successor Feature model which shows that learning Successor Features is equivalent to learning a Model-Reduction. A novel optimization objective is developed and we provide bounds showing that minimizing this objective results in an increasingly improved approximation of a Model-Reduction. Further, we provide transfer experiments on randomly generated MDPs which vary in their transition and reward functions but approximately preserve behavioural equivalence between states. These results demonstrate that Model Features are suitable for transfer between tasks with varying transition and reward functions.",0
"In Reinforcement Learning, a critical inquiry is determining which representation an agent can adopt to effectively reuse knowledge across different tasks. Recently, the Successor Representation has been proven to have practical advantages in transferring knowledge between tasks that share transition dynamics. This study introduces Model Features, a characteristic representation that groups states with similar behaviors and is comparable to a Model-Reduction. Additionally, a Successor Feature model is presented, demonstrating that learning Successor Features is equivalent to learning a Model-Reduction. A new optimization objective is developed, and we provide evidence that minimizing this objective leads to an increasingly better approximation of a Model-Reduction. Transfer experiments on randomly generated MDPs with varying transition and reward functions are also provided, demonstrating that Model Features are appropriate for transferring knowledge between tasks with different transition and reward functions while maintaining behavioral equivalence between states.",1
"Deep reinforcement learning has led to several recent breakthroughs, though the learned policies are often based on black-box neural networks. This makes them difficult to interpret and to impose desired specification constraints during learning. We present an iterative framework, MORL, for improving the learned policies using program synthesis. Concretely, we propose to use synthesis techniques to obtain a symbolic representation of the learned policy, which can then be debugged manually or automatically using program repair. After the repair step, we use behavior cloning to obtain the policy corresponding to the repaired program, which is then further improved using gradient descent. This process continues until the learned policy satisfies desired constraints. We instantiate MORL for the simple CartPole problem and show that the programmatic representation allows for high-level modifications that in turn lead to improved learning of the policies.",0
"Recent advances in deep reinforcement learning have resulted in significant breakthroughs. However, the policies derived from black-box neural networks can be challenging to comprehend and enforce specific constraints during the learning process. In this study, we propose an iterative framework, MORL, that utilizes program synthesis to enhance the learned policies. We achieve this by obtaining a symbolic representation of the learned policy using synthesis techniques, which can be debugged manually or automatically using program repair. After the repair stage, we use behavior cloning to obtain the policy corresponding to the fixed program. We then further enhance the policy using gradient descent until it satisfies the desired constraints. We apply MORL to the CartPole problem and demonstrate that the programmatic representation allows for high-level modifications, resulting in improved policy learning.",1
"This paper studies the potential of the return distribution for exploration in deterministic reinforcement learning (RL) environments. We study network losses and propagation mechanisms for Gaussian, Categorical and Gaussian mixture distributions. Combined with exploration policies that leverage this return distribution, we solve, for example, a randomized Chain task of length 100, which has not been reported before when learning with neural networks.",0
"The focus of this research is to investigate the possibility of utilizing the return distribution to facilitate exploration in deterministic reinforcement learning (RL) environments. The study delves into the propagation mechanisms and network losses of Gaussian, Categorical, and Gaussian mixture distributions. By incorporating exploration policies that capitalize on this return distribution, we were able to successfully solve a randomized Chain task of length 100, which has not been previously achieved with neural networks.",1
"We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.",0
"Our framework for continual learning domains involves a straightforward and scalable approach, allowing for sequential task learning. Our approach maintains a consistent number of parameters and aims to sustain performance on previously learned tasks while expediting learning on future ones. This is accomplished by training a two-component network, consisting of a knowledge base that can solve past problems and an active column that efficiently learns the current task. Once a new task is learned, the active column is compressed into the knowledge base, ensuring that previous skills are not compromised. This process involves no growth in architecture, no access to prior data, and no task-specific parameters. We showcase the efficacy of this progression and compression method on handwritten alphabet classification, as well as two reinforcement learning domains: Atari games and 3D maze navigation.",1
"The use of ensembles of neural networks (NNs) for the quantification of predictive uncertainty is widespread. However, the current justification is intuitive rather than analytical. This work proposes one minor modification to the normal ensembling methodology, which we prove allows the ensemble to perform Bayesian inference, hence converging to the corresponding Gaussian Process as both the total number of NNs, and the size of each, tend to infinity. This working paper provides early-stage results in a reinforcement learning setting, analysing the practicality of the technique for an ensemble of small, finite number. Using the uncertainty estimates produced by anchored ensembles to govern the exploration-exploitation process results in steadier, more stable learning.",0
"Ensemble of neural networks (NNs) are widely used to measure predictive uncertainty, but the justification for their use is currently based on intuition rather than analysis. This study suggests a minor modification to the traditional ensembling approach, which has been proven to enable Bayesian inference and converge to the Gaussian Process as the number and size of NNs increase. The paper presents initial findings in a reinforcement learning scenario, examining the feasibility of the method for a small and finite ensemble. Utilizing the uncertainty estimates generated by anchored ensembles to guide the exploration-exploitation process leads to more consistent and stable learning.",1
"In a large E-commerce platform, all the participants compete for impressions under the allocation mechanism of the platform. Existing methods mainly focus on the short-term return based on the current observations instead of the long-term return. In this paper, we formally establish the lifecycle model for products, by defining the introduction, growth, maturity and decline stages and their transitions throughout the whole life period. Based on such model, we further propose a reinforcement learning based mechanism design framework for impression allocation, which incorporates the first principal component based permutation and the novel experiences generation method, to maximize short-term as well as long-term return of the platform. With the power of trial-and-error, it is possible to optimize impression allocation strategies globally which is contribute to the healthy development of participants and the platform itself. We evaluate our algorithm on a simulated environment built based on one of the largest E-commerce platforms, and a significant improvement has been achieved in comparison with the baseline solutions.",0
"In the context of a vast E-commerce platform, all the players strive to obtain impressions through the platform's allocation mechanism. However, current methods prioritize short-term gain based on present observations instead of long-term results. This study introduces a lifecycle model for products that outlines the various stages of introduction, growth, maturity, and decline, along with their transitions throughout the product's lifespan. Building on this model, a reinforcement learning-based mechanism design framework is proposed for impression allocation. The framework includes a first principal component-based permutation and a unique experiences generation method, which aims to maximize both short-term and long-term returns for the platform. The trial-and-error approach enables the optimization of impression allocation strategies globally, ultimately contributing to the healthy development of the platform and its participants. The algorithm is evaluated using a simulated environment based on one of the largest E-commerce platforms, and a significant improvement was recorded compared to the baseline solutions.",1
"Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.",0
"Effective machine learning systems require reliable methods for reasoning under uncertainty. Although Bayesian methods offer a general framework for quantifying uncertainty, the accuracy of their uncertainty estimates is often compromised by model misspecification and approximate inference. This means that a 90% credible interval may not accurately contain the true outcome 90% of the time. To overcome this limitation, we propose a straightforward approach for calibrating any regression algorithm, which is able to produce calibrated uncertainty estimates when applied to Bayesian and probabilistic models with sufficient data. Our approach is based on Platt scaling and builds on previous work on classification. We demonstrate the effectiveness of this approach across Bayesian linear regression, feedforward, and recurrent neural networks, consistently producing well-calibrated credible intervals that enhance performance on time series forecasting and model-based reinforcement learning tasks.",1
"We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.",0
"Our proposed method for deep reinforcement learning (RL) surpasses traditional approaches in terms of effectiveness, generalization capacity, and interpretability, thanks to the use of structured perception and relational reasoning. By employing self-attention, our approach iteratively examines relations between entities in a scene and guides a model-free policy. Our experiments demonstrate that our agent solves a novel navigation and planning task called Box-World with interpretable solutions that outperform baselines in terms of sample complexity, ability to generalize to more complex scenes, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games, outperforming human grandmasters on four. With a focus on architectural inductive biases, our work advances new avenues for overcoming complex challenges in deep RL.",1
"One of the key challenges in applying reinforcement learning to real-life problems is that the amount of train-and-error required to learn a good policy increases drastically as the task becomes complex. One potential solution to this problem is to combine reinforcement learning with automated symbol planning and utilize prior knowledge on the domain. However, existing methods have limitations in their applicability and expressiveness. In this paper we propose a hierarchical reinforcement learning method based on abductive symbolic planning. The planner can deal with user-defined evaluation functions and is not based on the Herbrand theorem. Therefore it can utilize prior knowledge of the rewards and can work in a domain where the state space is unknown. We demonstrate empirically that our architecture significantly improves learning efficiency with respect to the amount of training examples on the evaluation domain, in which the state space is unknown and there exist multiple goals.",0
"Reinforcement learning poses a major challenge in solving real-world problems as more trial-and-error is required to develop a good policy in complex tasks. To overcome this issue, reinforcement learning can be combined with automated symbol planning and prior domain knowledge. However, current methods have limitations in application and expression. This paper proposes a hierarchical reinforcement learning approach that utilizes abductive symbolic planning. The planner can handle user-defined evaluation functions and does not rely on the Herbrand theorem, allowing it to use prior knowledge of rewards and work in unknown state spaces. Empirical results show significant improvement in learning efficiency in an evaluation domain with multiple goals and unknown state space.",1
"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",0
"Our objective is to use a solitary reinforcement learning agent with a single set of parameters to solve a vast array of tasks. However, the primary challenge we face is handling the large amount of data and prolonged training time. To address this challenge, we have developed IMPALA (Importance Weighted Actor-Learner Architecture), a new distributed agent that optimizes resource utilization during single-machine training and scales up to thousands of machines without compromising data efficiency. We ensure stable learning with high throughput by utilizing decoupled acting and learning alongside V-trace, an innovative off-policy correction method. We demonstrate the effectiveness of IMPALA on DMLab-30 (30 tasks from the DeepMind Lab environment) and Atari-57 (all available Atari games in the Arcade Learning Environment). Our results display that IMPALA outperforms previous agents with less data and exhibits positive transfer between tasks due to its multi-task approach.",1
"Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.",0
"Although dextrous multi-fingered hands are versatile, they are difficult to control due to their high dimensionality and numerous potential contacts. While deep reinforcement learning (DRL) provides a model-agnostic method to control complex systems, it has not yet been applied to high-dimensional dextrous manipulation and is challenging to deploy on physical systems due to sample inefficiency. Consequently, DRL has only been successful with simpler manipulators and tasks. However, in this study, we demonstrate that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand and solve them from scratch in simulated experiments. Moreover, using a small number of human demonstrations can significantly reduce sample complexity, allowing learning with sample sizes equivalent to a few hours of robot experience. Policies learned with demonstrations exhibit very natural movements and are surprisingly more robust.",1
"The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners.",0
"The dangers of overfitting in machine learning are widely recognized, but most of the solutions and methods for identifying the issue were created for supervised learning. This study seeks to provide fresh insights into the identification and prevention of overfitting in deep Reinforcement Learning (RL) techniques, with an emphasis on continuous domains. The research investigates various aspects, including the definition and diagnosis of overfitting in MDPs, and reducing risks by introducing diverse training. The study complements recent discoveries on the fragility of deep RL methods and provides practical observations for RL practitioners and researchers.",1
"In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.",0
"Over the past few years, deep learning has become increasingly popular due to its remarkable outcomes in various fields, including pattern recognition, speech and image recognition, and natural language processing. Moreover, recent studies have demonstrated that the combination of deep learning and reinforcement learning can be utilized to acquire valuable representations for problems involving high-dimensional raw data input. This chapter provides an overview of the latest advancements in deep reinforcement learning, particularly in relation to the most commonly employed deep architectures such as autoencoders, convolutional neural networks, and recurrent neural networks, which have been successfully integrated with the reinforcement learning framework.",1
"All-goals updating exploits the off-policy nature of Q-learning to update all possible goals an agent could have from each transition in the world, and was introduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work this was mostly explored in small-state RL problems that allowed tabular representations and where all possible goals could be explicitly enumerated and learned separately. In this paper we empirically explore 3 different extensions of the idea of updating many (instead of all) goals in the context of RL with deep neural networks (or DeepRL for short). First, in a direct adaptation of Kaelbling's approach we explore if many-goals updating can be used to achieve mastery in non-tabular visual-observation domains. Second, we explore whether many-goals updating can be used to pre-train a network to subsequently learn faster and better on a single main task of interest. Third, we explore whether many-goals updating can be used to provide auxiliary task updates in training a network to learn faster and better on a single main task of interest. We provide comparisons to baselines for each of the 3 extensions.",0
"Kaelbling (1993) introduced all-goals updating in Reinforcement Learning (RL), which takes advantage of the off-policy nature of Q-learning to update all possible goals an agent could have from each transition in the world. However, this approach has mainly been explored in small-state RL problems with tabular representations where all possible goals could be explicitly enumerated and learned separately. In this study, we investigate the feasibility of updating many goals instead of all goals in RL with deep neural networks (DeepRL). We explore three different extensions of this idea, including using many-goals updating to achieve mastery in non-tabular visual-observation domains, pre-training a network to subsequently learn faster and better on a single main task of interest, and providing auxiliary task updates in training a network to learn faster and better on a single main task of interest. We compare our results to baselines for each of the three extensions.",1
We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually unifies multiple previous methods in exploration. We also derive a practical algorithm that achieves efficient exploration on challenging control tasks.,0
"Our proposed framework combines distributional reinforcement learning with recent efforts to merge Bayesian parameter updates and deep reinforcement learning. Through this, we unify various exploration methods and present a practical algorithm that effectively handles difficult control tasks.",1
"Recognition of surgical gesture is crucial for surgical skill assessment and efficient surgery training. Prior works on this task are based on either variant graphical models such as HMMs and CRFs, or deep learning models such as Recurrent Neural Networks and Temporal Convolutional Networks. Most of the current approaches usually suffer from over-segmentation and therefore low segment-level edit scores. In contrast, we present an essentially different methodology by modeling the task as a sequential decision-making process. An intelligent agent is trained using reinforcement learning with hierarchical features from a deep model. Temporal consistency is integrated into our action design and reward mechanism to reduce over-segmentation errors. Experiments on JIGSAWS dataset demonstrate that the proposed method performs better than state-of-the-art methods in terms of the edit score and on par in frame-wise accuracy. Our code will be released later.",0
"Surgical skill assessment and efficient surgery training require the recognition of surgical gestures. Previous studies have used graphical models, such as HMMs and CRFs, or deep learning models, such as Recurrent Neural Networks and Temporal Convolutional Networks, to accomplish this task. However, these methods often suffer from over-segmentation, which leads to low segment-level edit scores. In contrast, our approach models the task as a sequential decision-making process and trains an intelligent agent using reinforcement learning with hierarchical features from a deep model. We integrate temporal consistency into our action design and reward mechanism to reduce over-segmentation errors. Our experiments on the JIGSAWS dataset demonstrate that our method outperforms state-of-the-art methods in terms of the edit score and performs on par in frame-wise accuracy. We will release our code at a later time.",1
"Neural networks allow Q-learning reinforcement learning agents such as deep Q-networks (DQN) to approximate complex mappings from state spaces to value functions. However, this also brings drawbacks when compared to other function approximators such as tile coding or their generalisations, radial basis functions (RBF) because they introduce instability due to the side effect of globalised updates present in neural networks. This instability does not even vanish in neural networks that do not have any hidden layers. In this paper, we show that simple modifications to the structure of the neural network can improve stability of DQN learning when a multi-layer perceptron is used for function approximation.",0
"The utilization of neural networks enables reinforcement learning agents like deep Q-networks (DQN) to estimate intricate mappings from state spaces to value functions. Nonetheless, this approach also has its downsides when compared to other function approximators such as tile coding or radial basis functions (RBF), as it introduces instability caused by the globalized updates inherent in neural networks. This instability still persists even in neural networks without hidden layers. This paper presents modifications to the neural network structure that can enhance the stability of DQN learning, particularly when utilizing a multi-layer perceptron for function approximation.",1
"Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, object positions and types, and other factors. We term this kind of imitation learning ""imitation-from-observation,"" and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations in the same environment configuration, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show the effectiveness of our approach in learning a wide range of real-world robotic tasks modeled after common household chores from videos of a human demonstrator, including sweeping, ladling almonds, pushing objects as well as a number of tasks in simulation.",0
"When an explicit reward function is not available, autonomous systems can use imitation learning to acquire control policies by imitating an expert's demonstrations, typically a human operator. However, current imitation learning methods assume that the agent receives observation-action tuples, which is different from how humans and animals learn through imitation. Humans observe behaviors and then determine which actions will achieve the desired results, adjusting for changes in viewpoint, surroundings, object positions, and other factors. This approach is called ""imitation-from-observation,"" and we propose a new method that combines video prediction with context translation and deep reinforcement learning. This method removes the assumption that demonstrations must take place in the same environment configuration and allows for learning of skills that involve tool use simply by observing videos of human tool use. Our experiments show that this approach is effective in learning real-world robotic tasks, including sweeping, ladling almonds, pushing objects, and several simulation tasks based on everyday household chores.",1
"Inverse reinforcement learning (IRL) aims to explain observed strategic behavior by fitting reinforcement learning models to behavioral data. However, traditional IRL methods are only applicable when the observations are in the form of state-action paths. This assumption may not hold in many real-world modeling settings, where only partial or summarized observations are available. In general, we may assume that there is a summarizing function $\sigma$, which acts as a filter between us and the true state-action paths that constitute the demonstration. Some initial approaches to extending IRL to such situations have been presented, but with very specific assumptions about the structure of $\sigma$, such as that only certain state observations are missing. This paper instead focuses on the most general case of the problem, where no assumptions are made about the summarizing function, except that it can be evaluated. We demonstrate that inference is still possible. The paper presents exact and approximate inference algorithms that allow full posterior inference, which is particularly important for assessing parameter uncertainty in this challenging inference situation. Empirical scalability is demonstrated to reasonably sized problems, and practical applicability is demonstrated by estimating the posterior for a cognitive science RL model based on an observed user's task completion time only.",0
"The purpose of Inverse reinforcement learning (IRL) is to explain strategic behavior by fitting reinforcement learning models to observed behavioral data. However, traditional IRL methods are limited to cases where the observations are in the form of state-action paths, which is not always the case in real-world modeling settings where only partial or summarized observations are available. It is assumed that there is a summarizing function $\sigma$ that acts as a filter between the true state-action paths that constitute the demonstration and us. Some initial approaches have been proposed to extend IRL to such situations, but they make specific assumptions about the structure of $\sigma$, such as only certain state observations being missing. This paper focuses on the most general problem of the situation, where the summarizing function is evaluated without any assumptions made about its structure. The paper presents exact and approximate inference algorithms that enable full posterior inference, which is crucial for assessing parameter uncertainty in this challenging inference scenario. The empirical scalability of the proposed algorithms is demonstrated for reasonably sized problems, and their practical applicability is shown by estimating the posterior for a cognitive science RL model based on an observed user's task completion time.",1
"Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.",0
"Probabilistic models, such as scalable and adaptable Bayesian neural networks with latent variables, are capable of accommodating uncertainty in the estimation of network weights and can capture intricate noise patterns in data using latent variables. Our research demonstrates a technique for separating and analyzing uncertainty into epistemic and aleatoric components to aid in decision-making. This approach enables us to effectively identify informative points for functions with heteroscedastic and bimodal noise through active learning. With the decomposition, we also introduce a new risk-sensitive criterion for reinforcement learning that identifies policies that balance expected cost, model-bias and noise aversion.",1
"In this paper, we provide two new stable online algorithms for the problem of prediction in reinforcement learning, \emph{i.e.}, estimating the value function of a model-free Markov reward process using the linear function approximation architecture and with memory and computation costs scaling quadratically in the size of the feature set. The algorithms employ the multi-timescale stochastic approximation variant of the very popular cross entropy (CE) optimization method which is a model based search method to find the global optimum of a real-valued function. A proof of convergence of the algorithms using the ODE method is provided. We supplement our theoretical results with experimental comparisons. The algorithms achieve good performance fairly consistently on many RL benchmark problems with regards to computational efficiency, accuracy and stability.",0
"This paper presents two novel and reliable online algorithms for reinforcement learning prediction. These algorithms aim to estimate the value function of a model-free Markov reward process using the linear function approximation architecture. They incur memory and computation costs that increase quadratically with the feature set size. The algorithms utilize the multi-timescale stochastic approximation variant of the cross entropy optimization method, which is a model-based search technique that finds the global optimum of a real-valued function. We provide proof of convergence of the algorithms using the ODE method and supplement our theoretical findings with experimental comparisons. The algorithms exhibit consistent and efficient performance on multiple RL benchmark problems in terms of accuracy, stability, and computational efficiency.",1
"In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.",0
"Our work expands on recent developments in distributional reinforcement learning to create a versatile, adaptable, and cutting-edge DQN variant. Our approach utilizes quantile regression to estimate the complete quantile function for the state-action return distribution. This technique reconfigures a distribution over the sample space, resulting in an implicitly defined return distribution and a broad range of risk-aware policies. Our experiments on the ALE's 57 Atari 2600 games demonstrate superior results, and we employ our algorithm's implicitly defined distributions to investigate the impacts of risk-conscious policies in Atari games.",1
"We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.",0
"A novel reinforcement learning algorithm, named as Maximum aposteriori Policy Optimisation (MPO), is introduced in this study. The algorithm is based on the relative entropy objective and employs coordinate ascent for its implementation. Our derivation establishes a direct relationship between our approach and several existing methods. We have formulated two off-policy algorithms, which have been shown to be equally competitive with the current state-of-the-art in deep reinforcement learning. Specifically, in the case of continuous control, our method outperforms other methods in terms of sample efficiency, premature convergence, and robustness to hyperparameter settings, while achieving comparable or superior final performance.",1
"We explore the use of deep learning and deep reinforcement learning for optimization problems in transportation. Many transportation system analysis tasks are formulated as an optimization problem - such as optimal control problems in intelligent transportation systems and long term urban planning. Often transportation models used to represent dynamics of a transportation system involve large data sets with complex input-output interactions and are difficult to use in the context of optimization. Use of deep learning metamodels can produce a lower dimensional representation of those relations and allow to implement optimization and reinforcement learning algorithms in an efficient manner. In particular, we develop deep learning models for calibrating transportation simulators and for reinforcement learning to solve the problem of optimal scheduling of travelers on the network.",0
"The application of deep learning and deep reinforcement learning is investigated for optimization issues in transportation. Various transportation system analysis tasks are structured as optimization problems, including intelligent transportation system optimal control problems and long-term urban planning. These transportation models contain intricate input-output interactions and voluminous datasets, making them challenging to utilize in optimization scenarios. However, the use of deep learning metamodels can provide a reduced dimensional portrayal of these relationships, making it feasible to execute optimization and reinforcement learning algorithms effectively. Specifically, we construct deep learning models to calibrate transportation simulators and employ reinforcement learning to address the optimal scheduling of travelers on the network.",1
"Recent work has shown that reinforcement learning (RL) is a promising approach to control dynamical systems described by partial differential equations (PDE). This paper shows how to use RL to tackle more general PDE control problems that have continuous high-dimensional action spaces with spatial relationship among action dimensions. In particular, we propose the concept of action descriptors, which encode regularities among spatially-extended action dimensions and enable the agent to control high-dimensional action PDEs. We provide theoretical evidence suggesting that this approach can be more sample efficient compared to a conventional approach that treats each action dimension separately and does not explicitly exploit the spatial regularity of the action space. The action descriptor approach is then used within the deep deterministic policy gradient algorithm. Experiments on two PDE control problems, with up to 256-dimensional continuous actions, show the advantage of the proposed approach over the conventional one.",0
"Recent research has indicated that reinforcement learning (RL) is a promising method for regulating dynamical systems represented by partial differential equations (PDE). This article demonstrates how RL can be applied to address more generalized PDE control problems that have continuous high-dimensional action spaces with spatial relationships among action dimensions. The authors suggest the idea of action descriptors, which incorporate patterns among spatially-extended action dimensions and enable the agent to manage high-dimensional action PDEs. Theoretical proof suggests that this approach is more efficient in terms of sample usage when compared to a conventional approach that handles each action dimension independently and does not explicitly employ the spatial consistency of the action space. The action descriptor approach is integrated into the deep deterministic policy gradient algorithm. Experiments on two PDE control problems, which involve up to 256-dimensional continuous actions, demonstrate the superiority of the proposed approach over the conventional one.",1
"Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalise across diverse problems.",0
"The goal of Active Learning (AL) is to develop high-performance classifiers with minimal annotation costs by identifying the most advantageous subset of unlabeled instances to label. The significance of AL has led to numerous studies that have proposed various AL algorithms with different theoretical and intuitive foundations. Instead of relying on these traditional methods, we suggest that designing active learning algorithms should be approached as a meta-learning problem, where the best criterion is learned from data. We have developed a deep neural network that functions as an active learning algorithm, taking in the base learner state and unlabeled point set as input and predicting the next best point to annotate. By training this active query policy network using reinforcement learning, we can produce the best non-myopic policy for a given dataset. However, the main challenge lies in achieving learner generalization across heterogeneous datasets. To address this, we propose a multi-task dataset-embedding approach that enables the training of dataset-agnostic active learners. Our results demonstrate that AL algorithms developed using this approach can generalize effectively across a variety of problems.",1
"There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels---such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.",0
"It is becoming increasingly clear that converting supervised learning targets into soft targets can lead to substantial improvements in performance. Previous studies have mainly focused on classification tasks, where hard zero-one values are transformed into soft labels by adding label noise, incorporating ambiguity, or using distillation. Concurrently, there is some indication from reinforcement learning experiments in the regression setting that learning distributions can also enhance performance. Our research delves into the underlying reasons for this improvement in a regression setting. We propose a new method of distributional regression loss, which we find significantly enhances prediction accuracy. While there are several theories that suggest that reducing overfitting and improving representations could be responsible for this improvement, our results suggest an alternative hypothesis. We find that this loss is simpler to optimize, with gradients that behave better, leading to improved generalization. We provide theoretical support for this idea by characterizing the norm of the gradients of this loss.",1
"Modern reinforcement learning algorithms reach super-human performance on many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce Variational State Tabulation (VaST), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular model. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.",0
"Despite achieving super-human performance on numerous board and video games, modern reinforcement learning algorithms are sample inefficient. This means they require significantly more playing experience than humans to reach an equal level of performance. To address this issue, agents can build a model of the environment and use planning methods to update their policy. In this article, we present the Variational State Tabulation (VaST) approach, which maps high-dimensional state space environments, such as those with visual inputs, to an abstract tabular model. By utilizing the highly efficient Prioritized Sweeping with small backups planning method, VaST can be used to update state-action values. We demonstrate how VaST can quickly learn how to maximize rewards in tasks like 3D navigation and adapt efficiently to sudden changes in rewards or transition probabilities.",1
"In traditional reinforcement learning, an agent maximizes the reward collected during its interaction with the environment by approximating the optimal policy through the estimation of value functions. Typically, given a state s and action a, the corresponding value is the expected discounted sum of rewards. The optimal action is then chosen to be the action a with the largest value estimated by value function. However, recent developments have shown both theoretical and experimental evidence of superior performance when value function is replaced with value distribution in context of deep Q learning [1]. In this paper, we develop a new algorithm that combines advantage actor-critic with value distribution estimated by quantile regression. We evaluated this new algorithm, termed Distributional Advantage Actor-Critic (DA2C or QR-A2C) on a variety of tasks, and observed it to achieve at least as good as baseline algorithms, and outperforming baseline in some tasks with smaller variance and increased stability.",0
"The conventional approach to reinforcement learning involves an agent maximizing its reward by estimating the optimal policy through value function approximation. The value of a state-action pair is determined by the expected discounted sum of rewards and the optimal action is chosen based on the largest estimated value. However, recent research has shown that using value distribution instead of value function can lead to superior performance. In this study, we present a new algorithm called Distributional Advantage Actor-Critic (DA2C or QR-A2C), which combines advantage actor-critic with value distribution estimated by quantile regression. Our evaluation on various tasks revealed that DA2C performs at least as well as baseline algorithms, and outperforms them in some tasks with increased stability and smaller variance.",1
"We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma's Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.",0
"Our research investigates the effective use of expert feedback in learning policies for sequential decision-making. Our focus is on addressing the challenges posed by sparse rewards and long time horizons in reinforcement learning. To tackle these challenges, we introduce a new algorithmic framework, called hierarchical guidance, which exploits the hierarchical structure of the problem to facilitate different modes of expert interaction. Our framework can integrate imitation learning (IL) and reinforcement learning (RL) at different levels, achieving significant reductions in both expert effort and exploration costs. We use long-horizon benchmarks, such as Montezuma's Revenge, to demonstrate that our approach outperforms hierarchical RL in terms of learning speed and standard IL in terms of label-efficiency. Additionally, we provide a theoretical analysis of the labeling cost for certain instantiations of our framework.",1
"In model-based reinforcement learning it is typical to decouple the problems of learning the dynamics model and learning the reward function. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. It is not clear a priori what value the reward function should assign to such states. This paper presents a novel error bound that accounts for the reward model's behavior in states sampled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned. Empirically, this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed.",0
"Typically, in model-based reinforcement learning, the learning of the dynamics model and the reward function are separate issues. However, if the dynamics model is incorrect, it may produce states that would not normally occur in the real environment. It is unclear how to assign a value to such states for the reward function. This paper introduces a new error bound that considers the behavior of the reward model in states that are sampled from the model. This bound is applied to enhance the existing Hallucinated DAgger-MC algorithm, which provides theoretical performance assurances in deterministic MDPs and does not presume that a perfect model can be learned. Empirically, this method of reward learning can greatly enhance control performance in cases where the dynamics model is inaccurate.",1
"We propose a fully automatic method to find standardized view planes in 3D image acquisitions. Standard view images are important in clinical practice as they provide a means to perform biometric measurements from similar anatomical regions. These views are often constrained to the native orientation of a 3D image acquisition. Navigating through target anatomy to find the required view plane is tedious and operator-dependent. For this task, we employ a multi-scale reinforcement learning (RL) agent framework and extensively evaluate several Deep Q-Network (DQN) based strategies. RL enables a natural learning paradigm by interaction with the environment, which can be used to mimic experienced operators. We evaluate our results using the distance between the anatomical landmarks and detected planes, and the angles between their normal vector and target. The proposed algorithm is assessed on the mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and 4.84mm, respectively.",0
"A method is proposed to automatically identify standardized view planes in 3D image acquisitions. Standard view images are crucial in clinical practice for performing biometric measurements from similar anatomical regions, but are often restricted to the native orientation of a 3D image acquisition, making it tiresome and dependent on the operator to navigate through target anatomy to find the required view plane. To address this issue, a multi-scale reinforcement learning (RL) agent framework is employed, with Deep Q-Network (DQN) based strategies extensively evaluated. The RL approach mimics experienced operators by learning through interaction with the environment. The accuracy of the proposed algorithm is evaluated using the distance between anatomical landmarks and detected planes, as well as the angles between their normal vector and target. The mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI are assessed, with accuracy achieved being 1.53mm, 1.98mm, and 4.84mm, respectively.",1
"The balance between exploration and exploitation is a key problem for reinforcement learning methods, especially for Q-learning. In this paper, a fidelity-based probabilistic Q-learning (FPQL) approach is presented to naturally solve this problem and applied for learning control of quantum systems. In this approach, fidelity is adopted to help direct the learning process and the probability of each action to be selected at a certain state is updated iteratively along with the learning process, which leads to a natural exploration strategy instead of a pointed one with configured parameters. A probabilistic Q-learning (PQL) algorithm is first presented to demonstrate the basic idea of probabilistic action selection. Then the FPQL algorithm is presented for learning control of quantum systems. Two examples (a spin- 1/2 system and a lamda-type atomic system) are demonstrated to test the performance of the FPQL algorithm. The results show that FPQL algorithms attain a better balance between exploration and exploitation, and can also avoid local optimal policies and accelerate the learning process.",0
"Reinforcement learning methods, particularly Q-learning, face a crucial challenge of maintaining a balance between exploration and exploitation. To address this issue, this paper introduces an approach called fidelity-based probabilistic Q-learning (FPQL), which is utilized for learning control of quantum systems. The FPQL approach utilizes fidelity to guide the learning process, and iteratively updates the probability of selecting each action at a given state, resulting in a natural exploration strategy rather than a targeted one with preset parameters. Initially, a probabilistic Q-learning (PQL) algorithm is presented to illustrate the fundamental concept of probabilistic action selection. Subsequently, the FPQL algorithm is introduced for learning control of quantum systems, and two examples (a spin-1/2 system and a lambda-type atomic system) are provided to evaluate the performance of the FPQL algorithm. The outcomes show that the FPQL algorithm achieves a superior balance between exploration and exploitation, prevents local optimal policies, and enhances the learning process.",1
"An appealing property of the natural gradient is that it is invariant to arbitrary differentiable reparameterizations of the model. However, this invariance property requires infinitesimal steps and is lost in practical implementations with small but finite step sizes. In this paper, we study invariance properties from a combined perspective of Riemannian geometry and numerical differential equation solving. We define the order of invariance of a numerical method to be its convergence order to an invariant solution. We propose to use higher-order integrators and geodesic corrections to obtain more invariant optimization trajectories. We prove the numerical convergence properties of geodesic corrected updates and show that they can be as computationally efficient as plain natural gradient. Experimentally, we demonstrate that invariance leads to faster optimization and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.",0
"The natural gradient has an attractive feature of being unaffected by arbitrary differentiable reparameterizations of the model, but this property requires infinitesimal steps and is not practical with small but finite step sizes. This study explores the invariance of numerical methods using a combined perspective of Riemannian geometry and numerical differential equation solving. The order of invariance of a numerical method is defined as its convergence order to an invariant solution. To achieve more invariant optimization trajectories, higher-order integrators and geodesic corrections are proposed. The numerical convergence properties of geodesic corrected updates are proven, and they are shown to be as computationally efficient as plain natural gradient. Experimental results demonstrate that invariance leads to faster optimization, and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.",1
"In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",0
"In this study, we approach hierarchical reinforcement learning from the perspective of representation learning. Our goal is to solve the problem of learning lower layers in a hierarchy by transforming it into the problem of learning trajectory-level generative models. To achieve this, we have developed a model called SeCTAR, which is inspired by variational autoencoders and learns continuous latent representations of trajectories. SeCTAR consists of a latent-conditioned policy and a latent-conditioned model, which work together to generate trajectories that match each other given the same latent input. This allows the model to predict the outcome of closed loop policy behavior and provides a built-in prediction mechanism. To perform hierarchical RL with SeCTAR, we have proposed a novel algorithm that combines model-based planning in the learned latent space with an unsupervised exploration objective. Our experiments show that SeCTAR is highly effective at reasoning over long horizons with sparse rewards, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",1
"We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.",0
"We have developed a novel transformation technique for efficient neural architecture search that preserves function. This transformation enables the reuse of previously trained networks and successful architectures, resulting in better sample efficiency. Our aim is to overcome the limitations of current network transformation operations that only modify layer-level architecture, such as adding or pruning filters, which fail to change connection path topology. Our proposed path-level transformation operations allow the meta-controller to modify the path topology of the network, maintaining the benefits of weight reuse while efficiently designing complex structures, such as Inception models. We also propose a bidirectional tree-structured reinforcement learning meta-controller that explores a versatile tree-structured architecture space, similar to multi-branch architectures. Our experiments on limited computational resources with image classification datasets show improved parameter efficiency and better test results, demonstrating the transferability and effectiveness of our designed architectures (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting).",1
"The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search.   In this paper, we describe how we interface GVGAI to the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of GVGAI games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions.",0
"The GVGAI competition and its framework are utilized to evaluate AI algorithms on various games coded in a specific language. Although the competition has garnered much attention, it has concentrated on online planning, which permits the usage of algorithms like Monte Carlo Tree Search. This study details the process of connecting GVGAI to the OpenAI Gym environment, a prevalent method of linking agents to reinforcement learning problems. Through this interface, we assess how prevalent implementations of various deep reinforcement learning algorithms perform on multiple GVGAI games. Additionally, we analyze the outcomes to provide an initial indication of the games' comparative difficulty and their similarity to those in the Arcade Learning Environment under comparable circumstances.",1
"Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.",0
"There is a high demand for reinforcement learning methods that can handle real-world sequential decision making problems that are partially observable and have an unknown environment model. This is due to the fact that only incomplete and noisy observations are available. This paper proposes a solution called deep variational reinforcement learning (DVRL), which involves an inductive bias that enables an agent to learn a generative model of the environment and make effective inferences in that model. An n-step approximation to the evidence lower bound (ELBO) is developed to train the model jointly with the policy, resulting in a suitable latent state representation for the control task. The proposed method is compared to previous approaches that rely on recurrent neural networks to encode the past in experiments on Mountain Hike and flickering Atari, and it is demonstrated that DVRL outperforms them.",1
"Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.",0
"Various applications have yielded exciting results through deep learning on graph structures. Despite this, there has been little attention given to the robustness of these models, in contrast to the extensive research on image or text adversarial attack and defense. In this paper, we concentrate on adversarial attacks that deceive the model by modifying the combinatorial structure of the data. Our research presents a reinforcement learning based attack method that learns a generalizable attack policy using only prediction labels from the target classifier. Additionally, we provide variants of genetic algorithms and gradient methods in scenarios where prediction confidence or gradients are available. By using both synthetic and real-world data, we demonstrate that a family of Graph Neural Network models are vulnerable to these attacks in both graph-level and node-level classification tasks. Furthermore, we show that these attacks can be used to diagnose the learned classifiers.",1
"In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.",0
"In various situations, a group of agents must coordinate their actions in a decentralised manner. However, in simulated or laboratory settings, agents can be trained centrally with access to global state information and no communication constraints. To take advantage of centralised learning, joint action-values conditioned on extra state information can be learned, but it is unclear how to extract decentralised policies. Our solution is QMIX, a novel method that can train decentralised policies in a centralised end-to-end fashion. QMIX estimates joint action-values using a network that combines per-agent values that only condition on local observations. We ensure that the joint-action value is monotonic in the per-agent values, allowing for easy maximisation of the joint action-value in off-policy learning and consistency between centralised and decentralised policies. We evaluate QMIX on challenging StarCraft II micromanagement tasks and show that it outperforms existing value-based multi-agent reinforcement learning methods.",1
"Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",0
"Deep reinforcement learning (RL) faces a significant challenge in domain adaptation. Obtaining data in many scenarios is difficult, leading to the need for agents to learn a source policy in an environment where data is easily available. The hope is that this policy will generalize well to the target domain. To address this problem, we present DARLA, a multi-stage RL agent that prioritizes learning to see before acting. DARLA's vision is based on acquiring a disentangled representation of the observed environment. Once DARLA can see, it can acquire source policies that are robust to domain shifts, even without access to the target domain. DARLA outperforms traditional baselines in zero-shot domain adaptation scenarios in a range of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C, and EC).",1
"We propose to learn a curriculum or a syllabus for supervised learning and deep reinforcement learning with deep neural networks by an attachable deep neural network, called ScreenerNet. Specifically, we learn a weight for each sample by jointly training the ScreenerNet and the main network in an end-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor requires to remember the past learning history. We show the networks augmented with the ScreenerNet achieve early convergence with better accuracy than the state-of-the-art curricular learning methods in extensive experiments using three popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a Cart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend other curriculum learning methods such as Prioritized Experience Replay (PER) for further accuracy improvement.",0
"Our proposal involves utilizing an attachable deep neural network, known as ScreenerNet, to acquire a curriculum or syllabus for deep reinforcement learning and supervised learning with deep neural networks. The ScreenerNet assists in training the main network by determining an appropriate weight for each sample through an end-to-end self-paced approach. In contrast to other methods, the ScreenerNet is not affected by sampling bias and does not require recollection of past learning experiences. Our experiments using popular vision datasets, including MNIST, CIFAR10, and Pascal VOC2012, as well as a Cart-pole task utilizing Deep Q-learning, demonstrate that the ScreenerNet augmented networks achieve early convergence and better accuracy than the current state-of-the-art curricular learning techniques. Furthermore, the ScreenerNet has the potential to enhance other curriculum learning methods, such as Prioritized Experience Replay (PER), to further improve accuracy.",1
"This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be not optimal. Compared to previous works that decouple agents in the game by assuming optimality in expert strategies, we introduce a new objective function that directly pits experts against Nash Equilibrium strategies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. In our setting the model and algorithm do not decouple by agent. In order to find Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In our numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to previous approaches using tabular representations. Moreover, with sub-optimal expert demonstrations our algorithms recover both reward functions and strategies with good quality.",0
"This article examines the issue of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are not optimal. The study introduces a new objective function that directly pits experts against Nash Equilibrium strategies, as opposed to prior research which assumed optimality in expert strategies and decoupled agents in the game. The article also presents an algorithm to solve for the reward function in the context of inverse reinforcement learning, utilizing deep neural networks as model approximations. The model and algorithm are not decoupled by agent in this setting. To address Nash Equilibrium in large-scale games, an adversarial training algorithm for zero-sum stochastic games is proposed, with the theoretical appeal of non-existence of local optima in its objective function. Numerical experiments show that the Nash Equilibrium and inverse reinforcement learning algorithms are effective in addressing games that were not previously possible with tabular representations. Additionally, with sub-optimal expert demonstrations, the algorithms recover both reward functions and strategies with good quality.",1
"Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework {\em PEORL} that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision-making in a dynamic environment with uncertainties.   Symbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL.",0
"Intelligent autonomous agents have been constructed using both reinforcement learning and symbolic planning. However, reinforcement learning necessitates a vast amount of experience gained from real-world interactions, making it impractical. On the other hand, symbolic planning relies on manually crafted symbolic knowledge that may not be adaptable to changes in the domain. The paper proposes a unified framework named PEORL, which integrates hierarchical reinforcement learning with symbolic planning to handle decision-making in a dynamic environment with uncertainties. The agent's task execution and learning are directed by symbolic plans, and the gained experience is used to enhance planning. This approach accelerates policy search and strengthens symbolic plans in intricate domains. The benchmark domains of HRL have been used to test the framework.",1
"When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov's smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm's sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.",0
"For decades, the challenge of solving the Bellman optimality equation with stability guarantees using function approximation has remained unresolved in reinforcement learning. This is due to the fact that the Bellman operator can potentially result in expansion and cause oscillating and divergent behavior of commonly used algorithms like Q-learning. In this study, we address this issue by reformulating the Bellman equation into a novel primal-dual optimization problem with Nesterov's smoothing technique and the Legendre-Fenchel transformation. Our proposed algorithm, Smoothed Bellman Error Embedding, can be applied to any differentiable function class and provides the first convergence guarantee for general nonlinear function approximation. We also analyze the sample complexity of the algorithm and compare it favorably to state-of-the-art baselines in various benchmark control problems.",1
"We introduce Mix&Match (M&M) - a training framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents. In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally. We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods. (2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.",0
"Introducing Mix&Match (M&M) - a framework for training RL agents that are difficult or slow to train. M&M utilizes a unique process to automatically create a curriculum for agents, allowing for the gradual training of more complex agents by building on the solutions found by simpler ones. Unlike traditional curriculum learning approaches, M&M does not modify tasks or environments presented, but instead alters how the policy is represented internally. Our method has been proven successful in three experimental setups: (1) training an agent to control over 700 actions in a challenging 3D first-person task, resulting in faster training and better final performance than traditional methods. (2) progressing through a curriculum of architectural variants defining an agent's internal state. (3) improving agent performance in a multitask setting.",1
Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.,0
"The presence of exogenous state variables and rewards may impede reinforcement learning by introducing uncontrolled fluctuations into the reward signal. We provide a formal definition for exogenous state variables and rewards and establish the circumstances under which an MDP with exogenous state can be separated into an exogenous Markov Reward Process containing only the exogenous state+reward and an endogenous Markov Decision Process based only on endogenous rewards. Moreover, we determine a variance-covariance requirement for Monte Carlo policy evaluation on the endogenous MDP to be accelerated as compared to utilizing the full MDP. This acceleration is likely to apply to all RL algorithms. We create two algorithms to identify the exogenous variables and test them on various MDPs. Our findings demonstrate that the algorithms are feasible and can substantially hasten reinforcement learning.",1
"Recent developments have established the vulnerability of deep reinforcement learning to policy manipulation attacks via intentionally perturbed inputs, known as adversarial examples. In this work, we propose a technique for mitigation of such attacks based on addition of noise to the parameter space of deep reinforcement learners during training. We experimentally verify the effect of parameter-space noise in reducing the transferability of adversarial examples, and demonstrate the promising performance of this technique in mitigating the impact of whitebox and blackbox attacks at both test and training times.",0
"Deep reinforcement learning has been found to be susceptible to policy manipulation attacks through the use of deliberately distorted inputs, also known as adversarial examples. To counteract this, we present a method that involves adding noise to the parameter space of deep reinforcement learners during their training, which helps to reduce the transferability of adversarial examples. Our experiments confirm that this approach is effective in mitigating the effects of whitebox and blackbox attacks during both training and testing, and we believe that it holds great promise for improving the overall performance of deep reinforcement learning systems.",1
"While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a ""relational inductive bias"": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object- and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more naive approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.",0
"Although deep learning systems are highly proficient in tasks such as language processing, object classification, and gameplay, they struggle with constructing or modifying complex systems like a tower of blocks. The authors propose that these systems lack a ""relational inductive bias,"" which is the ability to reason about inter-object relationships and make decisions using a structured description of a scene. To test this theory, the authors developed a task that involves gluing pairs of blocks together to stabilize a tower and measured human performance. They then introduced a deep reinforcement learning agent that used object- and relation-centric scene and policy representations to complete the task. The results showed that the structured representations allowed the agent to outperform both humans and less sophisticated approaches, indicating that relational inductive bias is a crucial element in solving structured reasoning problems and building intelligent, adaptable machines.",1
"Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.",0
"Decades ago, theoretical and empirical results using tabular representations and linear function approximators shaped our understanding of reinforcement learning (RL). These results suggested that temporal differencing (TD) RL methods were better than direct Monte Carlo estimation (MC). However, in modern deep RL, which involves complex environments and deep nonlinear models, we need to re-evaluate TD's role. To do this, we've created environments that control for specific factors that affect performance. Our findings show that, when comparing TD with infinite-horizon MC, we can replicate classic results in modern settings. However, finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. Therefore, deep RL can use MC as a viable alternative to TD.",1
"Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-task in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.",0
"The task of determining the minimum or maximum mean from a finite set of distributions is crucial in fields such as game tree search, planning, and reinforcement learning. To formalize this task, we propose sequentially testing the comparison of the minimum mean from the set of distributions to a given threshold. We have developed improved non-asymptotic lower bounds that demonstrate how different sampling behavior is necessary for low and high true minimums to achieve optimality. Existing policies such as Thompson Sampling and Lower Confidence Bounds policy are only effective for one of these cases. To address this, we introduce a new approach called Murphy Sampling that is optimal for both low and high true minimums. Additionally, we have designed advanced self-normalized deviation inequalities that enable more aggressive stopping rules. Our theoretical guarantees are supported by experiments that demonstrate the superior performance of Murphy Sampling in practical applications.",1
"The research on deep reinforcement learning which estimates Q-value by deep learning has been attracted the interest of researchers recently. In deep reinforcement learning, it is important to efficiently learn the experiences that an agent has collected by exploring environment. We propose NEC2DQN that improves learning speed of a poor sample efficiency algorithm such as DQN by using good one such as NEC at the beginning of learning. We show it is able to learn faster than Double DQN or N-step DQN in the experiments of Pong.",0
"Recently, researchers have been focusing on the study of deep reinforcement learning, where Q-value is estimated through deep learning. Learning the experiences that an agent has gathered by exploring the environment efficiently is crucial in this area. Our proposal, NEC2DQN, enhances the learning speed of a low sample efficiency algorithm like DQN by implementing a superior one such as NEC at the start of the learning process. Through our experiments with Pong, we demonstrate that NEC2DQN can learn at a faster rate than Double DQN or N-step DQN.",1
"The deep reinforcement learning method usually requires a large number of training images and executing actions to obtain sufficient results. When it is extended a real-task in the real environment with an actual robot, the method will be required more training images due to complexities or noises of the input images, and executing a lot of actions on the real robot also becomes a serious problem. Therefore, we propose an extended deep reinforcement learning method that is applied a generative model to initialize the network for reducing the number of training trials. In this paper, we used a deep q-network method as the deep reinforcement learning method and a deep auto-encoder as the generative model. We conducted experiments on three different tasks: a cart-pole game, an atari game, and a real-game with an actual robot. The proposed method trained efficiently on all tasks than the previous method, especially 2.5 times faster on a task with real environment images.",0
"To obtain satisfactory results, the deep reinforcement learning approach typically necessitates an abundance of training images and actions executed. In the event of extending this method to real-world tasks with an actual robot, obtaining adequate outcomes becomes more challenging due to the input images' complexities or noises, and performing numerous actions on the real robot poses a significant issue. To address this problem, we propose an advanced deep reinforcement learning method that employs a generative model to initialize the network and diminish the number of training attempts. Our approach employs a deep q-network method for deep reinforcement learning and a deep auto-encoder for the generative model. We conducted experiments on three distinct tasks: a cart-pole game, an atari game, and a real-game with an actual robot. Our proposed method exhibited more efficient training across all tasks than the previous method, particularly 2.5 times faster on a task involving real environment images.",1
"In recent years, deep reinforcement learning has been shown to be adept at solving sequential decision processes with high-dimensional state spaces such as in the Atari games. Many reinforcement learning problems, however, involve high-dimensional discrete action spaces as well as high-dimensional state spaces. This paper considers entropy bonus, which is used to encourage exploration in policy gradient. In the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. We apply these estimators to several models for the parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we test our algorithms on two environments: a multi-hunter multi-rabbit grid game and a multi-agent multi-arm bandit problem. The results show that our entropy estimators substantially improve performance with marginal additional computational cost.",0
"Recently, deep reinforcement learning has demonstrated its proficiency in tackling sequential decision-making processes with high-dimensional state spaces, such as those found in Atari games. Nevertheless, many reinforcement learning issues involve high-dimensional discrete action spaces, along with high-dimensional state spaces. This study explores the use of an entropy bonus to encourage policy gradient exploration. In instances where action spaces are high-dimensional, computing entropy and its gradient necessitates enumerating all actions in the action space and performing forward and backpropagation for each action, which may not be practical. We present several novel, unbiased estimators for the entropy bonus and its gradient. These estimators are applied to various models for parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we evaluate our algorithms on two environments: a multi-hunter multi-rabbit grid game and a multi-agent multi-arm bandit problem. The outcomes indicate that our entropy estimators can significantly improve performance with only a marginal increase in computational cost.",1
"Episodic memory is a psychology term which refers to the ability to recall specific events from the past. We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful. Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states. The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on. Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory. Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.",0
"The term ""episodic memory"" in psychology refers to the ability to recollect specific past events. One benefit of this memory type is its ability to assign credit to a particular state when recalled information is deemed useful. Taking inspiration from this, and the growing trend of external memory mechanisms in deep learning systems to handle long-term dependencies, we propose a new algorithm that employs a reservoir sampling technique to maintain an external memory of a fixed number of past states. This algorithm enables a deep reinforcement learning agent to learn online and selectively remember states that prove to be valuable for future recollection. Importantly, this method allows for efficient online computation of gradient estimates concerning the external memory's write process. Thus, unlike previous external memory mechanisms, it is practical to use in an online reinforcement learning environment.",1
"We study active object tracking, where a tracker takes as input the visual observation (i.e., frame sequence) and produces the camera control signal (e.g., move forward, turn left, etc.). Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many expensive trial-and-errors in realworld. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-toaction prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.",0
"The focus of our research is on active object tracking, whereby the tracker utilizes visual observations (i.e. frame sequences) to determine the camera control signal (such as moving forward, turning left, etc.). Typically, traditional methods tackle tracking and camera control separately, which poses a challenge in tuning them jointly. Additionally, this method requires extensive human labeling efforts and expensive trial-and-error processes in the real world. To address these problems, we propose an end-to-end solution using deep reinforcement learning, wherein a ConvNet-LSTM function approximator is utilized for direct frame-to-action prediction. In this paper, we also introduce an environment augmentation technique and a customized reward function, both of which are crucial for successful training. Our tracker, trained in simulators such as ViZDoom and Unreal Engine, exhibits good generalization by tracking unseen objects, moving paths, appearances, backgrounds, and distracting objects. It can also restore tracking even when the target is lost occasionally. Furthermore, based on our experiments on the VOT dataset, we find that the tracking ability obtained solely from simulators has the potential to transfer to real-world scenarios.",1
"Scheduling the transmission of time-sensitive data to multiple users over error-prone communication channels is studied with the goal of minimizing the long-term average age of information (AoI) at the users under a constraint on the average number of transmissions at the source node. After each transmission, the source receives an instantaneous ACK/NACK feedback from the intended receiver and decides on what time and to which user to transmit the next update. The optimal scheduling policy is first studied under different feedback mechanisms when the channel statistics are known; in particular, the standard automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols are considered. Then a reinforcement learning (RL) approach is introduced, which does not assume any a priori information on the random processes governing the channel states. Different RL methods are verified and compared through numerical simulations.",0
"The aim of this study is to minimize the long-term average age of information (AoI) at multiple users when transmitting time-sensitive data over communication channels with errors. The source node is required to adhere to an average number of transmissions constraint. The source node receives feedback in the form of ACK/NACK from the intended receiver after each transmission and decides when and to whom to transmit the next update. The optimal scheduling policy is studied using various feedback mechanisms when the channel statistics are known, such as the automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols. Additionally, a reinforcement learning (RL) approach is introduced, which requires no a priori information on the random processes governing the channel states. Numerical simulations are performed to compare and verify different RL methods.",1
"In recent years an increasing number of researchers and practitioners have been suggesting algorithms for large-scale neural network architecture search: genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. None of them, however, demonstrated high-performance without training new experiments in the presence of unseen datasets. We propose a new deep neural network accuracy predictor, that estimates in fractions of a second classification performance for unseen input datasets, without training. In contrast to previously proposed approaches, our prediction is not only calibrated on the topological network information, but also on the characterization of the dataset-difficulty which allows us to re-tune the prediction without any training. Our predictor achieves a performance which exceeds 100 networks per second on a single GPU, thus creating the opportunity to perform large-scale architecture search within a few minutes. We present results of two searches performed in 400 seconds on a single GPU. Our best discovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for CIFAR-100, verified by training. These networks are performance competitive with other automatically discovered state-of-the-art networks however we only needed a small fraction of the time to solution and computational resources.",0
"Recently, an increasing number of researchers and practitioners have proposed algorithms for large-scale neural network architecture search, including genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. However, none of these approaches have shown high performance without the need for new experiments when presented with unseen datasets. To address this issue, we introduce a new deep neural network accuracy predictor that can estimate classification performance for unseen input datasets in fractions of a second, without requiring any training. Unlike previous approaches, our predictor is calibrated not only on the topological network information but also on the dataset difficulty characterization, allowing us to re-tune the prediction without any additional training. Our predictor can perform over 100 networks per second on a single GPU, making it possible to conduct large-scale architecture search within a few minutes. We demonstrate the effectiveness of our approach by performing two searches in 400 seconds on a single GPU. Our best discovered networks achieve high accuracy rates of 93.67% for CIFAR-10 and 81.01% for CIFAR-100, which are competitive with other state-of-the-art networks that were discovered automatically. However, our approach requires only a small fraction of the time and computational resources.",1
Self-play is an unsupervised training procedure which enables the reinforcement learning agents to explore the environment without requiring any external rewards. We augment the self-play setting by providing an external memory where the agent can store experience from the previous tasks. This enables the agent to come up with more diverse self-play tasks resulting in faster exploration of the environment. The agent pretrained in the memory augmented self-play setting easily outperforms the agent pretrained in no-memory self-play setting.,0
"The unsupervised training technique of self-play allows reinforcement learning agents to navigate the environment without relying on external rewards. To enhance this approach, we introduce an external memory that enables the agent to save past experiences and generate more varied self-play tasks, leading to quicker exploration of the environment. As a result, the agent trained with the memory-augmented self-play method performs significantly better than the agent trained without memory augmentation.",1
"Most artificial intelligence models have limiting ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.",0
"Artificial intelligence models often struggle to quickly solve new tasks without losing previously learned knowledge. However, continual learning has emerged as a potential solution to this issue by allowing models to learn tasks sequentially. This study proposes a new approach to continual learning called Reinforced Continual Learning, which uses advanced reinforcement learning strategies to find the optimal neural architecture for each new task. This method effectively prevents catastrophic forgetting and performs well on new tasks. Experiments on sequential classification tasks using MNIST and CIFAR-100 datasets demonstrate that this approach outperforms other existing continual learning methods for deep networks.",1
"Much attention has been devoted recently to the development of machine learning algorithms with the goal of improving treatment policies in healthcare. Reinforcement learning (RL) is a sub-field within machine learning that is concerned with learning how to make sequences of decisions so as to optimize long-term effects. Already, RL algorithms have been proposed to identify decision-making strategies for mechanical ventilation, sepsis management and treatment of schizophrenia. However, before implementing treatment policies learned by black-box algorithms in high-stakes clinical decision problems, special care must be taken in the evaluation of these policies.   In this document, our goal is to expose some of the subtleties associated with evaluating RL algorithms in healthcare. We aim to provide a conceptual starting point for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new ways of treating patients. In the following, we describe how choices about how to summarize a history, variance of statistical estimators, and confounders in more ad-hoc measures can result in unreliable, even misleading estimates of the quality of a treatment policy. We also provide suggestions for mitigating these effects---for while there is much promise for mining observational health data to uncover better treatment policies, evaluation must be performed thoughtfully.",0
"Recently, there has been a significant focus on improving treatment policies in healthcare by developing machine learning algorithms. Reinforcement learning (RL) is a subfield of machine learning that aims to optimize long-term effects by learning how to make a sequence of decisions. RL algorithms have been proposed for the treatment of mechanical ventilation, sepsis management, and schizophrenia. However, before implementing any policies learned by black-box algorithms in high-stakes clinical decision-making, it is crucial to evaluate these policies carefully. Our objective is to highlight the complexities associated with evaluating RL algorithms in healthcare and provide a conceptual foundation for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new patient treatment methods. We explain how choices regarding summarizing a history, variance of statistical estimators, and confounders can potentially result in unreliable and misleading estimates of treatment policy quality. We also advise on how to mitigate these effects, as while there is potential for mining observational health data to uncover better treatment policies, careful evaluation is essential.",1
"We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results.",0
"A new method for deriving updates for policy gradients in reinforcement learning is proposed using Fourier analysis. This approach transforms integrals associated with expected policy gradients into multiplications by recasting them as convolutions. Analytical solutions are obtained which capture the low variance benefits of EPG in numerous settings. Trigonometric and radial basis functions are utilized for the critic, both of which have the universal approximation property. Policy selection can be nearly arbitrary, encompassing hybrid continuous-discrete probability distributions and mixtures. Additionally, a general family of sample-based estimators for stochastic policy gradients is derived, unifying existing approximation outcomes. It is believed that this technique has the potential to shape the next generation of policy gradient techniques, bolstered by analytical findings.",1
"Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.",0
"New perspectives in chemical synthesis can be achieved through deep generative models for graph-structured data. By using differentiable models to generate molecular graphs, the need for costly search procedures in the extensive and discrete space of chemical structures can be avoided. MolGAN is introduced as an implicit, likelihood-free generative model for small molecular graphs. It operates on graph-structured data, eliminating the need for expensive graph matching procedures or node ordering heuristics. Furthermore, our approach combines generative adversarial networks with reinforcement learning to encourage the generation of molecules with specific desired chemical properties. Our experiments on the QM9 chemical database reveal that our model can generate nearly 100% valid compounds. MolGAN outperforms recent proposals that use string-based (SMILES) representations of molecules and a likelihood-based method that directly generates graphs, despite being susceptible to mode collapse.",1
"The question of how to explore, i.e., take actions with uncertain outcomes to learn about possible future rewards, is a key question in reinforcement learning (RL). Here, we show a surprising result: We show that Q-learning with nonlinear Q-function and no explicit exploration (i.e., a purely greedy policy) can learn several standard benchmark tasks, including mountain car, equally well as, or better than, the most commonly-used $\epsilon$-greedy exploration. We carefully examine this result and show that both the depth of the Q-network and the type of nonlinearity are important to induce such deterministic exploration.",0
"A fundamental inquiry in reinforcement learning (RL) pertains to how to conduct actions with uncertain outcomes to gain insight into prospective rewards. Our study reveals a remarkable finding: employing Q-learning with a nonlinear Q-function and a solely greedy policy, without explicit exploration, can perform just as effectively, or even better, than the widely utilized $\epsilon$-greedy exploration, in various standard benchmark tasks, such as mountain car. We meticulously scrutinize this outcome and demonstrate that both the depth of the Q-network and the nonlinearity type play a crucial role in inducing such deterministic exploration.",1
"Despite significant advances in the field of deep Reinforcement Learning (RL), today's algorithms still fail to learn human-level policies consistently over a set of diverse tasks such as Atari 2600 games. We identify three key challenges that any algorithm needs to master in order to perform well on all games: processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently. In this paper, we propose an algorithm that addresses each of these challenges and is able to learn human-level policies on nearly all Atari games. A new transformed Bellman operator allows our algorithm to process rewards of varying densities and scales; an auxiliary temporal consistency loss allows us to train stably using a discount factor of $\gamma = 0.999$ (instead of $\gamma = 0.99$) extending the effective planning horizon by an order of magnitude; and we ease the exploration problem by using human demonstrations that guide the agent towards rewarding states. When tested on a set of 42 Atari games, our algorithm exceeds the performance of an average human on 40 games using a common set of hyper parameters. Furthermore, it is the first deep RL algorithm to solve the first level of Montezuma's Revenge.",0
"Despite significant advancements, today's deep Reinforcement Learning (RL) algorithms still struggle to consistently learn human-level policies across a diverse set of tasks, including Atari 2600 games. We have identified three key challenges that any algorithm must overcome to perform well on all games: processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently. Our proposed algorithm addresses each of these challenges and has the ability to learn human-level policies on almost all Atari games. By using a new transformed Bellman operator, our algorithm can process rewards of varying densities and scales. Additionally, an auxiliary temporal consistency loss allows for stable training using a discount factor of $\gamma = 0.999$, increasing the effective planning horizon by an order of magnitude. We also ease the exploration problem by utilizing human demonstrations that direct the agent towards rewarding states. When tested on 42 Atari games, our algorithm surpasses the performance of an average human on 40 games using a common set of hyperparameters. It is also the first deep RL algorithm to solve the first level of Montezuma's Revenge.",1
"We show that when a third party, the adversary, steps into the two-party setting (agent and operator) of safely interruptible reinforcement learning, a trade-off has to be made between the probability of following the optimal policy in the limit, and the probability of escaping a dangerous situation created by the adversary. So far, the work on safely interruptible agents has assumed a perfect perception of the agent about its environment (no adversary), and therefore implicitly set the second probability to zero, by explicitly seeking a value of one for the first probability. We show that (1) agents can be made both interruptible and adversary-resilient, and (2) the interruptibility can be made safe in the sense that the agent itself will not seek to avoid it. We also solve the problem that arises when the agent does not go completely greedy, i.e. issues with safe exploration in the limit. Resilience to perturbed perception, safe exploration in the limit, and safe interruptibility are the three pillars of what we call \emph{virtuously safe reinforcement learning}.",0
"In safely interruptible reinforcement learning, the presence of an adversary creates a trade-off between the likelihood of following the optimal policy and the likelihood of avoiding danger. Previous research on safely interruptible agents has assumed no adversary and focused solely on achieving a probability of one for the first likelihood. However, we demonstrate that interruptibility can be made safe and agents can be made both interruptible and adversary-resilient. Additionally, we address the issue of safe exploration in the limit. Our approach, which we call ""virtuously safe reinforcement learning,"" is built on the three pillars of resilience to perturbed perception, safe exploration in the limit, and safe interruptibility.",1
"In this paper, we propose to combine imitation and reinforcement learning via the idea of reward shaping using an oracle. We study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner's planning horizon as function of its accuracy: a globally optimal oracle can shorten the planning horizon to one, leading to a one-step greedy Markov Decision Process which is much easier to optimize, while an oracle that is far away from the optimality requires planning over a longer horizon to achieve near-optimal performance. Hence our new insight bridges the gap and interpolates between imitation learning and reinforcement learning. Motivated by the above mentioned insights, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximize the total reshaped reward over a finite planning horizon when the oracle is sub-optimal. We experimentally demonstrate that a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.",0
"This paper introduces a new approach that combines imitation and reinforcement learning through the use of an oracle to shape rewards. The study evaluates the effectiveness of the cost-to-go oracle on the planning horizon and shows that a globally optimal oracle can shorten the planning horizon to one step, resulting in a simpler Markov Decision Process. However, an oracle that is not optimal requires planning over a longer horizon to achieve near-optimal performance. This insight bridges the gap between imitation and reinforcement learning. Based on these findings, the paper proposes Truncated Horizon Policy Search (THOR), a method that searches for policies maximizing reshaped reward over a finite planning horizon when the oracle is sub-optimal. Results show that THOR outperforms both RL and IL baselines, even with a sub-optimal oracle.",1
"We consider the transfer of experience samples (i.e., tuples < s, a, s', r >) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.",0
"Our focus is on transferring experience samples from a set of source tasks to improve the learning process in a target task in reinforcement learning (RL). However, most existing approaches solely concentrate on selecting relevant source samples without considering model discrepancies. In contrast, we present a model-based technique that automatically evaluates the importance weight of each source sample and uses them in a batch RL algorithm to solve the target task. The importance weight determines the contribution of each sample to the learning process. We provide a finite-sample analysis of the proposed algorithm based on the results of importance weighting in supervised learning literature. Our empirical comparison with state-of-the-art approaches demonstrates that our algorithm achieves better learning performance and is highly resistant to negative transfer, even when some source tasks differ significantly from the target task.",1
"Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition. Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study.",0
"Although Deep reinforcement learning (DRL) has been successful in developing advanced video-game AI, most current DRL game agents rely on learning end-to-end from the video-output of the game, which is excessive for certain applications and can lead to additional issues. Moreover, working directly on pixel-based raw video data differs significantly from a human player's approach. To address these challenges, we propose a new approach that allows DRL agents to learn from object information. This is achieved using an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector to represent the current game-state and fulfills the DRL task simultaneously. We evaluate our OEN-based DRL agent against state-of-the-art methods on a range of games from the GVG-AI Competition. Our experimental results demonstrate that our object-based DRL agent performs comparably to the approaches used in our comparative study.",1
"Imitation learning (IL) consists of a set of tools that leverage expert demonstrations to quickly learn policies. However, if the expert is suboptimal, IL can yield policies with inferior performance compared to reinforcement learning (RL). In this paper, we aim to provide an algorithm that combines the best aspects of RL and IL. We accomplish this by formulating several popular RL and IL algorithms in a common mirror descent framework, showing that these algorithms can be viewed as a variation on a single approach. We then propose LOKI, a strategy for policy learning that first performs a small but random number of IL iterations before switching to a policy gradient RL method. We show that if the switching time is properly randomized, LOKI can learn to outperform a suboptimal expert and converge faster than running policy gradient from scratch. Finally, we evaluate the performance of LOKI experimentally in several simulated environments.",0
"The set of tools known as Imitation Learning (IL) allows for policies to be quickly learned by utilizing expert demonstrations. However, policies generated through IL may not perform as well as those created through Reinforcement Learning (RL) if the expert is not optimal. This paper aims to combine the best aspects of both RL and IL by formulating various popular algorithms from each in a common mirror descent framework. By doing so, the algorithms can be viewed as variations of a single approach. The proposed strategy, LOKI, first performs a small number of IL iterations before switching to a policy gradient RL method. Proper randomization of the switching time allows LOKI to outperform suboptimal experts and converge faster than running policy gradient from scratch. Lastly, the performance of LOKI is tested in various simulated environments.",1
"We consider reinforcement learning in changing Markov Decision Processes where both the state-transition probabilities and the reward functions may vary over time. For this problem setting, we propose an algorithm using a sliding window approach and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. We also characterize the optimal window size suitable for our algorithm. These results are complemented by a sample complexity bound on the number of sub-optimal steps taken by the algorithm. Finally, we present some experimental results to support our theoretical analysis.",0
"Our focus is on reinforcement learning in Markov Decision Processes that undergo changes, allowing for variations in both the reward functions and state-transition probabilities. To address this problem, we introduce an algorithm that uses a sliding window approach and offer assurances of its performance by comparing its regret to that of the optimal non-stationary policy. Additionally, we determine the optimal window size for our algorithm and provide a sample complexity bound to estimate the number of sub-optimal steps taken by the algorithm. Finally, we present experimental results that support our theoretical analysis.",1
"Policy evaluation with linear function approximation is an important problem in reinforcement learning. When facing high-dimensional feature spaces, such a problem becomes extremely hard considering the computation efficiency and quality of approximations. We propose a new algorithm, LSTD($\lambda$)-RP, which leverages random projection techniques and takes eligibility traces into consideration to tackle the above two challenges. We carry out theoretical analysis of LSTD($\lambda$)-RP, and provide meaningful upper bounds of the estimation error, approximation error and total generalization error. These results demonstrate that LSTD($\lambda$)-RP can benefit from random projection and eligibility traces strategies, and LSTD($\lambda$)-RP can achieve better performances than prior LSTD-RP and LSTD($\lambda$) algorithms.",0
"Reinforcement learning faces a significant challenge when evaluating policies using linear function approximation, especially when dealing with high-dimensional feature spaces. The computation efficiency and quality of approximations are major concerns. To address these challenges, we propose a new algorithm, LSTD($\lambda$)-RP, which combines random projection techniques and eligibility traces. We conducted theoretical analysis and found that LSTD($\lambda$)-RP can achieve better performance than previous algorithms, such as LSTD-RP and LSTD($\lambda$). Our results demonstrate that LSTD($\lambda$)-RP benefits from random projection and eligibility traces strategies, with upper bounds on the estimation error, approximation error, and total generalization error providing meaningful insights.",1
"We design a new myopic strategy for a wide class of sequential design of experiment (DOE) problems, where the goal is to collect data in order to to fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling (MPS), is inspired by the classical posterior (Thompson) sampling algorithm for multi-armed bandits and leverages the flexibility of probabilistic programming and approximate Bayesian inference to address a broad set of problems. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of DOE tasks, and more importantly, enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies.",0
"We have developed a new tactic, known as Myopic Posterior Sampling (MPS), to tackle a range of sequential design of experiment (DOE) problems. The aim of these problems is to gather information to meet a specific goal. Our MPS approach draws inspiration from the classical posterior (Thompson) sampling algorithm for multi-armed bandits, while also utilising probabilistic programming and approximate Bayesian inference to handle a broad range of problems. Our strategy is versatile and has proven competitive with more specialised methods across a variety of DOE tasks. It also enables the tackling of complex DOE targets that existing methods cannot address. Theoretical analysis has shown that MPS can achieve sublinear regret against natural benchmark policies, thanks to our use of adaptive submodularity and reinforcement learning concepts.",1
"The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a return. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance.",0
"Reinforcement learning algorithms aim to estimate and optimize the value function, but unlike supervised learning, there is no available teacher or oracle to provide the true value function. Instead, they estimate and optimize a proxy for the value function based on a sampled and bootstrapped approximation called a return. The choice of return significantly impacts the algorithm's nature, including the rate of future reward discount, bootstrapping timing and methods, and reward nature. These decisions are critical to the success of RL algorithms. This article discusses a gradient-based meta-learning algorithm that can adapt the return's nature while interacting and learning from the environment. The algorithm achieved a new state-of-the-art performance when applied to 57 games on the Atari 2600 environment over 200 million frames.",1
"Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.",0
"Deep Reinforcement Learning (RL) has emerged as a competitive approach for learning in sequential decision-making problems with observable environments, such as computer Go. However, there has been limited progress in using deep RL to handle partially observable environments. To address this, we propose a new architecture called the Action-specific Deep Recurrent Q-Network (ADRQN) that improves learning performance in partially observable domains. The ADRQN encodes actions using a fully connected layer and couples them with a convolutional observation to create an action-observation pair. An LSTM layer integrates the time series of action-observation pairs, learning latent states that a fully connected layer uses to compute Q-values, similar to conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of the ADRQN architecture in various partially observable domains, including flickering Atari games.",1
"A core novelty of Alpha Zero is the interleaving of tree search and deep learning, which has proven very successful in board games like Chess, Shogi and Go. These games have a discrete action space. However, many real-world reinforcement learning domains have continuous action spaces, for example in robotic control, navigation and self-driving cars. This paper presents the necessary theoretical extensions of Alpha Zero to deal with continuous action space. We also provide some preliminary experiments on the Pendulum swing-up task, empirically showing the feasibility of our approach. Thereby, this work provides a first step towards the application of iterated search and learning in domains with a continuous action space.",0
"Alpha Zero's unique feature is its combination of tree search and deep learning, which has been successful in board games such as Chess, Shogi, and Go. However, real-world reinforcement learning domains often involve continuous action spaces, such as robotic control, navigation, and self-driving cars. This study presents theoretical extensions of Alpha Zero to address continuous action spaces and provides preliminary experiments on the Pendulum swing-up task to demonstrate the feasibility of the approach. This work is a crucial initial step towards integrating iterated search and learning in continuous action space domains.",1
"Dyna-style reinforcement learning is a powerful approach for problems where not much real data is available. The main idea is to supplement real trajectories, or sequences of sampled states over time, with simulated ones sampled from a learned model of the environment. However, in large state spaces, the problem of learning a good generative model of the environment has been open so far. We propose to use deep belief networks to learn an environment model for use in Dyna. We present our approach and validate it empirically on problems where the state observations consist of images. Our results demonstrate that using deep belief networks, which are full generative models, significantly outperforms the use of linear expectation models, proposed in Sutton et al. (2008)",0
"When real data is scarce, Dyna-style reinforcement learning can be a valuable approach. This involves supplementing real trajectories with simulated ones generated from a learned model of the environment. However, the challenge of developing a robust generative model of the environment remains unsolved in large state spaces. To address this issue, we propose using deep belief networks in Dyna. Our methodology is tested with problems that involve image state observations, and our results indicate that deep belief networks, which are complete generative models, outperform the linear expectation models proposed in Sutton et al. (2008).",1
"We introduce reinforcement learning for heterogeneous teams in which rewards for an agent are additively factored into local costs, stimuli unique to each agent, and global rewards, those shared by all agents in the domain. Motivating domains include coordination of varied robotic platforms, which incur different costs for the same action, but share an overall goal. We present two templates for learning in this setting with factored rewards: a generalization of Perkins' Monte Carlo exploring starts for POMDPs to canonical MPOMDPs, with a single policy mapping joint observations of all agents to joint actions (MCES-MP); and another with each agent individually mapping joint observations to their own action (MCES-FMP). We use probably approximately local optimal (PALO) bounds to analyze sample complexity, instantiating these templates to PALO learning. We promote sample efficiency by including a policy space pruning technique, and evaluate the approaches on three domains of heterogeneous agents demonstrating that MCES-FMP yields improved policies in less samples compared to MCES-MP and a previous benchmark.",0
"Reinforcement learning is implemented for diverse groups comprising agents whose rewards are factored into local costs, individual stimuli, and global rewards. The objective is to coordinate dissimilar robotic platforms that incur varying costs for the same action while sharing an overall goal. The study presents two learning templates with factored rewards: a modified version of Perkins' Monte Carlo exploring starts for POMDPs to canonical MPOMDPs, with a single policy mapping joint observations of all agents to joint actions (MCES-MP); and another with each agent individually mapping joint observations to their own action (MCES-FMP). Sample complexity is analyzed using probably approximately local optimal (PALO) bounds. The templates are instantiated to PALO learning to promote sample efficiency by including a policy space pruning technique. The study evaluates the approaches on three domains of heterogeneous agents and demonstrates that MCES-FMP yields improved policies in less time compared to MCES-MP and a previous benchmark.",1
"In e-commerce platforms such as Amazon and TaoBao, ranking items in a search session is a typical multi-step decision-making problem. Learning to rank (LTR) methods have been widely applied to ranking problems. However, such methods often consider different ranking steps in a session to be independent, which conversely may be highly correlated to each other. For better utilizing the correlation between different ranking steps, in this paper, we propose to use reinforcement learning (RL) to learn an optimal ranking policy which maximizes the expected accumulative rewards in a search session. Firstly, we formally define the concept of search session Markov decision process (SSMDP) to formulate the multi-step ranking problem. Secondly, we analyze the property of SSMDP and theoretically prove the necessity of maximizing accumulative rewards. Lastly, we propose a novel policy gradient algorithm for learning an optimal ranking policy, which is able to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments are conducted in simulation and TaoBao search engine. The results demonstrate that our algorithm performs much better than online LTR methods, with more than 40% and 30% growth of total transaction amount in the simulation and the real application, respectively.",0
"Ranking items in search sessions on e-commerce platforms like Amazon and TaoBao involves a multi-step decision-making process. To address this issue, learning to rank (LTR) methods have been commonly used. However, these methods often treat each ranking step independently, although they may be highly correlated. To better utilize the correlation between different ranking steps, this paper proposes using reinforcement learning (RL) to learn an optimal ranking policy that maximizes the expected accumulative rewards during a search session. The concept of search session Markov decision process (SSMDP) is formally defined to formulate the multi-step ranking problem, and the necessity of maximizing accumulative rewards is theoretically proven. Lastly, a novel policy gradient algorithm is proposed to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments conducted in both simulation and TaoBao search engine show that our algorithm outperforms online LTR methods, with over 40% and 30% growth of total transaction amount in the simulation and the real application, respectively.",1
"Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.",0
"When agents trained in simulation are deployed into the real world, they may make errors due to differences between their training and execution environments. These mistakes can be dangerous and difficult to detect as the agent is unable to predict them in advance. To mitigate the occurrence of costly errors in real-world applications, we suggest using oracle feedback to develop a predictive model of these blind spots. Our focus is on blind spots that arise in reinforcement learning due to limited state representation, where the agent is unable to differentiate between multiple states as it lacks the necessary features to accurately represent the true state of the world. We formalize the problem of identifying blind spots in RL as a supervised learning problem with class imbalance and noise. We use various forms of oracle feedback, including demonstrations and corrections, to train models to recognize blind spots in unexplored regions of the state space. Our approach combines techniques for label aggregation, calibration, and supervised learning. Through our evaluation on two domains, we demonstrate that our method outperforms baseline approaches and that the learned model can be employed to selectively seek oracle feedback at execution time to prevent errors. We also examine the biases of different types of feedback and their impact on discovering blind spots.",1
"In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user's policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user's actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user's input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) flying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user's private information through observations, but receives a reward signal and user input that both depend on the user's intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user's input. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",0
"Shared autonomy involves combining user input with semi-autonomous control to achieve a common goal, which may be unknown beforehand. However, current methods rely on assumptions about the environment dynamics, user policy, and possible user goals, limiting their applicability in real-world scenarios. To address this, we propose a deep reinforcement learning framework for model-free shared autonomy that uses human-in-the-loop reinforcement learning with neural network function approximation. Our approach learns an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only supervision. We balance the need to follow user commands closely with the need to deviate from suboptimal user actions by discarding low-value actions and selecting the remaining action closest to the user's input. Our algorithm is demonstrated to assist users in real-time control tasks without direct access to the user's private information, with controlled studies involving users and synthetic pilots playing a video game and a pilot study with users flying a real quadrotor. This paper illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",1
"We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action continuous-state reinforcement learning. MAC is a policy gradient algorithm that uses the agent's explicit representation of all action values to estimate the gradient of the policy, rather than using only the actions that were actually executed. We prove that this approach reduces variance in the policy gradient estimate relative to traditional actor-critic methods. We show empirical results on two control domains and on six Atari games, where MAC is competitive with state-of-the-art policy search algorithms.",0
"Our proposed algorithm, known as Mean Actor-Critic (MAC), is designed for reinforcement learning with continuous-state and discrete-action systems. Unlike traditional actor-critic methods, MAC employs an explicit representation of all action values to estimate the policy gradient, resulting in reduced variance. We have demonstrated the effectiveness of this approach through empirical testing on two control domains and six Atari games, where MAC has proven to be competitive with current state-of-the-art policy search algorithms.",1
"Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",0
"The task of generating a program that meets certain specifications automatically is known as program synthesis. Recently, various neural approaches have been proposed for program synthesis, many of which use a sequence generation paradigm similar to neural machine translation. In this approach, sequence-to-sequence models are trained to maximize the probability of known reference programs. Despite its impressive results, this strategy has two main drawbacks. Firstly, it overlooks the fact that many different programs may fulfill a given specification, particularly with incomplete specifications like a few input-output examples. By maximizing the probability of only one reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer's performance. Secondly, this approach ignores the fact that programs have a strict syntax that can be checked efficiently. To overcome these limitations, we apply reinforcement learning to a supervised model that explicitly maximizes the probability of generating semantically correct programs. Furthermore, we introduce a training method that directly maximizes the probability of generating syntactically correct programs that meet the specification. Our contributions lead to enhanced accuracy of the models, particularly when the training data is limited.",1
"We study the inverse optimal control problem in social sciences: we aim at learning a user's true cost function from the observed temporal behavior. In contrast to traditional phenomenological works that aim to learn a generative model to fit the behavioral data, we propose a novel variational principle and treat user as a reinforcement learning algorithm, which acts by optimizing his cost function. We first propose a unified KL framework that generalizes existing maximum entropy inverse optimal control methods. We further propose a two-step Wasserstein inverse optimal control framework. In the first step, we compute the optimal measure with a novel mass transport equation. In the second step, we formulate the learning problem as a generative adversarial network. In two real world experiments - recommender systems and social networks, we show that our framework obtains significant performance gains over both existing inverse optimal control methods and point process based generative models.",0
"Our focus in social sciences is on the inverse optimal control problem, which involves learning a user's true cost function from their observed behavior over time. Rather than using traditional phenomenological approaches to fit behavioral data with a generative model, we propose a novel variational principle that treats the user as a reinforcement learning algorithm that optimizes their cost function. Our approach includes a unified KL framework that builds on existing maximum entropy inverse optimal control methods, as well as a two-step Wasserstein inverse optimal control framework. In the first step, we leverage a novel mass transport equation to compute the optimal measure, while in the second step we use a generative adversarial network to formulate the learning problem. Our experiments on recommender systems and social networks demonstrate that our approach outperforms existing inverse optimal control methods and point process-based generative models.",1
"In this paper, we explore using deep reinforcement learning for problems with multiple agents. Most existing methods for deep multi-agent reinforcement learning consider only a small number of agents. When the number of agents increases, the dimensionality of the input and control spaces increase as well, and these methods do not scale well. To address this, we propose casting the multi-agent reinforcement learning problem as a distributed optimization problem. Our algorithm assumes that for multi-agent settings, policies of individual agents in a given population live close to each other in parameter space and can be approximated by a single policy. With this simple assumption, we show our algorithm to be extremely effective for reinforcement learning in multi-agent settings. We demonstrate its effectiveness against existing comparable approaches on co-operative and competitive tasks.",0
"The aim of this study is to investigate the use of deep reinforcement learning in managing problems with multiple agents. Although current deep multi-agent reinforcement learning methods only consider a limited number of agents, the number of agents and the input and control spaces' dimensionality increases as the number of agents increases, resulting in poor scalability. To overcome this challenge, we propose treating the multi-agent reinforcement learning issue as a distributed optimization problem. Our approach assumes that in multi-agent settings, individual agents' policies are located close to each other in parameter space and can be modeled by a single policy, resulting in a highly effective reinforcement learning algorithm for multi-agent settings. We demonstrate its efficacy in cooperative and competitive tasks, outperforming comparable existing methods.",1
"We introduce a method to disentangle controllable and uncontrollable factors of variation by interacting with the world. Disentanglement leads to good representations and is important when applying deep neural networks (DNNs) in fields where explanations are required. This study attempts to improve an existing reinforcement learning (RL) approach to disentangle controllable and uncontrollable factors of variation, because the method lacks a mechanism to represent uncontrollable obstacles. To address this problem, we train two DNNs simultaneously: one that represents the controllable object and another that represents uncontrollable obstacles. For stable training, we applied a pretraining approach using a model robust against uncontrollable obstacles. Simulation experiments demonstrate that the proposed model can disentangle independently controllable and uncontrollable factors without annotated data.",0
"In this study, we present a technique to separate factors of variation that can be controlled from those that cannot be controlled by interacting with the environment. This disentanglement is crucial for creating effective representations, particularly in fields where explanations are needed and deep neural networks (DNNs) are used. However, an existing reinforcement learning (RL) approach lacks the ability to represent uncontrollable obstacles, which limits its effectiveness. To address this issue, we simultaneously train two DNNs: one for controllable objects and one for uncontrollable obstacles. We use a pretraining method with a robust model to ensure stable training. The proposed model can disentangle controllable and uncontrollable factors without the need for annotated data, as demonstrated in simulation experiments.",1
"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",0
"The problem of Inverse Reinforcement Learning (IRL) involves learning an agent's preferences from its behavior on a task. Although this problem has been thoroughly researched, the problem of online IRL has received less attention. Online IRL is when observations are incrementally gathered, but rerunning an IRL method is not feasible due to the demands of the application. We have introduced a new framework for online IRL, called incremental IRL (I2RL), and a new method that improves maximum entropy IRL with hidden variables in this setting. Our formal analysis shows that the new method has an increasing performance with more demonstration data and a probabilistically bounded error, both under full and partial observability. In a simulated robotic application of penetrating a continuous patrol under occlusion, experiments show the new method's relatively improved performance and speed up, validating the usefulness of online IRL.",1
"The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.",0
"The mathematical formalization of intelligent decision making provided by the framework of reinforcement learning or optimal control is powerful and widely applicable. Although the reinforcement learning problem is effective in reasoning about uncertainty, the connection between reinforcement learning and probabilistic models' inference is not immediately apparent. However, this connection is valuable in algorithm design, as formalizing a problem as probabilistic inference allows for the use of a wide range of approximate inference tools, flexible and powerful model extension, and reasoning about partial observability and compositionality. This article explores how a generalization of the reinforcement learning or optimal control problem, known as maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics and variational inference in the case of stochastic dynamics. The article provides a detailed derivation of this framework, reviews previous work that has employed these ideas to propose new reinforcement learning and control algorithms, and discusses future research perspectives.",1
"We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information.",0
"A new technique for deep reinforcement learning is introduced in this study, which detects moving objects automatically and utilizes relevant information for action selection. The technique employs unsupervised detection of moving objects by utilizing structure from motion, rather than learning a policy directly from raw images. The agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the agent's policy on the moving objects. As the agent interacts with the environment, it identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, named Motion-Oriented Reinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Additionally, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. Practitioners can gain insight into the policy by examining the segmentation and motion of each object detected by the agent, enabling them to confirm if the policy is based on reasonable information.",1
"Meta-learning approaches have been proposed to tackle the few-shot learning problem.Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined.Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",0
"The issue of few-shot learning has been addressed through the use of meta-learning techniques. Typically, a meta-learner is trained on various tasks to increase its adaptability to new tasks. However, over-training during the meta-training phase may result in a meta-learner that is biased towards existing tasks and unable to adapt to new ones, particularly with limited examples for updating the model. To prevent this bias and enhance the meta-learner's generalizability, we propose a novel strategy called Task-Agnostic Meta-Learning (TAML) algorithms. Our entropy-based approach meta-learns an unbiased initial model with the highest uncertainty over output labels by preventing over-performance in classification tasks. Additionally, we introduce a more general TAML approach that minimizes inequality of initial losses beyond classification tasks, where applicable. Our experiments on benchmarked datasets demonstrate the superiority of our proposed methods over other meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",1
"In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance it is crucial to guarantee the safety of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel \emph{Lyapunov} method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.",0
"In reinforcement learning (RL) problems in the real world, agents must avoid violating constraints while optimizing their objective function. Ensuring agent safety is critical during both training and deployment, as actions could damage hardware. To incorporate safety into RL, we utilize constrained Markov decision problems (CMDPs), which extend standard MDPs with constraints on expected cumulative costs. Our approach uses a novel Lyapunov method, which constructs Lyapunov functions to guarantee global safety of behavior policy through local, linear constraints. We demonstrate how to transform DP and RL algorithms into their safe counterparts using the Lyapunov approach. We evaluate these algorithms in various CMDP tasks on a safety benchmark domain and prove their effectiveness in balancing constraint satisfaction and performance.",1
"Good temporal representations are crucial for video understanding, and the state-of-the-art video recognition framework is based on two-stream networks. In such framework, besides the regular ConvNets responsible for RGB frame inputs, a second network is introduced to handle the temporal representation, usually the optical flow (OF). However, OF or other task-oriented flow is computationally costly, and is thus typically pre-computed. Critically, this prevents the two-stream approach from being applied to reinforcement learning (RL) applications such as video game playing, where the next state depends on current state and action choices. Inspired by the early vision systems of mammals and insects, we propose a fast event-driven representation (EDR) that models several major properties of early retinal circuits: (1) logarithmic input response, (2) multi-timescale temporal smoothing to filter noise, and (3) bipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the directional information for fast speed (> 9000 fps), EDR en-ables fast real-time inference/learning in video applications that require interaction between an agent and the world such as game-playing, virtual robotics, and domain adaptation. In this vein, we use EDR to demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games, something that has not been possible with pre-computed OF. Moreover, with UCF-101 video action recognition experiments, we show that EDR performs near state-of-the-art in accuracy while achieving a 1,500x speedup in input representation processing, as compared to optical flow.",0
"The effectiveness of video understanding relies heavily on accurate temporal representations. Presently, the leading approach for video recognition involves two-stream networks, in which a second network is implemented to handle temporal representation, often using optical flow. However, optical flow is computationally expensive and is typically pre-computed, which makes it unsuitable for reinforcement learning (RL) applications such as video game playing. To address this issue, we propose a fast event-driven representation (EDR) that mimics early retinal circuits found in mammals and insects. EDR provides fast real-time inference and learning for video applications that require interaction between an agent and the world. By using EDR, we have achieved performance improvements in reinforcement learning algorithms for Atari games, which was not possible with pre-computed optical flow. Additionally, our UCF-101 video action recognition experiments demonstrate that EDR performs close to state-of-the-art in accuracy while providing a 1,500x speedup in input representation processing compared to optical flow.",1
"Reinforcement learning (RL) algorithms have made huge progress in recent years by leveraging the power of deep neural networks (DNN). Despite the success, deep RL algorithms are known to be sample inefficient, often requiring many rounds of interaction with the environments to obtain satisfactory performance. Recently, episodic memory based RL has attracted attention due to its ability to latch on good actions quickly. In this paper, we present a simple yet effective biologically inspired RL algorithm called Episodic Memory Deep Q-Networks (EMDQN), which leverages episodic memory to supervise an agent during training. Experiments show that our proposed method can lead to better sample efficiency and is more likely to find good policies. It only requires 1/5 of the interactions of DQN to achieve many state-of-the-art performances on Atari games, significantly outperforming regular DQN and other episodic memory based RL algorithms.",0
"The utilization of deep neural networks (DNN) has greatly improved reinforcement learning (RL) algorithms in recent times. However, DNN-based RL algorithms are often inefficient in their use of samples, and require numerous rounds of interaction with the environment to achieve satisfactory results. Episodic memory based RL has recently become a popular approach due to its ability to quickly identify good actions. Our paper introduces a new RL algorithm, called Episodic Memory Deep Q-Networks (EMDQN), which is inspired by biological processes and uses episodic memory to supervise the agent during training. Our experiments demonstrate that EMDQN offers superior sample efficiency and better policy outcomes than regular DQN and other episodic memory based RL algorithms. It achieves state-of-the-art performance on Atari games, using only 1/5 of the interactions required by DQN.",1
"Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and $N$-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg",0
"The ability for robots to navigate complex environments independently is crucial for their practical use. Traditional methods involve the robot creating a map of the environment and then using a localization and planning system to move through it. However, these methods can be problematic due to their assumptions, high computational requirements, and inability to learn from errors. In contrast, learning-based methods improve with experience, but their high sample complexity makes them difficult to use in real-world situations. To overcome this issue, we propose a generalized computation graph that integrates value-based model-free and model-based methods, with various instantiations between them. We apply this graph to create a navigation model that can learn from visual input and requires minimal samples. Our experiments using a simulated car demonstrate the effectiveness of our approach, outperforming other methods. Furthermore, our real-world experiments using an RC car show that our model can navigate complex indoor environments with only a few hours of autonomous, self-supervised training. Videos and code for the experiments are available at github.com/gkahn13/gcg.",1
"To improve the quality of computation experience for mobile devices, mobile-edge computing (MEC) is a promising paradigm by providing computing capabilities in close proximity within a sliced radio access network (RAN), which supports both traditional communication and MEC services. Nevertheless, the design of computation offloading policies for a virtual MEC system remains challenging. Specifically, whether to execute a computation task at the mobile device or to offload it for MEC server execution should adapt to the time-varying network dynamics. In this paper, we consider MEC for a representative mobile user in an ultra-dense sliced RAN, where multiple base stations (BSs) are available to be selected for computation offloading. The problem of solving an optimal computation offloading policy is modelled as a Markov decision process, where our objective is to maximize the long-term utility performance whereby an offloading decision is made based on the task queue state, the energy queue state as well as the channel qualities between MU and BSs. To break the curse of high dimensionality in state space, we first propose a double deep Q-network (DQN) based strategic computation offloading algorithm to learn the optimal policy without knowing a priori knowledge of network dynamics. Then motivated by the additive structure of the utility function, a Q-function decomposition technique is combined with the double DQN, which leads to novel learning algorithm for the solving of stochastic computation offloading. Numerical experiments show that our proposed learning algorithms achieve a significant improvement in computation offloading performance compared with the baseline policies.",0
"Mobile-edge computing (MEC) offers a promising solution to enhance the quality of computation experience for mobile devices. This paradigm provides computing capabilities in close proximity through a sliced radio access network (RAN), supporting both traditional communication and MEC services. However, designing computation offloading policies for virtual MEC systems is challenging. The decision to execute a task on the mobile device or offload it for MEC server execution must adapt to the time-varying network dynamics. This paper presents a Markov decision process model for solving an optimal computation offloading policy for a representative mobile user in an ultra-dense sliced RAN, where multiple base stations (BSs) are available for selection. Our objective is to maximize long-term utility performance by making offloading decisions based on task queue state, energy queue state, and channel qualities between mobile users and BSs. To address the high dimensionality in state space, we propose a double deep Q-network (DQN) based strategic computation offloading algorithm to learn the optimal policy without prior knowledge of network dynamics. Additionally, we combine a Q-function decomposition technique with the double DQN to solve stochastic computation offloading. Numerical experiments demonstrate that our proposed learning algorithms significantly improve computation offloading performance compared to baseline policies.",1
"We formulate the problem of sampling and recovering clustered graph signal as a multi-armed bandit (MAB) problem. This formulation lends naturally to learning sampling strategies using the well-known gradient MAB algorithm. In particular, the sampling strategy is represented as a probability distribution over the individual arms of the MAB and optimized using gradient ascent. Some illustrative numerical experiments indicate that the sampling strategies based on the gradient MAB algorithm outperform existing sampling methods.",0
The clustered graph signal sampling and recovery problem can be framed as a multi-armed bandit (MAB) problem. This approach allows for the development of sampling strategies through the gradient MAB algorithm. The sampling strategy is modeled as a probability distribution over the MAB arms and optimized using gradient ascent. Experimental results demonstrate that the gradient MAB algorithm-based sampling strategies perform better than current methods.,1
"A common problem in Machine Learning and statistics consists in detecting whether the current sample in a stream of data belongs to the same distribution as previous ones, is an isolated outlier or inaugurates a new distribution of data. We present a hierarchical Bayesian algorithm that aims at learning a time-specific approximate posterior distribution of the parameters describing the distribution of the data observed. We derive the update equations of the variational parameters of the approximate posterior at each time step for models from the exponential family, and show that these updates find interesting correspondents in Reinforcement Learning (RL). In this perspective, our model can be seen as a hierarchical RL algorithm that learns a posterior distribution according to a certain stability confidence that is, in turn, learned according to its own stability confidence. Finally, we show some applications of our generic model, first in a RL context, next with an adaptive Bayesian Autoregressive model, and finally in the context of Stochastic Gradient Descent optimization.",0
"Detecting whether a sample in a stream of data belongs to the same distribution as previous samples, is an isolated outlier, or inaugurates a new distribution is a common issue in Machine Learning and statistics. To address this, we introduce a hierarchical Bayesian algorithm that learns an approximate posterior distribution of the observed data's distribution parameters at each specific time. We demonstrate how to update the variational parameters of the approximate posterior for models from the exponential family, which also have correspondences in Reinforcement Learning. Our model can be viewed as a hierarchical RL algorithm that learns a posterior distribution based on a stability confidence, which is learned according to its own stability confidence. We also provide some examples of our generic model's application, including in a RL context, with an adaptive Bayesian Autoregressive model, and in the context of Stochastic Gradient Descent optimization.",1
"Reinforcement learning (RL) agents performing complex tasks must be able to remember observations and actions across sizable time intervals. This is especially true during the initial learning stages, when exploratory behaviour can increase the delay between specific actions and their effects. Many new or popular approaches for learning these distant correlations employ backpropagation through time (BPTT), but this technique requires storing observation traces long enough to span the interval between cause and effect. Besides memory demands, learning dynamics like vanishing gradients and slow convergence due to infrequent weight updates can reduce BPTT's practicality; meanwhile, although online recurrent network learning is a developing topic, most approaches are not efficient enough to use as replacements. We propose a simple, effective memory strategy that can extend the window over which BPTT can learn without requiring longer traces. We explore this approach empirically on a few tasks and discuss its implications.",0
"When performing complex tasks, reinforcement learning (RL) agents need to retain information about observations and actions over significant periods of time. This is particularly important during the initial stages of learning, when experimental behavior can lead to delays between actions and their consequences. Although backpropagation through time (BPTT) is a popular method for learning distant correlations, it necessitates storing observation traces for a long enough period to cover the gap between cause and effect. Furthermore, BPTT's practicality is hampered by issues such as memory requirements, vanishing gradients, and slow convergence due to infrequent weight updates. While online recurrent network learning is an area of active research, most approaches are not efficient enough to replace BPTT. Our proposal is a straightforward, effective memory technique that can extend the learning window of BPTT without necessitating longer traces. We experimentally investigate this approach on several tasks and consider its implications.",1
"Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function. Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimalist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We introduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss function (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning.",0
"Acquiring locomotion skills is a difficult task that typically involves the use of motion capture, finite state machines, or morphology-specific knowledge to guide motion generation algorithms in order to create realistic and smooth movements. Deep reinforcement learning (DRL) is a promising approach for creating automatic locomotion control, which has been applied to the creation of a running controller for biped characters. However, the resulting motions often look unrealistic. This paper proposes a minimalist approach to the locomotion problem that does not rely on motion examples, finite state machines, or morphology-specific knowledge. Instead, it introduces two modifications to the DRL approach that produce symmetric, low-energy, and more realistic movements. The first modification adds a new term to the loss function to encourage symmetric actions, while the second introduces a new curriculum learning method that provides physical assistance to help the character with balance and movement. The algorithm gradually reduces the assistance until the character can move without help. Since the method does not require motion capture data, it can be applied to various character morphologies. The paper provides examples of locomotion controllers for bipeds, full humanoids, quadrupeds, and hexapods, showing that the learned policies can produce symmetric, low-energy gaits and appropriate gait patterns without guidance from motion examples or contact planning.",1
"In recent years, attention has been focused on the relationship between black-box optimiza- tion problem and reinforcement learning problem. In this research, we propose the Mirror Descent Search (MDS) algorithm which is applicable both for black box optimization prob- lems and reinforcement learning problems. Our method is based on the mirror descent method, which is a general optimization algorithm. The contribution of this research is roughly twofold. We propose two essential algorithms, called MDS and Accelerated Mirror Descent Search (AMDS), and two more approximate algorithms: Gaussian Mirror Descent Search (G-MDS) and Gaussian Accelerated Mirror Descent Search (G-AMDS). This re- search shows that the advanced methods developed in the context of the mirror descent research can be applied to reinforcement learning problem. We also clarify the relationship between an existing reinforcement learning algorithm and our method. With two evaluation experiments, we show our proposed algorithms converge faster than some state-of-the-art methods.",0
"The focus of recent years has been on the connection between black-box optimization and reinforcement learning problems. The Mirror Descent Search (MDS) algorithm has been proposed in this study, capable of solving both types of problems. The algorithm is based on the mirror descent method, a general optimization algorithm. The research contributes in two ways: proposing two essential algorithms, MDS and Accelerated Mirror Descent Search (AMDS), and two approximate algorithms, Gaussian Mirror Descent Search (G-MDS) and Gaussian Accelerated Mirror Descent Search (G-AMDS). Additionally, the research demonstrates that the advanced methods developed in mirror descent research can be applied to reinforcement learning problems. The study also clarifies the relationship between the proposed method and an existing reinforcement learning algorithm. Two evaluation experiments indicate that the proposed algorithms converge faster compared to some state-of-the-art methods.",1
"We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.   An interactive version of this paper is available at https://worldmodels.github.io/",0
"The focus of our investigation is the construction of generative neural network models for widely used reinforcement learning environments. Our model of the world can undergo unsupervised training to rapidly acquire a condensed spatial and temporal understanding of the environment. By utilizing characteristics derived from the world model as inputs for an agent, we can develop a streamlined and straightforward policy that is capable of accomplishing the necessary task. Moreover, we can even train our agent exclusively within its own simulated dream world created by the world model and transfer this policy back to the real environment. For an interactive version of our study, please visit https://worldmodels.github.io/.",1
