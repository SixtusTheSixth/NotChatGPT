"While the satellite-based Global Positioning System (GPS) is adequate for some outdoor applications, many other applications are held back by its multi-meter positioning errors and poor indoor coverage. In this paper, we study the feasibility of real-time video-based localization on resource-constrained platforms. Before commencing a localization task, a video-based localization system downloads an offline model of a restricted target environment, such as a set of city streets, or an indoor shopping mall. The system is then able to localize the user within the model, using only video as input.   To enable such a system to run on resource-constrained embedded systems or smartphones, we (a) propose techniques for efficiently building a 3D model of a surveyed path, through frame selection and efficient feature matching, (b) substantially reduce model size by multiple compression techniques, without sacrificing localization accuracy, (c) propose efficient and concurrent techniques for feature extraction and matching to enable online localization, (d) propose a method with interleaved feature matching and optical flow based tracking to reduce the feature extraction and matching time in online localization.   Based on an extensive set of both indoor and outdoor videos, manually annotated with location ground truth, we demonstrate that sub-meter accuracy, at real-time rates, is achievable on smart-phone type platforms, despite challenging video conditions.",0
"Although the Global Positioning System (GPS) that uses satellites is sufficient for some outdoor applications, it has limitations such as multi-meter positioning errors and inadequate indoor coverage, which restricts many other applications. This research explores the possibility of video-based real-time localization on platforms with limited resources. To initiate a localization task, the system downloads an offline model of a specific restricted environment, such as a city street or indoor shopping mall, before using video input to determine the user's location within the model. To make the system functional on embedded systems or smartphones, we propose techniques to efficiently construct a 3D model of a surveyed path, reduce model size without compromising localization accuracy, and suggest efficient feature extraction and matching techniques for online localization. Additionally, we recommend interleaved feature matching and tracking with optical flow to reduce feature extraction and matching time in online localization. Based on a comprehensive set of both indoor and outdoor videos manually annotated with location ground truth, we demonstrate that sub-meter accuracy can be achieved on smart-phone type platforms in real-time, even with challenging video conditions.",1
"In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue.Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system.",0
"This study introduces a monocular visual odometry (VO) algorithm that combines geometry-based methods with deep learning. Current VO/SLAM systems that offer high performance rely on geometry and must be tailored for specific application scenarios. Additionally, most monocular systems experience scale-drift problems. Although recent deep learning approaches attempt to solve VO in an end-to-end manner, they still cannot match the performance of geometry-based methods. This study revisits the fundamentals of VO and explores the most effective way to integrate deep learning with epipolar geometry and the Perspective-n-Point (PnP) technique. Specifically, the researchers train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. Using these deep predictions, they devise a simple but robust frame-to-frame VO algorithm (DF-VO) that surpasses pure deep learning-based and geometry-based methods. Furthermore, their system overcomes scale-drift issues with the aid of a scale-consistent single-view depth CNN. Their extensive experiments on the KITTI dataset demonstrate the robustness of their system, and their detailed ablation study reveals the impact of various factors within their system.",1
"Motion blurry images challenge many computer vision algorithms, e.g, feature detection, motion estimation, or object recognition. Deep convolutional neural networks are state-of-the-art for image deblurring. However, obtaining training data with corresponding sharp and blurry image pairs can be difficult. In this paper, we present a differentiable reblur model for self-supervised motion deblurring, which enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our key insight is that motion cues obtained from consecutive images yield sufficient information to inform the deblurring task. We therefore formulate deblurring as an inverse rendering problem, taking into account the physical image formation process: we first predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We use both synthetic and real dataset for experimental evaluations. Our experiments demonstrate that self-supervised single image deblurring is really feasible and leads to visually compelling results.",0
"Many computer vision algorithms struggle with motion blurry images, including feature detection, motion estimation, and object recognition. While deep convolutional neural networks are currently the best option for image deblurring, gathering the necessary training data with corresponding sharp and blurry image pairs can be challenging. This paper proposes a differentiable reblur model for self-supervised motion deblurring that enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. The approach takes advantage of motion cues obtained from consecutive images to inform the deblurring task, formulating it as an inverse rendering problem that considers the physical image formation process. The method predicts two deblurred images from which it estimates corresponding optical flow, and then re-renders the blurred images to minimize the difference with respect to the original blurry inputs. Both synthetic and real datasets are used for experimental evaluations, and results demonstrate that self-supervised single image deblurring is feasible and produces visually compelling outcomes.",1
"We introduce the first very large detection dataset for event cameras. The dataset is composed of more than 39 hours of automotive recordings acquired with a 304x240 ATIS sensor. It contains open roads and very diverse driving scenarios, ranging from urban, highway, suburbs and countryside scenes, as well as different weather and illumination conditions. Manual bounding box annotations of cars and pedestrians contained in the recordings are also provided at a frequency between 1 and 4Hz, yielding more than 255,000 labels in total. We believe that the availability of a labeled dataset of this size will contribute to major advances in event-based vision tasks such as object detection and classification. We also expect benefits in other tasks such as optical flow, structure from motion and tracking, where for example, the large amount of data can be leveraged by self-supervised learning methods.",0
"We are presenting the initial detection dataset for event cameras with a large scale. The dataset includes over 39 hours of automotive recordings obtained with a 304x240 ATIS sensor, which encompasses various driving scenarios like urban, highway, suburbs, and countryside scenes, along with different weather and illumination conditions. Furthermore, manual bounding box annotations of cars and pedestrians present in the recordings are given at a frequency between 1 and 4Hz, resulting in more than 255,000 labels overall. We are confident that the availability of such a labeled dataset will facilitate significant developments in event-based vision tasks such as object detection and classification. Additionally, we anticipate advantages in optical flow, structure from motion, and tracking, where the vast amount of data can be utilized by self-supervised learning methods.",1
"With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",0
"The availability of multi-modal video data has increased with the prevalence of RGB-D cameras, making human action recognition more feasible. However, effectively utilizing the complementary information presented by different modalities remains a major challenge. To address this issue, we propose the Modality Compensation Network (MCN), which enhances representations for human action recognition by exploring the relationships among different modalities. Specifically, we treat RGB/optical flow videos as source modalities and skeletons as an auxiliary modality. Our objective is to extract more discriminative features from the source modalities with the aid of the auxiliary modality. To achieve adaptive representation learning, our model, which combines deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, uses a modality adaptation block to bridge data from the source and auxiliary modalities. The network learns to compensate for the loss of skeletons during both training and testing by exploring multiple adaptation schemes that narrow the distance between source and auxiliary modal distributions from different levels. Importantly, skeletons are only needed during training. Our model improves recognition performance when testing with source data and outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",1
"Most of Multiple Object Tracking (MOT) approaches compute individual target features for two subtasks: estimating target-wise motions and conducting pair-wise Re-Identification (Re-ID). Because of the indefinite number of targets among video frames, both subtasks are very difficult to scale up efficiently in end-to-end Deep Neural Networks (DNNs). In this paper, we design an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), that addresses the above issues with two efficient techniques: target flowing and target fusing. Specifically, in target flowing, a FlowTracker DNN module learns the indefinite number of target-wise motions jointly from pixel-level optical flows. In target fusing, a FuseTracker DNN module refines and fuses targets proposed by FlowTracker and frame-wise object detection, instead of trusting either of the two inaccurate sources of target proposal. Because FlowTracker can explore complex target-wise motion patterns and FuseTracker can refine and fuse targets from FlowTracker and detectors, our approach can achieve the state-of-the-art results on several MOT benchmarks. As an online MOT approach, FFT produced the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks, surpassing all the online and offline methods in existing publications.",0
"The majority of Multiple Object Tracking (MOT) methods calculate individual target characteristics for two tasks: determining target-specific movements and performing pair-wise Re-Identification (Re-ID). Since the number of targets in video frames is uncertain, both tasks present challenges for efficiently scaling up end-to-end Deep Neural Networks (DNNs). This article introduces an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), which addresses these problems using two effective techniques: target flowing and target fusing. Target flowing involves learning the indefinite number of target-specific movements jointly from pixel-level optical flows using a FlowTracker DNN module. Target fusing, on the other hand, refines and combines targets proposed by FlowTracker and frame-wise object detection through a FuseTracker DNN module, rather than relying on either of the two inaccurate sources of target proposal. Our approach achieves state-of-the-art results on several MOT benchmarks because FlowTracker can explore complex target-specific motion patterns and FuseTracker can refine and fuse targets from FlowTracker and detectors. As a real-time MOT approach, FFT outperforms all online and offline methods in existing publications, producing the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks.",1
"Video denoising is to remove noise from noise-corrupted data, thus recovering true signals via spatiotemporal processing. Existing approaches for spatiotemporal video denoising tend to suffer from motion blur artifacts, that is, the boundary of a moving object tends to appear blurry especially when the object undergoes a fast motion, causing optical flow calculation to break down. In this paper, we address this challenge by designing a first-image-then-video two-stage denoising neural network, consisting of an image denoising module for spatially reducing intra-frame noise followed by a regular spatiotemporal video denoising module. The intuition is simple yet powerful and effective: the first stage of image denoising effectively reduces the noise level and, therefore, allows the second stage of spatiotemporal denoising for better modeling and learning everywhere, including along the moving object boundaries. This two-stage network, when trained in an end-to-end fashion, yields the state-of-the-art performances on the video denoising benchmark Vimeo90K dataset in terms of both denoising quality and computation. It also enables an unsupervised approach that achieves comparable performance to existing supervised approaches.",0
"The goal of video denoising is to eliminate noise from data that has been corrupted by noise, thereby recovering authentic signals through spatiotemporal processing. However, current techniques for spatiotemporal video denoising often encounter issues with motion blur artifacts, whereby the border of a moving object appears fuzzy, particularly during rapid motion, which can disrupt optical flow calculations. To overcome this challenge, we propose a two-stage denoising neural network that first employs an image denoising module to decrease intra-frame noise and then utilizes a regular spatiotemporal video denoising module. This method is effective because the initial stage of image denoising reduces the noise level, allowing the second stage to model and learn more accurately, including along the boundaries of moving objects. This two-stage network, when trained end-to-end, achieves state-of-the-art performance on the Vimeo90K video denoising benchmark dataset in terms of both denoising quality and computation. Additionally, it enables an unsupervised approach that achieves performance comparable to existing supervised methods.",1
"In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to no significant roles in optical flow, we generated simplified texture-less and background-less videos and utilized the synthetic data to train a Temporal Segment Network (TSN). The results demonstrated that augmenting TSN with simplified synthetic data improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51 when adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training using simplified synthetic videos alone on 25 classes of UCF-101 achieved 30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos. Finally, results showed that when reducing the number of real videos of UCF-25 to 10% and combining them with synthetic videos, the accuracy drops to only 85.41%, compared to a drop to 77.4% when no synthetic data is added.",0
"The objective of this research is to explore the effectiveness of synthetic videos as training data for neural networks utilized in action categorization. Our approach is motivated by the fact that optical flow is not significantly influenced by texture and background. To this end, we generated texture-less and background-less videos and incorporated them into the training of a Temporal Segment Network (TSN). The findings of our study indicate that the inclusion of synthetic data enhances the performance of the TSN, as evidenced by an improvement from 68.5% to 71.8% accuracy on HMDB-51 when 4,000 videos were added, and 72.4% accuracy when 8,000 videos were added. Moreover, training solely on simplified synthetic videos resulted in 30.71% accuracy on 25 classes of UCF-101 when 2,500 videos were used, and 52.7% accuracy when 5,000 videos were used. Lastly, our results demonstrate that when real videos of UCF-25 are reduced to 10% and combined with synthetic videos, the accuracy only drops to 85.41%, compared to 77.4% when no synthetic data is included.",1
"The existing approaches for salient motion segmentation are unable to explicitly learn geometric cues and often give false detections on prominent static objects. We exploit multiview geometric constraints to avoid such shortcomings. To handle the nonrigid background like a sea, we also propose a robust fusion mechanism between motion and appearance-based features. We find dense trajectories, covering every pixel in the video, and propose trajectory-based epipolar distances to distinguish between background and foreground regions. Trajectory epipolar distances are data-independent and can be readily computed given a few features' correspondences between the images. We show that by combining epipolar distances with optical flow, a powerful motion network can be learned. Enabling the network to leverage both of these features, we propose a simple mechanism, we call input-dropout. Comparing the motion-only networks, we outperform the previous state of the art on DAVIS-2016 dataset by 5.2% in the mean IoU score. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform the previous methods on DAVIS-2016, 2017 and Segtrackv2 dataset.",0
"The current methods for identifying important motion in a video are inadequate because they do not consider geometric cues and often falsely identify stationary objects. To overcome these limitations, we utilize multiview geometric constraints. We also propose a fusion mechanism that combines motion and appearance-based features to handle nonrigid backgrounds like the ocean. We use dense trajectories that cover every pixel in the video and introduce trajectory-based epipolar distances to distinguish between foreground and background regions. These distances are data-independent and can be easily computed with a few features' correspondences between the images. By combining epipolar distances with optical flow, we can create a powerful motion network. We introduce a simple mechanism called input-dropout, which enables the network to utilize both features. By comparing motion-only networks, we outperform previous state-of-the-art methods by 5.2% in the mean IoU score on the DAVIS-2016 dataset. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform previous methods on the DAVIS-2016, 2017, and Segtrackv2 datasets.",1
"For a long time, the vision community tries to learn the spatio-temporal representation by combining convolutional neural network together with various temporal models, such as the families of Markov chain, optical flow, RNN and temporal convolution. However, these pipelines consume enormous computing resources due to the alternately learning process for spatial and temporal information. One natural question is whether we can embed the temporal information into the spatial one so the information in the two domains can be jointly learned once-only. In this work, we answer this question by presenting a simple yet powerful operator -- temporal interlacing network (TIN). Instead of learning the temporal features, TIN fuses the two kinds of information by interlacing spatial representations from the past to the future, and vice versa. A differentiable interlacing target can be learned to control the interlacing process. In this way, a heavy temporal model is replaced by a simple interlacing operator. We theoretically prove that with a learnable interlacing target, TIN performs equivalently to the regularized temporal convolution network (r-TCN), but gains 4% more accuracy with 6x less latency on 6 challenging benchmarks. These results push the state-of-the-art performances of video understanding by a considerable margin. Not surprising, the ensemble model of the proposed TIN won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. Code is made available to facilitate further research at https://github.com/deepcs233/TIN",0
"The vision community has been attempting to learn how to represent spatio-temporal data by combining convolutional neural networks with various temporal models. However, these methods require a large amount of computing resources due to the alternating learning process for spatial and temporal information. Therefore, a natural question arises as to whether temporal information can be embedded into spatial information to enable joint learning. This work proposes a simple yet effective operator called the temporal interlacing network (TIN) that fuses spatial representations from the past and future by interlacing them. The interlacing process can be controlled by a differentiable interlacing target. TIN replaces the need for a heavy temporal model with a simple interlacing operator. Theoretical analysis shows that TIN performs equivalently to the regularized temporal convolution network (r-TCN), but with 4% more accuracy and 6x less latency on 6 challenging benchmarks. The TIN ensemble model won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. The code is available at https://github.com/deepcs233/TIN to facilitate further research.",1
"We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation.",0
"The problem of determining the camera pose without supervision is being considered. The aim is to estimate the camera's movement between consecutive frames when given a video sequence. Traditionally, this issue is addressed by imposing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We suggest an alternative method that employs a compositional re-estimation process for camera pose estimation. Initially, we estimate a depth map from the input. Our method then iteratively determines the camera motion based on the estimated depth map. Our approach enhances the predicted camera motion in terms of both quantitative and visual aspects. Additionally, the re-estimation technique solves the problem of out-of-boundary pixels in a unique and straightforward manner. Our approach is also adaptable to other camera pose estimation methods. The KITTI benchmark dataset was experimentally analyzed, and our technique outperformed current state-of-the-art unsupervised camera ego-motion estimation techniques.",1
"Recently, 3D convolutional networks yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 20.5% and 12.5% points improvements over top-1 accuracy can be achieved on the UCF101 and HMDB51 datasets when trained from scratch. Because residual frames contain little information of object appearance, we further use a 2D convolutional network to extract appearance features and combine them with the results from residual frames to form a two-path solution. In three benchmark datasets, our two-path solution achieved better or comparable performances than those using additional optical flow methods, especially outperformed the state-of-the-art models on Mini-kinetics dataset. Further analysis indicates that better motion features can be extracted using residual frames with 3D ConvNets, and our residual-frame-input path is a good supplement for existing RGB-frame-input models.",0
"Recently, 3D convolutional networks have shown promising results in action recognition. However, incorporating optical flow stream is essential for better performance, despite its high cost. This paper proposes an efficient method to extract motion features from videos using residual frames as input data in 3D ConvNets. By replacing stacked RGB frames with residual ones, a significant increase in top-1 accuracy of 20.5% and 12.5% on the UCF101 and HMDB51 datasets, respectively, can be obtained when trained from scratch. As residual frames contain minimal object appearance information, a 2D convolutional network is used to extract appearance features, which are then combined with the results from residual frames to create a two-path solution. Our two-path solution outperforms existing models on Mini-kinetics dataset and achieves comparable or better performances than those using additional optical flow methods on three benchmark datasets. Further analysis reveals that residual frames with 3D ConvNets can extract better motion features, making our residual-frame-input path an excellent complement to existing RGB-frame-input models.",1
"Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion extremely reliably in the most sophisticated scenarios but they come at a price - modern event-based vision sensors have extremely low resolution and produce a lot of noise. Moreover, the asynchronous nature of the event stream calls for novel algorithms.   This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal) component of the event stream, and not only the spatial component, at every moment of time. This is done by approximating the 3D geometry of the event stream with a parametric model; as a result, the algorithm is capable of producing the motion-compensated event stream (effectively approximating egomotion), and without using any form of external sensors in extremely low-light and noisy conditions without any form of feature tracking or explicit optical flow computation. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.",0
"Dynamic Vision Sensor (DVS) and other event-based vision sensors are well-suited for real-time motion analysis due to their unique features, including high temporal resolution, superior light sensitivity, and low latency. However, these advantages come with a drawback - these sensors have low resolution and produce a lot of noise. Additionally, the asynchronous nature of the event stream requires new algorithms. In this paper, we introduce a novel approach to object tracking with asynchronous cameras. Our method utilizes a parametric model to approximate the 3D geometry of the event stream, allowing us to incorporate information about the dynamic component of the event stream at every moment of time. With this approach, we can produce a motion-compensated event stream without external sensors, even in low-light and noisy conditions. We demonstrate the effectiveness of our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to identify objects with different movements, even in situations of fast motion.",1
"Visual odometry is an essential key for a localization module in SLAM systems. However, previous methods require tuning the system to adapt environment changes. In this paper, we propose a learning-based approach for frame-to-frame monocular visual odometry estimation. The proposed network is only learned by disparity maps for not only covering the environment changes but also solving the scale problem. Furthermore, attention block and skip-ordering scheme are introduced to achieve robust performance in various driving environment. Our network is compared with the conventional methods which use common domain such as color or optical flow. Experimental results confirm that the proposed network shows better performance than other approaches with higher and more stable results.",0
"Visual odometry is crucial for localizing SLAM systems, but previous methods required system tuning to adapt to environmental changes. In this study, we present a learning-based approach for estimating frame-to-frame monocular visual odometry. The proposed network utilizes only disparity maps, addressing both environment changes and the scale problem. To achieve robust performance in diverse driving environments, we introduce an attention block and skip-ordering scheme. Our network is compared to conventional methods using color or optical flow. Experimental results demonstrate that our approach outperforms other methods with higher and more consistent results.",1
"VBM3D is an extension to video of the well known image denoising algorithm BM3D, which takes advantage of the sparse representation of stacks of similar patches in a transform domain. The extension is rather straightforward: the similar 2D patches are taken from a spatio-temporal neighborhood which includes neighboring frames. In spite of its simplicity, the algorithm offers a good trade-off between denoising performance and computational complexity. In this work we revisit this method, providing an open-source C++ implementation reproducing the results. A detailed description is given and the choice of parameters is thoroughly discussed. Furthermore, we discuss several extensions of the original algorithm: (1) a multi-scale implementation, (2) the use of 3D patches, (3) the use of optical flow to guide the patch search. These extensions allow to obtain results which are competitive with even the most recent state of the art.",0
"VBM3D is a video denoising algorithm that builds upon BM3D, a well-known image denoising technique utilizing the sparse representation of stacks of similar patches in a transform domain. VBM3D uses a spatio-temporal neighborhood to extract similar 2D patches, including neighboring frames. Despite its straightforward implementation, VBM3D strikes a balance between denoising performance and computational complexity. In this study, we present an open-source C++ implementation of the algorithm, providing a detailed description and discussing parameter choices. Additionally, we explore several extensions of the original algorithm, including a multi-scale implementation, the use of 3D patches, and optical flow-guided patch search. These extensions yield results that are comparable to the most recent state-of-the-art methods.",1
"Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.",0
"The goal of video super-resolution (SR) is to create a series of high-resolution (HR) frames that accurately reflect the details of their low-resolution (LR) counterparts in a consistent and believable manner. However, the primary challenge of video SR lies in effectively utilizing the temporal dependency between consecutive frames. Current deep learning methods rely on estimating optical flows between LR frames to provide this temporal dependency, but the resolution discrepancy between LR optical flows and HR outputs can impede the recovery of fine details. In this study, we propose an end-to-end video SR network that can super-resolve both optical flows and images. By using HR optical flows to encode temporal dependency, we can improve the accuracy of video SR. Our approach involves using an optical flow reconstruction network (OFRnet) to generate HR optical flows and motion compensation to account for temporal dependency. The compensated LR inputs are then fed into a super-resolution network (SRnet) to produce the final SR results. Our experiments demonstrate that the use of HR optical flows significantly improves SR performance. Our network achieves state-of-the-art results on the Vid4 and DAVIS-10 datasets.",1
"This paper tackles the problem of real-time semantic segmentation of high definition videos using a hybrid GPU / CPU approach. We propose an Efficient Video Segmentation(EVS) pipeline that combines:   (i) On the CPU, a very fast optical flow method, that is used to exploit the temporal aspect of the video and propagate semantic information from one frame to the next. It runs in parallel with the GPU.   (ii) On the GPU, two Convolutional Neural Networks: A main segmentation network that is used to predict dense semantic labels from scratch, and a Refiner that is designed to improve predictions from previous frames with the help of a fast Inconsistencies Attention Module (IAM). The latter can identify regions that cannot be propagated accurately.   We suggest several operating points depending on the desired frame rate and accuracy. Our pipeline achieves accuracy levels competitive to the existing real-time methods for semantic image segmentation(mIoU above 60%), while achieving much higher frame rates. On the popular Cityscapes dataset with high resolution frames (2048 x 1024), the proposed operating points range from 80 to 1000 Hz on a single GPU and CPU.",0
"The aim of this paper is to address the challenge of real-time semantic segmentation of high definition videos through a hybrid GPU / CPU method. The authors present an Efficient Video Segmentation (EVS) pipeline which involves two elements. Firstly, on the CPU, a rapid optical flow method is employed to exploit the temporal aspect of the video and propagate semantic data from one frame to another, while running in parallel with the GPU. Secondly, two Convolutional Neural Networks are used on the GPU: a primary segmentation network that predicts dense semantic labels from scratch, and a Refiner that enhances predictions from previous frames using a fast Inconsistencies Attention Module (IAM) to identify regions that cannot be propagated accurately. The authors offer a variety of operational points based on desired accuracy and frame rate, with their pipeline achieving competitive accuracy levels for real-time semantic image segmentation (mIoU above 60%), and significantly higher frame rates. The suggested operating points for the Cityscapes dataset with high resolution frames (2048 x 1024) range from 80 to 1000 Hz on a single GPU and CPU.",1
"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.",0
"The technique we present involves the automatic generation of believable depth maps from videos, using non-parametric depth sampling. We showcase how our technique succeeds in scenarios where previous methods have failed, such as with dynamic scenes and non-translating cameras. Our approach is not only applicable to single images but also to videos. When working with videos, we employ local motion cues to enhance the inferred depth maps and ensure temporal depth consistency using optical flow. To train and assess our technique, we use a Kinect-based system to collect a vast dataset that comprises stereoscopic videos with known depths. Our results indicate that our depth estimation technique surpasses the state-of-the-art on benchmark databases. Moreover, our approach can convert a monoscopic video into stereo for 3D visualization automatically. We provide a range of visually appealing outcomes for indoor and outdoor scenes, including examples from the feature film Charade.",1
"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.",0
"We have devised a method that can generate believable depth maps from videos by utilizing non-parametric depth sampling. Our approach can overcome the limitations of previous techniques when dealing with non-translating cameras and dynamic scenes. It can be applied to both individual images and videos, with the latter benefiting from improved depth maps due to local motion cues and temporal depth consistency through optical flow. Our method was evaluated using a large dataset of stereoscopic videos with known depths collected through a Kinect-based system. We have demonstrated that our technique surpasses the current state-of-the-art performance on benchmark databases. Our approach can also automatically transform a monoscopic video into a 3D stereo format for visualization. We have showcased the effectiveness of our method with various indoor and outdoor scenes, including a feature film called Charade.",1
"Event cameras provide a number of benefits over traditional cameras, such as the ability to track incredibly fast motions, high dynamic range, and low power consumption. However, their application into computer vision problems, many of which are primarily dominated by deep learning solutions, has been limited by the lack of labeled training data for events. In this work, we propose a method which leverages the existing labeled data for images by simulating events from a pair of temporal image frames, using a convolutional neural network. We train this network on pairs of images and events, using an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses utilize a pair of pre-trained self-supervised networks which perform optical flow estimation and image reconstruction from events, and constrain our network to generate events which result in accurate outputs from both of these networks. Trained fully end to end, our network learns a generative model for events from images without the need for accurate modeling of the motion in the scene, exhibited by modeling based methods, while also implicitly modeling event noise. Using this simulator, we train a pair of downstream networks on object detection and 2D human pose estimation from events, using simulated data from large scale image datasets, and demonstrate the networks' abilities to generalize to datasets with real events.",0
"Traditional cameras have limitations that event cameras do not, such as the ability to track fast movements, low power consumption, and high dynamic range. However, event cameras have been limited in their application to computer vision problems dominated by deep learning solutions due to the lack of labeled training data for events. This study proposes a method to solve this issue by simulating events from a pair of temporal image frames, using a convolutional neural network. The network is trained on pairs of images and events, with an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses use pre-trained self-supervised networks to constrain the network to generate events that result in accurate outputs from the optical flow estimation and image reconstruction networks. The network learns a generative model for events from images, without the need for accurate modeling of motion in the scene or explicitly modeling event noise. Using this simulator, the study demonstrates the network's ability to generalize to datasets with real events by training downstream networks on object detection and 2D human pose estimation from events, using simulated data from large scale image datasets.",1
"Two-stream networks have achieved great success in video recognition. A two-stream network combines a spatial stream of RGB frames and a temporal stream of Optical Flow to make predictions. However, the temporal redundancy of RGB frames as well as the high-cost of optical flow computation creates challenges for both the performance and efficiency. Recent works instead use modern compressed video modalities as an alternative to the RGB spatial stream and improve the inference speed by orders of magnitudes. Previous works create one stream for each modality which are combined with an additional temporal stream through late fusion. This is redundant since some modalities like motion vectors already contain temporal information. Based on this observation, we propose a compressed domain two-stream network IP TSN for compressed video recognition, where the two streams are represented by the two types of frames (I and P frames) in compressed videos, without needing a separate temporal stream. With this goal, we propose to fully exploit the motion information of P-stream through generalized distillation from optical flow, which largely improves the efficiency and accuracy. Our P-stream runs 60 times faster than using optical flow while achieving higher accuracy. Our full IP TSN, evaluated over public action recognition benchmarks (UCF101, HMDB51 and a subset of Kinetics), outperforms other compressed domain methods by large margins while improving the total inference speed by 20%.",0
"Video recognition has seen great success with the use of two-stream networks, which combine RGB frames and Optical Flow to make predictions. However, RGB frames have temporal redundancy, and optical flow computation is costly, making it difficult to achieve high performance and efficiency. To address this, recent research has explored using compressed video modalities instead of the RGB spatial stream. Previous approaches use one stream for each modality and combine them through late fusion, but this is redundant as some modalities already contain temporal information. To overcome this, we propose the compressed domain two-stream network IP TSN for compressed video recognition, using I and P frames without a separate temporal stream. We also use generalized distillation from optical flow to fully exploit the motion information of P-stream, which significantly improves efficiency and accuracy. Our P-stream runs 60 times faster than using optical flow, with higher accuracy. Our IP TSN outperforms other compressed domain methods on public action recognition benchmarks (UCF101, HMDB51, and a subset of Kinetics), improving total inference speed by 20%.",1
"High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction.",0
"To adapt to climate change effectively, high-resolution nowcasting is crucial, especially for extreme weather. As Deep Learning (DL) techniques have been successful in various fields, including geosciences, we demonstrate the use of DL in precipitation nowcasting. This involves predicting high-resolution (1 km x 1 km) short-term (1 hour) precipitation. We approach forecasting as an image-to-image translation problem and use the UNET convolutional neural network. Our results show that this outperforms three commonly used models: optical flow, persistence, and NOAA's numerical one-hour HRRR nowcasting prediction.",1
"Learning-based visual odometry and SLAM methods demonstrate a steady improvement over past years. However, collecting ground truth poses to train these methods is difficult and expensive. This could be resolved by training in an unsupervised mode, but there is still a large gap between performance of unsupervised and supervised methods. In this work, we focus on generating synthetic data for deep learning-based visual odometry and SLAM methods that take optical flow as an input. We produce training data in a form of optical flow that corresponds to arbitrary camera movement between a real frame and a virtual frame. For synthesizing data we use depth maps either produced by a depth sensor or estimated from stereo pair. We train visual odometry model on synthetic data and do not use ground truth poses hence this model can be considered unsupervised. Also it can be classified as monocular as we do not use depth maps on inference. We also propose a simple way to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. We demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method build upon this model yields state-of-the-art results among unsupervised methods on KITTI dataset and shows promising results on a challenging EuRoC dataset.",0
"Visual odometry and SLAM techniques that rely on machine learning have shown gradual improvement over the years. However, obtaining accurate and reliable ground truth poses for training these methods is a difficult and costly process. One solution to this issue is to train models in an unsupervised manner, but this approach still has a significant performance gap compared to supervised methods. In this study, the focus is on generating synthetic training data for deep learning-based visual odometry and SLAM techniques that use optical flow as an input. The training data is produced in the form of optical flow, which corresponds to the camera's movement between a real and a virtual frame. Synthetic data is generated using depth maps obtained from a depth sensor or estimated from a stereo pair. The visual odometry model is trained on synthetic data without relying on ground truth poses, making it an unsupervised and monocular approach as depth maps are not used during inference. Additionally, a simple method is proposed to convert any visual odometry model into a SLAM technique based on frame matching and graph optimization. The results demonstrate that the synthetically-trained visual odometry model and the proposed SLAM method built on top of this model outperform other unsupervised methods on the KITTI dataset. The method also shows promising results on the challenging EuRoC dataset.",1
"Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper.",0
"Current methods for multi-person pose estimation in videos rely heavily on dense annotations, which can be time-consuming and costly to obtain. To address this issue, our proposed PoseWarper network uses training videos with sparse annotations to learn dense temporal pose propagation and estimation. By training our model to predict human pose in a labeled frame using the features from an unlabeled frame, we can leverage our trained PoseWarper for various applications. We can propagate pose information from manually annotated frames to unlabeled frames, generate pose annotations for the entire video with only a few labeled frames, improve the accuracy of a pose estimator, and aggregate temporal pose information from neighboring frames during inference. Our warping mechanism is more compact and accurate than modern label propagation methods based on optical flow. We have made our code available at https://github.com/facebookresearch/PoseWarper.",1
"The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single- and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.",0
"Human optical flow is widely recognized as a valuable tool for analyzing human movement. Recent methods for optical flow have focused on using deep networks to address this issue. However, the training data employed by these methods fails to encompass the human motion domain. To tackle this challenge, we have created a multi-human optical flow dataset and trained optical flow networks on it. We have generated realistic flow fields in single- and multi-person images using motion capture data and a 3D model of the human body. Subsequently, we have trained optical flow networks to estimate human flow fields from image pairs. We have shown that our trained networks outperform a variety of top methods on test data and can adapt well to real image sequences. The dataset, trained models, and code are available for research purposes.",1
"Moving Object Detection (MOD) is a critical task for autonomous vehicles as moving objects represent higher collision risk than static ones. The trajectory of the ego-vehicle is planned based on the future states of detected moving objects. It is quite challenging as the ego-motion has to be modelled and compensated to be able to understand the motion of the surrounding objects. In this work, we propose a real-time end-to-end CNN architecture for MOD utilizing spatio-temporal context to improve robustness. We construct a novel time-aware architecture exploiting temporal motion information embedded within sequential images in addition to explicit motion maps using optical flow images.We demonstrate the impact of our algorithm on KITTI dataset where we obtain an improvement of 8% relative to the baselines. We compare our algorithm with state-of-the-art methods and achieve competitive results on KITTI-Motion dataset in terms of accuracy at three times better run-time. The proposed algorithm runs at 23 fps on a standard desktop GPU targeting deployment on embedded platforms.",0
"Detecting moving objects is a crucial task for autonomous vehicles since they pose a higher risk of collision compared to stationary objects. To plan the trajectory of the ego-vehicle, it is necessary to consider the future states of detected moving objects. However, this is a challenging task as the motion of the ego-vehicle must be modeled and accounted for to understand the motion of surrounding objects. In this study, we introduce a real-time end-to-end CNN architecture for Moving Object Detection (MOD) that utilizes spatio-temporal context to enhance robustness. Our innovative time-aware architecture incorporates temporal motion information and explicit motion maps using optical flow images. Our algorithm improves the MOD performance by 8% relative to existing methods on the KITTI dataset. Furthermore, our algorithm achieves competitive results on the KITTI-Motion dataset with three times better run-time efficiency. Our proposed algorithm runs at 23 fps on a standard desktop GPU, making it suitable for deployment on embedded platforms.",1
"Deep learning-based video salient object detection has recently achieved great success with its performance significantly outperforming any other unsupervised methods. However, existing data-driven approaches heavily rely on a large quantity of pixel-wise annotated video frames to deliver such promising results. In this paper, we address the semi-supervised video salient object detection task using pseudo-labels. Specifically, we present an effective video saliency detector that consists of a spatial refinement network and a spatiotemporal module. Based on the same refinement network and motion information in terms of optical flow, we further propose a novel method for generating pixel-level pseudo-labels from sparsely annotated frames. By utilizing the generated pseudo-labels together with a part of manual annotations, our video saliency detector learns spatial and temporal cues for both contrast inference and coherence enhancement, thus producing accurate saliency maps. Experimental results demonstrate that our proposed semi-supervised method even greatly outperforms all the state-of-the-art fully supervised methods across three public benchmarks of VOS, DAVIS, and FBMS.",0
"Recently, deep learning-based video salient object detection has proven to be highly successful, surpassing other unsupervised techniques in terms of performance. However, current data-driven methods rely heavily on a substantial amount of pixel-wise annotated video frames to achieve such impressive outcomes. This study introduces a semi-supervised approach to video salient object detection using pseudo-labels. A spatial refinement network and a spatiotemporal module comprise the proposed effective video saliency detector. Additionally, a novel method for generating pixel-level pseudo-labels from sparsely annotated frames is presented, which uses the same refinement network and motion information in terms of optical flow. By incorporating the generated pseudo-labels with some manual annotations, the video saliency detector can learn spatial and temporal cues for both contrast inference and coherence enhancement, resulting in precise saliency maps. Experimental results show that the proposed semi-supervised method significantly outperforms all the state-of-the-art fully supervised methods in three public benchmarks of VOS, DAVIS, and FBMS.",1
"A major challenge for video semantic segmentation is the lack of labeled data. In most benchmark datasets, only one frame of a video clip is annotated, which makes most supervised methods fail to utilize information from the rest of the frames. To exploit the spatio-temporal information in videos, many previous works use pre-computed optical flows, which encode the temporal consistency to improve the video segmentation. However, the video segmentation and optical flow estimation are still considered as two separate tasks. In this paper, we propose a novel framework for joint video semantic segmentation and optical flow estimation. Semantic segmentation brings semantic information to handle occlusion for more robust optical flow estimation, while the non-occluded optical flow provides accurate pixel-level temporal correspondences to guarantee the temporal consistency of the segmentation. Moreover, our framework is able to utilize both labeled and unlabeled frames in the video through joint training, while no additional calculation is required in inference. Extensive experiments show that the proposed model makes the video semantic segmentation and optical flow estimation benefit from each other and outperforms existing methods under the same settings in both tasks.",0
"Video semantic segmentation faces a significant hurdle due to the scarcity of labeled data, with only one frame of a video clip being annotated in most benchmark datasets. This limitation prevents supervised methods from utilizing information from the remaining frames. To tackle this issue, previous works have used pre-computed optical flows to leverage spatio-temporal information. However, video segmentation and optical flow estimation are still viewed as separate tasks. To address this, we present a new approach for the joint estimation of optical flow and video semantic segmentation, which allows semantic information to handle occlusions for more robust optical flow estimation. The non-occluded optical flow, in turn, provides accurate pixel-level temporal correspondences to ensure the segmentation's temporal consistency. Our framework can use both labeled and unlabeled frames in the video during joint training, without any additional computation required during inference. Our experimental results demonstrate that our proposed model benefits from the combination of video semantic segmentation and optical flow estimation, surpassing current methods in both tasks under the same settings.",1
"Majority of state-of-the-art monocular depth estimation methods are supervised learning approaches. The success of such approaches heavily depends on the high-quality depth labels which are expensive to obtain. Some recent methods try to learn depth networks by leveraging unsupervised cues from monocular videos which are easier to acquire but less reliable. In this paper, we propose to resolve this dilemma by transferring knowledge from synthetic videos with easily obtainable ground-truth depth labels. Due to the stylish difference between synthetic and real images, we propose a temporally-consistent domain adaptation (TCDA) approach that simultaneously explores labels in the synthetic domain and temporal constraints in the videos to improve style transfer and depth prediction. Furthermore, we make use of the ground-truth optical flow and pose information in the synthetic data to learn moving mask and pose prediction networks. The learned moving masks can filter out moving regions that produces erroneous temporal constraints and the estimated poses provide better initializations for estimating temporal constraints. Experimental results demonstrate the effectiveness of our method and comparable performance against state-of-the-art.",0
"The majority of modern techniques for estimating monocular depth involve supervised learning methods. However, these approaches rely heavily on high-quality depth labels, which can be expensive to obtain. Some recent methods have attempted to leverage unsupervised cues from monocular videos, which are easier to acquire but less reliable. In this paper, we propose a solution to this dilemma by transferring knowledge from synthetic videos that have readily available ground-truth depth labels. Because synthetic and real images differ stylistically, we introduce a temporally-consistent domain adaptation (TCDA) approach that simultaneously explores labels in the synthetic domain and temporal constraints in the videos to improve depth prediction and style transfer. Additionally, we utilize ground-truth optical flow and pose information in the synthetic data to develop moving mask and pose prediction networks. The learned moving masks can filter out erroneous temporal constraints produced by moving regions, and the estimated poses provide better initializations for estimating temporal constraints. Our experimental results demonstrate the effectiveness of our method and comparable performance to state-of-the-art techniques.",1
"We propose a torus model for high-contrast patches of optical flow. Our model is derived from a database of ground-truth optical flow from the computer-generated video \emph{Sintel}, collected by Butler et al.\ in \emph{A naturalistic open source movie for optical flow evaluation}. Using persistent homology and zigzag persistence, popular tools from the field of computational topology, we show that the high-contrast $3\times 3$ patches from this video are well-modeled by a \emph{torus}, a nonlinear 2-dimensional manifold. Furthermore, we show that the optical flow torus model is naturally equipped with the structure of a fiber bundle, related to the statistics of range image patches.",0
"A torus model is suggested for optical flow's high-contrast patches. Our model is derived from a ground-truth optical flow database from the computer-generated video, ""Sintel"", gathered by Butler et al. in ""A naturalistic open source movie for optical flow evaluation."" We utilize popular tools from computational topology, persistent homology and zigzag persistence, to demonstrate that the high-contrast $3 \times 3$ patches from the video can be appropriately modeled by a nonlinear 2-dimensional manifold, the torus. Additionally, we demonstrate that the optical flow torus model is naturally structured as a fiber bundle that relates to the range image patch statistics.",1
"Scene flow is a challenging task aimed at jointly estimating the 3D structure and motion of the sensed environment. Although deep learning solutions achieve outstanding performance in terms of accuracy, these approaches divide the whole problem into standalone tasks (stereo and optical flow) addressing them with independent networks. Such a strategy dramatically increases the complexity of the training procedure and requires power-hungry GPUs to infer scene flow barely at 1 FPS. Conversely, we propose DWARF, a novel and lightweight architecture able to infer full scene flow jointly reasoning about depth and optical flow easily and elegantly trainable end-to-end from scratch. Moreover, since ground truth images for full scene flow are scarce, we propose to leverage on the knowledge learned by networks specialized in stereo or flow, for which much more data are available, to distill proxy annotations. Exhaustive experiments show that i) DWARF runs at about 10 FPS on a single high-end GPU and about 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, with moderate drop in accuracy compared to 10x deeper models, ii) learning from many distilled samples is more effective than from the few, annotated ones available. Code available at: https://github.com/FilippoAleotti/Dwarf-Tensorflow",0
"The task of scene flow involves estimating the 3D structure and motion of the environment, which is challenging. Although deep learning approaches are accurate, they typically use separate networks for stereo and optical flow, making the training process complex and requiring powerful GPUs for inference. To address this, we propose a lightweight architecture called DWARF that can easily and elegantly infer full scene flow by jointly reasoning about depth and optical flow. We also suggest leveraging the knowledge learned by specialized networks in stereo or flow to distill proxy annotations since ground truth images are scarce. Our experiments show that DWARF runs at 10 FPS on a high-end GPU and 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, with a moderate drop in accuracy compared to deeper models. Additionally, learning from many distilled samples is more effective than from a few annotated ones. The code for DWARF is available at: https://github.com/FilippoAleotti/Dwarf-Tensorflow.",1
"Spatial-temporal feature learning is of vital importance for video emotion recognition. Previous deep network structures often focused on macro-motion which extends over long time scales, e.g., on the order of seconds. We believe integrating structures capturing information about both micro- and macro-motion will benefit emotion prediction, because human perceive both micro- and macro-expressions. In this paper, we propose to combine micro- and macro-motion features to improve video emotion recognition with a two-stream recurrent network, named MIMAMO (Micro-Macro-Motion) Net. Specifically, smaller and shorter micro-motions are analyzed by a two-stream network, while larger and more sustained macro-motions can be well captured by a subsequent recurrent network. Assigning specific interpretations to the roles of different parts of the network enables us to make choice of parameters based on prior knowledge: choices that turn out to be optimal. One of the important innovations in our model is the use of interframe phase differences rather than optical flow as input to the temporal stream. Compared with the optical flow, phase differences require less computation and are more robust to illumination changes. Our proposed network achieves state of the art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset. The most significant gains are for arousal prediction, for which motion information is intuitively more informative. Source code is available at https://github.com/wtomin/MIMAMO-Net.",0
"The ability to learn spatial-temporal features is crucial for accurately recognizing emotions in videos. Traditional deep network structures have primarily focused on capturing macro-motion, which occurs over long time periods, such as several seconds. However, as humans perceive both micro- and macro-expressions, we believe that incorporating features that capture information about both types of motion will enhance emotion prediction. In this study, we introduce the MIMAMO (Micro-Macro-Motion) Net, a two-stream recurrent network that combines micro- and macro-motion features to improve video emotion recognition. The two-stream network analyzes smaller and shorter micro-motions, while a subsequent recurrent network captures larger and more sustained macro-motions. By assigning specific roles to different parts of the network, we can select optimal parameters based on prior knowledge. One innovative aspect of our model is the use of interframe phase differences instead of optical flow as input to the temporal stream, which requires less computation and is more robust to illumination changes. Our proposed network achieves state-of-the-art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset, with the most significant improvements observed in arousal prediction. The source code for our model is available at https://github.com/wtomin/MIMAMO-Net.",1
"Moving object detection is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, LiDAR sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work, we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset ""Dark KITTI"". We obtain a 10.1% relative improvement on Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 18 fps on a standard desktop GPU using $256\times1224$ resolution images.",0
"Detecting moving objects is a crucial task for autonomous vehicles, as dynamic objects pose a higher risk of collision than static ones. Therefore, when planning our own path, we must take into account the future states of the moving elements in the scene. To perceive motion, we can use temporal information such as optical flow. However, conventional optical flow computation based on camera sensors is susceptible to failure in low-light conditions. In contrast, LiDAR sensors are not affected by illumination as they measure the time-of-flight of their own emitted lasers. This study proposes a real-time CNN architecture for Moving Object Detection (MOD) that captures motion information from both camera and LiDAR sensors to achieve robustness under low-light conditions. The proposed algorithm was tested on the KITTI dataset, including a new dataset called ""Dark KITTI,"" which simulates a low-light environment. Results showed a 10.1% relative improvement on Dark-KITTI and a 4.25% improvement on standard KITTI compared to the baselines. The algorithm runs at 18 fps on a standard desktop GPU with $256\times1224$ resolution images.",1
"It is expensive to generate real-life image labels and there is a domain gap between real-life and simulated images, hence a model trained on the latter cannot adapt to the former. Solving this can totally eliminate the need for labeling real-life datasets completely. Class balanced self-training is one of the existing techniques that attempt to reduce the domain gap. Moreover, augmenting RGB with flow maps has improved performance in simple semantic segmentation and geometry is preserved across domains. Hence, by augmenting images with dense optical flow map, domain adaptation in semantic segmentation can be improved.",0
"Generating labels for real-life images is costly and poses a challenge due to the domain gap between real-life and simulated images. Consequently, models trained on simulated images cannot adapt well to real-life images. Addressing this challenge can eliminate the need for labeling real-life datasets altogether. One technique that aims to reduce the domain gap is class balanced self-training. Additionally, incorporating flow maps with RGB has shown to enhance the performance of simple semantic segmentation while preserving geometry across domains. Therefore, improving domain adaptation in semantic segmentation can be achieved by augmenting images with dense optical flow maps.",1
"In this paper we present a novel approach for depth map enhancement from an RGB-D video sequence. The basic idea is to exploit the shading information in the color image. Instead of making assumption about surface albedo or controlled object motion and lighting, we use the lighting variations introduced by casual object movement. We are effectively calculating photometric stereo from a moving object under natural illuminations. The key technical challenge is to establish correspondences over the entire image set. We therefore develop a lighting insensitive robust pixel matching technique that out-performs optical flow method in presence of lighting variations. In addition we present an expectation-maximization framework to recover the surface normal and albedo simultaneously, without any regularization term. We have validated our method on both synthetic and real datasets to show its superior performance on both surface details recovery and intrinsic decomposition.",0
"A new method for improving depth maps from RGB-D video sequences is introduced in this paper. The approach utilizes shading information from the color image, instead of relying on assumptions about surface albedo or controlled object motion and lighting. Lighting variations introduced by natural object movement are used to effectively calculate photometric stereo. The main technical hurdle involves establishing correspondences across the entire image set, which is accomplished through a lighting-insensitive, robust pixel matching technique that outperforms optical flow methods in the presence of lighting variations. Additionally, an expectation-maximization framework is presented for recovering surface normal and albedo simultaneously, without the need for regularization terms. Results from both synthetic and real datasets demonstrate the method's superior performance in recovering surface details and intrinsic decomposition.",1
"Synthetic visual data can provide practically infinite diversity and rich labels, while avoiding ethical issues with privacy and bias. However, for many tasks, current models trained on synthetic data generalize poorly to real data. The task of 3D human pose estimation is a particularly interesting example of this sim2real problem, because learning-based approaches perform reasonably well given real training data, yet labeled 3D poses are extremely difficult to obtain in the wild, limiting scalability. In this paper, we show that standard neural-network approaches, which perform poorly when trained on synthetic RGB images, can perform well when the data is pre-processed to extract cues about the person's motion, notably as optical flow and the motion of 2D keypoints. Therefore, our results suggest that motion can be a simple way to bridge a sim2real gap when video is available. We evaluate on the 3D Poses in the Wild dataset, the most challenging modern benchmark for 3D pose estimation, where we show full 3D mesh recovery that is on par with state-of-the-art methods trained on real 3D sequences, despite training only on synthetic humans from the SURREAL dataset.",0
"Using synthetic visual data can offer a wide range of options and accurate labels without raising concerns about privacy or bias. However, current models trained on synthetic data often fall short when it comes to real-world applications. The challenge of estimating 3D human pose is a prime example of this problem, as it is difficult to obtain labeled 3D poses outside of a controlled environment. Despite this limitation, we have found that standard neural-network approaches can perform well when pre-processed to include cues about a person's motion, such as optical flow and the motion of 2D keypoints. By leveraging motion, we can bridge the gap between synthetic and real data when video is available. Our evaluation on the challenging 3D Poses in the Wild dataset reveals that our method produces full 3D mesh recovery that is comparable to state-of-the-art methods trained on real 3D sequences, despite only using synthetic humans from the SURREAL dataset for training.",1
"Architecture optimization, which is a technique for finding an efficient neural network that meets certain requirements, generally reduces to a set of multiple-choice selection problems among alternative sub-structures or parameters. The discrete nature of the selection problem, however, makes this optimization difficult. To tackle this problem we introduce a novel concept of a trainable gate function. The trainable gate function, which confers a differentiable property to discretevalued variables, allows us to directly optimize loss functions that include non-differentiable discrete values such as 0-1 selection. The proposed trainable gate can be applied to pruning. Pruning can be carried out simply by appending the proposed trainable gate functions to each intermediate output tensor followed by fine-tuning the overall model, using any gradient-based training methods. So the proposed method can jointly optimize the selection of the pruned channels while fine-tuning the weights of the pruned model at the same time. Our experimental results demonstrate that the proposed method efficiently optimizes arbitrary neural networks in various tasks such as image classification, style transfer, optical flow estimation, and neural machine translation.",0
"The process of architecture optimization involves finding an efficient neural network that meets specific requirements by selecting from various sub-structures or parameters. However, due to the discrete nature of this selection problem, optimization can be challenging. To address this issue, we propose a new concept called a trainable gate function that makes discrete-valued variables differentiable, enabling us to optimize loss functions that include non-differentiable discrete values like 0-1 selection. This trainable gate function can be applied to pruning, which involves appending the proposed function to each intermediate output tensor and fine-tuning the overall model using gradient-based training methods. This approach allows for the joint optimization of pruned channel selection and weight fine-tuning. Our experimental results demonstrate that this method is effective in optimizing various neural networks for tasks such as image classification, style transfer, optical flow estimation, and neural machine translation.",1
"Generating temporal action proposals remains a very challenging problem, where the main issue lies in predicting precise temporal proposal boundaries and reliable action confidence in long and untrimmed real-world videos. In this paper, we propose an efficient and unified framework to generate temporal action proposals named Dense Boundary Generator (DBG), which draws inspiration from boundary-sensitive methods and implements boundary classification and action completeness regression for densely distributed proposals. In particular, the DBG consists of two modules: Temporal boundary classification (TBC) and Action-aware completeness regression (ACR). The TBC aims to provide two temporal boundary confidence maps by low-level two-stream features, while the ACR is designed to generate an action completeness score map by high-level action-aware features. Moreover, we introduce a dual stream BaseNet (DSB) to encode RGB and optical flow information, which helps to capture discriminative boundary and actionness features. Extensive experiments on popular benchmarks ActivityNet-1.3 and THUMOS14 demonstrate the superiority of DBG over the state-of-the-art proposal generator (e.g., MGG and BMN). Our code will be made available upon publication.",0
"The task of generating temporal action proposals is quite difficult as it involves predicting precise boundaries and reliable confidence in long and untrimmed real-world videos. This paper presents a novel and efficient framework called Dense Boundary Generator (DBG) that draws inspiration from boundary-sensitive methods. The DBG consists of two modules, namely Temporal Boundary Classification (TBC) and Action-Aware Completeness Regression (ACR), which respectively aim to provide temporal boundary confidence maps and action completeness score maps. Additionally, a dual stream BaseNet (DSB) is introduced to encode RGB and optical flow information, enabling the capture of discriminative boundary and actionness features. Extensive experiments conducted on popular benchmarks ActivityNet-1.3 and THUMOS14 reveal that DBG outperforms state-of-the-art proposal generators such as MGG and BMN. The code will be made available upon publication.",1
"Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",0
"Video enhancement algorithms often use optical flow to align frames in a video sequence, but precise flow estimation is difficult and optical flow may not always be the best option for certain video processing tasks. This paper introduces task-oriented flow (TOFlow), a motion representation that is learned in a self-supervised, task-specific manner. A neural network is developed with a motion estimation component and a video processing component, which are trained together to learn TOFlow. A large, high-quality video dataset called Vimeo-90K is created to test TOFlow's performance in low-level video processing tasks. Results show that TOFlow outperforms traditional optical flow on standard benchmarks and in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",1
"In recent years, artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest. DL is widely used today and has expanded into various interesting areas. It is becoming more popular in cross-subject research, such as studies of smart city systems, which combine computer science with engineering applications. Human action detection is one of these areas. Human action detection is an interesting challenge due to its stringent requirements in terms of computing speed and accuracy. High-accuracy real-time object tracking is also considered a significant challenge. This paper integrates the YOLO detection network, which is considered a state-of-the-art tool for real-time object detection, with motion vectors and the Coyote Optimization Algorithm (COA) to construct a real-time human action localization and tracking system. The proposed system starts with the extraction of motion information from a compressed video stream and the extraction of appearance information from RGB frames using an object detector. Then, a fusion step between the two streams is performed, and the results are fed into the proposed action tracking model. The COA is used in object tracking due to its accuracy and fast convergence. The basic foundation of the proposed model is the utilization of motion vectors, which already exist in a compressed video bit stream and provide sufficient information to improve the localization of the target action without requiring high consumption of computational resources compared with other popular methods of extracting motion information, such as optical flows. This advantage allows the proposed approach to be implemented in challenging environments where the computational resources are limited, such as Internet of Things (IoT) systems.",0
"The global interest in artificial intelligence (AI) based on deep learning (DL) has recently increased significantly. DL has expanded into various areas, including cross-subject research, such as smart city systems that combine computer science with engineering applications. Human action detection is a challenging area that requires both high computing speed and accuracy. Real-time object tracking is also a significant challenge. To address these challenges, this paper proposes a real-time human action localization and tracking system that integrates the YOLO detection network, motion vectors, and the Coyote Optimization Algorithm (COA). The system starts by extracting motion and appearance information from a compressed video stream using an object detector. The two streams are then fused, and the results are fed into the proposed action tracking model. The COA is used for object tracking because of its accuracy and fast convergence. The proposed model utilizes motion vectors, which already exist in a compressed video bit stream and provide sufficient information to improve target action localization without requiring high computational resources. This feature makes the proposed approach suitable for challenging environments with limited computational resources, such as Internet of Things (IoT) systems.",1
"Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.",0
"The detection of fine-grained actions is a significant task for various fields such as robotics and human-computer interaction. Currently, the common approach involves a two-stage process comprising the extraction of local spatio-temporal features, followed by temporal modeling to capture long-term dependencies. While recent research has focused on long-term modeling, this study aims to improve the efficiency of feature production for fine-grained motion. To achieve this goal, a novel locally-consistent deformable convolution approach is proposed, which enforces a local coherency constraint and utilizes the change in receptive fields to capture motion information effectively. Our model learns spatio-temporal features jointly, instead of using spatial and temporal streams separately. The temporal component is learned from the feature space instead of pixel space such as optical flow. The produced features can be used flexibly with other long-temporal modeling networks like ST-CNN, DilatedTCN, and ED-TCN. Our proposed approach outperforms the original long-temporal models on two fine-grained action datasets, 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39%, respectively.",1
"Inspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.",0
"The concept behind Curriculum Learning (CL) is based on the way humans and animals learn, where the difficulty of the training data is gradually increased. In this study, we examine whether CL can be used to solve complex geometry problems such as monocular Visual Odometry (VO). Unlike traditional CL approaches, we introduce a new CL method for learning monocular VO geometry by progressively increasing the difficulty of the learning objective during training. For this purpose, we suggest a novel geometry-aware objective function that optimizes relative and composite transformations using bounded pose regression loss over small windows. To learn this objective, we develop CL-VO, a model consisting of a cascade optical flow network followed by a recurrent network with a differentiable windowed composition layer. Our experiments on three real-world datasets show that CL-VO outperforms existing feature-based and learning-based VO methods.",1
"The deep learning-based visual tracking algorithms such as MDNet achieve high performance leveraging to the feature extraction ability of a deep neural network. However, the tracking efficiency of these trackers is not very high due to the slow feature extraction for each frame in a video. In this paper, we propose an effective tracking algorithm to alleviate the time-consuming problem. Specifically, we design a deep flow collaborative network, which executes the expensive feature network only on sparse keyframes and transfers the feature maps to other frames via optical flow. Moreover, we raise an effective adaptive keyframe scheduling mechanism to select the most appropriate keyframe. We evaluate the proposed approach on large-scale datasets: OTB2013 and OTB2015. The experiment results show that our algorithm achieves considerable speedup and high precision as well.",0
"MDNet and other deep learning-based visual tracking algorithms rely on the feature extraction capability of deep neural networks to achieve high performance. However, these trackers suffer from low efficiency due to the slow feature extraction process for each video frame. To address this issue, we propose an efficient tracking algorithm in this paper. Our solution is based on a deep flow collaborative network that utilizes the feature network only on sparse keyframes and transfers the feature maps to other frames using optical flow. Additionally, we introduce an adaptive keyframe scheduling mechanism to select the optimal keyframe. Our proposed approach is evaluated on large-scale datasets: OTB2013 and OTB2015, and the results demonstrate a significant improvement in both speed and accuracy.",1
"Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving.",0
"It is extremely difficult to predict future video frames due to the various factors that contribute to how frames change over time. Previous solutions have required complex inductive biases in network architectures, such as segmentation masks, optical flow, and foreground and background separation. However, we challenge the necessity of such handcrafted architectures and propose a different approach: maximizing network capacity while finding minimal inductive bias for video prediction. To investigate this, we conducted a large-scale empirical study and obtained state-of-the-art performance by training large models on three different datasets, each for modeling different scenarios.",1
"The paper addresses the problem of motion saliency in videos, that is, identifying regions that undergo motion departing from its context. We propose a new unsupervised paradigm to compute motion saliency maps. The key ingredient is the flow inpainting stage. Candidate regions are determined from the optical flow boundaries. The residual flow in these regions is given by the difference between the optical flow and the flow inpainted from the surrounding areas. It provides the cue for motion saliency. The method is flexible and general by relying on motion information only. Experimental results on the DAVIS 2016 benchmark demonstrate that the method compares favourably with state-of-the-art video saliency methods.",0
"The paper deals with the issue of motion saliency in videos, which involves identifying regions that exhibit motion that is different from their surroundings. We present a novel unsupervised approach for computing motion saliency maps, which relies on the flow inpainting phase. The initial step involves identifying potential regions based on the optical flow boundaries. The residual flow in these areas is determined by comparing the optical flow with the flow that is inpainted from the surrounding regions, providing a cue for motion saliency. The method is versatile and general, as it solely relies on motion information. The experimental outcomes on the DAVIS 2016 benchmark display that the method performs well compared to current video saliency techniques.",1
"Robust and computationally efficient anomaly detection in videos is a problem in video surveillance systems. We propose a technique to increase robustness and reduce computational complexity in a Convolutional Neural Network (CNN) based anomaly detector that utilizes the optical flow information of video data. We reduce the complexity of the network by denoising the intermediate layer outputs of the CNN and by using powers-of-two weights, which replaces the computationally expensive multiplication operations with bit-shift operations. Denoising operation during inference forces small valued intermediate layer outputs to zero. The number of zeros in the network significantly increases as a result of denoising, we can implement the CNN about 10% faster than a comparable network while detecting all the anomalies in the testing set. It turns out that denoising operation also provides robustness because the contribution of small intermediate values to the final result is negligible. During training we also generate motion vector images by a Generative Adversarial Network (GAN) to improve the robustness of the overall system. We experimentally observe that the resulting system is robust to background motion.",0
"Video surveillance systems face the challenge of detecting anomalies in videos efficiently and with reliability. To address this issue, we present a method that enhances the Convolutional Neural Network (CNN) based anomaly detector's robustness and reduces its computational complexity by incorporating optical flow information. We simplify the CNN by denoising intermediate layer outputs and utilizing powers-of-two weights instead of multiplication operations. This denoising operation during inference removes small valued intermediate layer outputs, resulting in a considerable increase in zeros within the network. By implementing the CNN with this approach, we can detect all anomalies in the testing set while achieving a 10% faster processing speed than a comparable network. Furthermore, the denoising operation also contributes to the system's robustness by reducing the impact of small intermediate values on the final result. To further improve the system's resilience to background motion, we generate motion vector images using a Generative Adversarial Network (GAN) during training. Through experimentation, we observed that the resulting system is highly resilient to background motion.",1
"In this research, Piano performances have been analyzed only based on visual information. Computer vision algorithms, e.g., Hough transform and binary thresholding, have been applied to find where the keyboard and specific keys are located. At the same time, Convolutional Neural Networks(CNNs) has been also utilized to find whether specific keys are pressed or not, and how much intensity the keys are pressed only based on visual information. Especially for detecting intensity, a new method of utilizing spatial, temporal CNNs model is devised. Early fusion technique is especially applied in temporal CNNs architecture to analyze hand movement. We also make a new dataset for training each model. Especially when finding an intensity of a pressed key, both of video frames and their optical flow images are used to train models to find effectiveness.",0
"The study focused on analyzing piano performances using visual information only. Computer vision techniques, such as Hough transform and binary thresholding, were employed to identify the location of the keyboard and certain keys. Additionally, Convolutional Neural Networks (CNNs) were utilized to determine if specific keys were pressed and the amount of pressure applied, solely based on visual data. To detect the intensity of key presses, a novel spatial-temporal CNN model was developed. The temporal CNNs architecture employed the early fusion technique to analyze hand movements. A new dataset was also created to train each model, with video frames and their optical flow images used to enhance the effectiveness of identifying key intensity.",1
"We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.",0
"SENSE is a compact network designed for holistic scene flow estimation. It shares encoder features among four closely-related tasks - optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our approach is based on the idea that sharing features can reduce network size, improve feature representation, and optimize interactions among tasks for handling partially labeled data. During training, we can add decoders for different tasks using a shared encoder, resulting in a modular design that leads to an efficient model at inference time. By exploiting interactions among the tasks, we can introduce distillation and self-supervised losses in addition to supervised losses, which can improve performance on real-world data. SENSE outperforms state-of-the-art models on optical flow benchmarks and runs as fast as networks designed specifically for optical flow. It also compares favorably against the state of the art on stereo and scene flow while using much less memory.",1
"Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approaches tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators of [70], we introduce a technique to establish dense correspondences between pixel embeddings of a reference ""anchor"" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of $81.7\%$, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the ViSal video saliency dataset, showing results competitive with the state of the art.",0
"Typically, unsupervised video object segmentation is tackled using recurrent neural networks and optical flow methods. While these approaches are complex, they tend to prioritize short-term temporal dependencies, leading to inaccuracies and drift over time. Additionally, simple image segmentation models can perform competitively against these methods. Therefore, we propose exploring simple yet effective strategies to model long-term temporal dependencies. Our approach involves establishing dense correspondences between pixel embeddings of a reference anchor frame and the current frame to learn pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Our method achieves a mean IoU of 81.7% on the DAVIS-2016 leaderboard of unsupervised methods, ranking first, and remains competitive against state-of-the-art online semi-supervised approaches, without online supervision. We also demonstrate competitive results on the FBMS dataset and the ViSal video saliency dataset.",1
"Action recognition is a key problem in computer vision that labels videos with a set of predefined actions. Capturing both, semantic content and motion, along the video frames is key to achieve high accuracy performance on this task. Most of the state-of-the-art methods rely on RGB frames for extracting the semantics and pre-computed optical flow fields as a motion cue. Then, both are combined using deep neural networks. Yet, it has been argued that such models are not able to leverage the motion information extracted from the optical flow, but instead the optical flow allows for better recognition of people and objects in the video. This urges the need to explore different cues or models that can extract motion in a more informative fashion. To tackle this issue, we propose to explore the predictive coding network, so called PredNet, a recurrent neural network that propagates predictive coding errors across layers and time steps. We analyze whether PredNet can better capture motions in videos by estimating over time the representations extracted from pre-trained networks for action recognition. In this way, the model only relies on the video frames, and does not need pre-processed optical flows as input. We report the effectiveness of our proposed model on UCF101 and HMDB51 datasets.",0
"The task of recognizing actions in videos is a crucial problem in computer vision. To achieve high accuracy performance, it is important to capture both the semantic content and motion across video frames. Most current methods utilize RGB frames to extract semantics and pre-calculated optical flow fields for motion cues, which are then combined using deep neural networks. However, there is a debate on whether these models effectively use motion information from optical flow or if it simply enhances recognition of people and objects in the video. This highlights the need to explore alternative cues or models that can extract motion in a more informative manner. To address this, we propose using the PredNet, a recurrent neural network that propagates predictive coding errors across layers and time steps. We investigate whether the PredNet can better capture motion in videos by estimating the representations extracted from pre-trained networks for action recognition over time. Our proposed model only relies on video frames and does not require pre-processed optical flows as input. We demonstrate the effectiveness of our model on UCF101 and HMDB51 datasets.",1
"Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.",0
"State-of-the-art performance in optical flow estimation is accomplished by deep neural nets. Given that optical flow is used in safety-critical applications like self-driving cars, it is essential to understand the robustness of these techniques. Recent studies have demonstrated that deep neural networks are easily fooled by adversarial attacks causing misclassification of objects. However, the vulnerability of optical flow networks to adversarial attacks has yet to be explored. This research extends adversarial patch attacks to optical flow networks and reveals that such attacks can compromise their performance. The study shows that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. These attacks result in noisy flow estimates that extend beyond the region of the attack, potentially erasing the motion of objects in the scene. While encoder-decoder architecture networks are highly vulnerable to these attacks, spatial pyramid architecture networks are less affected. The research analyses the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques that are resilient to these attacks. Additionally, the study demonstrates the practicality of these attacks by placing a printed pattern into real scenes.",1
"Though machine learning has achieved notable success in modeling sequential and spatial data for speech recognition and in computer vision, applications to remote sensing and climate science problems are seldom considered. In this paper, we demonstrate techniques from unsupervised learning of future video frame prediction, to increase the accuracy of ice flow tracking in multi-spectral satellite images. As the volume of cryosphere data increases in coming years, this is an interesting and important opportunity for machine learning to address a global challenge for climate change, risk management from floods, and conserving freshwater resources. Future frame prediction of ice melt and tracking the optical flow of ice dynamics presents modeling difficulties, due to uncertainties in global temperature increase, changing precipitation patterns, occlusion from cloud cover, rapid melting and glacier retreat due to black carbon aerosol deposition, from wildfires or human fossil emissions. We show the adversarial learning method helps improve the accuracy of tracking the optical flow of ice dynamics compared to existing methods in climate science. We present a dataset, IceNet, to encourage machine learning research and to help facilitate further applications in the areas of cryospheric science and climate change.",0
"While machine learning has been successful in modeling sequential and spatial data for speech recognition and computer vision, it has not been widely utilized in remote sensing and climate science. This paper showcases how unsupervised learning techniques used in future video frame prediction can increase the accuracy of ice flow tracking in multi-spectral satellite images. With the increasing amount of cryosphere data, this presents a crucial opportunity for machine learning to address global challenges such as climate change, flood risk management, and freshwater conservation. However, predicting ice melt and tracking ice dynamics optical flow is challenging due to uncertainties in global temperature increase, changing precipitation patterns, occlusion from cloud cover, and rapid melting caused by black carbon aerosol deposition from wildfires or human fossil emissions. Through the use of adversarial learning methods, we demonstrate improved accuracy in tracking ice dynamics optical flow compared to current methods in climate science. Additionally, we introduce the IceNet dataset to encourage further machine learning research and applications in cryospheric science and climate change.",1
"Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets.",0
"The progress of unsupervised learning for depth extraction from videos has been impressive lately, and it has produced results that are similar to those obtained by fully supervised methods in outdoor environments such as KITTI. Nonetheless, using this technology directly in indoor settings poses significant challenges, such as large areas of non-textured regions, more complicated ego-motion of handheld cameras, and transparent glasses and shiny objects. To address these issues, we have introduced a novel training paradigm based on optical flow that reduces the difficulty of unsupervised learning by providing a clearer training target and handling non-texture regions. Our experimental analysis has shown that our approach's output is comparable to fully supervised methods on the NYU Depth V2 benchmark. To our knowledge, this is the first quantitative outcome of a purely unsupervised learning method on indoor datasets.",1
"We address the challenging task of video-based person re-identification. Recent works have shown that splitting the video sequences into clips and then aggregating clip based similarity is appropriate for the task. We show that using a learned clip similarity aggregation function allows filtering out hard clip pairs, e.g. where the person is not clearly visible, is in a challenging pose, or where the poses in the two clips are too different to be informative. This allows the method to focus on clip-pairs which are more informative for the task. We also introduce the use of 3D CNNs for video-based re-identification and show their effectiveness by performing equivalent to previous works, which use optical flow in addition to RGB, while using RGB inputs only. We give quantitative results on three challenging public benchmarks and show better or competitive performance. We also validate our method qualitatively.",0
"Our focus is on the difficult task of person re-identification using video. Previous studies have found that dividing video sequences into clips and combining the similarities between these clips is a suitable approach. We demonstrate that employing a learned function to aggregate clip similarities can eliminate challenging clip pairs, such as those where people are not easily seen, are in awkward positions, or where the poses in the two clips differ too much to be informative. This enables the method to concentrate on clip-pairs that offer more useful information for the task. Additionally, we introduce the use of 3D CNNs for video-based re-identification and show that they are effective, achieving results equivalent to previous works that use both optical flow and RGB inputs, while using only RGB inputs. We present quantitative results using three challenging public benchmarks and demonstrate better or comparative performance. Furthermore, we validate our method qualitatively.",1
"Driver drowsiness increases crash risk, leading to substantial road trauma each year. Drowsiness detection methods have received considerable attention, but few studies have investigated the implementation of a detection approach on a mobile phone. Phone applications reduce the need for specialised hardware and hence, enable a cost-effective roll-out of the technology across the driving population. While it has been shown that three-dimensional (3D) operations are more suitable for spatiotemporal feature learning, current methods for drowsiness detection commonly use frame-based, multi-step approaches. However, computationally expensive techniques that achieve superior results on action recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create bottlenecks for real-time, safety-critical applications on mobile devices. Here, we show how depthwise separable 3D convolutions, combined with an early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. In particular, increased accuracy is achieved when assessment requires motion information, for example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based smartphone application shows the true impact of various approaches on inference times and demonstrates the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. Our model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. Overall, our research is a step towards practical deep learning applications, potentially preventing micro-sleeps and reducing road trauma.",0
"Every year, road accidents are caused by drivers who become drowsy while driving, leading to significant road injuries. Although there has been considerable attention given to detecting driver drowsiness, few studies have explored the implementation of a detection approach on mobile phones. The use of phone applications reduces the need for specialized hardware and is therefore a cost-effective way to deploy this technology to all drivers. While three-dimensional (3D) operations have been shown to be more suitable for spatiotemporal feature learning, current approaches for drowsiness detection commonly use frame-based, multi-step methods. However, these computationally expensive techniques create bottlenecks for real-time, safety-critical applications on mobile devices. This study demonstrates that depthwise separable 3D convolutions, combined with early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. Furthermore, a custom TensorFlow-based smartphone application shows the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. The model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. This research is a significant step towards practical deep learning applications that could prevent micro-sleeps and reduce road trauma.",1
"The automatic detection and tracking of general objects (like persons, animals or cars), text and logos in a video is crucial for many video understanding tasks, and usually real-time processing as required. We propose OmniTrack, an efficient and robust algorithm which is able to automatically detect and track objects, text as well as brand logos in real-time. It combines a powerful deep learning based object detector (YoloV3) with high-quality optical flow methods. Based on the reference YoloV3 C++ implementation, we did some important performance optimizations which will be described. The major steps in the training procedure for the combined detector for text and logo will be presented. We will describe then the OmniTrack algorithm, consisting of the phases preprocessing, feature calculation, prediction, matching and update. Several performance optimizations have been implemented there as well, like doing the object detection and optical flow calculation asynchronously. Experiments show that the proposed algorithm runs in real-time for standard definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.",0
"Detecting and tracking objects, text, and logos in a video is essential for video understanding tasks, and real-time processing is often necessary. To address this need, we introduce the OmniTrack algorithm, which efficiently and robustly detects and tracks these elements in real-time. The algorithm combines a deep learning-based object detector (YoloV3) with high-quality optical flow methods. We optimized the algorithm's performance based on the YoloV3 C++ implementation and present the key steps of the training procedure for the combined detector. The OmniTrack algorithm consists of preprocessing, feature calculation, prediction, matching, and update phases, and we implemented several performance optimizations, such as asynchronous object detection and optical flow calculation. Our experiments demonstrate that the proposed algorithm can run in real-time for standard definition ($720x576$) video on a Quadro RTX 5000 GPU-equipped PC.",1
"Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",0
"Recent years have seen great success in deep video action recognition models, but acquiring the large amounts of manually annotated data required for these models is expensive and labor-intensive. This study explores the use of synthetic data for video action recognition, as synthetic data has proven effective for other computer vision tasks. The authors propose an interpretable parametric generative model for human action videos that employs procedural generation, physics models, and other components of modern game engines. Using this model, they generate a diverse, realistic, and physically plausible dataset of human action videos (PHAV). PHAV includes 39,982 videos, with over 1,000 examples for each of 35 action categories. The video generation approach is not limited to existing motion capture sequences, with 14 of the categories being procedurally defined synthetic actions. Additionally, each video is represented with 6 different data modalities, generated almost simultaneously using modern GPU technology. To utilize PHAV, the authors introduce a deep multi-task representation learning architecture that can learn from both synthetic and real video datasets, even when their action categories differ. Experimental results on the UCF-101 and HMDB-51 benchmarks show that combining synthetic videos with small real-world datasets can improve recognition performance, and that the proposed approach outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",1
"Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available.",0
"The objective of video salient object detection is to identify the most visually distinct objects in a video. One crucial aspect is how to consider object motion effectively during this process. However, current methods either do not take motion cues into account or neglect spatial contexts within optical flow images. To address this issue, we have developed a motion-guided video salient object detection network that performs two sub-tasks using two sub-networks. One sub-network identifies salient objects in still images, while the other detects motion saliency in optical flow images. Additionally, we have introduced motion-guided attention modules that use the motion saliency sub-network to enhance the still image sub-network. Both sub-networks are adapted to each other through end-to-end training. Our approach significantly outperforms existing state-of-the-art algorithms on various benchmarks. We expect that our straightforward and efficient approach will serve as a reliable starting point and facilitate future research in video salient object detection. Code and models will be provided.",1
"In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.",0
"Our proposed approach for semi-supervised video object segmentation involves using capsules instead of the current frame-based methods. These methods often require optical flow to ensure temporal consistency across frames, which can be challenging to calculate. Our CapsuleVOS network can segment multiple frames simultaneously, depending on a reference frame and segmentation mask, thanks to a unique routing algorithm for attention-based efficient capsule selection. Our approach addresses two common challenges in video object segmentation: the segmentation of small objects and occlusion of objects over time. We combat the former with our zooming module that processes small spatial regions of the video, while the latter is addressed through our novel memory module based on recurrent networks that tracks objects when they go out of frame or become occluded. Our network is trained end-to-end and is effective on two benchmark video object segmentation datasets. It outperforms current offline approaches on the Youtube-VOS dataset and is almost twice as fast as competing methods. The code we used is available to the public at https://github.com/KevinDuarte/CapsuleVOS.",1
"Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.",0
"The use of synthetic data has become increasingly popular for training deep learning models, specifically in computer vision and other fields. This study aims to provide a comprehensive overview of the various developments and applications of synthetic data. The first section covers synthetic datasets used for low-level and high-level computer vision problems, as well as synthetic environments for autonomous driving, indoor navigation, aerial navigation, and robotics. The study also delves into how synthetic data can be used outside of computer vision, including in neural programming, bioinformatics, and NLP. Furthermore, the study explores techniques for improving synthetic data development and alternative methods such as GANs. The second section concentrates on the synthetic-to-real domain adaptation problem, including synthetic-to-real refinement and domain adaptation at the feature/model level. The third section looks at privacy-related applications of synthetic data, discussing the work on generating synthetic datasets with differential privacy guarantees. Finally, the study concludes by identifying the most promising areas for future research in synthetic data studies.",1
"We propose a novel conditional GAN (cGAN) model for continuous fine-grained human action segmentation, that utilises multi-modal data and learned scene context information. The proposed approach utilises two GANs: termed Action GAN and Auxiliary GAN, where the Action GAN is trained to operate over the current RGB frame while the Auxiliary GAN utilises supplementary information such as depth or optical flow. The goal of both GANs is to generate similar `action codes', a vector representation of the current action. To facilitate this process a context extractor that incorporates data and recent outputs from both modes is used to extract context information to aid recognition. The result is a recurrent GAN architecture which learns a task specific loss function from multiple feature modalities. Extensive evaluations on variants of the proposed model to show the importance of utilising different information streams such as context and auxiliary information in the proposed network; and show that our model is capable of outperforming state-of-the-art methods for three widely used datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities, comprising both static and dynamic camera settings.",0
"Our study presents a new approach to human action segmentation using a conditional GAN (cGAN) model that combines multi-modal data and scene context information. The model consists of two GANs, namely Action GAN and Auxiliary GAN, which operate on RGB frames and supplementary information such as depth or optical flow, respectively. The ultimate objective of both GANs is to generate similar action codes, which are vector representations of the current action. To aid recognition, a context extractor that considers data and recent outputs from both modes is used to extract context information. This leads to a recurrent GAN architecture that learns a task-specific loss function from multiple feature modalities. Our evaluations demonstrate the importance of utilizing different information streams such as context and auxiliary information in the proposed network. Moreover, our model outperforms state-of-the-art methods for three widely used datasets, namely 50 Salads, MERL Shopping, and Georgia Tech Egocentric Activities, which comprise both static and dynamic camera settings.",1
"This paper proposes a vision-based fire and smoke segmentation system which use spatial, temporal and motion information to extract the desired regions from the video frames. The fusion of information is done using multiple features such as optical flow, divergence and intensity values. These features extracted from the images are used to segment the pixels into different classes in an unsupervised way. A comparative analysis is done by using multiple clustering algorithms for segmentation. Here the Markov Random Field performs more accurately than other segmentation algorithms since it characterizes the spatial interactions of pixels using a finite number of parameters. It builds a probabilistic image model that selects the most likely labeling using the maximum a posteriori (MAP) estimation. This unsupervised approach is tested on various images and achieves a frame-wise fire detection rate of 95.39%. Hence this method can be used for early detection of fire in real-time and it can be incorporated into an indoor or outdoor surveillance system.",0
"A system for fire and smoke segmentation using vision-based technology is proposed in this paper. The system utilizes spatial, temporal and motion information to extract desired regions from video frames. Multiple features such as optical flow, divergence and intensity values are fused to extract information from the images, and unsupervised segmentation is utilized to classify pixels into different classes. A comparative analysis is conducted using multiple clustering algorithms for segmentation, with Markov Random Field performing most accurately due to its ability to characterize spatial interactions of pixels with a finite number of parameters. The system achieves a frame-wise fire detection rate of 95.39% in various image tests, making it suitable for real-time early detection of fires and integration into indoor or outdoor surveillance systems.",1
"Infrared human action recognition has many advantages, i.e., it is insensitive to illumination change, appearance variability, and shadows. Existing methods for infrared action recognition are either based on spatial or local temporal information, however, the global temporal information, which can better describe the movements of body parts across the whole video, is not considered. In this letter, we propose a novel global temporal representation named optical-flow stacked difference image (OFSDI) and extract robust and discriminative feature from the infrared action data by considering the local, global, and spatial temporal information together. Due to the small size of the infrared action dataset, we first apply convolutional neural networks on local, spatial, and global temporal stream respectively to obtain efficient convolutional feature maps from the raw data rather than train a classifier directly. Then these convolutional feature maps are aggregated into effective descriptors named three-stream trajectory-pooled deep-convolutional descriptors by trajectory-constrained pooling. Furthermore, we improve the robustness of these features by using the locality-constrained linear coding (LLC) method. With these features, a linear support vector machine (SVM) is adopted to classify the action data in our scheme. We conduct the experiments on infrared action recognition datasets InfAR and NTU RGB+D. The experimental results show that the proposed approach outperforms the representative state-of-the-art handcrafted features and deep learning features based methods for the infrared action recognition.",0
"The benefits of infrared human action recognition include insensitivity to changes in illumination, variations in appearance, and shadows. However, existing methods only rely on spatial or local temporal information, neglecting global temporal information that can better describe whole-body movements. To address this issue, we introduce the optical-flow stacked difference image (OFSDI) as a new global temporal representation and combine it with local, global, and spatial temporal information to extract robust and discriminative features from infrared action data. To deal with the small size of the dataset, we apply convolutional neural networks separately to obtain efficient convolutional feature maps, which are then combined into three-stream trajectory-pooled deep-convolutional descriptors using trajectory-constrained pooling. We further enhance the robustness of these features by using the locality-constrained linear coding (LLC) method and classify the action data using a linear support vector machine (SVM). Our approach outperforms previous handcrafted and deep learning features-based methods on two datasets, InfAR and NTU RGB+D, demonstrating its effectiveness.",1
