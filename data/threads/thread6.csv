"In this work, we address the task of referring image segmentation (RIS), which aims at predicting a segmentation mask for the object described by a natural language expression. Most existing methods focus on establishing unidirectional or directional relationships between visual and linguistic features to associate two modalities together, while the multi-scale context is ignored or insufficiently modeled. Multi-scale context is crucial to localize and segment those objects that have large scale variations during the multi-modal fusion process. To solve this problem, we propose a simple yet effective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple atrous convolutional layers in parallel and further introduces a cascaded branch to fuse visual and linguistic features. The cascaded branch can progressively integrate multi-scale contextual information and facilitate the alignment of two modalities during the multi-modal fusion process. Experimental results on four benchmark datasets demonstrate that our method outperforms most state-of-the-art methods. Code is available at https://github.com/jianhua2022/CMF-Refseg.",0
"In recent years, there have been advances made in deep learning techniques such as computer vision that has allowed us to make accurate predictions through image segmentation tasks. One particular challenge faced by these models is dealing with complex real world situations where ground truth annotations cannot always capture all possible variations seen in images due to occlusion, partial occlusions, blurred boundaries etc.. These issues can lead to increased difficulty in model training as well as reducing the accuracy of predictions.  To address some of the shortcomings present in current state of art approaches we propose a new method based on multi modal fusion called CMF:Cascade multi-modal fusion architecture for referring image segmentation. Our approach uses multiple individual semantic and instance mask heads within a single decoder which allows for better encoding of contextual relationships across multiple classes compared to previous methods using only one head per class. We use this architecture along with a novel cascading mechanism that sequentially refines the intermediate outputs from each masking task. This ensures high quality output for each step of the process allowing for more stable and coherent final results even if each single stage might perform poorly compared to traditional pipelines. Additionally our cascading design provides an extra regularization effect during optimization that stabilizes convergence, leading to better generalization performance overall.  The experiments conducted show that our proposed CMF approach yields significant improvement over existing methods on challenging datasets, demonstrating that our system is able to accurately model both object shape semantics and pixelwise instances for high resolution images while generalizing well across domains",1
"Variational autoencoders have been widely applied for natural language generation, however, there are two long-standing problems: information under-representation and posterior collapse. The former arises from the fact that only the last hidden state from the encoder is transformed to the latent space, which is insufficient to summarize data. The latter comes as a result of the imbalanced scale between the reconstruction loss and the KL divergence in the objective function. To tackle these issues, in this paper we propose the discrete variational attention model with categorical distribution over the attention mechanism owing to the discrete nature in languages. Our approach is combined with an auto-regressive prior to capture the sequential dependency from observations, which can enhance the latent space for language generation. Moreover, thanks to the property of discreteness, the training of our proposed approach does not suffer from posterior collapse. Furthermore, we carefully analyze the superiority of discrete latent space over the continuous space with the common Gaussian distribution. Extensive experiments on language generation demonstrate superior advantages of our proposed approach in comparison with the state-of-the-art counterparts.",0
"Abstract: Modern deep learning models have achieved state-of-the-art results on tasks such as language modeling and translation. However, these models often struggle with generating coherent and contextually relevant outputs due to their reliance on local patterns and lack of global understanding. Recently proposed attention mechanisms attempt to address this issue by allowing neural networks to selectively focus on different parts of input data. Inspired by these methods, we introduce Discrete Variational Attention (DVA), which extends variational autoencoders to incorporate discrete latent variables that control the allocation of attention. Our approach enables flexible encoding and decoding of textual representations while preserving spatial dependencies within them. We evaluate our method using various benchmark datasets and demonstrate competitive performance compared to several baseline models. Additionally, we provide insightful visualizations that showcase how DVAs learn meaningful attention maps during training, emphasizing its effectiveness for natural language generation tasks. Overall, our work represents an important step towards developing more efficient and expressive generative models capable of capturing complex relationships in high-dimensional spaces.",1
"We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first ""tokenize"" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.",0
"We present BEiT, which pre-trains vision transformers on image-text data by predicting masked tokens given corresponding human descriptions. This results in improved text understanding performance on several NLP benchmarks without any changes to model architecture or parameters. We establish that our pre-training improves zero-shot transfer compared to previous methods, including CLIP, achieving SOTA in most cases. BEiT reduces gaps across many datasets, but some discrepancies remain unexplained, offering promising directions for future work.",1
"Transformer models have demonstrated superior performance in natural language processing. The dot product self-attention in Transformer allows us to model interactions between words. However, this modeling comes with significant computational overhead. In this work, we revisit the memory-compute trade-off associated with Transformer, particularly multi-head attention, and show a memory-heavy but significantly more compute-efficient alternative to Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron (MLP), models the pairwise interaction between words by explicit pairwise word embeddings. As a result, PairConnect substitutes self dot product with a simple embedding lookup. We show mathematically that despite being an MLP, our compute-efficient PairConnect is strictly more expressive than Transformer. Our experiment on language modeling tasks suggests that PairConnect could achieve comparable results with Transformer while reducing the computational cost associated with inference significantly.",0
"In recent years, deep learning has revolutionized several fields by providing state-of-the-art results across domains such as computer vision, natural language processing (NLP), speech recognition, and many others. These advances have been facilitated by advancements in hardware technology, which enable training on massive amounts of data using complex models that contain millions of parameters. However, even with these advancements, computational limitations remain a significant challenge faced during both training and inference, especially for larger models and datasets. One approach proposed to tackle this issue is attention mechanisms, which selectively focus on certain parts of a sequence rather than considering all elements equally. While effective, attention adds additional complexity to the model, resulting in increased computation requirements compared to traditional feedforward neural networks (FNNs). This work presents PairConnect, an alternative approach based on multi-layer perceptrons (MLPs) that can achieve competitive performance while requiring fewer computations relative to conventional FNNs equipped with attentional components. We demonstrate this through extensive experiments on image classification, text generation, and machine translation tasks. Our findings show that our method achieves comparable accuracy and speed compared to popular transformer architectures. As such, PairConnect provides a promising direction for designing efficient NLP systems in low-resource settings where limited compute budgets must support high demand loads.",1
"Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.",0
"This could refer directly to any number of different things (the author, the topic) etc. A better option would be ""an"" as that implies there may well be others. In addition, you might consider changing ""survey"" to something like ""overview"" as that gives more flavour without using up quite so many characters:--- An Overview of Transformer Architectures Abstract: Recent advances in natural language processing have been enabled by large pre-trained models such as BERT [8] and GPT-4 [7]. These models are based on self attention mechanisms [9], which allows them to effectively weigh input features and perform reasoning across global dependencies. However, these architectures can suffer from quadratic memory complexity with respect to input length. Various techniques for reducing the computational cost have been proposed [2]. Notably, multiheaded dot product attention [6] and relative positional encoding methods were introduced [3] to reduce the quadratic blowup while maintaining the quality of model predictions. Other approaches [5] attempt to approximate these models at inference time to enable faster computation but sacrifice some accuracy. We aim to provide a comprehensive overview of these trends and highlight recent works towards efficient and accurate deep learning models for NLP tasks. Keywords: transformer architecture, efficient deep learning, natural language processing",1
"The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",0
"Abstract:  This paper explores the concept of divergence frontiers as applied to generative models. Divergence frontiers refer to points in model space where there is little difference between two distributions. By understanding these frontiers, we can gain insight into how different aspects of model performance relate to each other and identify areas for improvement. We study sample complexity, quantization level, and frontier integral as key parameters that affect divergence frontiers and discuss their impact on generator outputs. Our findings suggest that optimizing models based solely on metrics like perplexity may result in suboptimal results due to their sensitivity to noise. This research underscores the importance of considering trade-offs in model design choices and provides guidance on selecting appropriate evaluation measures. Overall, our work contributes to developing more robust and effective generative models by highlighting the complex interplay among model characteristics.",1
"Despite impressive performance on many text classification tasks, deep neural networks tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author's native language is Swedish). We propose a method that represents the latent topical confounds and a model which ""unlearns"" confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.",0
"""Latent confounding is a common issue in natural language processing that occurs when hidden factors influence the relationship between the independent variables and dependent variables in a model, leading to reduced accuracy and interpretability. In text classification tasks, latent confounding can arise from various sources such as topic ambiguity, context sensitivity, and measurement errors. Ignoring these confounders may result in poor performance, biased estimates, and misleading conclusions. This paper presents an overview of the most common topics that cause latent confounding issues in text classification, providing insights into their impact on model performance and suggestions for mitigating them. By demoting these confounds, researchers can improve the reliability and validity of their findings, enabling more accurate predictions and better decision-making.""",1
"Despite significant improvements in natural language understanding models with the advent of models like BERT and XLNet, these neural-network based classifiers are vulnerable to blackbox adversarial attacks, where the attacker is only allowed to query the target model outputs. We add two more realistic restrictions on the attack methods, namely limiting the number of queries allowed (query budget) and crafting attacks that easily transfer across different pre-trained models (transferability), which render previous attack models impractical and ineffective. Here, we propose a target model agnostic adversarial attack method with a high degree of attack transferability across the attacked models. Our empirical studies show that in comparison to baseline methods, our method generates highly transferable adversarial sentences under the restriction of limited query budgets.",0
"This paper presents a novel approach to adversarial attacks against language understanding models that is agnostic to the target model and can work within query budgets. Existing methods have limitations in terms of their dependence on specific models or the lack of ability to function under resource constraints. Our method addresses these issues by using a simple yet effective strategy based on random search and gradient estimation. We evaluate our approach on several popular benchmarks and show significant improvements over baseline methods across different datasets, architectures, and settings. Our results highlight the effectiveness of our method as well as its robustness to changes in the environment. Overall, we believe that this research has important implications for advancing the field of natural language processing and improving the security of language understanding systems.",1
"Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-of-the-art results on the two public datasets.",0
"This work presents a novel approach for automatic chest x-ray report generation using contrastive attention mechanisms. We propose a deep learning framework that leverages both image features and textual radiologist reports as inputs to generate accurate and informative chest x-ray reports. Our model employs a dual-stream architecture consisting of a convolutional neural network (CNN) and a recurrent neural network (RNN), which capture spatial and temporal patterns respectively. To better attend to relevant regions in the images, we introduce a region-specific contrastive module that learns joint representations of the input images and their corresponding radiology reports. Experiments on two large datasets demonstrate the effectiveness of our method, achieving state-of-the-art performance in terms of quantitative metrics such as ROUGE scores and F1 measures, as well as qualitatively through visual inspections by domain experts. Our results suggest that incorporating attention mechanism can significantly improve the accuracy and coherence of generated chest x-ray reports compared to existing methods without attention modules. Overall, our study highlights the potential of deep learning techniques for automating medical reporting tasks, offering promising directions for future research.",1
Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model's performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.,0
"In recent years, knowledge graph completion has become increasingly important in Artificial Intelligence research due to the abundance of structured data available online. This task involves predicting missing links (or edges) between entities within a large knowledge graph. Previous approaches have utilized embedding techniques and neural networks to achieve good results on benchmark datasets such as DBpedia. However, there remains room for improvement in terms of robustness and generalization performance across different tasks.  In this work, we propose a novel approach called Stacked Convolutional Neural Networks (SCNN), which combines the strengths of convolutional operations and recurrent architectures. Our model consists of multiple convolutional layers that learn to extract hierarchical representations from the input graphs. We also introduce a student re-ranking network to increase the robustness of our predictions by leveraging the uncertainty estimated by the SCNN.  We evaluate the effectiveness of our method against state-of-the-art baselines using three popular benchmark datasets: FB15K-237, WN18, and YAGO43. Experimental results demonstrate significant improvements over existing methods across all metrics, including accuracy, precision, recall, F1 score, and mean reciprocal rank. Furthermore, ablation studies show that each component of our proposed architecture contributes significantly towards improving overall performance.  Overall, our framework provides a powerful tool for completing knowledge graphs while achieving better robustness and generalization capabilities. Its scalability makes it suitable for real-world applications where high-quality knowledge representations are crucial. Future directions involve extending these concepts to more complex and dynamic settings, such as learning continually from new data streams or incorporating advanced reasoning mechanisms into the pipeline.",1
"Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are ""dense"", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.",0
"In this work we present the first attempt at scaling up the state-of-the-art dense vision model architecture Mask R-CNN (He et al., 2017) without sacrificing accuracy; our method uses only a fraction of the parameters compared to other proposed methods while still achieving comparable results on popular benchmark datasets such as COCO Stuff (Caesar et al., 2018). Our approach builds upon previous research by adopting their use of attention mechanisms to distill knowledge from higher capacity models into smaller ones but generalizes it to dense architectures where self-attention becomes computationally prohibitive, requiring us instead to resort to sparse mixtures of experts. We introduce two novelties: adding spatial consistency requirements to these mixtures that prevent degenerate solutions, which allows using much less computation during training without loss of quality, and a simple yet effective reparameterization of the mixing weights’ posterior distribution making optimization much easier than vanilla importance sampling techniques used thus far. Overall, the presented system represents a promising step forward towards efficient realtime object detection systems deployable in today’s mainstream hardware configurations.",1
"Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4% and its comparable variant DINO by 1.0\%. For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch pre-training.",0
"Title: Unlocking the Potential of Self-Supervision for Vision: Masked Self-Supervised Transformer (MST)  Self-supervised learning has emerged as a powerful alternative to supervised learning in the field of computer vision. By leveraging large amounts of unlabelled data and cleverly designed pretext tasks, self-supervised models have been shown to achieve state-of-the-art results on various benchmarks. Despite their impressive performance, these methods often struggle to fully exploit the potential of self-supervision due to limitations in their design. In particular, existing techniques tend to rely heavily on heuristics and handcrafted features, which can limit their effectiveness and generalizability.  In this work, we present Masked Self-Supervised Transformer (MST), a new approach that addresses these issues by combining masked self-attention with advanced visual representations based on transformers. Our method learns meaningful representations through two complementary objectives: predicting missing tokens in masked images and predicting image-level labels from transformed feature vectors. These tasks allow us to directly optimize the quality of learned representations without relying on arbitrary architectural constraints or heuristics.  We evaluate our approach on several challenging benchmarks including ImageNet, COCO, and Cityscapes, demonstrating significant improvements over previous self-supervised methods across all metrics. Our results show that MST effectively captures high-quality visual representations through self-supervision alone, even outperforming strongly supervised models trained on smaller datasets. Additionally, we demonstrate that our model exhibits strong transfer capabilities, achieving competitive results on multiple downstream tasks with limited fine-tuning. Overall, our study highlights the potential of MST as a flexible and effective framework for unleashing the power of self-supervision in computer vision.",1
"The global spread of COVID-19, the disease caused by the novel coronavirus SARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation continues to evolve, predicting localized disease severity is crucial for advanced resource allocation. This paper proposes a method named COURAGE (COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of 2-week-ahead COVID-19 related deaths for each county in the United States, leveraging modern deep learning techniques. Specifically, our method adopts a self-attention model from Natural Language Processing, known as the transformer model, to capture both short-term and long-term dependencies within the time series while enjoying computational efficiency. Our model fully utilizes publicly available information of COVID-19 related confirmed cases, deaths, community mobility trends and demographic information, and can produce state-level prediction as an aggregation of the corresponding county-level predictions. Our numerical experiments demonstrate that our model achieves the state-of-the-art performance among the publicly available benchmark models.",0
"This paper presents a new approach to predicting COVID-19 cases using county aggregation mixed up augmentation (COURAGE). We propose that by combining data from multiple counties and applying randomized transformations to the data, we can improve predictions of future case numbers. Our model uses machine learning algorithms trained on these combined datasets to make probabilistic predictions at each time step. We evaluate our method against existing baseline models and show that COURAGE significantly outperforms them in terms of accuracy and stability across different areas and time periods. Additionally, we demonstrate how the uncertainty estimates provided by our method can inform decision making related to public health interventions such as contact tracing and vaccination efforts. Overall, COURAGE represents a promising new tool for the prediction and management of COVID-19 cases in real-time.",1
"After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\%-50\%$ in leading NLP models such as ALBERT and T5.",0
"This paper investigates how different Transformer architectures can affect the performance of Natural Language Processing (NLP) models on specific types of datasets. We identify that one of the key factors influencing model accuracy is the amount of unique tokens each dataset contains. By using a novel tokenization method and evaluating several popular Transformer architectures across four benchmark NLP tasks, we show that larger models are more effective at handling large datasets with diverse and complex terminology. Our findings highlight the importance of considering both the size and complexity of the dataset when selecting a suitable Transformer architecture for NLP tasks. Ultimately, our research contributes towards improving model selection for better overall NLP performance.",1
"Neural language models can be successfully trained on source code, leading to applications such as code completion. However, their versatile autoregressive self-supervision objective overlooks important global sequence-level features that are present in the data such as syntactic correctness or compilability. In this work, we pose the problem of learning to generate compilable code as constraint satisfaction. We define an Energy-Based Model (EBM) representing a pre-trained generative model with an imposed constraint of generating only compilable sequences. We then use the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021) to train a generative model approximating the EBM. We conduct experiments showing that our proposed approach is able to improve compilability rates without sacrificing diversity and complexity of the generated samples.",0
This is a complex task that requires expertise and attention to detail to ensure accuracy and clarity. Please provide all necessary details and any specific requirements or guidelines for writing the abstract.,1
"The recent advanced deep learning techniques have shown the promising results in various domains such as computer vision and natural language processing. The success of deep neural networks in supervised learning heavily relies on a large amount of labeled data. However, obtaining labeled data with target labels is often challenging due to various reasons such as cost of labeling and privacy issues, which challenges existing deep models. In spite of that, it is relatively easy to obtain data with \textit{inexact supervision}, i.e., having labels/tags related to the target task. For example, social media platforms are overwhelmed with billions of posts and images with self-customized tags, which are not the exact labels for target classification tasks but are usually related to the target labels. It is promising to leverage these tags (inexact supervision) and their relations with target classes to generate labeled data to facilitate the downstream classification tasks. However, the work on this is rather limited. Therefore, we study a novel problem of labeled data generation with inexact supervision. We propose a novel generative framework named as ADDES which can synthesize high-quality labeled data for target classification tasks by learning from data with inexact supervision and the relations between inexact supervision and target classes. Experimental results on image and text datasets demonstrate the effectiveness of the proposed ADDES for generating realistic labeled data from inexact supervision to facilitate the target classification task.",0
"This research presents a methodology for generating labeled training data using inexactly specified supervisory guidance. The generation process involves two main components: an image generator that produces candidate images based on a textual prompt, and a discriminator that evaluates the generated images for quality and relevance. The inexact supervision comes from human annotators who provide high-level labels or descriptions of the desired output rather than explicit annotations for every pixel or object within the image. To handle these imprecise guidelines, we develop algorithms for inferring a soft label distribution over possible interpretations and refining the image generation parameters accordingly. Our approach achieves state-of-the-art results across several benchmark datasets for image generation tasks, demonstrating the effectiveness of our framework for leveraging limited or ambiguous supervision signals. By enabling more flexible forms of annotation input, we hope to broaden the scope of machine learning applications that can benefit from large-scale data synthesis processes like those proposed here.",1
"Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provide a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer model. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.",0
"Here’s a new one I made up: “The application of transformer models has significantly improved natural language processing tasks over traditional approaches such as recurrent neural networks (RNNs). However, their use in modeling physical systems still remains limited due to several challenges related to nonlinearity, time dependence, and multimodality. In this work, we aim to address these limitations by developing novel architectures that incorporate physics-informed constraints into the transformer framework using mechanisms such as attention masking and adaptive filtering. Our contributions provide insights on how to effectively leverage transformers in complex domains involving physical systems while maintaining efficiency and interpretability. We evaluate our approach on a variety of problems across different disciplines including fluid dynamics, heat transfer, and structural analysis demonstrating state-of-the-art performance.”",1
"We study the task of conversational fashion image retrieval via multiturn natural language feedback. Most previous studies are based on single-turn settings. Existing models on multiturn conversational fashion image retrieval have limitations, such as employing traditional models, and leading to ineffective performance. We propose a novel framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts. One characteristic of the framework is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history. Furthermore, the image fashion attribute information is leveraged via a mutual attention strategy. Since there is no existing fashion dataset suitable for the multiturn setting of our task, we derive a large-scale multiturn fashion dataset via additional manual annotation efforts on an existing single-turn dataset. The experiments show that our proposed model significantly outperforms existing state-of-the-art methods.",0
"This paper proposes a novel approach to fashion image retrieval using conversational feedback from users. Our method utilizes natural language queries and multiturn interactions to improve accuracy and relevance in retrieved images. In contrast to traditional approaches that rely solely on visual features, our system leverages textual descriptions provided by users to capture their preferences and narrow down search results. Experimental evaluations show significant improvements over state-of-the-art methods in terms of precision, recall, and user satisfaction. We conclude that conversational fashion image retrieval holds great potential for enhancing online shopping experiences and facilitating more personalized recommendations.",1
"Although deep learning models have driven state-of-the-art performance on a wide array of tasks, they are prone to learning spurious correlations that should not be learned as predictive clues. To mitigate this problem, we propose a causality-based training framework to reduce the spurious correlations caused by observable confounders. We give theoretical analysis on the underlying general Structural Causal Model (SCM) and propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely Counterfactual Maximum Likelihood Estimation (CMLE). As the interventional distribution, in general, is hidden from the observational data, we then derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. We conduct experiments on two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations.",0
"In recent years, deep learning has emerged as one of the most effective approaches to artificial intelligence, allowing machines to learn and make predictions based on large amounts of data. One challenge in training deep networks lies in optimizing their parameters to minimize the difference between predicted outputs and actual outcomes. This task can be difficult due to the complex nature of these models and the limited availability of ground truth data.  To address this issue, we propose a novel method called counterfactual maximum likelihood estimation (CME) that leverages counterfactual examples to improve the accuracy of deep network training. CME works by generating hypothetical inputs that would lead the model to produce different outputs than the actual ones, thus enabling us to evaluate how well the model generalizes to situations outside its training set. By maximizing the likelihood of these counterfactuals, our approach helps guide the optimization process towards better predictive performance.  We demonstrate the effectiveness of CME through extensive experiments using several benchmark datasets across different domains such as computer vision and natural language processing. Our results show that incorporating counterfactuals into the training process leads to significant improvements over state-of-the-art methods. Moreover, we observe that the proposed method requires fewer labels than traditional supervised techniques while maintaining competitive accuracy levels. These findings highlight the promise of CME in advancing the development of robust deep neural networks. Overall, our work contributes to the growing body of literature on efficient yet accurate deep learning training methods.",1
"Generative Adversarial Networks (GAN) have promoted a variety of applications in computer vision, natural language processing, etc. due to its generative model's compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey paper to summarize those state-of-the-art works systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey paper conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this paper also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.",0
"Abstract—In this survey we explore emerging directions in generative adversarial networks (GAN) research that aim towards private and secure applications. We highlight key insights from recent work across several areas including privacy preserving data generation, robustness against adversaries and defenses, interpretability for security assurance, federated learning for distributed training, and real world deployments in critical infrastructure systems. Key benefits of GANs towards these applications are their ability to generate diverse high quality outputs, their capacity for generating novel problem specific solutions, and robustness even under uncertainty conditions during deployment. Challenges still remain which hinder wider adoption such as overfitting, sensitivity to hyperparameters, lack of interpretability of decision making process and issues related to generalization performance. Finally, future directions and opportunities for further advancements in addressing real world problems and challenges are discussed.",1
"The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.",0
"In recent years, point clouds have become increasingly popular due to their ability to capture three-dimensional information accurately and efficiently. However, processing and analyzing these massive datasets can still pose significant challenges. To address these difficulties, we propose a novel approach called ""Point Cloud Transformer"" (PCT), which leverages the power of deep learning techniques to process large-scale point cloud data effectively.  Our method utilizes the attention mechanism from natural language processing models such as Transformers, enabling it to focus on different parts of the input while capturing global context. We demonstrate that PCT achieves state-of-the-art results on several benchmarks including completion, segmentation, and classification tasks. Our proposed model outperforms other existing methods by up to 4% on semantic segmentation and more than 6% on object recognition tasks. Moreover, our ablation studies reveal that each component of the PCT architecture contributes significantly to improving performance.  In summary, this work presents a powerful new tool for point cloud analysis, paving the way for further advancements in computer vision and related fields. With its impressive performance and general applicability across multiple domains, PCT has the potential to revolutionize how researchers tackle complex problems involving high-resolution 3D data. While there remains room for improvement, our contributions offer valuable insights into the development and use of advanced machine learning algorithms for real-world applications.",1
"Transformer-based deep learning models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. In this paper, we propose a compression-compilation co-design framework that can guarantee the identified model to meet both resource and real-time specifications of mobile devices. Our framework applies a compiler-aware neural architecture optimization method (CANAO), which can generate the optimal compressed model that balances both accuracy and latency. We are able to achieve up to 7.8x speedup compared with TensorFlow-Lite with only minor accuracy loss. We present two types of BERT applications on mobile devices: Question Answering (QA) and Text Generation. Both can be executed in real-time with latency as low as 45ms. Videos for demonstrating the framework can be found on https://www.youtube.com/watch?v=_WIRvK_2PZI",0
"This abstract presents a framework that can compress large pre-trained language models (PLMs) such as BERT while maintaining their accuracy on mobile devices without sacrificing speed. The proposed approach involves training a secondary model using smaller subsets of the data from the original PLM, resulting in reduced computational requirements while still preserving high levels of performance. Furthermore, our method also utilizes techniques like knowledge distillation and quantization to further reduce memory footprint and improve inference times. Our experiments demonstrate that we can achieve comparable results to full-size BERT models on several benchmark datasets while only requiring half the amount of time and taking up just one-twentieth of the space. These significant improvements make real-time natural language processing applications feasible on modern smartphones and other resource-constrained environments. Overall, our work represents an important step towards enabling powerful NLP capabilities on mobile platforms without compromising user experience due to slow or overly complex models.",1
"Can we teach a robot to recognize and make predictions for activities that it has never seen before? We tackle this problem by learning models for video from text. This paper presents a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to video. Given a portion of an instructional video, our model recognizes and predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the capabilities of our model, we introduce the \emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot learning, recognition and anticipation. Extensive experiments with various evaluation metrics demonstrate the potential of our method for generalization, given limited video data for training models.",0
"In recent years, deep learning techniques have made significant advances in image recognition and understanding tasks. However, there still exist limitations in modeling complex dynamic scenes using these methods due to their reliance on large amounts of labeled training data. This paper proposes a novel approach that utilizes textual descriptions to train video models capable of predicting future frames with high accuracy, even in situations where little or no visual data is available (i.e., zero-shot anticipation). The proposed method relies on procedural action representations derived from natural language processing techniques which enable the representation of multi-step actions as sequences of steps or atoms. These representations capture spatial relationships and affordances present in both images and videos, enabling effective transfer across domains. Our experiments demonstrate the effectiveness of our approach in several benchmark datasets including AnticipationDB and PFVSD, outperforming state-of-the-art approaches under zero-shot settings. The results showcase the potential of zero-shot anticipation in applications such as autonomous driving, robotics, and video generation.",1
"There is a growing interest in the community in making an embodied AI agent perform a complicated task while interacting with an environment following natural language directives. Recent studies have tackled the problem using ALFRED, a well-designed dataset for the task, but achieved only very low accuracy. This paper proposes a new method, which outperforms the previous methods by a large margin. It is based on a combination of several new ideas. One is a two-stage interpretation of the provided instructions. The method first selects and interprets an instruction without using visual information, yielding a tentative action sequence prediction. It then integrates the prediction with the visual information etc., yielding the final prediction of an action and an object. As the object's class to interact is identified in the first stage, it can accurately select the correct object from the input image. Moreover, our method considers multiple egocentric views of the environment and extracts essential information by applying hierarchical attention conditioned on the current instruction. This contributes to the accurate prediction of actions for navigation. A preliminary version of the method won the ALFRED Challenge 2020. The current version achieves the unseen environment's success rate of 4.45% with a single view, which is further improved to 8.37% with multiple views.",0
"This abstract outlines findings from experiments conducted in natural language processing (NLP) aimed at improving performance on interactive instruction-following tasks using deep learning techniques and algorithms that integrate external data sources such as web search results. By utilizing these methods, significant improvements were observed in overall task accuracy and efficiency compared to traditional NLP approaches. These promising results demonstrate the potential benefits of incorporating additional contextual information into text understanding models for enhanced performance in interactive settings.",1
"Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.",0
"This paper presents a novel attack on Stochastic Gradient Descent (SGD), a widely used optimization algorithm in machine learning, by manipulating the order of training data that is fed into the system. We show how carefully crafted input sequences can lead to significant deviation from the model's original behavior, resulting in incorrect predictions and degradation of performance. Our experiments demonstrate the effectiveness of these attacks across multiple models and datasets, highlighting the importance of considering data ordering as a potential vulnerability in SGD systems. In addition, we discuss several possible defense mechanisms against such attacks and evaluate their efficacy. Overall, our work sheds light on the security risks associated with data ordering in SGD algorithms and emphasizes the need for further research in this area.",1
"Text-based video segmentation is a challenging task that segments out the natural language referred objects in videos. It essentially requires semantic comprehension and fine-grained video understanding. Existing methods introduce language representation into segmentation models in a bottom-up manner, which merely conducts vision-language interaction within local receptive fields of ConvNets. We argue that such interaction is not fulfilled since the model can barely construct region-level relationships given partial observations, which is contrary to the description logic of natural language/referring expressions. In fact, people usually describe a target object using relations with other objects, which may not be easily understood without seeing the whole video. To address the issue, we introduce a novel top-down approach by imitating how we human segment an object with the language guidance. We first figure out all candidate objects in videos and then choose the refereed one by parsing relations among those high-level objects. Three kinds of object-level relations are investigated for precise relationship understanding, i.e., positional relation, text-guided semantic relation, and temporal relation. Extensive experiments on A2D Sentences and J-HMDB Sentences show our method outperforms state-of-the-art methods by a large margin. Qualitative results also show our results are more explainable. Besides, based on the inspiration, we win the first place in CVPR2021 Referring Youtube-VOS challenge.",0
"An important task in computer vision is video segmentation, which involves separating objects from their backgrounds. Traditionally, this has been done using pixel-wise annotations and handcrafted features. However, these methods often struggle with handling large object occlusions and variations in appearance. In this work, we propose a novel approach called ""ClawCraneNet"" that leverages object-level relation reasoning for text-based video segmentation. Our method uses semantic bounding boxes as input and learns visual representations through attention mechanisms that capture spatial dependencies among pixels conditioned on instance labels. We evaluate our model on two challenging datasets and demonstrate significant improvements over previous state-of-the-art methods, showing the effectiveness of our proposed approach. Overall, our results highlight the potential of integrating high-level semantics into video segmentation tasks, opening up new opportunities for computer vision research.",1
"Vision transformer (ViT) has recently showed its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ a novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extract global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information. Extensive experiments on three vision tasks, including image classification, object detection and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models will be publicly available.",0
"This paper introduces the region-based vision transformer (RegionViT), which can efficiently localize important regions on images and videos while still maintaining strong global reasoning ability. We achieve this by replacing the self attention mechanism in vision transformers with regional attention that takes into account both intra-regional relationships within local windows as well as inter-relationships across all regions in the input space. By doing so we are able to model objects at different scales, from coarse object boundaries down to fine details like hair and fur. Our method also extends naturally to video where it models spatio-temporal representations capturing motions such as human walking patterns. RegionViT significantly outperforms current state-of-the-art alternatives including recent efficient models designed specifically for image classification tasks on the ImageNet benchmark. These results showcase the broad applicability of our approach beyond just imaging analysis to other computer vision problems, providing compelling evidence supporting adoption of RegionViTs for many real world applications of deep learning.",1
"The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning -- Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures -- in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as BGRL, best self-supervised methods, and fully supervised ones while requiring substantially fewer hyperparameters and converging in an order of magnitude training steps earlier.",0
"Graphs have become increasingly important representations of complex data, with applications ranging from social networks to scientific simulations. However, many machine learning algorithms require large amounts of labeled training data to achieve good performance on graph tasks, which can be expensive and time-consuming to obtain. In recent years, there has been significant interest in developing methods for learning representations of graphs that do not require massive amounts of labeled data. One promising approach is self-supervised learning, where models learn by predicting missing or corrupted information within their own inputs rather than relying on external labels. This work presents a new self-supervised representation learning method called Graph Barlow Twins (GBT). GBT modifies traditional GCNs so they consistently make use of multiple views generated from different random augmentations of each node during training. Our results show that using just one view yields only a minor improvement over no pretraining at all on several benchmark datasets, while our multi-view variant significantly improves both the quality of learned embeddings, as well as outperforms existing unsupervised baselines across a range of downstream graph inference task evaluation metrics.",1
"Real-world machine learning systems are achieving remarkable performance in terms of coarse-grained metrics like overall accuracy and F-1 score. However, model improvement and development often require fine-grained modeling on individual data subsets or slices, for instance, the data slices where the models have unsatisfactory results. In practice, it gives tangible values for developing such models that can pay extra attention to critical or interested slices while retaining the original overall performance. This work extends the recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of attentions (MoA) to learn slice-aware dual attentive representations. We empirically show that the MoA approach outperforms the baseline method as well as the original SBL approach on monitored slices with two natural language understanding (NLU) tasks.",0
"Recently developed deep learning architectures have improved state of the art results on many challenging problems such as object detection, image classification and semantic segmentation. These successes can largely attributed to advances in convolutional neural network designs that enable more efficient representation learning. However, despite these advancements there remains large gaps between human performance and the current systems. In our work we develop methods that adaptively selective attention mechanisms during training. Our method learns representations that are optimized for different size chunks (slices) of the input data thereby allowing the model to learn richer features that better capture relevant information from larger spatial contexts. We evaluate our approach using multiple datasets where our models achieve strong results across several tasks including scene understanding, object recognition and medical diagnosis. Finally, we provide analyses showing how models trained with Mixture of Attention perform comparably with respect to other recent approaches while achieving higher accuracy in certain cases.",1
"When a human asks questions online, or when a conversational virtual agent asks human questions, questions triggering emotions or with details might more likely to get responses or answers. we explore how to automatically rewrite natural language questions to improve the response rate from people. In particular, a new task of Visual Question Rewriting(VQR) task is introduced to explore how visual information can be used to improve the new questions. A data set containing around 4K bland questions, attractive questions and images triples is collected. We developed some baseline sequence to sequence models and more advanced transformer based models, which take a bland question and a related image as input and output a rewritten question that is expected to be more attractive. Offline experiments and mechanical Turk based evaluations show that it is possible to rewrite bland questions in a more detailed and attractive way to increase the response rate, and images can be helpful.",0
"Abstract: This study aimed to investigate the effectiveness of visual question rewriting on increasing response rate. We hypothesized that by providing more detailed and specific questions through image manipulation techniques such as highlighting, cropping, and adding arrows, participants would be more likely to provide accurate responses. To test our hypothesis, we conducted two experiments using a repeated measures design where participants were presented with both original and rewritten versions of the same questions. Our results showed that there was indeed a significant increase in response rates for questions with visually rewritten queries compared to those without any modifications. These findings suggest that incorporating image manipulation techniques into survey designs can significantly improve participation rates, ultimately leading to higher quality data collection efforts across different domains. Further research may explore other potential factors affecting response rates, including individual differences and cultural influences. Overall, these findings have important implications for researchers who rely heavily on participant engagement and input.",1
"We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an information vector and an attention gate using attention results and current context; and then adds another attention to generate final attended information by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves the state-of-the-art performance.",0
"Visual question answering (VQA) has emerged as a challenging task that requires both computer vision and natural language processing techniques to successfully solve. One key component of many VQA systems is attention mechanisms, which allow the model to focus on specific parts of an image when processing questions related to visual content. In this work, we propose an improved attention mechanism for VQA that enhances the ability of models to accurately determine the most relevant image regions for given questions. Our method utilizes dynamic channel-wise feature scaling to learn a global contextual relevance score, allowing the model to selectively attend to different channels within each spatial location based on their importance. We evaluate our proposed method using several benchmark datasets and demonstrate consistent improvement over strong baseline methods. Overall, our results showcase the effectiveness of our improved attention mechanism in improving VQA performance.",1
"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",0
"This paper presents a novel approach to image recognition that leverages deep learning techniques to improve accuracy and scalability. By utilizing transformer networks, we propose a model capable of processing large images without losing valuable contextual information. Our method achieves state-of-the-art performance on challenging benchmarks and demonstrates significant improvements over traditional convolutional neural network architectures. Furthermore, our model can effectively handle high resolution images, opening up new applications in fields such as medical imaging and self-driving cars. Finally, we provide detailed analysis and ablation studies to validate the effectiveness of our proposed approach. Overall, our work represents a step forward towards enabling efficient and accurate image understanding algorithms.",1
"With the advent of state of the art nature-inspired pure attention based models i.e. transformers, and their success in natural language processing (NLP), their extension to machine vision (MV) tasks was inevitable and much felt. Subsequently, vision transformers (ViTs) were introduced which are giving quite a challenge to the established deep learning based machine vision techniques. However, pure attention based models/architectures like transformers require huge data, large training times and large computational resources. Some recent works suggest that combinations of these two varied fields can prove to build systems which have the advantages of both these fields. Accordingly, this state of the art survey paper is introduced which hopefully will help readers get useful information about this interesting and potential research area. A gentle introduction to attention mechanisms is given, followed by a discussion of the popular attention based deep architectures. Subsequently, the major categories of the intersection of attention mechanisms and deep learning for machine vision (MV) based are discussed. Afterwards, the major algorithms, issues and trends within the scope of the paper are discussed.",0
This survey article provides an overview of attention mechanisms used in deep neural networks for image recognition tasks. We describe current approaches that leverage these techniques in conjunction with convolutional neural networks (CNNs) and explore their potential benefits relative to traditional CNN methods. Our goal is to provide insights into how attention mechanisms can enhance image classification models and encourage new research directions in this field. Keywords: attention mechanism; deep learning; image processing; object detection; scene understanding,1
"Training a reinforcement learning agent to carry out natural language instructions is limited by the available supervision, i.e. knowing when the instruction has been carried out. We adapt the CLEVR visual question answering dataset to generate complex natural language navigation instructions and accompanying scene graphs, yielding an environment-agnostic supervised dataset. To demonstrate the use of this data set, we map the scenes to the VizDoom environment and use the architecture in \citet{gatedattention} to train an agent to carry out these more complex language instructions.",0
"""Ground"" means the process of anchoring things in reality so that they make sense. So a grounded approach might mean you look at the real world implications before taking action on something abstract like instructions: if those instructions lead to confusion, then how could we change them? Perhaps by using pictures! This paper presents a novel use case applying computer vision algorithms to generate graphical scene graphs from everyday natural language navigation instructions (for example, how to walk a dog). These graphs can be used as interfaces between complex machine tasks (such as self driving cars) and simple user requests (""hey car, go pick up my son""). They should also help disambiguate instruction sets during execution (e.g., ""go left"", but which way!?) which seems important for making AIs smarter in the future without needing more compute resources. The key contributions herein include: developing heuristics and software tools capable of generating scene graphs from raw human instructions; presenting evaluation data comparing user input against automatically generated graphs for quality and accuracy; and discussing open research problems impacting the scalability of our model based system moving forward.",1
"Despite their ubiquity in core AI fields like natural language processing, the mechanics of deep attention-based neural networks like the Transformer model are not fully understood. In this article, we present a new perspective towards understanding how Transformers work. In particular, we show that the ""dot-product attention"" that is the core of the Transformer's operation can be characterized as a kernel learning method on a pair of Banach spaces. In particular, the Transformer's kernel is characterized as having an infinite feature dimension. Along the way we consider an extension of the standard kernel learning problem to a binary setting, where data come from two input domains and a response is defined for every cross-domain pair. We prove a new representer theorem for these binary kernel machines with non-Mercer (indefinite, asymmetric) kernels (implying that the functions learned are elements of reproducing kernel Banach spaces rather than Hilbert spaces), and also prove a new universal approximation theorem showing that the Transformer calculation can learn any binary non-Mercer reproducing kernel Banach space pair. We experiment with new kernels in Transformers, and obtain results that suggest the infinite dimensionality of the standard Transformer kernel is partially responsible for its performance. This paper's results provide a new theoretical understanding of a very important but poorly understood model in modern machine~learning.",0
"Title: ""Deep Learning With Non-Mercerizable Kernels""  This paper presents an innovative approach to deep learning by using non-mercerizable kernels to improve performance on a wide range of tasks. Conventional deep learning models rely on Mercer kernels, which have limitations in terms of expressiveness and computational efficiency. By utilizing non-mercerizable kernels instead, we can train more powerful representations that are capable of capturing complex relationships between data points. Our method achieves state-of-the-art results across several benchmark datasets, demonstrating its effectiveness in image classification, speech recognition, and natural language processing. Furthermore, our model has the advantage of being able to handle high-dimensional input spaces, such as images and audio signals. Overall, this work advances the field of deep learning by introducing a new class of models that pushes the boundaries of what was previously thought possible.",1
"Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning.",0
"Improving pre-trained language models has become increasingly important as these models have been shown to effectively capture knowledge from large amounts of data and perform well on various natural language processing tasks. However, often times, using such pre-trained models out-of-the-box can result in suboptimal performance due to lack of domain specificity and task adaptability. In this work, we present methods that enable personalization of pre-trained models through fine-tuning and distillation techniques, which improve both accuracy and efficiency. Our approach involves identifying relevant features and parameters in the model that contribute towards better performance while minimizing the risk of overfitting. We demonstrate significant improvements across different domains including sentiment analysis, question answering, and text generation. Our findings show that personalized models not only generalize better but also achieve state-of-the-art results in some cases. This research provides valuable insights into how one can leverage transfer learning to create high performing NLP systems adapted to their needs without incurring the cost of retraining from scratch.",1
"Convolutional neural networks (CNNs) are ubiquitous in computer vision, with a myriad of effective and efficient variations. Recently, Transformers -- originally introduced in natural language processing -- have been increasingly adopted in computer vision. While early adopters continue to employ CNN backbones, the latest networks are end-to-end CNN-free Transformer solutions. A recent surprising finding shows that a simple MLP based solution without any traditional convolutional or Transformer components can produce effective visual representations. While CNNs, Transformers and MLP-Mixers may be considered as completely disparate architectures, we provide a unified view showing that they are in fact special cases of a more general method to aggregate spatial context in a neural network stack. We present the \model (CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head context aggregation that can exploit long-range interactions \emph{a la} Transformers while still exploiting the inductive bias of the local convolution operation leading to faster convergence speeds, often seen in CNNs. In contrast to Transformer-based methods that do not scale well to downstream tasks that rely on larger input image resolutions, our efficient network, named \modellight, can be employed in object detection and instance segmentation networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50 backbone with a comparable compute and parameter size. Our method also achieves promising results on self-supervised learning compared to DeiT on the DINO framework.",0
"Abstract:  This research presents a new deep learning architecture called ""Container"" that uses context aggregation networks (CANs) to improve image classification performance on challenging datasets like COCO and ImageNet. By leveraging CANs, Containers enable effective communication between multiple layers within the network while reducing computational complexity compared to existing methods. We evaluate Containers using several ablation studies, demonstrating their effectiveness in improving accuracy across different dataset sizes and training configurations. Our approach significantly outperforms prior state-of-the-art models by achieving higher top-1 and top-5 accuracy on COCO and comparable results on ImageNet. Overall, our work advances the field of computer vision by introducing a simple yet powerful method for enhancing the representation capabilities of convolutional neural networks.",1
"In computer vision and natural language processing, innovations in model architecture that lead to increases in model capacity have reliably translated into gains in performance. In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algorithms often use only small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overfitting; however, this hypothesis is untested. In this paper we investigate how RL agents are affected by exchanging the small MLPs with larger modern networks with skip connections and normalization, focusing specifically on soft actor-critic (SAC) algorithms. We verify, empirically, that na\""ively adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that intrinsic instability from the actor in SAC taking gradients through the critic is the culprit. We demonstrate that a simple smoothing method can mitigate this issue, which enables stable training with large modern architectures. After smoothing, larger models yield dramatic performance improvements for state-of-the-art agents -- suggesting that more ""easy"" gains may be had by focusing on model architectures in addition to algorithmic innovations.",0
"In recent years deep reinforcement learning (DRL) has emerged as one of the most promising approaches for training agents that can perform complex sequential decision making tasks. While DRL algorithms have achieved impressive results in many domains, they still face significant challenges due to their limited understanding of the underlying environment and the lack of ability to incorporate prior knowledge into the learning process. This paper presents an overview of some of these limitations and proposes novel methods towards addressing them by leveraging advanced techniques from representation learning, model uncertainty estimation, and meta learning. We demonstrate through extensive experiments on real world benchmarks that our proposed methods significantly improve sample efficiency, scalability and performance over state-of-the art DRL algorithms, paving the pathway toward deeper exploration-exploitation tradeoffs and ultimately achieving more human-like intelligence in artificial agents. Finally, we discuss open research questions and future directions that could lead to even greater breakthroughs in the field. Overall, this work represents a major step forward in developing intelligent agents capable of solving increasingly difficult problems autonomously and efficiently.",1
"Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efficiently. Our model ranks first place on CVPR2021 Referring Youtube-VOS challenge.",0
"This is an interesting paper that explores cross-modal interaction from a top-down perspective for referring video object segmentation. In particular, the authors propose a new framework called TSLSNet (Top-Down Spatial Layout Stream Networks) which uses a two-stream architecture to model spatial layout and semantic context jointly with explicit supervision on bounding boxes, category labels, and instance-level masks. This approach enables more accurate reasoning about complex relationships within scenes by leveraging contextual dependencies across multiple modalities. Experiments conducted using DAVIS and YouTube-VOS datasets demonstrate significant improvements over state-of-the-art methods, highlighting the effectiveness of the proposed methodology. Overall, this research offers valuable insights into advancing the field of referral video object segmentation through novel techniques grounded in cognitive psychology principles.",1
"Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and, thus, are too resource-hungry and computation-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted a lot of research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.",0
"This paper presents a method for compressing large-scale transformer-based models such as BERT (Bidirectional Encoder Representations from Transformers). We evaluate our approach using a variety of techniques including pruning, quantization, and Huffman coding. Our results show that we can achieve significant compression rates without sacrificing model accuracy. Furthermore, we demonstrate how our method can be applied to other types of neural networks beyond just those based on the Transformer architecture. Finally, we discuss future directions for research into further improving the efficiency and scalability of deep learning models.",1
"In traditional software programs, we take for granted how easy it is to debug code by tracing program logic from variables back to input, apply unit tests and assertion statements to block erroneous behavior, and compose programs together. But as the programs we write grow more complex, it becomes hard to apply traditional software to applications like computer vision or natural language. Although deep learning programs have demonstrated strong performance on these applications, they sacrifice many of the functionalities of traditional software programs. In this paper, we work towards bridging the benefits of traditional and deep learning programs by jointly training a generative model to constrain neural network activations to ""decode"" back to inputs. Doing so enables practitioners to probe and track information encoded in activation(s), apply assertion-like constraints on what information is encoded in an activation, and compose separate neural networks together in a plug-and-play fashion. In our experiments, we demonstrate applications of decodable representations to out-of-distribution detection, adversarial examples, calibration, and fairness -- while matching standard neural networks in accuracy.",0
"Abstract: In recent years, there has been significant progress in developing powerful neural networks that can perform complex tasks such as image classification, speech recognition, and natural language processing. However, despite their impressive performance on benchmark datasets, these models often lack interpretability and transparency. In other words, it is difficult to understand how they make predictions and decisions. One of the main challenges facing researchers is improving the compositionality of neural networks – i.e., ensuring that the outputs are meaningful and interpretable components of the input data.  To address this problem, we propose a novel approach that involves decoding neural network representations back into inputs. By doing so, we can gain insights into how different parts of the input affect the model’s output, as well as identify important features that contribute to its decision making process. We evaluate our method using several state-of-the-art models across multiple domains, including image generation, text completion, and question answering. Our results demonstrate that our proposed technique significantly improves the compositional nature of neural networks, enabling them to produce more explainable and transparent outputs. Overall, our work represents an important step towards building more trustworthy and reliable artificial intelligence systems.",1
"Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) dataset by just modifying the learning rate schedule of a high performing model.",0
"One approach to balancing exploration (trying new things) and exploitation (sticking with what works) is based on the idea that finding the global minimum can be thought of as searching for a dense region of solution space. If we model local search heuristically using Gaussian processes (GPs), we can use variational inference techniques from GP regression to approximate a posterior over functions which includes uncertainty about whether there may still be unknown valleys close by. Then we can define a measure of ""density"" in this space which reflects how far we think other potential solutions might be. This hypothesis suggests then making exploratory steps into more uncertain areas in order to find better optima. We explore some simple instantiations of this intuition and compare their performance on a few benchmark problems where prior work has established baselines; overall our methods perform significantly better than current state-of-the-art blackbox optimizers, while maintaining competitive runtimes with sequential models. In summary we present evidence towards wide minima density as an unifying principle behind several powerful Bayesian optimization algorithms and provide clear empirical justification for this formulation in high dimensions on real-world tasks beyond those used during training.",1
"Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing -- with an emphasis on interdisciplinary collaboration -- will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.",0
"Reliability testing has become increasingly important as natural language processing systems (NLPs) continue to play a larger role in our daily lives. NLPs interact with humans through textual interfaces such as chatbots and virtual assistants, making them essential tools that must provide accurate and reliable answers under all circumstances. In this study, we evaluate three commonly used reliability metrics: recall, precision, and F1 score, and investigate their effectiveness at measuring the accuracy of different types of NLP models. We also compare these reliability metrics against human evaluation methods and demonstrate how they complement each other in assessing NLP system performance. Our results show that while each metric provides valuable insights into the strengths and weaknesses of individual NLP systems, a combination of multiple evaluations may yield more comprehensive and nuanced understanding of overall reliability. These findings can guide future research efforts toward creating more robust and effective NLP technologies that better serve end users. This work highlights the need for continuous improvement and rigorous examination of NLP systems, especially those that have significant impact on society.",1
"Machine-learning systems such as self-driving cars or virtual assistants are composed of a large number of machine-learning models that recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, etc. Models in these systems are often developed and trained independently, which raises an obvious concern: Can improving a machine-learning model make the overall system worse? We answer this question affirmatively by showing that improving a model can deteriorate the performance of downstream models, even after those downstream models are retrained. Such self-defeating improvements are the result of entanglement between the models in the system. We perform an error decomposition of systems with multiple machine-learning models, which sheds light on the types of errors that can lead to self-defeating improvements. We also present the results of experiments which show that self-defeating improvements emerge in a realistic stereo-based detection system for cars and pedestrians.",0
"In machine learning, performance improvements may sometimes produce unintended negative consequences that can degrade overall system effectiveness over time. We identify and explore a class of such self-defeating improvements, which we call fixes that fail, where well intentioned modifications actually make things worse rather than better by creating new problems while attempting to solve existing ones. By analyzing real world case studies from industry and research literature, we demonstrate how seemingly innocuous changes can lead to cascading effects throughout the entire system and ultimately reduce overall system utility. We provide guidance on detecting the symptoms of issues that could evolve into full blown fixes that fail, as well as techniques to mitigate their impact before they occur. Our work highlights potential pitfalls facing machine learners today but more importantly provides insights into building robust systems with adaptive capacity to survive them.",1
"Vehicle search is one basic task for the efficient traffic management in terms of the AI City. Most existing practices focus on the image-based vehicle matching, including vehicle re-identification and vehicle tracking. In this paper, we apply one new modality, i.e., the language description, to search the vehicle of interest and explore the potential of this task in the real-world scenario. The natural language-based vehicle search poses one new challenge of fine-grained understanding of both vision and language modalities. To connect language and vision, we propose to jointly train the state-of-the-art vision models with the transformer-based language model in an end-to-end manner. Except for the network structure design and the training strategy, several optimization objectives are also re-visited in this work. The qualitative and quantitative experiments verify the effectiveness of the proposed method. Our proposed method has achieved the 1st place on the 5th AI City Challenge, yielding competitive performance 18.69% MRR accuracy on the private test set. We hope this work can pave the way for the future study on using language description effectively and efficiently for real-world vehicle retrieval systems. The code will be available at https://github.com/ShuaiBai623/AIC2021-T5-CLV.",0
Title: Connecting Language and Vision for Natural Language-Based Vehicle Retrieval,1
"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. Along with producing more distinct and well-structured patterns that improve the performance, we also show that dropping clauses increases learning robustness. To explore the effects clause dropping has on accuracy, training time, and interpretability, we conduct extensive experiments on various benchmark datasets in natural language processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and CIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2x to 4x faster learning. We further employ the Convolutional TM to document interpretable results on the CIFAR10 dataset. To the best of our knowledge, this is the first time an interpretable machine learning algorithm has been used to produce pixel-level human-interpretable results on CIFAR10. Also, unlike previous interpretable methods that focus on attention visualisation or gradient interpretability, we show that the TM is a more general interpretable method. That is, by producing rule-based propositional logic expressions that are \emph{human}-interpretable, the TM can explain how it classifies a particular instance at the pixel level for computer vision and at the word level for NLP.",0
"This paper presents a method for enhancing human interpretability in Tsetlin Machines (TM) by introducing drop clauses. By incorporating randomness into clause selection, drop clauses provide an added layer of stochasticity that allows for greater flexibility and interpretabilit",1
"Although Transformer has made breakthrough success in widespread domains especially in Natural Language Processing (NLP), applying it to time series forecasting is still a great challenge. In time series forecasting, the autoregressive decoding of canonical Transformer models could introduce huge accumulative errors inevitably. Besides, utilizing Transformer to deal with spatial-temporal dependencies in the problem still faces tough difficulties.~To tackle these limitations, this work is the first attempt to propose a Non-Autoregressive Transformer architecture for time series forecasting, aiming at overcoming the time delay and accumulative error issues in the canonical Transformer. Moreover, we present a novel spatial-temporal attention mechanism, building a bridge by a learned temporal influence map to fill the gaps between the spatial and temporal attention, so that spatial and temporal dependencies can be processed integrally. Empirically, we evaluate our model on diversified ego-centric future localization datasets and demonstrate state-of-the-art performance on both real-time and accuracy.",0
"This paper presents NAST, a novel neural network architecture designed specifically for time series forecasting tasks. Unlike most existing methods that rely on autoregressive models or require extensive engineering domain knowledge, our approach utilizes non-autoregressive spatial-temporal transformer networks. This allows us to capture complex temporal dependencies while addressing the scalability issues typically faced by traditional sequence modeling techniques. Our experimental results across several real-world datasets demonstrate the effectiveness of our method compared to state-of-the-art approaches. Additionally, we provide detailed analysis and ablation studies highlighting the benefits of using non-autoregressivity in time series prediction tasks. Overall, our work represents a significant step forward in advancing the field of time series forecasting with deep learning models.",1
"A number of problems in the processing of sound and natural language, as well as in other areas, can be reduced to simultaneously reading an input sequence and writing an output sequence of generally different length. There are well developed methods that produce the output sequence based on the entirely known input. However, efficient methods that enable such transformations on-line do not exist. In this paper we introduce an architecture that learns with reinforcement to make decisions about whether to read a token or write another token. This architecture is able to transform potentially infinite sequences on-line. In an experimental study we compare it with state-of-the-art methods for neural machine translation. While it produces slightly worse translations than Transformer, it outperforms the autoencoder with attention, even though our architecture translates texts on-line thereby solving a more difficult problem than both reference methods.",0
"In recent years, deep learning has achieved great success in many areas such as computer vision and natural language processing. However, most deep learning algorithms are based on supervised or unsupervised learning techniques that require large amounts of labeled data, which can be time consuming and expensive to collect. To address these limitations, reinforcement learning (RL) has emerged as a promising alternative approach for training artificial intelligence agents. RL allows agents to learn from trial and error by receiving rewards or penalties depending on their actions.  In particular, one promising direction of research within RL is sequence transformation, where the agent transforms sequences of input elements into desired output sequences. This problem arises naturally in many real-world applications, including text generation, speech recognition, and robotics. Traditional approaches to sequence transformation have focused mainly on rule-based methods or statistical models. However, these methods suffer from several drawbacks, such as limited generalization ability and poor robustness to noise.  Our work proposes a new model for on-line sequence transformation using deep reinforcement learning. Our method builds upon previous work in RL, but incorporates novel architectural components designed to handle sequential inputs and outputs more effectively. Specifically, we use a recurrent neural network architecture to encode the state of the environment and store information over time. We then train our agent using proximal policy optimization (PPO), a popular RL algorithm known for its stability and sample efficiency.  We evaluate our approach on two benchmark datasets: one for generating human-like speech synthesis and another for controlling simulated robots to reach specific goals. Experimental results show that our method outperforms strong baselines across both tasks, achieving significant improvements in accuracy and speed while requiring fewer training samples. Furthermore, our approach demonstrates good generalizability across different domains, indicating its potential to solve similar problems beyond those studied here. Overall, our work represents an important step towards solving the challe…",1
"In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning",0
"Here we present weighted training (WT), a simple but effective approach to cross-task learning that leverages insights from transfer learning without explicitly performing fine-tuning. By adjusting the contribution of each task based on their similarity to the target task during model training, WT can achieve better generalization performance than standard multi-task learning methods across several benchmark datasets. Empirical results showcase both improved overall accuracy as well as better calibration compared to competitive baselines. Our findings suggest that our method could have broad implications for natural language processing tasks by allowing models to effectively adapt to new domains while mitigating overfitting. Furthermore, our framework can potentially facilitate researchers interested in applying pretrained language models towards novel application areas where limited labeled data may be available.",1
"Borrowing from the transformer models that revolutionized the field of natural language processing, self-supervised feature learning for visual tasks has also seen state-of-the-art success using these extremely deep, isotropic networks. However, the typical AI researcher does not have the resources to evaluate, let alone train, a model with several billion parameters and quadratic self-attention activations. To facilitate further research, it is necessary to understand the features of these huge transformer models that can be adequately studied by the typical researcher. One interesting characteristic of these transformer models is that they remove most of the inductive biases present in classical convolutional networks. In this work, we analyze the effect of these and more inductive biases on small to moderately-sized isotropic networks used for unsupervised visual feature learning and show that their removal is not always ideal.",0
"Artificial intelligence (AI) models rely heavily on inductive biases – assumptions encoded into their architectures that enable them to learn complex representations from data. However, many recent developments have focused on removing these constraints altogether and training models without any explicit inductive biases. In this work, we argue that bias against inductive biases can lead to suboptimal model performance and hinder scientific progress. We show how inductive biases play crucial roles in allowing neural networks to capture important properties of the world and facilitating efficient learning through modularity. By analyzing a range of state-of-the-art models across different tasks, we demonstrate that these limitations arise due to insufficient bias rather than excessive bias. We conclude by discussing implications of our findings and suggesting future directions for research in this area. Our results highlight the importance of considering both empirical evidence and theoretical principles in designing effective AI systems.",1
"Purpose: This study evaluates the effectiveness and impact of automated order-based protocol assignment for magnetic resonance imaging (MRI) exams using natural language processing (NLP) and deep learning (DL).   Methods: NLP tools were applied to retrospectively process orders from over 116,000 MRI exams with 200 unique sub-specialized protocols (""Local"" protocol class). Separate DL models were trained on 70\% of the processed data for ""Local"" protocols as well as 93 American College of Radiology (""ACR"") protocols and 48 ""General"" protocols. The DL Models were assessed in an ""auto-protocoling (AP)"" inference mode which returns the top recommendation and in a ""clinical decision support (CDS)"" inference mode which returns up to 10 protocols for radiologist review. The accuracy of each protocol recommendation was computed and analyzed based on the difference between the normalized output score of the corresponding neural net for the top two recommendations.   Results: The top predicted protocol in AP mode was correct for 82.8%, 73.8%, and 69.3% of the test cases for ""General"", ""ACR"", and ""Local"" protocol classes, respectively. Higher levels of accuracy over 96% were obtained for all protocol classes in CDS mode. However, at current validation performance levels, the proposed models offer modest, positive, financial impact on large-scale imaging networks.   Conclusions: DL-based protocol automation is feasible and can be tuned to route substantial fractions of exams for auto-protocoling, with higher accuracy with more general protocols. Economic analyses of the tested algorithms indicate that improved algorithm performance is required to yield a practical exam auto-protocoling tool for sub-specialized imaging exams.",0
"Artificial intelligence (AI) has been increasingly used in healthcare to improve patient outcomes and reduce costs. One area where AI can have a significant impact is in automating protocol definition for advanced diagnostic imaging exams. Traditionally, radiologists manually determine which sequences and parameters should be included in these complex exams, which can lead to variability and suboptimal results. In this study, we propose using deep learning techniques to automatically generate optimized imaging protocols that take into account factors such as disease type and patient characteristics. We demonstrate the feasibility of our approach through experiments on real clinical data, showing improved image quality and robustness compared to traditional manual methods. Our work represents a step towards more efficient and effective use of advanced medical imaging technologies and could ultimately benefit patients by providing higher quality care while reducing exam times and radiation exposure.",1
"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.",0
"Effectively measuring natural language inference (NLI) performance has been hindered by what we call the ""hypothesis-only"" bias: existing metrics only consider whether a hypothesis matches a premise sentence and ignore potentially important differences among hypotheses that may indicate one is preferred over another. To address this problem, we introduce adversarial training to encourage NLI models to predict multiple alternative plausible hypotheses that better reflect different interpretations underlying both premises; our method achieves improved correlations compared to human annotations on four benchmark datasets (SNLI, HANS, SciTail, and TriviaQA). We argue that ensemble methods that capture more diverse knowledge sources can mitigate the limitations of individual systems---our framework, Ensemble Adversarial Training (EAT), represents a promising approach towards robust evaluation of NLI models and other tasks requiring reasoning beyond simple pattern matching.",1
"This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of \cite{dosovitskiy2020image} for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of vision Longformer, which is a variant of Longformer \cite{beltagy2020longformer}, originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work \cite{wang2021pyramid}, on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at \url{https://github.com/microsoft/vision-longformer}.",0
"In recent years, vision transformers have emerged as powerful models that achieve state-of-the-art results on a wide range of computer vision tasks. However, these models suffer from computational limitations due to their reliance on self-attention mechanisms. These difficulties become even more pronounced for high-resolution images, which are difficult to process using existing transformer architectures. To address these challenges, we propose Multi-Scale Vision Longformer (MVL), a new vision transformer architecture designed specifically for encoding high-resolution images at multiple scales. MVL leverages multi-scale processing by applying downsampling operations at different points during encoding, allowing for efficient computation while capturing crucial contextual relationships among pixels at varying spatial resolutions. Our experimental analysis demonstrates the effectiveness of our proposed model across several benchmark datasets and applications such as image classification, object detection, and segmentation. We conclude that MVL is capable of achieving competitive performance with significantly fewer computations compared to previous approaches, making it well suited for resource-constrained scenarios like edge computing and mobile devices. Overall, MVL represents a significant step forward in enabling effective and efficient processing of complex visual data within vision transformers.",1
"Transformer networks are effective at modeling long-range contextual information and have recently demonstrated exemplary performance in the natural language processing domain. Conventionally, the temporal action proposal generation (TAPG) task is divided into two main sub-tasks: boundary prediction and proposal confidence prediction, which rely on the frame-level dependencies and proposal-level relationships separately. To capture the dependencies at different levels of granularity, this paper intuitively presents a unified temporal action proposal generation framework with original Transformers, called TAPG Transformer, which consists of a Boundary Transformer and a Proposal Transformer. Specifically, the Boundary Transformer captures long-term temporal dependencies to predict precise boundary information and the Proposal Transformer learns the rich inter-proposal relationships for reliable confidence evaluation. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG Transformer outperforms state-of-the-art methods. Equipped with the existing action classifier, our method achieves remarkable performance on the temporal action localization task. Codes and models will be available.",0
"In recent years, there has been significant progress in natural language processing using deep learning techniques such as transformers. One area that has seen particular advancement is action proposal generation (APG), which involves generating candidate actions based on natural language input. However, most existing APG models focus solely on the present tense, ignoring temporal relationships between actions. This can lead to incomplete or incorrect proposals, particularly for complex tasks involving multiple steps over time. To address this limitation, we propose a novel approach called Temporal Action Proposal Generation with Transformers (TAPRO). Our method uses transformer networks to encode both spatial and temporal information, allowing us to generate more accurate and complete sets of actions across different time frames. We evaluate our model on several benchmark datasets and demonstrate improved performance compared to state-of-the-art methods. Overall, TAPRO represents a promising new direction for NLP research and has the potential to enable more advanced applications in areas such as robotics and intelligent assistants.",1
"Interactive robots navigating photo-realistic environments face challenges underlying vision-and-language navigation (VLN), but in addition, they need to be trained to handle the dynamic nature of dialogue. However, research in Cooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts with a guide in natural language in order to reach a goal, treats the dialogue history as a VLN-style static instruction. In this paper, we present VISITRON, a navigator better suited to the interactive regime inherent to CVDN by being trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. We perform extensive ablations with VISITRON to gain empirical insights and improve performance on CVDN. VISITRON is competitive with models on the static CVDN leaderboard. We also propose a generalized interactive regime to fine-tune and evaluate VISITRON and future such models with pre-trained guides for adaptability.",0
"In recent years, there has been increasing interest in developing machine learning algorithms that can accurately classify objects within images, especially those with complex backgrounds. One promising approach is to utilize visual semantics, which involves analyzing the relationships between different objects and their surroundings within an image. This allows for more accurate classification by taking into account contextual cues.  This paper presents a new algorithm called VISITRON (Visual Semantics-Aligned Interactively Trained Object-Navigator) that leverages visual semantics to improve object detection accuracy in cluttered scenes. Unlike existing methods, VISITRON combines both supervised and interactive learning techniques to achieve better results. Specifically, VISITRON first pretrains on large amounts of labeled data using convolutional neural networks (CNNs). Then, a human operator provides feedback on the model's initial predictions, guiding the system towards more accurate detections through an iterative process. By incorporating this user input, VISITRON can adapt to specific domains and tasks and learn from human expertise.  Experimental evaluations demonstrate the effectiveness of our proposed method compared to state-of-the-art approaches across multiple datasets. Furthermore, we show that the combined use of supervised and interactive training leads to significant performance improvements over single-mode approaches. Our findings suggest that integrating human intelligence into computer vision systems can lead to enhanced object recognition capabilities in complex scenarios. Overall, VISITRON represents an important step forward in understanding how humans and machines can collaborate effectively to solve real-world problems in artificial intelligence.",1
"Recently, learning a model that generalizes well on out-of-distribution (OOD) data has attracted great attention in the machine learning community. In this paper, after defining OOD generalization via Wasserstein distance, we theoretically show that a model robust to input perturbation generalizes well on OOD data. Inspired by previous findings that adversarial training helps improve input-robustness, we theoretically show that adversarially trained models have converged excess risk on OOD data, and empirically verify it on both image classification and natural language understanding tasks. Besides, in the paradigm of first pre-training and then fine-tuning, we theoretically show that a pre-trained model that is more robust to input perturbation provides a better initialization for generalization on downstream OOD data. Empirically, after fine-tuning, this better-initialized model from adversarial pre-training also has better OOD generalization.",0
"Abstract: This work proposes an improved method for out-of-distribution (OOD) generalization using adversarial training and pre-training techniques. In recent years, deep learning models have demonstrated impressive performance on many tasks, but their ability to generalize well beyond their training data remains limited. One reason for this limitation is that these models often struggle to identify instances from different distributions as belonging to separate groups rather than simply representing random variation within a single distribution. To address this challenge, we propose leveraging both adversarial training and pre-training methods in order to improve model robustness and promote better OOD generalization. Our approach involves training a generative adversarial network to produce synthetic examples that can confuse the original classifier. These generated images are then used during fine-tuning to enhance the capacity of our model to distinguish real samples from generated ones. We evaluate our proposed method on several benchmark datasets including CIFAR10, SVHN, TinyImageNet, and LSUN, demonstrating consistent improvements over existing approaches. Overall, our study suggests that incorporating both adversarial training and pre-training techniques provides significant benefits in terms of enhancing OOD generalization abilities.",1
"Searching for objects in indoor organized environments such as homes or offices is part of our everyday activities. When looking for a target object, we jointly reason about the rooms and containers the object is likely to be in; the same type of container will have a different probability of having the target depending on the room it is in. We also combine geometric and semantic information to infer what container is best to search, or what other objects are best to move, if the target object is hidden from view. We propose to use a 3D scene graph representation to capture the hierarchical, semantic, and geometric aspects of this problem. To exploit this representation in a search process, we introduce Hierarchical Mechanical Search (HMS), a method that guides an agent's actions towards finding a target object specified with a natural language description. HMS is based on a novel neural network architecture that uses neural message passing of vectors with visual, geometric, and linguistic information to allow HMS to reason across layers of the graph while combining semantic and geometric cues. HMS is evaluated on a novel dataset of 500 3D scene graphs with dense placements of semantically related objects in storage locations, and is shown to be significantly better than several baselines at finding objects and close to the oracle policy in terms of the median number of actions required. Additional qualitative results can be found at https://ai.stanford.edu/mech-search/hms.",0
"Title: Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search  Abstract: This paper proposes a new approach for semantic and geometric modeling of complex 3D scenes using neural message passing in scene graphs. Traditional methods for modeling such scenes rely on manual encoding and feature engineering. However, these approaches can be time-consuming and may miss important contextual details. Our proposed method leverages advances in deep learning to automatically learn representations that capture both the semantic and geometric structure of the scene. We use graph convolutional networks (GCN) to encode relationships among objects in the scene graph. This enables us to capture spatial relationships between nodes while preserving their hierarchical organization. Furthermore, we introduce a novel attention mechanism that allows us to selectively focus on specific features or aspects of the scene at each iteration of message passing. By doing so, our model can better represent fine-grained differences across different scenarios and variations. Finally, we demonstrate how our approach outperforms existing baseline methods on challenging benchmark datasets for mechanical search problems involving complex 3D scenes. These results suggest that our model holds great potential for enabling more advanced applications in areas such as computer graphics, robotics, and virtual reality.",1
"Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies are mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose Convolutional Visual Transformers to tackle FER in the wild by two main steps. First, we propose an attentional selective fusion (ASF) for leveraging the feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with global-local attention. The fused feature maps are then flattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with global self-attention. The proposed method are evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. We also conduct cross-dataset evaluation on CK+ show the generalization capability of the proposed method.",0
"Title: Robust Facial Expression Recognition with Convolutional Visual Transformers Abstract This research presents a novel approach for robust facial expression recognition using convolutional visual transformers (CVT). The proposed method utilizes deep learning techniques to model spatial relationships within image data while capturing global dependencies across the sequence of frames in a video clip. CVTs improve upon traditional methods by enabling parallel processing at multiple scales and providing attention mechanisms that weigh different regions of the input based on their importance. Our experiments demonstrate improved performance over several baseline models, particularly under challenging conditions such as pose variation, occlusion, and background noise. Overall, this work represents a significant advance in facial expression recognition technology, with potential applications ranging from affective computing systems to social signal processing and human-computer interaction studies. Keywords: Facial expression recognition, convolutional neural networks, visual transformers, multi-scale representation, attention mechanism",1
"Automated image captioning is one of the applications of Deep Learning which involves fusion of work done in computer vision and natural language processing, and it is typically performed using Encoder-Decoder architectures. In this project, we have implemented and experimented with various flavors of multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19 based CNN Encoders and Attention based LSTM Decoders were explored. We have studied the effect of beam size and the use of pretrained word embeddings and compared them to baseline CNN encoder and RNN decoder architecture. The goal is to analyze the performance of each approach using various evaluation metrics including BLEU, CIDEr, ROUGE and METEOR. We have also explored model explainability using Visual Attention Maps (VAM) to highlight parts of the images which has maximum contribution for predicting each word of the generated caption.",0
"This paper presents an empirical analysis of image caption generation using deep learning techniques. We evaluate different approaches to generating text descriptions from images, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer models. Our results show that transformer models outperform other architectures on metrics such as accuracy, coherence, and diversity. We also investigate the impact of pretraining strategies and fine-tuning techniques on model performance. Overall, we conclude that deep learning holds great promise for automating image captioning tasks and provide recommendations for future research directions in this area.",1
"In this paper, we consider hybrid parallelism -- a paradigm that employs both Data Parallelism (DP) and Model Parallelism (MP) -- to scale distributed training of large recommendation models. We propose a compression framework called Dynamic Communication Thresholding (DCT) for communication-efficient hybrid training. DCT filters the entities to be communicated across the network through a simple hard-thresholding function, allowing only the most relevant information to pass through. For communication efficient DP, DCT compresses the parameter gradients sent to the parameter server during model synchronization. The threshold is updated only once every few thousand iterations to reduce the computational overhead of compression. For communication efficient MP, DCT incorporates a novel technique to compress the activations and gradients sent across the network during the forward and backward propagation, respectively. This is done by identifying and updating only the most relevant neurons of the neural network for each training sample in the data. We evaluate DCT on publicly available natural language processing and recommender models and datasets, as well as recommendation systems used in production at Facebook. DCT reduces communication by at least $100\times$ and $20\times$ during DP and MP, respectively. The algorithm has been deployed in production, and it improves end-to-end training time for a state-of-the-art industrial recommender model by 37\%, without any loss in performance.",0
"Here’s a possible abstract for this work:  In recent years, recommender systems have become increasingly important tools for making personalized predictions and recommendations based on large amounts of data. However, training these models can be challenging due to their complexity, scale, and resource requirements. In this work, we present a scalable methodology that enables efficient training of recommendation models using communication-efficient model and data parallelism techniques. We demonstrate how our approach outperforms current state-of-the-art methods in terms of speed, accuracy, and scalability by applying it to real-world datasets from different domains such as entertainment, online shopping, and news aggregation. Our results show that the proposed system achieves significantly better rankings than baseline models, improving user satisfaction and engagement while reducing computation costs. This research has implications for industry practitioners looking to improve recommendation quality, efficiency, and performance, ultimately leading to improved user experiences and business success.",1
"Attention layers are widely used in natural language processing (NLP) and are beginning to influence computer vision architectures. Training very large transformer models allowed significant improvement in both fields, but once trained, these networks show symptoms of over-parameterization. For instance, it is known that many attention heads can be pruned without impacting accuracy. This work aims to enhance current understanding on how multiple heads interact. Motivated by the observation that attention heads learn redundant key/query projections, we propose a collaborative multi-head attention layer that enables heads to learn shared projections. Our scheme decreases the number of parameters in an attention layer and can be used as a drop-in replacement in any transformer architecture. Our experiments confirm that sharing key/query dimensions can be exploited in language understanding, machine translation and vision. We also show that it is possible to re-parametrize a pre-trained multi-head attention layer into our collaborative attention layer. Collaborative multi-head attention reduces the size of the key and query projections by 4 for same accuracy and speed. Our code is public.",0
"The authors present a new approach to processing sequences with deep neural networks called ""multi-head attention"". In contrast to previous methods that concatenate vectors from different sources to form a single representation, multi-head attention allows these vectors to collaborate with one another as they learn about their context within the sequence. This method enables parallel computation over the sequence elements, allowing more efficient modeling than prior approaches. By employing novel mechanisms for scaling dot-products, the authors achieve state-of-the-art results on several challenging natural language processing tasks. Overall, this work demonstrates the effectiveness and efficiency of the proposed architecture for analyzing complex sequential data.",1
"Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate and therefore inconsistencies in smart home operations can lead a user to wonder ""why did the smart home do that?"" In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques to contribute computational methods for explainable activity recognition. Specifically, we generate explanations for smart home activity recognition systems that explain what about an activity led to the given classification. To do so, we introduce four computational techniques for generating natural language explanations of smart home data and compare their effectiveness at generating meaningful explanations. Through a study with everyday users, we evaluate user preferences towards the four explanation types. Our results show that the leading approach, SHAP, has a 92% success rate in generating accurate explanations. Moreover, 84% of sampled scenarios users preferred natural language explanations over a simple activity label, underscoring the need for explainable activity recognition systems. Finally, we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model, while others lead users to gain confidence. Taking all studied factors into consideration, we make a recommendation regarding which existing XAI method leads to the best performance in the domain of smart home automation, and discuss a range of topics for future work in this area.",0
"Activity recognition has become increasingly important as smart home systems have gained popularity. Understanding human behavior within these environments can lead to better automation, personalization, and assistance. However, current activity recognition methods often lack transparency and interpretability, which limits their adoption and trustworthiness. In this work, we propose an explainable approach that utilizes natural language processing techniques to generate descriptions of activities based on sensor data. Our method first maps raw sensor readings into semantic meaning representations using pre-trained models. Then, it generates textual explanations by reasoning over those intermediate representations through graph algorithms that model relationships between actions and objects present in a scene. We evaluate our approach across multiple datasets, demonstrating state-of-the-art performance while providing detailed explanations for each recognized action. This study represents a significant step towards more comprehensible and reliable activity recognition solutions for smart homes. Future research directions and potential applications are discussed.",1
"Chinese word segmentation (CWS) is the basic of Chinese natural language processing (NLP). The quality of word segmentation will directly affect the rest of NLP tasks. Recently, with the artificial intelligence tide rising again, Long Short-Term Memory (LSTM) neural network, as one of easily modeling in sequence, has been widely utilized in various kinds of NLP tasks, and functions well. Attention mechanism is an ingenious method to solve the memory compression problem on LSTM. Furthermore, inspired by the powerful abilities of bidirectional LSTM models for modeling sequence and CRF model for decoding, we propose a Bidirectional LSTM-CRF Attention-based Model in this paper. Experiments on PKU and MSRA benchmark datasets show that our model performs better than the baseline methods modeling by other neural networks.",0
"This paper presents a novel bidirectional LSTM-CRF attention-based model (BLSTMCA) that significantly improves upon state-of-the-art methods on two benchmark datasets by exploiting interdependencies among character contexts. Our approach combines both biLSTM and convolutional neural network architectures while explicitly considering character dependencies through conditional random fields (CRF). We apply selfattention mechanisms to focus specifically on important character features, which allows us to better handle OOV characters than previous models. Experiments demonstrate our BLSTMCA achieves substantial improvements over existing solutions, including those based on CRF alone. Additionally, we explore the robustness of our system against noise insertion during training time and observe only minor degradations in performance. These findings confirm the effectiveness and applicability of our method as a standalone segmenter or incorporated into larger language processing pipelines.",1
"Leveraging the advances of natural language processing, most recent scene text recognizers adopt an encoder-decoder architecture where text images are first converted to representative features and then a sequence of characters via `direct decoding'. However, scene text images suffer from rich noises of different sources such as complex background and geometric distortions which often confuse the decoder and lead to incorrect alignment of visual features at noisy decoding time steps. This paper presents I2C2W, a novel scene text recognizer that is accurate and tolerant to various noises in scenes. I2C2W consists of an image-to-character module (I2C) and a character-to-word module (C2W) which are complementary and can be trained end-to-end. I2C detects characters and predicts their relative positions in a word. It strives to detect all possible characters including incorrect and redundant ones based on different alignments of visual features without the restriction of time steps. Taking the detected characters as input, C2W learns from character semantics and their positions to filter out incorrect and redundant detection and produce the final word recognition. Extensive experiments over seven public datasets show that I2C2W achieves superior recognition performances and outperforms the state-of-the-art by large margins on challenging irregular scene text datasets.",0
"This paper presents a new approach for scene text recognition using transformer models. We introduce a novel architecture called Image-to-Character-to-Word (I2C2W) transformers that directly predicts character sequences from input images without any intermediate steps such as detecting characters first. Our model is designed specifically for handling real world scenarios where we observe significant variation in scale, aspect ratio and background clutter present in images, making traditional approaches less effective. We evaluate our proposed method on publicly available datasets and demonstrate state-of-the-art performance over previous methods across all benchmark metrics including accuracy, precision and recall. Finally, we analyze the learned representation spaces by visualizing attention maps which provide insights into the decision making process of our model leading to improved understanding of these systems",1
"Performance of recommender systems (RS) relies heavily on the amount of training data available. This poses a chicken-and-egg problem for early-stage products, whose amount of data, in turn, relies on the performance of their RS. On the other hand, zero-shot learning promises some degree of generalization from an old dataset to an entirely new dataset. In this paper, we explore the possibility of zero-shot learning in RS. We develop an algorithm, dubbed ZEro-Shot Recommenders (ZESRec), that is trained on an old dataset and generalize to a new one where there are neither overlapping users nor overlapping items, a setting that contrasts typical cross-domain RS that has either overlapping users or items. Different from categorical item indices, i.e., item ID, in previous methods, ZESRec uses items' natural-language descriptions (or description embeddings) as their continuous indices, and therefore naturally generalize to any unseen items. In terms of users, ZESRec builds upon recent advances on sequential RS to represent users using their interactions with items, thereby generalizing to unseen users as well. We study two pairs of real-world RS datasets and demonstrate that ZESRec can successfully enable recommendations in such a zero-shot setting, opening up new opportunities for resolving the chicken-and-egg problem for data-scarce startups or early-stage products.",0
"Abstract: The success of modern recommender systems largely depends on their ability to capture complex relationships among users, items, and features through effective modeling techniques such as collaborative filtering (CF) and content-based filtering (CBF). However, these approaches typically require large amounts of labeled training data which can be expensive and time consuming to collect. As a result, there has been growing interest in developing zero-shot learning models that can effectively make recommendations without any prior knowledge of user preferences or item characteristics. This paper proposes a novel framework that combines state-of-the-art deep learning techniques with zero-shot learning principles to achieve accurate recommendation outcomes across a wide range of datasets and evaluation metrics. Our approach leverages multi-modal representations that encode both semantic and visual information about items into embeddings, making our method highly scalable and applicable to diverse recommendation scenarios. Through extensive experiments, we demonstrate that our model significantly improves upon existing baselines while reducing computational cost compared to traditional CF methods. We hope this work will contribute towards bridging the gap between high quality recommendations and efficient resource usage in real-world applications.",1
"When working to understand usage of a data format, examples of the data format are often more representative than the format's specification. For example, two different applications might use very different JSON representations, or two PDF-writing applications might make use of very different areas of the PDF specification to realize the same rendered content. The complexity arising from these distinct origins can lead to large, difficult-to-understand attack surfaces, presenting a security concern when considering both exfiltration and data schizophrenia. Grammar inference can aid in describing the practical language generator behind examples of a data format. However, most grammar inference research focuses on natural language, not data formats, and fails to support crucial features such as type recursion. We propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply them to understanding de facto data formats. After reviewing existing grammar inference solutions, it was determined that a new, more flexible scaffold could be found in Reinforcement Learning (RL). Within this work, we lay out the many algorithmic changes required to adapt RL from its traditional, sequential-time environment to the highly interdependent environment of parsing. The result is an algorithm which can demonstrably learn recursive control structures in simple data formats, and can extract meaningful structure from fragments of the PDF format. Whereas prior work in grammar inference focused on either regular languages or constituency parsing, we show that RL can be used to surpass the expressiveness of both classes, and offers a clear path to learning context-sensitive languages. The proposed algorithm can serve as a building block for understanding the ecosystems of de facto data formats.",0
"This abstract presents RL-GRIT, which stands for “Reinforcement Learning for Grammar Inference”. Our approach takes advantage of two key insights to scale up unsupervised grammar inference using deep learning methods. Firstly, we apply recent advances in meta-learning to enable our model to rapidly adapt to new grammatical systems by leveraging experiences gained from solving prior tasks, including those that arise in diverse languages. Second, rather than directly predicting target structures, we train recurrent models to perform online repair operations guided by neural string edit distance minimization. We demonstrate the utility of our approach on several benchmark tests of cross-linguistic generality, where our method significantly outperforms strong baseline techniques without handcrafted features. These results support the viability of our novel meta-learning framework for efficient acquisition of complex linguistic knowledge, holding promise both as a foundation for future work on language generation and natural language understanding problems, and perhaps ultimately for enabling machine translation without explicit supervision.",1
"Given a natural language expression and an image/video, the goal of referring segmentation is to produce the pixel-level masks of the entities described by the subject of the expression. Previous approaches tackle this problem by implicit feature interaction and fusion between visual and linguistic modalities in a one-stage manner. However, human tends to solve the referring problem in a progressive manner based on informative words in the expression, i.e., first roughly locating candidate entities and then distinguishing the target one. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I (Image) module and a CMPC-V (Video) module to improve referring image and video segmentation models. For image data, our CMPC-I module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the target entity as well as suppress other irrelevant ones by spatial graph reasoning. For video data, our CMPC-V module further exploits action words based on CMPC-I to highlight the correct entity matched with the action cues by temporal graph reasoning. In addition to the CMPC, we also introduce a simple yet effective Text-Guided Feature Exchange (TGFE) module to integrate the reasoned multimodal features corresponding to different levels in the visual backbone under the guidance of textual information. In this way, multi-level features can communicate with each other and be mutually refined based on the textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or video version referring segmentation frameworks and our frameworks achieve new state-of-the-art performances on four referring image segmentation benchmarks and three referring video segmentation benchmarks respectively.",0
"This paper presents a novel algorithm for cross-modal referring segmentation using deep learning techniques that allows for progressive comprehension across modalities. We describe how our model can learn from visual prompts to generate text descriptions, which can then be used as input to refine the referred object region. Our approach uses a variational autoencoder framework coupled with adversarial training, allowing us to effectively model both the uncertainties in the learned representation and the discrepancies between different modalities. Experiments on several challenging datasets show that our method outperforms state-of-the-art methods by a significant margin while reducing computational costs. We believe that our work represents an important step forward towards truly efficient and effective multi-modal understanding systems, capable of seamlessly integrating data from multiple sources.",1
"Explaining the behavior of black box machine learning models through human interpretable rules is an important research area. Recent work has focused on explaining model behavior locally i.e. for specific predictions as well as globally across the fields of vision, natural language, reinforcement learning and data science. We present a novel model-agnostic approach that derives rules to globally explain the behavior of classification models trained on numerical and/or categorical data. Our approach builds on top of existing local model explanation methods to extract conditions important for explaining model behavior for specific instances followed by an evolutionary algorithm that optimizes an information theory based fitness function to construct rules that explain global model behavior. We show how our approach outperforms existing approaches on a variety of datasets. Further, we introduce a parameter to evaluate the quality of interpretation under the scenario of distributional shift. This parameter evaluates how well the interpretation can predict model behavior for previously unseen data distributions. We show how existing approaches for interpreting models globally lack distributional robustness. Finally, we show how the quality of the interpretation can be improved under the scenario of distributional shift by adding out of distribution samples to the dataset used to learn the interpretation and thereby, increase robustness. All of the datasets used in our paper are open and publicly available. Our approach has been deployed in a leading digital marketing suite of products.",0
"This is an interdisciplinary collaboration between computer scientists and biologists aimed at understanding how explanations for complex natural phenomena emerge from large data sets. We develop a mathematical framework based on information theory that allows us to quantify and compare different types of global explanations across multiple levels of organization (e.g., genotype–phenotype maps, gene regulatory networks). Using high performance computing resources we then apply our methodology to genomics data, showing that some well known principles of human health make perfect sense within the model agnostic information-based framework we introduce here. These findings could lead to the development of new strategies for early disease diagnosis or drug discovery. The broad applicability of these results to other domains of science and technology highlights both a need for a deeper theoretical understanding of scientific explanation as well as new machine learning algorithms capable of generating explanations de novo. While work remains before these goals can be achieved, the present study represents one step toward them by formalizing relevant aspects of the process via information theory.",1
"Language-queried video actor segmentation aims to predict the pixel-level mask of the actor which performs the actions described by a natural language query in the target frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolutions are amenable to recognizing which actor is performing the queried actions, it also inevitably introduces misaligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccurate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which contains a 3D temporal encoder over the video clip to recognize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to flexibly integrate spatial and temporal features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial- and temporal-relevant linguistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new state-of-the-art performance on two popular benchmarks with less computational overhead than previous approaches.",0
"In recent years there has been increasing interest from both industry and academia in developing methods that can automatically segment actors from complex videos containing multiple performers, motion blur, occlusions, and other challenges. This problem remains difficult even for state-of-the-art techniques due to the wide variety of scenes found in movies, TV shows, plays, dances, and online videos, among others. This article describes how natural language queries can significantly improve actor segmentation accuracy by providing strong priors about scene context, object relationships, human postures and movements, camera motion, and visual cues such as color or texture. We show how deep neural networks trained on large datasets of video frames, annotations, and natural language descriptions can learn latent representations that capture these diverse factors while enabling efficient inference using standard graphics processing units (GPUs). Our system processes input video streams frame-by-frame, taking into account previously predicted masks, new sensor data, and textual guidance. Our evaluation demonstrates significant improvements over baseline approaches across a range of scenarios without relying exclusively on any particular task-specific features.",1
"This paper is based on a machine learning project at the Norwegian University of Science and Technology, fall 2020. The project was initiated with a literature review on the latest developments within time-series forecasting methods in the scientific community over the past five years. The paper summarizes the essential aspects of this research. Furthermore, in this paper, we introduce an LSTM cell's architecture, and explain how different components go together to alter the cell's memory and predict the output. Also, the paper provides the necessary formulas and foundations to calculate a forward iteration through an LSTM. Then, the paper refers to some practical applications and research that emphasize the strength and weaknesses of LSTMs, shown within the time-series domain and the natural language processing (NLP) domain. Finally, alternative statistical methods for time series predictions are highlighted, where the paper outline ARIMA and exponential smoothing. Nevertheless, as LSTMs can be viewed as a complex architecture, the paper assumes that the reader has some knowledge of essential machine learning aspects, such as the multi-layer perceptron, activation functions, overfitting, backpropagation, bias, over- and underfitting, and more.",0
"Recurrent neural networks (RNNs) have been used extensively in natural language processing tasks such as machine translation, sentiment analysis and speech recognition because they can effectively capture temporal patterns from sequential data. However, vanilla recurrent architectures like LSTM suffer from the gradient exploding/vanishing problem which makes training them difficult. In this work, we propose Long Short-term Memory RNN architecture (LS-RNN), that combines memory cell state with forget gate gating mechanism that controls flow of gradients to enable better optimization by mitigating the vanishing gradient problem and improving the stability and accuracy of RNN models. We evaluate our approach on several benchmark datasets for five different NLP tasks, including text classification, machine translation and sentiment analysis and show consistent improvements over baseline systems using both LSTM and GRU cells.",1
"Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at https://github.com/YyzHarry/imbalanced-regression.",0
"The paper presents an overview of deep imbalanced regression, which refers to the use of machine learning algorithms such as neural networks and decision trees to predict continuous outcomes from datasets where one class has far more instances than the other classes. This can lead to biased predictions that favor the majority class, so we need methods to address class imbalances like resampling techniques like undersampling and oversampling. We discuss these challenges and present our own approach using generative adversarial networks (GANs). GANs have proven effective at generating synthetic data samples that closely mimic real examples, but their utility for improving regression models remains unexplored. Our experiments show promising results: generated samples can reduce bias by upweighting minority instances; using them for model training leads to better performance across metrics on benchmark datasets; we provide analysis comparing different strategies for integrating generated samples. These findings make an important contribution to handling imbalance in regression problems and highlight new opportunities for leveraging GANs beyond image generation tasks they were primarily designed for.",1
"Spherical data is distributed on the sphere. The data appears in various fields such as meteorology, biology, and natural language processing. However, a method for analysis of spherical data does not develop enough yet. One of the important issues is an estimation of the number of clusters in spherical data. To address the issue, I propose a new method called the Spherical X-means (SX-means) that can estimate the number of clusters on d-dimensional sphere. The SX-means is the model-based method assuming that the data is generated from a mixture of von Mises-Fisher distributions. The present paper explains the proposed method and shows its performance of estimation of the number of clusters.",0
"Here we consider estimation of the number of clusters from data sampled uniformly at random across the surface of a D-dimensional unit sphere S^D. For convenience we restrict attention to even dimensions (although most results extend directly to odd dimensions) with an integer M such that D = 2M or D=2M+1; thus the dimension ranges from two upwards. We are interested in estimating K, which is assumed fixed throughout. Data were generated by drawing N independent samples each containing P points; the total size of the data set was thus NP (and we took M=N, so N>D). Several alternative methods of estimating K were considered: using nearest neighbour distance matrices either via singular value decomposition with low-rank approximation, or else through kernel density estimates with bandwidth determined locally; both techniques yielded comparable results. Simulations showed how well these clustering algorithms worked as functions of several variables, including P,N,K, andD. Overall, though some regimes gave disappointing performance, good accuracy could typically be obtained over wide swathes of parameter space. However, all simulations used perfect knowledge of true underlying geometry of clustering – i.e., we knew apriori where centroids lay. In practice such geometric information may itself need to be estimated, complicating matters further still; but preliminary work indicates that this too can be done reasonably accurately, and we hope shortly to report fully on our investigations into this more realistic scenario",1
"Recurrent neural networks (RNNs) have been applied to a broad range of applications, including natural language processing, drug discovery, and video recognition. Their vulnerability to input perturbation is also known. Aligning with a view from software defect detection, this paper aims to develop a coverage guided testing approach to systematically exploit the internal behaviour of RNNs, with the expectation that such testing can detect defects with high possibility. Technically, the long short term memory network (LSTM), a major class of RNNs, is thoroughly studied. A family of three test metrics are designed to quantify not only the values but also the temporal relations (including both step-wise and bounded-length) exhibited when LSTM processing inputs. A genetic algorithm is applied to efficiently generate test cases. The test metrics and test case generation algorithm are implemented into a tool TestRNN, which is then evaluated on a set of LSTM benchmarks. Experiments confirm that TestRNN has advantages over the state-of-art tool DeepStellar and attack-based defect detection methods, owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation. Furthermore, TestRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step towards interpretable neural network testing.",0
"""Recurrent neural networks (RNNs) have been widely used for natural language processing tasks such as text generation and sentiment analysis due to their ability to capture sequential patterns from input data. However, testing RNNs can be challenging because they generate outputs over time steps, making it difficult to determine whether all valid output sequences have been covered by test cases.  Coverage guided testing (CGT) is a software testing approach that uses coverage metrics, such as branch coverage or path coverage, to systematically improve test suite quality through iterative refinement. CGT has proven effective in improving the fault detection rate for non-recursive programs but applying CGT to RNNs presents several new challenges. Firstly, due to their feedback loop structure, generating meaningful coverages requires reasoning about future program states. Secondly, defining meaningful covers for arbitrary long inputs and different network architectures remains open. Lastly, most existing approaches require manual effort or significant domain knowledge to define these criteria.  This work addresses these challenges by proposing an automated method called Coverage Guided Testing for Recurrent Neural Networks (CGTRNN). Our proposed method leverages recent advances in model checking for probabilistic systems to efficiently compute reachable output distributions under given test trajectories starting at different internal states. These computed outputs, including the probability distribution values, allow us to automatically formulate meaningfulcoverages based on a user-specified threshold. Additionally, we propose heuristics tailored specifically for RNNs which substantially reduce the search space required for generating diverse testcases. By integrating these methods into our approach, we demonstrate how CGTRNN significantly outperforms baseline methods across multiple benchmark problems drawn from real use cases.""",1
"Source code summarization aims to generate natural language summaries from structured code snippets for better understanding code functionalities. However, automatic code summarization is challenging due to the complexity of the source code and the language gap between the source code and natural language summaries. Most previous approaches either rely on retrieval-based (which can take advantage of similar examples seen from the retrieval database, but have low generalization performance) or generation-based methods (which have better generalization performance, but cannot take advantage of similar examples). This paper proposes a novel retrieval-augmented mechanism to combine the benefits of both worlds. Furthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on capturing global graph structure information of source code, we propose a novel attention-based dynamic graph to complement the static graph representation of the source code, and design a hybrid message passing GNN for capturing both the local and global structural information. To evaluate the proposed approach, we release a new challenging benchmark, crawled from diversified large-scale open-source C projects (total 95k+ unique functions in the dataset). Our method achieves the state-of-the-art performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of BLEU-4, ROUGE-L and METEOR.",0
"In recent years, there has been significant interest in developing natural language processing (NLP) techniques that can automatically generate summaries of source code for developers. One promising approach to generating high-quality code summaries is using a hybrid graph neural network (GNN). However, existing approaches based on hybrid GNNs for code summary generation still have several limitations. For example, they may miss important parts of the original code due to insufficient attention or lack of diversity. To address these challenges, we propose retrieval augmentation as a method to improve the quality of generated code summaries. By leveraging pretrained models such as BERT and RoBERTa, our proposed technique effectively selects appropriate candidates from different codes and improves overall summary accuracy through inference. Our experimental results show that the proposed model significantly outperforms state-of-the-art baselines across various evaluation metrics, including ROUGE, METEOR, and CIDEr. Overall, our work demonstrates the effectiveness of combining retrieval and generative approaches to improve code summarization, making it more efficient and accurate for developers. This research paves the way for future advancements in the field by providing valuable insights into hybrid GNN architectures and their applications in NLP tasks.",1
"Graph Neural Networks have revolutionized many machine learning tasks in recent years, ranging from drug discovery, recommendation systems, image classification, social network analysis to natural language understanding. This paper shows their efficacy in modeling relationships between products and making predictions for unseen product networks. By representing products as nodes and their relationships as edges of a graph, we show how an inductive graph neural network approach, named GraphSAGE, can efficiently learn continuous representations for nodes and edges. These representations also capture product feature information such as price, brand, or engineering attributes. They are combined with a classification model for predicting the existence of the relationship between products. Using a case study of the Chinese car market, we find that our method yields double the prediction performance compared to an Exponential Random Graph Model-based method for predicting the co-consideration relationship between cars. While a vanilla GraphSAGE requires a partial network to make predictions, we introduce an `adjacency prediction model' to circumvent this limitation. This enables us to predict product relationships when no neighborhood information is known. Finally, we demonstrate how a permutation-based interpretability analysis can provide insights on how design attributes impact the predictions of relationships between products. This work provides a systematic method to predict the relationships between products in many different markets.",0
"In recent years, product relationship prediction has gained significant attention from researchers due to its potential applications in recommendation systems, market analysis, and customer segmentation. Traditional methods used for product relationship prediction rely on manually engineered features that often fail to capture complex relationships among products. To overcome these limitations, we propose a novel approach based on graph neural networks (GNNs) that can learn latent representations of products and their interactions automatically. Our method takes into account both explicit and implicit feedback data such as purchase history and clickthrough behavior to predict missing entries in a matrix where each cell represents whether two items have been purchased together. We demonstrate the effectiveness of our proposed model through extensive experiments using real-world datasets. Results show that our GNN-based approach significantly outperforms state-of-the-art baselines across different metrics, including accuracy, precision, recall, F1 score, and mean average precision. Additionally, we conduct ablation studies to investigate the impact of different components in our model, providing valuable insights into how each component contributes to overall performance. This work provides new opportunities for enhancing product recommendation systems by leveraging advanced deep learning techniques like graph neural networks.",1
"Customers of machine learning systems demand accountability from the companies employing these algorithms for various prediction tasks. Accountability requires understanding of system limit and condition of erroneous predictions, as customers are often interested in understanding the incorrect predictions, and model developers are absorbed in finding methods that can be used to get incremental improvements to an existing system. Therefore, we propose an accountable error characterization method, AEC, to understand when and where errors occur within the existing black-box models. AEC, as constructed with human-understandable linguistic features, allows the model developers to automatically identify the main sources of errors for a given classification system. It can also be used to sample for the set of most informative input points for a next round of training. We perform error detection for a sentiment analysis task using AEC as a case study. Our results on the sample sentiment task show that AEC is able to characterize erroneous predictions into human understandable categories and also achieves promising results on selecting erroneous samples when compared with the uncertainty-based sampling.",0
"""This paper proposes an accountable error characterization methodology that utilizes machine learning techniques to improve reliability and robustness of engineering design systems. This methodology allows engineers to evaluate and quantify uncertainties related to system performance prediction, which can then be used to optimize designs and increase overall efficiency. By leveraging advanced computational tools, such as artificial intelligence and data analytics, the proposed approach enables real-time monitoring and identification of potential failures, ultimately leading to more accurate predictions and better decision making. The effectiveness of the proposed methodology is demonstrated through case studies involving complex mechanical systems. Overall, this work contributes to the development of a new framework for improving the quality, safety, and cost-effectiveness of engineering design.""",1
"Keeping in mind the necessity of intelligent system in educational sector, this paper proposes a text analysis based automated approach for automatic evaluation of the descriptive answers in an examination. In particular, the research focuses on the use of intelligent concepts of Natural Language Processing and Data Mining for computer aided examination evaluation system. The paper present an architecture for fair evaluation of answer sheet. In this architecture, the examiner creates a sample answer sheet for given sets of question. By using the concept of text summarization, text semantics and keywords summarization, the final score for each answer is calculated. The text similarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of this research were compared to manually graded assignments and other existing system. This approach was found to be very efficient in order to be implemented in an institution or in an university.",0
"This paper presents a methodology for evaluating the quality of descriptive answers generated by natural language processing (NLP) systems. The proposed approach uses text similarity measures to compare the output of NLP systems against human-generated answers and determine their degree of overlap. The results show that while some NLP systems perform well on simple factual questions, they struggle to generate detailed and accurate responses for more complex queries. Additionally, we found that traditional metrics like precision and recall are insufficient in measuring the quality of descriptive answers, as they fail to capture important aspects such as coherency and readability. Overall, our work demonstrates the potential of using NLP for generating high-quality descriptive answers, but also highlights areas where current methods fall short and require further improvement.",1
"Recent advancement of research in biometrics, computer vision, and natural language processing has discovered opportunities for person retrieval from surveillance videos using textual query. The prime objective of a surveillance system is to locate a person using a description, e.g., a short woman with a pink t-shirt and white skirt carrying a black purse. She has brown hair. Such a description contains attributes like gender, height, type of clothing, colour of clothing, hair colour, and accessories. Such attributes are formally known as soft biometrics. They help bridge the semantic gap between a human description and a machine as a textual query contains the person's soft biometric attributes. It is also not feasible to manually search through huge volumes of surveillance footage to retrieve a specific person. Hence, automatic person retrieval using vision and language-based algorithms is becoming popular. In comparison to other state-of-the-art reviews, the contribution of the paper is as follows: 1. Recommends most discriminative soft biometrics for specifiic challenging conditions. 2. Integrates benchmark datasets and retrieval methods for objective performance evaluation. 3. A complete snapshot of techniques based on features, classifiers, number of soft biometric attributes, type of the deep neural networks, and performance measures. 4. The comprehensive coverage of person retrieval from handcrafted features based methods to end-to-end approaches based on natural language description.",0
"Abstract: This paper presents a review on person retrieval methods using textual queries in surveillance systems. In recent years, advancements in artificial intelligence (AI) have enabled the development of more efficient techniques that can accurately identify individuals from surveillance footage based on natural language descriptions of their appearance or behavior. These methods leverage large datasets of annotated images, deep learning algorithms, computer vision models, and machine translation tools to improve accuracy and scalability. We discuss several approaches including attribute-based querying, keyword matching, neural network-based similarity search, jointly embedded retrieval frameworks, and hybrid features extracted by multi-model fusion. Each approach has been tested in various settings such as crowded places, airports, and public transportation centers demonstrating high retrieval performance and robustness under diverse lighting conditions, occlusions, background clutter, etc. While these methods show great potential, they still face challenges related to dealing with noisy texts, addressing privacy concerns, adapting to dynamic environments, handling new types of data like video sequences or multimodal streams, and ensuring explainability of results, among others. As future research directions, we recommend exploring advanced analytics combining multiple sources of evidence, incorporating domain knowledge or contextual constraints into retrieval pipelines, improving interpretability through attention mechanisms or counterfactual reasoning, enhancing security via tamper detection and authenticity validation, and fostering interdisciplinary collaborations across fields involved in intelligent surveillance systems design, deployment, ethical consideration, social impact assessment, etc. Overall, our literature review illustrates both tremendous progress made in person retrieval from textual cues and opportunities f",1
"Image captioning is one of the most challenging tasks in AI, which aims to automatically generate textual sentences for an image. Recent methods for image captioning follow encoder-decoder framework that transforms the sequence of salient regions in an image into natural language descriptions. However, these models usually lack the comprehensive understanding of the contextual interactions reflected on various visual relationships between objects. In this paper, we explore explicit and implicit visual relationships to enrich region-level representations for image captioning. Explicitly, we build semantic graph over object pairs and exploit gated graph convolutional networks (Gated GCN) to selectively aggregate local neighbors' information. Implicitly, we draw global interactions among the detected objects through region-based bidirectional encoder representations from transformers (Region BERT) without extra relational annotations. To evaluate the effectiveness and superiority of our proposed method, we conduct extensive experiments on Microsoft COCO benchmark and achieve remarkable improvements compared with strong baselines.",0
"In recent years, computer vision has made significant progress towards solving problems that were previously considered difficult. One particular task is image caption generation, which involves generating descriptive text summaries of images. To achieve this goal, researchers have proposed several approaches, such as using convolutional neural networks (CNNs) and recurrent neural networks (RNNs). However, these methods still face challenges in understanding relationships within images, particularly implicit relationships. This study seeks to address this challenge by exploring explicit and implicit visual relationships in image captioning. We propose a new method called Heterogeneous Attention Networks (HetNet), which utilizes both explicit relationships represented through spatial coordinates and object detectors, and implicit relationships inferred from scene context. Our experiments show that our approach outperforms existing state-of-the-art methods on two popular datasets - MSCOCO and Flickr8k. Additionally, we conduct an analysis on the effectiveness of different attention mechanisms in capturing visual relationships in image captioning tasks. Our results demonstrate the importance of incorporating both explicit and implicit relationships into image captioning models for improved performance.",1
"In most real-world applications, it is seldom the case that a given observable evolves independently of its environment. In social networks, users' behavior results from the people they interact with, news in their feed, or trending topics. In natural language, the meaning of phrases emerges from the combination of words. In general medicine, a diagnosis is established on the basis of the interaction of symptoms. Here, we propose a new model, the Interactive Mixed Membership Stochastic Block Model (IMMSBM), which investigates the role of interactions between entities (hashtags, words, memes, etc.) and quantifies their importance within the aforementioned corpora. We find that interactions play an important role in those corpora. In inference tasks, taking them into account leads to average relative changes with respect to non-interactive models of up to 150\% in the probability of an outcome. Furthermore, their role greatly improves the predictive power of the model. Our findings suggest that neglecting interactions when modeling real-world phenomena might lead to incorrect conclusions being drawn.",0
"This paper presents a framework that uses the stochastic block model to analyze interactions across different groups of nodes (e.g., individuals) within complex networks. We first derive analytical expressions for the joint probability distribution over network statistics, including both connectivity and node attributes. Next, we show how these results can be used to fit maximum likelihood estimates of group affiliations, edge probabilities, attribute effects, and other key parameters associated with multi-layered SBMs. Our findings suggest several new insights into data on social interactions and knowledge diffusion, including evidence of assortative mixing patterns and nonlinearities in tie formation rates across diverse types of actors. In addition, our work extends prior research by accounting for variations among edges, such as homophily along multiple dimensions or external influences like common interests or structural roles. Finally, our numerical simulations demonstrate the accuracy and flexibility of the proposed approach, which has important implications for understanding how to measure and predict interconnectedness based on large datasets from digital platforms, surveys, or other sources. Overall, the study contributes novel methodological tools for studying heterogeneous network structures and their consequences, expanding opportunities for researchers in fields ranging from sociology to computer science and economics.",1
"In recent years, sentiment analysis and emotion classification are two of the most abundantly used techniques in the field of Natural Language Processing (NLP). Although sentiment analysis and emotion classification are used commonly in applications such as analyzing customer reviews, the popularity of candidates contesting in elections, and comments about various sporting events; however, in this study, we have examined their application for epidemic outbreak detection. Early outbreak detection is the key to deal with epidemics effectively, however, the traditional ways of outbreak detection are time-consuming which inhibits prompt response from the respective departments. Social media platforms such as Twitter, Facebook, Instagram, etc. allow the users to express their thoughts related to different aspects of life, and therefore, serve as a substantial source of information in such situations. The proposed study exploits the bilingual (Urdu and English) data from Twitter and NEWS websites related to the dengue epidemic in Pakistan, and sentiment analysis and emotion classification are performed to acquire deep insights from the data set for gaining a fair idea related to an epidemic outbreak. Machine learning and deep learning algorithms have been used to train and implement the models for the execution of both tasks. The comparative performance of each model has been evaluated using accuracy, precision, recall, and f1-measure.",0
"In recent years, social media platforms have become popular sources of information during public health crises such as epidemics. However, these platforms often contain vast amounts of unstructured text data that need to be analyzed to identify trends and sentiments related to health issues. This study presents a methodology for sentiment and emotion classification of bilingual English/Spanish tweets related to COVID-19. We use deep learning techniques including transfer learning and fine-tuning pretrained models to classify emotions into six categories: negative, positive, neutral, fear, sadness, and joy. Our approach shows high accuracy (76%) on Spanish datasets, which represents one of the most linguistically diverse regions impacted by COVID-19. These results demonstrate the feasibility of using deep learning methods to analyze multilingual social media content and provide insights into community emotional response during global pandemics. Furthermore, our work highlights the importance of cross-linguistic research in understanding how people communicate their experiences across different languages during crisis events.",1
"Visual question answering (VQA) is a task that combines both the techniques of computer vision and natural language processing. It requires models to answer a text-based question according to the information contained in a visual. In recent years, the research field of VQA has been expanded. Research that focuses on the VQA, examining the reasoning ability and VQA on scientific diagrams, has also been explored more. Meanwhile, more multimodal feature fusion mechanisms have been proposed. This paper will review and analyze existing datasets, metrics, and models proposed for the VQA task.",0
"Visual Question Answering (VQA) has gained significant attention over recent years due to its potential applications in areas such as education, entertainment, and healthcare. In order to develop effective VQA systems, large datasets and diverse approaches are necessary to train and evaluate these models. This paper presents a comprehensive review of existing VQA datasets and approaches, aimed at providing insights into their characteristics, strengths, and limitations. We analyze several popular VQA datasets, including VQAv2, vqa_realworld, Open Image Dataset, and GQA, discussing their sizes, question types, images, annotations, and evaluation metrics. We then examine state-of-the-art VQA approaches, such as attention mechanisms, generative and transformer architectures, coattention networks, hybrid models, and transfer learning techniques, analyzing their features, performance, and shortcomings. Finally, we provide suggestions for future research directions, identifying challenges that need to be addressed to further advance the field of VQA. Overall, our study serves as a valuable resource for researchers interested in exploring the latest trends and advancements in VQA. Keywords: Visual Question Answering, Datasets, Approaches, Attention Mechanisms, Generative Models, Transfer Learning",1
"There has been recently a growing interest in studying adversarial examples on natural language models in the black-box setting. These methods attack natural language classifiers by perturbing certain important words until the classifier label is changed. In order to find these important words, these methods rank all words by importance by querying the target model word by word for each input sentence, resulting in high query inefficiency. A new interesting approach was introduced that addresses this problem through interpretable learning to learn the word ranking instead of previous expensive search. The main advantage of using this approach is that it achieves comparable attack rates to the state-of-the-art methods, yet faster and with fewer queries, where fewer queries are desirable to avoid suspicion towards the attacking agent. Nonetheless, this approach sacrificed the useful information that could be leveraged from the target classifier for that sake of query efficiency. In this paper we study the effect of leveraging the target model outputs and data on both attack rates and average number of queries, and we show that both can be improved, with a limited overhead of additional queries.",0
"This paper presents a new method of generating text adversarial attacks that utilizes target information to improve their effectiveness and efficiency. By incorporating knowledge of the target model's parameters and architecture into the attack process, we demonstrate a significant increase in success rate while reducing computational costs compared to traditional methods. We evaluate our approach on several benchmark datasets and models showing consistent improvements across different tasks and architectures. Our results indicate that leveraging target information can greatly enhance the power of text adversarial attacks as well as provide insights into how these attacks work. Overall, our study highlights the importance of considering both offensive and defensive aspects of machine learning systems. The ability of deep neural networks (DNN) to generalize has been studied intensively over recent years. However, less attention has been paid to analyze why DNNs fail during training, even though such analysis would have profound theoretical implications and allow one to better optimize the training procedure. In previous researches, some works propose to add noise during the forward pass in order to regularize the weights in the network so that each unit contributes approximately equally to the final output. Those works showed improvement in the robustness of the learned models against small perturbations but did not investigate their relationship to the problem of vanishing gradients which appears early on in training. Based on this observation, in this work, we aim at studying the connection between gradient obfuscation and generalization. For example, whether adding noise or increasing batch size could prevent vanishing gradients? And if yes, under which conditions? What kind of noise/batchsize combination leads to optimal performance? Moreover, since a deeper understanding of gradient flowing dynamics might lead us to develop more effective optimization algorithms that converge faster than SGD, thus allowing us to train larger and more powerful models without encountering the limitations brought by computation budget.",1
"Person search has drawn increasing attention due to its real-world applications and research significance. Person search aims to find a probe person in a gallery of scene images with a wide range of applications, such as criminals search, multicamera tracking, missing person search, etc. Early person search works focused on image-based person search, which uses person image as the search query. Text-based person search is another major person search category that uses free-form natural language as the search query. Person search is challenging, and corresponding solutions are diverse and complex. Therefore, systematic surveys on this topic are essential. This paper surveyed the recent works on image-based and text-based person search from the perspective of challenges and solutions. Specifically, we provide a brief analysis of highly influential person search methods considering the three significant challenges: the discriminative person features, the query-person gap, and the detection-identification inconsistency. We summarise and compare evaluation results. Finally, we discuss open issues and some promising future research directions.",0
"This paper presents a comprehensive survey on person search challenges and solutions, exploring both traditional methods and recent advances in technology. We focus on various aspects related to person search, including image and video analysis, natural language processing (NLP), social media, and crowdsourcing. Our aim is to provide researchers, practitioners, and policymakers with a clear understanding of current techniques used in person search operations, as well as identify future directions for research in this domain. Through our extensive review process, we uncover many promising approaches that could improve person search processes, such as deep learning algorithms and machine learning models that can accurately detect individuals in large collections of images and videos. We discuss the benefits and limitations of these technologies and highlight their potential contributions toward solving some of today’s most pressing person search problems. Our findings suggest there is significant room for improvement in terms of efficiency, accuracy, and scalability, particularly given the rapid growth of online data sources, which means the landscape may shift rapidly in the coming years. Overall, this work contributes to enhancing public safety and security efforts by providing insights into person search strategies and tools available today, along with opportunities for further advancements.",1
"As one of the most fundamental tasks in graph theory, subgraph matching is a crucial task in many fields, ranging from information retrieval, computer vision, biology, chemistry and natural language processing. Yet subgraph matching problem remains to be an NP-complete problem. This study proposes an end-to-end learning-based approximate method for subgraph matching task, called subgraph matching network (Sub-GMN). The proposed Sub-GMN firstly uses graph representation learning to map nodes to node-level embedding. It then combines metric learning and attention mechanisms to model the relationship between matched nodes in the data graph and query graph. To test the performance of the proposed method, we applied our method on two databases. We used two existing methods, GNN and FGNN as baseline for comparison. Our experiment shows that, on dataset 1, on average the accuracy of Sub-GMN are 12.21\% and 3.2\% higher than that of GNN and FGNN respectively. On average running time Sub-GMN runs 20-40 times faster than FGNN. In addition, the average F1-score of Sub-GMN on all experiments with dataset 2 reached 0.95, which demonstrates that Sub-GMN outputs more correct node-to-node matches.   Comparing with the previous GNNs-based methods for subgraph matching task, our proposed Sub-GMN allows varying query and data graphes in the test/application stage, while most previous GNNs-based methods can only find a matched subgraph in the data graph during the test/application for the same query graph used in the training stage. Another advantage of our proposed Sub-GMN is that it can output a list of node-to-node matches, while most existing end-to-end GNNs based methods cannot provide the matched node pairs.",0
"In recent years, graph neural networks (GNNs) have shown great promise as powerful models for understanding complex data structures such as graphs and networks. However, despite their successes, there remain many challenges facing GNNs in terms of scalability, computational complexity, and model expressiveness. To address these limitations, we present a new graph convolutional network architecture called ""Sub-GMN"", which stands for Subgraph Matching Network.  Our proposed approach leverages several key insights from traditional computer vision techniques and applies them to subgraph matching problems using a GCN design. Specifically, our model first identifies anchor patches across both input graphs based on low-level features like edge lengths and node degrees. Then, it performs local feature aggregation within each anchor patch using self-attention modules before computing global features that encode structural similarity relationships between the two input graphs. Finally, we apply a fully connected layer with Sigmoid activation function over the output nodes of last FC layer for generating probability scores over all the labeled binary substructures.  Experimental results show that our method outperforms state-of-the-art approaches on standard benchmark datasets, demonstrating improved accuracy and robustness even at very high levels of sparsity where most existing methods struggle. Our work represents another step forward towards making GNNs more flexible and powerful tools for solving real-world graph mining problems while maintaining high interpretability and explainability guarantees.  -----  The performance and applicability of graph neural networks (GNNs) continue to grow rapidly, but they still face some formidable challenges in terms of scale, complexity, and expressive power. We propose a novel graph convolutional network architecture called “Sub-GMN” to address these limitations and improve upon current approaches to graph mining. Based o",1
"Remarkable performance from Transformer networks in Natural Language Processing promote the development of these models in dealing with computer vision tasks such as image recognition and segmentation. In this paper, we introduce a novel framework, called Multi-level Multi-scale Point Transformer (MLMSPT) that works directly on the irregular point clouds for representation learning. Specifically, a point pyramid transformer is investigated to model features with diverse resolutions or scales we defined, followed by a multi-level transformer module to aggregate contextual information from different levels of each scale and enhance their interactions. While a multi-scale transformer module is designed to capture the dependencies among representations across different scales. Extensive evaluation on public benchmark datasets demonstrate the effectiveness and the competitive performance of our methods on 3D shape classification, part segmentation and semantic segmentation tasks.",0
"Advance image generation techniques have enabled computer vision researchers to generate high resolution 2D images from 3D point clouds. These generative models often rely on convolutional neural networks (CNNs) which tend to capture local features and lack the ability to model interdependencies among different parts of a scene. In contrast, transformer architectures process input sequences as a whole and can effectively capture global dependencies and relationships. This makes them ideal candidates for learning spatial relations from large datasets such as LiDAR scans which comprise millions of points representing real world environments at fine scales. The goal of our work is to propose a novel approach that combines the advantages of both CNNs and transformers by leveraging a hybrid architecture that enables end-to-end training of high quality 2D maps directly from raw 3D lidar data. We validate our method using qualitative and quantitative evaluations including cross validation against ground truth maps and user studies where we measure perceived visual fidelity of generated 2D maps over state-of-the-art methods. Our results show significant improvements across all metrics indicating the effectiveness of our approach in generating accurate 2D maps from high definition 3D point cloud data. Overall, our work represents an important step towards developing intelligent systems capable of processing massive amounts of sensor data collected in complex urban scenarios and producing interpretable representations required for decision making tasks.",1
"Social Media Platforms (SMPs) like Facebook, Twitter, Instagram etc. have large user base all around the world that generates huge amount of data every second. This includes a lot of posts by fake and spam users, typically used by many organisations around the globe to have competitive edge over others. In this work, we aim at detecting such user accounts in Twitter using a novel approach. We show how to distinguish between Genuine and Spam accounts in Twitter using a combination of Graph Representation Learning and Natural Language Processing techniques.",0
"In this research work, we study the problem of detecting fake users on social media platforms (SMPs) using natural language processing (NLP) techniques and graph embeddings. With the rise of deepfakes and automated bots on online spaces, identifying authentic user behavior has become increasingly difficult. In our approach, we propose a multi-modal framework that utilizes both textual cues and network features to model user behavior and identify anomalous activities. Specifically, we analyze three types of data: text posts, images, and interactions within a user’s social network. We then use graph embedding methods to encode these features into fixed-length vectors which capture both local and global structure in the graph. Finally, we train machine learning models such as XGBoost and Random Forest to distinguish real from fake profiles with high accuracy. Our experiments demonstrate significant improvement over state-of-the-art baselines across multiple datasets. Our findings have implications for various applications ranging from cybersecurity to digital marketing, emphasizing the importance of developing effective solutions to mitigate the impact of deepfakes and malicious actors on online communities. Overall, our proposed methodology provides an important step towards building secure and trustworthy online environments by addressing the pressing issue of identifying fake users in large-scale networks.",1
"Self-attention mechanism recently achieves impressive advancement in Natural Language Processing (NLP) and Image Processing domains. And its permutation invariance property makes it ideally suitable for point cloud processing. Inspired by this remarkable success, we propose an end-to-end architecture, dubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point cloud representation learning. First, a point-wise feature pyramid module is introduced to hierarchically extract features from different scales or resolutions. Then a cross-level cross-attention is designed to model long-range inter-level and intra-level dependencies. Finally, we develop a cross-scale cross-attention module to capture interactions between-and-within scales for representation enhancement. Compared with state-of-the-art approaches, our network can obtain competitive performance on challenging 3D object classification, point cloud segmentation tasks via comprehensive experimental evaluation.",0
"Artificial Intelligence (AI) has seen rapid advances over the past decade due to the development of novel architectures such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), etc. These architectures have been designed based on the notion that data can exhibit complex patterns across different scales and levels. This implies that learning effective representations from raw data often requires models capable of capturing these complex relationships. In recent years, deep point cloud representation has become increasingly important thanks largely to advancements in LiDAR technology which enables acquisition of high-resolution point clouds at low cost and time. The most commonly used methods for feature extraction from LiDAR points rely heavily on handcrafted features like Harris corners, SIFT descriptors, and spin images. While these techniques work well, they tend to lack generalization capabilities because they require engineered features specific to each application domain. Recent works try to replace these handengineered features with learnable ones but suffer from two major issues: either limited capacity of individual network components or suboptimal aggregation of local features. To address these limitations, we introduce the Cross-Level Cross-Scale Cross Attention Network(CCSN) architecture to jointly model interrelationships among multiple levels of abstraction and their corresponding scales. CCSN consists of three main parts: the encoder module, the cross-level attention module, and the decoder module. Our proposed method achieved state-of-the-art performance on SemanticKITTI dataset in terms of accuracy metrics and inference speed compared against other methods. Overall, our findings show promise towards improving the effectiveness of p",1
"Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.",0
"This paper proposes a novel dual transformer network architecture for point cloud analysis tasks such as semantic segmentation and object detection. Our approach uses two parallel encoders based on multi-head self attention mechanisms to capture global and local context respectively. These encoded representations are then fused to obtain a more comprehensive feature representation that can better encode high-level scene understanding and detailed geometry information. Extensive experiments demonstrate state-of-the-art performance across multiple benchmark datasets, outperforming prior arts by significant margins. We further analyze our method qualitatively and quantitatively, showing improved accuracy and robustness especially near object boundaries where most methods suffer from loss of detail. Overall, we show that our dual transformer framework provides powerful generalization abilities through strong inductive biases introduced via effective fusion mechanisms.",1
"Deep learning has yielded extraordinary results in vision and natural language processing, but this achievement comes at a cost. Most deep learning models require enormous resources during training, both in terms of computation and in human labeling effort. In this paper, we show that one can achieve similar accuracy to traditional deep-learning models, while using less training data. Much of the previous work in this area relies on using uncertainty or some form of diversity to select subsets of a larger training set. Submodularity, a discrete analogue of convexity, has been exploited to model diversity in various settings including data subset selection. In contrast to prior methods, we propose a novel diversity driven objective function, and balancing constraints on class labels and decision boundaries using matroids. This allows us to use efficient greedy algorithms with approximation guarantees for subset selection. We outperform baselines on standard image classification datasets such as CIFAR-10, CIFAR-100, and ImageNet. In addition, we also show that the proposed balancing constraints can play a key role in boosting the performance in long-tailed datasets such as CIFAR-100-LT.",0
"In many real-world applications, selecting a subset of data can improve performance while reducing computational costs. However, finding the optimal subset that balances constraints and maximizes utility is challenging. This paper presents a new framework that combines submodularity and constraint handling to efficiently select high-quality subsets. We provide theoretical guarantees on both the quality of our solution and the running time of our algorithm. Our extensive experimental evaluation across diverse domains demonstrates that our approach outperforms state-of-the-art baselines by significant margins, often returning solutions of higher quality within tight time limits. Overall, we believe that our work has broad impact and could enable efficient and effective subset selection in numerous application areas.",1
"In many real-world scenarios where extrinsic rewards to the agent are extremely sparse, curiosity has emerged as a useful concept providing intrinsic rewards that enable the agent to explore its environment and acquire information to achieve its goals. Despite their strong performance on many sparse-reward tasks, existing curiosity approaches rely on an overly holistic view of state transitions, and do not allow for a structured understanding of specific aspects of the environment. In this paper, we formulate curiosity based on grounded question answering by encouraging the agent to ask questions about the environment and be curious when the answers to these questions change. We show that natural language questions encourage the agent to uncover specific knowledge about their environment such as the physical properties of objects as well as their spatial relationships with other objects, which serve as valuable curiosity rewards to solve sparse-reward tasks more efficiently.",0
"This paper presents a framework for natural language question answering that enables robots to engage in curiosity-driven exploration and knowledge acquisition. Our approach, called ""Ask & Explore"", involves two key components: (1) active question generation, where the robot generates questions based on its current beliefs and goals, and (2) grounded question answering, where the robot uses perception, reasoning, and external resources to provide accurate answers to these questions. We demonstrate the effectiveness of our framework through experiments conducted on both real-world robots and simulated environments, showing that our method can enable robots to achieve significant improvements in their understanding of novel situations and phenomena. Additionally, we compare our approach against state-of-the-art methods in curiosity-driven learning and discuss its advantages in terms of efficiency, scalability, and adaptability to different domains and tasks. Overall, this work contributes to the development of autonomous agents capable of flexible and effective self-directed learning in complex, dynamic, and uncertain environments.",1
"Natural language-based vehicle retrieval is a task to find a target vehicle within a given image based on a natural language description as a query. This technology can be applied to various areas including police searching for a suspect vehicle. However, it is challenging due to the ambiguity of language descriptions and the difficulty of processing multi-modal data. To tackle this problem, we propose a deep neural network called SBNet that performs natural language-based segmentation for vehicle retrieval. We also propose two task-specific modules to improve performance: a substitution module that helps features from different domains to be embedded in the same space and a future prediction module that learns temporal information. SBnet has been trained using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three unique natural language descriptions each and tested 530 unique vehicle tracks and their corresponding query sets. SBNet achieved a significant improvement over the baseline in the natural language-based vehicle tracking track in the AI City Challenge 2021.",0
"Incorporate keywords such as natural language processing (NLP), deep learning, computer vision, vehicle search, segmentation, object detection. Include key findings/results and contributions from the research. Please make the abstract clear, concise, and compelling. The proposed approach leverages state-of-the-art NLP methods combined with powerful visual representation learned by deep neural networks to develop a robust model capable of accurately classifying vehicles according to their textual descriptions. By incorporating high quality contextualized image features into our models through fine-tuning, we achieved significant improvements over baseline techniques on standard benchmark datasets. Additionally, we present a novel network architecture that utilizes advanced image segmentation strategies, allowing us to generate more precise regional representations for improved performance. Our results indicate that these advances enable significantly better accuracy compared to traditional approaches across multiple metrics. Overall, our work contributes new insights into the integration of NLP and visual processing for enhanced automotive applications and beyond.",1
"Chest radiography is the most common radiographic examination performed in daily clinical practice for the detection of various heart and lung abnormalities. The large amount of data to be read and reported, with more than 100 studies per day for a single radiologist, poses a challenge in consistently maintaining high interpretation accuracy. The introduction of large-scale public datasets has led to a series of novel systems for automated abnormality classification. However, the labels of these datasets were obtained using natural language processed medical reports, yielding a large degree of label noise that can impact the performance. In this study, we propose novel training strategies that handle label noise from such suboptimal data. Prior label probabilities were measured on a subset of training data re-read by 4 board-certified radiologists and were used during training to increase the robustness of the training model to the label noise. Furthermore, we exploit the high comorbidity of abnormalities observed in chest radiography and incorporate this information to further reduce the impact of label noise. Additionally, anatomical knowledge is incorporated by training the system to predict lung and heart segmentation, as well as spatial knowledge labels. To deal with multiple datasets and images derived from various scanners that apply different post-processing techniques, we introduce a novel image normalization strategy. Experiments were performed on an extensive collection of 297,541 chest radiographs from 86,876 patients, leading to a state-of-the-art performance level for 17 abnormalities from 2 datasets. With an average AUC score of 0.880 across all abnormalities, our proposed training strategies can be used to significantly improve performance scores.",0
"This paper presents a new method for improving classification accuracy from noisy labels using additional sources of knowledge. The proposed approach integrates both clinical expertise and existing classifiers into one model, which can effectively handle the problematic labels often encountered in medical image analysis tasks. We evaluate our approach on chest radiography abnormality assessments and show that it significantly outperforms traditional methods, demonstrating its effectiveness at addressing label noise challenges.",1
"Referring expression comprehension aims to localize objects identified by natural language descriptions. This is a challenging task as it requires understanding of both visual and language domains. One nature is that each object can be described by synonymous sentences with paraphrases, and such varieties in languages have critical impact on learning a comprehension model. While prior work usually treats each sentence and attends it to an object separately, we focus on learning a referring expression comprehension model that considers the property in synonymous sentences. To this end, we develop an end-to-end trainable framework to learn contrastive features on the image and object instance levels, where features extracted from synonymous sentences to describe the same object should be closer to each other after mapping to the visual domain. We conduct extensive experiments to evaluate the proposed algorithm on several benchmark datasets, and demonstrate that our method performs favorably against the state-of-the-art approaches. Furthermore, since the varieties in expressions become larger across datasets when they describe objects in different ways, we present the cross-dataset and transfer learning settings to validate the ability of our learned transferable features.",0
"In natural language understanding, synonymous referring expressions (SREs) play a crucial role in conveying meaning and establishing reference relations. However, automatically distinguishing SREs from non-synonyms remains a challenging task for NLP systems. This paper presents a novel approach that leverages contrastive features to accurately identify SREs based on their subtle differences compared to non-synonyms. Our method extracts multiple types of features including lexical, semantic, and contextual cues, then applies a machine learning model trained on these features to make predictions. We evaluate our approach using four standard benchmark datasets and achieve state-of-the-art results across all metrics. Furthermore, we conduct human evaluations that demonstrate the effectiveness of our proposed method. Overall, our work provides insights into how NLP can better handle SREs and improve performance in applications such as text summarization, question answering, and machine translation.",1
"Text-video retrieval is a challenging task that aims to search relevant video contents based on natural language descriptions. The key to this problem is to measure text-video similarities in a joint embedding space. However, most existing methods only consider the global cross-modal similarity and overlook the local details. Some works incorporate the local comparisons through cross-modal local matching and reasoning. These complex operations introduce tremendous computation. In this paper, we design an efficient global-local alignment method. The multi-modal video sequences and text features are adaptively aggregated with a set of shared semantic centers. The local cross-modal similarities are computed between the video feature and text feature within the same center. This design enables the meticulous local comparison and reduces the computational cost of the interaction between each text-video pair. Moreover, a global alignment method is proposed to provide a global cross-modal measurement that is complementary to the local perspective. The global aggregated visual features also provide additional supervision, which is indispensable to the optimization of the learnable semantic centers. We achieve consistent improvements on three standard text-video retrieval benchmarks and outperform the state-of-the-art by a clear margin.",0
"This paper presents T2VLAD, a novel global-local sequence alignment method that enables efficient text-video retrieval by capturing both semantically meaningful features and subtle appearance differences across videos. We first propose a Temporal Pyramid Network (TPN) which encodes video into a multi-scale feature pyramid representation. Then we apply local alignment networks such as Dynamic Routing Networks on top of the pyramid representation to capture fine-grained correspondences between the query text and video segments. To ensure robustness against noisy and irrelevant matchings, we design a probabilistic framework based on Variational Autoencoder for inference and evaluation. Extensive experiments demonstrate that our approach outperforms existing methods significantly on large-scale benchmark datasets while achieving real-time speed. Our work sheds light on effective feature encoding and local sequence alignment for multimedia retrieval tasks.",1
"Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new \textbf{Convolution-enhanced image Transformer (CeiT)} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: \textbf{1)} instead of the straightforward tokenization from raw input images, we design an \textbf{Image-to-Tokens (I2T)} module that extracts patches from generated low-level features; \textbf{2)} the feed-froward network in each encoder block is replaced with a \textbf{Locally-enhanced Feed-Forward (LeFF)} layer that promotes the correlation among neighboring tokens in the spatial dimension; \textbf{3)} a \textbf{Layer-wise Class token Attention (LCA)} is attached at the top of the Transformer that utilizes the multi-level representations.   Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with $3\times$ fewer training iterations, which can reduce the training cost significantly\footnote{Code and models will be released upon acceptance.}.",0
"Artificial intelligence (AI) has made significant strides in recent years due to advances in deep learning technologies such as convolutional neural networks (CNNs) and transformer models. While CNNs have been widely used for image recognition tasks, they suffer from several limitations including poor parallelization, slow convergence, and limited modeling capacity for global dependencies. On the other hand, transformers provide superior parallelism and can effectively capture complex relationships among input elements but lack the ability to explicitly consider spatial hierarchies found in images. To address these challenges, we propose incorporating convolution designs into visual transformers by hybridizing their strengths while mitigating their weaknesses. This approach allows us to achieve state-of-the-art performance on image classification benchmarks while improving computational efficiency and robustness against noise and perturbations. Our experimental results demonstrate that our proposed method outperforms both standalone CNNs and transformers on several popular datasets, thus paving the way for future research in this direction. By combining the complementary advantages of CNNs and transformers through judicious design choices, our work represents an important step towards realizing efficient and effective AI systems for computer vision applications.",1
"Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised ""gold"" labels. Previous works, such as CLIP, use a simple pretraining task of predicting the pairings between images and text captions. CLIP, however, is data hungry and requires more than 400M image text pairs for training. We propose a data-efficient contrastive distillation method that uses soft labels to learn from noisy image-text pairs. Our model transfers knowledge from pretrained image and sentence encoders and achieves strong performance with only 3M image text pairs, 133x smaller than CLIP. Our method exceeds the previous SoTA of general zero-shot learning on ImageNet 21k+1k by 73% relatively with a ResNet50 image encoder and DeCLUTR text encoder. We also beat CLIP by 10.5% relatively on zero-shot evaluation on Google Open Images (19,958 classes).",0
"This paper describes how we can use language supervision to train machine learning models more efficiently by taking advantage of self-distillation. We show that by distilling knowledge from pretrained models and using it as additional supervision, we can achieve state-of-the-art zero-shot performance on several natural language understanding tasks. Our approach works well across different model architectures, and requires only a small amount of additional data to produce significant improvements over baseline methods.",1
"Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves substantial improvement in performance on image-caption retrieval w.r.t. similar methods. Second, we show that retrieval-augmented multi-modal transformers using the trained alignment model improve results on VQA over strong baselines. We further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.",0
"In this paper we propose a cross modal retrieval augmentation approach to improve multi-modal classification accuracy. Our method utilizes complementary modalities as auxiliary input data sources to increase the quality and quantity of training samples available to train deep neural networks which achieve better generalization on benchmark datasets such as HAVIC [2] and MUGA-II [7]. We provide extensive analysis using multiple variations of our proposed approach including ablation studies to demonstrate the effectiveness of each component. Finally, through experiments we show that our approach outperforms strong baselines such as DenseNet [4], ResNet [9], and VGG Net [6] achieving higher mean average precision (mAP) on these challenging multi-label benchmarks by up to +4%. Additionally, our model size remains small making deployment feasible across platforms where compute power may be limited. This research provides valuable insights into how to optimize deep learning models for real world problems involving complex, uncertain sensory inputs derived from the physical world. This paper proposes a novel approach to improving multi-modal classification accuracy using cross-modal retrieval augmentation. By leveraging complementary modalities as additional input data sources, the proposed method increases both the quality and quantity of training data available to deep neural networks, leading to improved generalization performance on benchmark datasets like HAVIC and MUGA-II. Thorough evaluation has been conducted, including ablation studies, to demonstrate the impact of each individual component of the methodology. Experiments have shown that the proposed approach can significantly surpass established models like DenseNet, ResNet, and VGG Net by as much as 4% mAP while maintaining compact model sizes suitable for resource-constrained environments. Ultimately, this work contributes important findings regarding the optimization of deep learning models for dealing wi",1
"People's visual experiences of the world are easy to carve up and examine along natural language boundaries, e.g., by category labels, attribute labels, etc. However, it is more difficult to elicit detailed visuospatial information about what a person attends to, e.g., the specific shape of a tree. Paying attention to the shapes of things not only feeds into well defined tasks like visual category learning, but it is also what enables us to differentiate similarly named objects and to take on creative visual pursuits, like poetically describing the shape of a thing, or finding shapes in the clouds or stars. We use a new data collection method that elicits people's prioritized attention to shapes during visual photo inspection by asking them to trace important parts of the image under varying time constraints. Using data collected via crowdsourcing over a set of 187 photographs, we examine changes in patterns of visual attention across individuals, across image types, and across time constraints.",0
"This study investigates how time constraints affect attention to shape during visual photo inspection tasks. We hypothesized that when participants were under time pressure, they would prioritize attending to shapes over other aspects of the images. To test this hypothesis, we conducted two experiments using a visual inspection task where participants had either unlimited time or limited time to complete the task. Results showed that when time was limited, participants attended more to shapes than when time was unlimited. These findings suggest that time constraints can re-prioritize attention to shapes during visual photo inspection tasks. Further research is necessary to explore how different types of time constraints may impact attention to various features of images and how these effects might vary across individuals and situations. Overall, our results contribute to understanding how attention and perception are influenced by time pressures and have implications for designing effective visual displays in real-world settings where time constraints may be present.",1
"Machine learning applications to symbolic mathematics are becoming increasingly popular, yet there lacks a centralized source of real-world symbolic expressions to be used as training data. In contrast, the field of natural language processing leverages resources like Wikipedia that provide enormous amounts of real-world textual data. Adopting the philosophy of ""mathematics as language,"" we bridge this gap by introducing a pipeline for distilling mathematical expressions embedded in Wikipedia into symbolic encodings to be used in downstream machine learning tasks. We demonstrate that a $\textit{mathematical}$ $\textit{language}$ $\textit{model}$ trained on this ""corpus"" of expressions can be used as a prior to improve the performance of neural-guided search for the task of symbolic regression.",0
This sounds like a fascinating topic! Can you tell me more about it?,1
"In recent years, graph neural networks (GNNs) have been widely adopted in the representation learning of graph-structured data and provided state-of-the-art performance in various applications such as link prediction, node classification, and recommendation. Motivated by recent advances of self-supervision for representation learning in natural language processing and computer vision, self-supervised learning has been recently studied to leverage unlabeled graph-structured data. However, employing self-supervision tasks as auxiliary tasks to assist a primary task has been less explored in the literature on graphs. In this paper, we propose a novel self-supervised auxiliary learning framework to effectively learn graph neural networks. Moreover, this work is the first study showing that a meta-path prediction is beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our method is learning to learn a primary task with various auxiliary tasks to improve generalization performance. The proposed method identifies an effective combination of auxiliary tasks and automatically balances them to improve the primary task. Our methods can be applied to any graph neural network in a plug-in manner without manual labeling or additional data. Also, it can be extended to any other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves the performance of node classification and link prediction.",0
"This work introduces self-supervised auxiliary learning for graph neural networks (GNN) via meta-learning. We use meta-learning to optimize over task distributions that generate training tasks on existing data sets, leading to better generalization performance at low computational cost. Our approach uses a single GNN model as opposed to multiple models which have to learn each time from scratch like traditional fine-tuning methods. Additionally, our method can make predictions without additional data annotation costs. For example, we show that compared to supervised baselines, our algorithm achieves higher accuracy. Finally, our results suggest that auxiliary self-supervision can significantly improve standard semi-supervised learning approaches.",1
"The computational prediction algorithm of neural network, or deep learning, has drawn much attention recently in statistics as well as in image recognition and natural language processing. Particularly in statistical application for censored survival data, the loss function used for optimization has been mainly based on the partial likelihood from Cox's model and its variations to utilize existing neural network library such as Keras, which was built upon the open source library of TensorFlow. This paper presents a novel application of the neural network to the quantile regression for survival data with right censoring, which is adjusted by the inverse of the estimated censoring distribution in the check function. The main purpose of this work is to show that the deep learning method could be flexible enough to predict nonlinear patterns more accurately compared to existing quantile regression methods such as traditional linear quantile regression and nonparametric quantile regression with total variation regularization, emphasizing practicality of the method for censored survival data. Simulation studies were performed to generate nonlinear censored survival data and compare the deep learning method with existing quantile regression methods in terms of prediction accuracy. The proposed method is illustrated with two publicly available breast cancer data sets with gene signatures. The method has been built into a package and is freely available at \url{https://github.com/yicjia/DeepQuantreg}.",0
"This article presents a deep learning method for quantile regression under right censorship. We use a modified version of the popular DenseNet architecture that preserves information on both the input variables and intermediate feature maps, resulting in better performance than traditional machine learning approaches. Our model effectively captures nonlinear relationships between covariates and survival outcomes by utilizing deep neural networks. In our experiments, we demonstrate improved accuracy compared to several benchmark methods across multiple datasets. Overall, this work contributes a new approach for tackling the challenges inherent in survival analysis with censored data.",1
"Compressing large neural networks is an important step for their deployment in resource-constrained computational platforms. In this context, vector quantization is an appealing framework that expresses multiple parameters using a single code, and has recently achieved state-of-the-art network compression on a range of core vision and natural language processing tasks. Key to the success of vector quantization is deciding which parameter groups should be compressed together. Previous work has relied on heuristics that group the spatial dimension of individual convolutional filters, but a general solution remains unaddressed. This is desirable for pointwise convolutions (which dominate modern architectures), linear layers (which have no notion of spatial dimension), and convolutions (when more than one filter is compressed to the same codeword). In this paper we make the observation that the weights of two adjacent layers can be permuted while expressing the same function. We then establish a connection to rate-distortion theory and search for permutations that result in networks that are easier to compress. Finally, we rely on an annealed quantization algorithm to better compress the network and achieve higher final accuracy. We show results on image classification, object detection, and segmentation, reducing the gap with the uncompressed model by 40 to 70% with respect to the current state of the art.",0
"Neural network compression has become increasingly important as we strive to make deep learning models more efficient without sacrificing accuracy. In this paper, we propose three novel techniques that address different aspects of the compression problem: permutation, quantization, and fine-tuning. We show how these methods can be combined to achieve state-of-the-art results on several benchmark datasets across a variety of architectures and tasks. Our contributions include a comprehensive evaluation framework that allows us to compare our approach against other compression algorithms, as well as a thorough analysis of the tradeoffs involved in each step of the process. Overall, we demonstrate that by carefully choosing which layers to compress and which operations to apply at runtime, significant reductions in model size and computational cost can be achieved while maintaining strong predictive performance. These findings have implications for real-world applications where storage constraints, latency requirements, and energy efficiency are critical factors.",1
"A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE -- an end-to-end data exploration system -- that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Reearch, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM.",0
"In our paper, we describe how to build an end-to-end data exploration system that provides interactive visualizations on arbitrary datasets in real time. Our approach is based on the idea of using inode queries to identify interesting subspaces within a dataset. We use a combination of database indexing techniques and GPU acceleration to make these queries as fast and efficient as possible. By combining these techniques with traditional machine learning algorithms, we can create a powerful tool for data analysis and discovery. While there have been many previous attempts at building similar systems, none have achieved the level of performance and ease of use that we demonstrate in our work. We believe that our approach has the potential to revolutionize the field of data science by making it easy for anyone to explore complex data sets in real time.",1
"We algorithmically identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are found using confident learning and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by 5%. Traditionally, ML practitioners choose which model to deploy based on test accuracy -- our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets.",0
"""Pervasive label errors"" refers to the fact that machine learning models often have very high accuracy on their test sets--high enough to make them look impressively good at a given task. However, these test sets aren't always representative of real data distributions (e.g., because they were sampled from different populations), which can cause discrepancies between model performance and actual utility. In some cases, these discrepancies lead to substantial ""bias variance dilemma,"" whereby machine learning practitioners must balance underfitting/overfitting against model misspecification. This paper argues that researchers need better ways to identify and correct pervasive label errors in benchmark datasets to ensure more accurate evaluation and reproducibility across applications. Through case studies involving established benchmarks, we demonstrate how simple cleaning techniques can sometimes mitigate these errors and significantly improve model calibration/generalization performance. We discuss several open challenges involved in detecting/addressing label noise in large-scale, complex domains while maintaining scalability and usability. Overall, our study underscores the importance of addressing pervasive label errors to achieve reliable progress in machine learning and artificial intelligence.",1
"Self-supervised learning methods are gaining increasing traction in computer vision due to their recent success in reducing the gap with supervised learning. In natural language processing (NLP) self-supervised learning and transformers are already the methods of choice. The recent literature suggests that the transformers are becoming increasingly popular also in computer vision. So far, the vision transformers have been shown to work well when pretrained either using a large scale supervised data or with some kind of co-supervision, e.g. in terms of teacher network. These supervised pretrained vision transformers achieve very good results in downstream tasks with minimal changes. In this work we investigate the merits of self-supervised learning for pretraining image/vision transformers and then using them for downstream classification tasks. We propose Self-supervised vIsion Transformers (SiT) and discuss several self-supervised training mechanisms to obtain a pretext model. The architectural flexibility of SiT allows us to use it as an autoencoder and work with multiple self-supervised tasks seamlessly. We show that a pretrained SiT can be finetuned for a downstream classification task on small scale datasets, consisting of a few thousand images rather than several millions. The proposed approach is evaluated on standard datasets using common protocols. The results demonstrate the strength of the transformers and their suitability for self-supervised learning. We outperformed existing self-supervised learning methods by large margin. We also observed that SiT is good for few shot learning and also showed that it is learning useful representation by simply training a linear classifier on top of the learned features from SiT. Pretraining, finetuning, and evaluation codes will be available under: https://github.com/Sara-Ahmed/SiT.",0
"Vision transformers have revolutionized computer vision by achieving top results on several benchmarks without using manual feature engineering, yet their success relies heavily on large amounts of labeled data and pretext tasks that may limit transferability to downstream objectives. In our work we introduce a simple self supervised approach (SiT) that trains a pure-transformer on jigsaw puzzles obtained from public datasets, significantly outperforming previous methods under weakly supervised settings, while being more generic, efficient and simpler than prior works. We further showcase SiT’s strong performance can boost the quality of linear probing and semi-supervised fine-tuning on challenging domains like semantic segmentation on plant cells where only very few annotations are available. Our findings pave the way towards fully unsupervised visual representation learning that generalizes across different domains.",1
"We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing vision-language models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to ""See Out of tHe bOx"" that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, 6.7% accuracy on SNLI-VE test split, respectively.",0
"This is an abstract on the paper ""Seeing out of the box"". In the research community, there has been significant interest recently in learning transferable representations that perform well across multiple NLP tasks without task-specific fine tuning. Although vision-language pretraining using multi-modal models has shown promise, previous methods have only used textual supervision and limited amounts of image data. This paper presents end-to-end pretraining for vision-language representation learning by jointly training the model on large amounts of both image and language data. Experiments show significantly better results compared to prior art and demonstrate that these learned representations generalize effectively to downstream NLP tasks. Overall, this work takes an important step towards more capable general-purpose intelligent systems.",1
"Over the last decade, the use of Deep Learning in many applications produced results that are comparable to and in some cases surpassing human expert performance. The application domains include diagnosing diseases, finance, agriculture, search engines, robot vision, and many others. In this paper, we are proposing an architecture that utilizes image/video captioning methods and Natural Language Processing systems to generate a title and a concise abstract for a video. Such a system can potentially be utilized in many application domains, including, the cinema industry, video search engines, security surveillance, video databases/warehouses, data centers, and others. The proposed system functions and operates as followed: it reads a video; representative image frames are identified and selected; the image frames are captioned; NLP is applied to all generated captions together with text summarization; and finally, a title and an abstract are generated for the video. All functions are performed automatically. Preliminary results are provided in this paper using publicly available datasets. This paper is not concerned about the efficiency of the system at the execution time. We hope to be able to address execution efficiency issues in our subsequent publications.",0
"In this paper we present a system which automatically generates descriptive titles for video clips using deep learning techniques. We use pre-trained convolutional neural networks (CNNs) which have been trained on large amounts of image data to extract features from each frame in the clip. These features are then used as input for a recurrent neural network (RNN), which generates a sequence of tokens representing candidate titles. Finally, these tokens are combined into a single string using a beam search algorithm, resulting in a natural language description of the content depicted in the video clip. To evaluate our method we performed experiments comparing automatic and manual generation of titels using a dataset of over one thousand YouTube videos. Results showed that out system was able to accurately capture important aspects of the video content such as objects, actions, and scene categories while producing descriptive titles which were comparable to those generated by human annotators. Furthermore we show how our system can be applied to real world applications such as content retrieval, where automatic generation of descriptive titles could greatly improve accessibility for visually impaired users. This research demonstrates how recent advancements in computer vision and machine learning can be applied to generate natural language descriptions of visual media such as videos, addressing one of the major challenges in multimedia retrieval and accessibility. Our work represents an important step towards developing more advanced AI systems capable of generating rich semantic representations of complex sensory inputs. As future work we plan to explore other types of multimedia such as audio recordings or text documents and extend our approach to support multimodal fusion of different types of data sources. Additionally we aim to investigate further the impact of different design choices on t",1
"Object detection with transformers (DETR) reaches competitive performance with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by the great success of pre-training transformers in natural language processing, we propose a pretext task named random query patch detection to Unsupervisedly Pre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the original image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we freeze the CNN backbone and propose a patch feature reconstruction branch which is jointly optimized with patch detection. (2) To perform multi-query localization, we introduce UP-DETR from single-query patch and extend it to multi-query patches with object query shuffle and attention mask. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr.",0
"Title: Unsupervised Pre-training for Object Detection with Transformers Authors: [Authors here] Abstract: This work addresses the problem of unsupervised pre-training for object detection using transformer networks. We present a new method called UP-DETR that leverages self-attention mechanisms to learn representations from raw input images without any labeled data or external supervision. Our approach achieves state-of-the-art results on several benchmark datasets, including COCO and Pascal VOC, outperforming previous unsupervised methods by significant margins. In addition, our model demonstrates strong transferability across different domains and tasks, making it a promising tool for a wide range of computer vision applications. By providing a simple yet effective framework for learning visual representations using only raw inputs and no explicit guidance, we hope to further advance the field of unsupervised learning for object detection and beyond. Keywords: Unsupervised learning, object detection, transformers, self-attention",1
"While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active$\mathbf{^2}$ Learning (A$\mathbf{^2}$L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A$\mathbf{^2}$L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by $\approx \mathbf{3-25\%}$ on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.",0
"Redundancy refers to redundant annotations which have been annotated but contribute no new information - as they can be inferred from other information present in the data (either from previous examples of such things in general or more specifically). In Active learning one usually has at least two types of samples available: positive ones that should be labelled and negative ones that needn’t be. Positive samples may often contain many overlapping parts (redundancies) making labelling them less efficient than it could be. This work presents a method named Active$^2$ that actively reduces these redundencies using simple heuristics. After implementing it we noticed very interesting improvements in accuracy on multiple data sets across both sequence tagging and machine translation domains. Active learning is a popular technique used in natural language processing tasks such as sequence tagging and machine translation to improve model performance by actively selecting informative training instances. However, traditional active learning approaches suffer from redundancies in the selected data, leading to wasted annotation efforts and slow progress in model improvement. To address this issue, we propose a novel approach called Active$^2$, which uses simple heuristics to reduce redundancies before selecting active instances. Our experiments demonstrate significant improvements in accuracy across several benchmark datasets in both sequence tagging and machine translation domains, compared to baseline active learning methods without reduction. We show that our proposed approach effectively leads to more efficient use of human annotations and faster convergence rates, thereby promoting cost efficiency in real-world applications. Overall, our findings highlight the importance of redundancy reduction in active learning to achieve better model performance.",1
"COVID-19 pandemic has an unprecedented impact all over the world since early 2020. During this public health crisis, reliable forecasting of the disease becomes critical for resource allocation and administrative planning. The results from compartmental models such as SIR and SEIR are popularly referred by CDC and news media. With more and more COVID-19 data becoming available, we examine the following question: Can a direct data-driven approach without modeling the disease spreading dynamics outperform the well referred compartmental models and their variants? In this paper, we show the possibility. It is observed that as COVID-19 spreads at different speed and scale in different geographic regions, it is highly likely that similar progression patterns are shared among these regions within different time periods. This intuition lead us to develop a new neural forecasting model, called Attention Crossing Time Series (\textbf{ACTS}), that makes forecasts via comparing patterns across time series obtained from multiple regions. The attention mechanism originally developed for natural language processing can be leveraged and generalized to materialize this idea. Among 13 out of 18 testings including forecasting newly confirmed cases, hospitalizations and deaths, \textbf{ACTS} outperforms all the leading COVID-19 forecasters highlighted by CDC.",0
"Artificial Intelligence (AI) has emerged as one of the most promising tools for forecasting the spread of infectious diseases such as COVID-19. In particular, attention mechanisms have been shown to improve the accuracy of models by allowing them to focus on relevant features of the data. However, traditional attention mechanisms operate solely within a single time step or across multiple time steps but without considering interdependencies across different regions at the same time step. In this work, we present a novel model called Inter-Series Attention Network (ISAN), which can capture both intra-series dependencies within each region and interdependencies among different regions simultaneously. Our approach outperforms state-of-the-art baseline methods and demonstrates significant improvement over several metrics including mean absolute error, root mean squared error, and correlation coefficient. This paper contributes to the growing literature on machine learning applications for predicting infectious disease epidemics. Future directions could explore incorporating other external variables like economic indicators, weather patterns, or social media data into ISAN to further enhance its performance. Overall, ISAN provides a valuable tool for public health officials responsible for making informed decisions related to containment measures, allocation of resources, and overall management of pandemics.",1
"Natural Language (NL) descriptions can be one of the most convenient or the only way to interact with systems built to understand and detect city scale traffic patterns and vehicle-related events. In this paper, we extend the widely adopted CityFlow Benchmark with NL descriptions for vehicle targets and introduce the CityFlow-NL Benchmark. The CityFlow-NL contains more than 5,000 unique and precise NL descriptions of vehicle targets, making it the first multi-target multi-camera tracking with NL descriptions dataset to our knowledge. Moreover, the dataset facilitates research at the intersection of multi-object tracking, retrieval by NL descriptions, and temporal localization of events. In this paper, we focus on two foundational tasks: the Vehicle Retrieval by NL task and the Vehicle Tracking by NL task, which take advantage of the proposed CityFlow-NL benchmark and provide a strong basis for future research on the multi-target multi-camera tracking by NL description task.",0
"Automatically generated abstract based on text provided:  CityFlow-NL is a system that allows users to track and retrieve vehicles using natural language descriptions of their locations. The system uses real-time image processing algorithms to detect and identify vehicles on city streets, and then matches them against natural language queries. This enables users to easily locate any vehicle within the city by simply describing where they last saw it, without needing detailed knowledge of the vehicle’s license plate number or other identifying details. Additionally, CityFlow-NL can also retrieve historical data on vehicle movements within the city, providing valuable insights into traffic patterns and congestion levels. Overall, CityFlow-NL represents a major step forward in vehicle tracking technology, allowing cities to better manage traffic flow and improve public safety.",1
"We propose a novel Siamese Natural Language Tracker (SNLT), which brings the advancements in visual tracking to the tracking by natural language (NL) descriptions task. The proposed SNLT is applicable to a wide range of Siamese trackers, providing a new class of baselines for the tracking by NL task and promising future improvements from the advancements of Siamese trackers. The carefully designed architecture of the Siamese Natural Language Region Proposal Network (SNL-RPN), together with the Dynamic Aggregation of vision and language modalities, is introduced to perform the tracking by NL task. Empirical results over tracking benchmarks with NL annotations show that the proposed SNLT improves Siamese trackers by 3 to 7 percentage points with a slight tradeoff of speed. The proposed SNLT outperforms all NL trackers to-date and is competitive among state-of-the-art real-time trackers on LaSOT benchmarks while running at 50 frames per second on a single GPU.",0
"We present the first end-to-end deep learning approach to track objects using natural language descriptions as input. This method leverages state-of-the-art siamese network architectures to learn correspondences between visual features and textual cues. Our system can effectively detect objects described in phrases like ""the red car"" or even longer sentences such as ""the small dog sitting on the mat"". In addition to object tracking results, our model generates image captions that closely match human annotations. With applications ranging from multimedia data management to assistive technologies for visually impaired individuals, we believe that our work represents an important step towards generalizing computer vision models into more intuitive and flexible tools.",1
"Video editing tools are widely used nowadays for digital design. Although the demand for these tools is high, the prior knowledge required makes it difficult for novices to get started. Systems that could follow natural language instructions to perform automatic editing would significantly improve accessibility. This paper introduces the language-based video editing (LBVE) task, which allows the model to edit, guided by text instruction, a source video into a target video. LBVE contains two features: 1) the scenario of the source video is preserved instead of generating a completely different video; 2) the semantic is presented differently in the target video, and all changes are controlled by the given instruction. We propose a Multi-Modal Multi-Level Transformer (M$^3$L-Transformer) to carry out LBVE. The M$^3$L-Transformer dynamically learns the correspondence between video perception and language semantic at different levels, which benefits both the video understanding and video frame synthesis. We build three new datasets for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M$^3$L-Transformer is effective for video editing and that LBVE can lead to a new field toward vision-and-language research.",0
"""Language-Based"" vs ""Data Driven"". What are the main differences? Is one better than another? This sounds like a very broad question, but I am interested in learning more about the key aspects that differentiate these two approaches for video editing, so please elaborate as necessary. Can you explain how each approach might be used depending on the application (e.g., consumer use case vs professional use case)? Are there any other key features/elements of the language-based versus data driven approach that would impact their respective applications beyond just video editing? Thank you!",1
"Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.",0
"This survey provides a comprehensive overview of the field of explainable artificial intelligence (XAI), specifically as it pertains to time series data. XAI has gained increasing importance due to the rise of machine learning algorithms that can produce complex results without revealing how those results were arrived at. In the context of time series analysis, where these models are used for predictive analytics in areas such as finance, healthcare, and climate science, understanding model decisions becomes even more crucial. The authors review existing research on time series XAI techniques, their limitations and challenges, and future directions towards enhancing transparency and accountability. They highlight promising approaches, including feature attribution methods, visualization tools, interpretable model architectures, and hybrid frameworks combining multiple XAI methods. Ultimately, they emphasize the need for further study into real-world applications, evaluation metrics, and ethical considerations. This work serves as a valuable resource for both researchers and practitioners interested in advancing the field of XAI on time series data.",1
"The Transformer architecture has become increasingly popular over the past two years, owing to its impressive performance on a number of natural language processing (NLP) tasks. However, all Transformer computations occur at the level of word representations and therefore, it may be argued that Transformer models do not explicitly attempt to learn hierarchical structure which is widely assumed to be integral to language. In the present work, we introduce hierarchical processing into the Transformer model, taking inspiration from the U-Net architecture, popular in computer vision for its hierarchical view of natural images. We empirically demonstrate that the proposed architecture outperforms both the vanilla Transformer and some strong baselines in the domain of chit-chat dialogue.",0
"Artificial intelligence agents that must make decisions in realtime need some understanding of time in order to operate effectively. In recent years we have seen deep learning models like transformers provide state-of-the-art performance on problems across many domains. Recurrent neural networks (RNNs) were previously used for most temporal data because they model memory explicitly, but now transformer-based systems can rival these results. But RNNs allow us to easily condition on past observations in a hierarchical manner using attention mechanisms; while some attention architectures can accomplish similar things with sequential inputs processed one at a time, they still lack strong explicit support for hierarchy and often require external supervision signals even if there are clear dependencies in the data itself. We propose injecting hierarchy into transformer architectures by extending them in two ways: First, we modify self-attention heads so each can attend only to specific levels of representation abstraction in other parts of the model rather than attending to all previous context equally. Secondly, we create a novel recurrence mechanism where internal representations propagate via a new type of update gate that modulates the strength of the signal flowing through different components in response to current conditions. Combinations of these modifications allow us to outperform prior methods on three tasks (prediction, zero-shot prompting, and multi-task finetuning), achieving state-of-the-art results without requiring task-specific modules or additional sources of supervisory signals beyond raw input text.",1
"A special purpose learning system assumes knowledge of admissible tasks at design time. Adapting such a system to unforeseen tasks requires architecture manipulation such as adding an output head for each new task or dataset. In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text. The system supports a wide range of vision tasks such as classification, localization, question answering, captioning, and more. We evaluate the system's ability to learn multiple skills simultaneously, to perform tasks with novel skill-concept combinations, and to learn new skills efficiently and without forgetting.",0
"Abstract: We describe a general framework towards building vision systems that can tackle any task given just pixels as input. Our proposed approach leverages recent advances in computer vision such as semi supervised learning techniques, self attention mechanisms, meta learning approaches etc to improve performance across different benchmarks. Our experiments show significant improvement over baselines on several datasets. In summary we believe our research takes steps towards realizing general purpose vision systems which could open up applications in areas like robotics, agriculture, medicine etc where automation currently faces limitations due to lack of accurate perception systems capable of handling diverse scenarios.",1
"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",0
"Artificial neural networks, particularly deep convolutional neural networks (DCNNs), have achieved state-of-the-art performance across several computer vision tasks such as image classification, object detection, and semantic segmentation. Recently, there has been growing interest in exploiting DCNNs beyond their original use case and applying them directly on raw sensor data - typically RGB images - without any explicit engineering effort dedicated to low-level feature extraction required by traditional computer vision algorithms like structure from motion (SfM) or multi-view geometry. Inspired by the seminal work of Mildenhall et al., who proposed a method that infers depth maps and scene geometry from single monocular images based solely on learned features, we introduce a framework for few-shot view synthesis using deep neural rendering networks. Our approach trains a network for depth estimation, and then uses it for novel view synthesis through point cloud warping and refinement. We demonstrate significant improvements over current methods, both quantitatively on benchmark datasets and qualitatively via visualizations. We hope our work fosters research into new applications exploiting high resolution generative models trained end-to-end, allowing us to finally put Neural Radiance Fields on a diet!",1
"We consider the task of grasping a target object based on a natural language command query. Previous work primarily focused on localizing the object given the query, which requires a separate grasp detection module to grasp it. The cascaded application of two pipelines incurs errors in overlapping multi-object cases due to ambiguity in the individual outputs. This work proposes a model named Command Grasping Network(CGNet) to directly output command satisficing grasps from RGB image and textual command inputs. A dataset with ground truth (image, command, grasps) tuple is generated based on the VMRD dataset to train the proposed network. Experimental results on the generated test set show that CGNet outperforms a cascaded object-retrieval and grasp detection baseline by a large margin. Three physical experiments demonstrate the functionality and performance of CGNet.",0
"This article presents a novel method called “Joint network for grasp detection conditioned on natural language commands” that can detect objects in images as well as output instructions for how they should be grasped by robots based on those objects’ contexts. The goal is to enable robots to perform a wide variety of manipulation tasks involving different objects under natural language guidance. We propose a system architecture where deep convolutional neural networks (CNNs) estimate pixelwise object proposals, followed by a graphical model which predicts one of the top N proposals using full image features including local low level edge maps, part Affordance fields and global scene evidence such as class confidence scores estimated from the same CNNs. Then we use reinforcement learning from human feedback to jointly optimize our perception module parameters, policy parameters and reward function to maximize task completion rates given natural language requests. Our experiments demonstrate significant improvement over baseline methods for both RGBD data collection tasks and 2D webcam imagery datasets with simulated depth maps. We also provide detailed ablation studies analyzing effects of each component contribution to overall performance. Results show substantial progress towards solving complex manipulation problems guided solely by natural language instructions.",1
"The video captioning task is to describe the video contents with natural language by the machine. Many methods have been proposed for solving this task. A large dataset called MSR Video to Text (MSR-VTT) is often used as the benckmark dataset for testing the performance of the methods. However, we found that the human annotations, i.e., the descriptions of video contents in the dataset are quite noisy, e.g., there are many duplicate captions and many captions contain grammatical problems. These problems may pose difficulties to video captioning models for learning. We cleaned the MSR-VTT annotations by removing these problems, then tested several typical video captioning models on the cleaned dataset. Experimental results showed that data cleaning boosted the performances of the models measured by popular quantitative metrics. We recruited subjects to evaluate the results of a model trained on the original and cleaned datasets. The human behavior experiment demonstrated that trained on the cleaned dataset, the model generated captions that were more coherent and more relevant to contents of the video clips. The cleaned dataset is publicly available.",0
"Title: Improving Video Caption Quality through Automatic Annotations Cleaning  Annotation quality plays a critical role in the performance of video captioning models, which heavily rely on accurate annotations for training and evaluation. However, manually annotating large-scale datasets is time-consuming and expensive, leading to lower annotation accuracy or insufficient data size. In this work, we address this challenge by introducing an automatic annotation cleaning method that improves the quality of video captions in the widely used MSR-VTT dataset. Our approach first utilizes language model selection and filtering techniques to identify low-quality captions, followed by human validation to refine and correct erroneous annotations. We evaluate our method using standard metrics such as BLEU and METEOR, demonstrating significant improvements over the original annotations across different languages. Our study highlights the importance of high-quality annotations in the development of robust video captioning systems and presents a viable solution for efficient and effective dataset curation. Overall, this research contributes towards advancing the state-of-the-art in natural language processing and computer vision domains.",1
"Natural language video localization (NLVL), which aims to locate a target moment from a video that semantically corresponds to a text query, is a novel and challenging task. Toward this end, in this paper, we present a comprehensive survey of the NLVL algorithms, where we first propose the pipeline of NLVL, and then categorize them into supervised and weakly-supervised methods, following by the analysis of the strengths and weaknesses of each kind of methods. Subsequently, we present the dataset, evaluation protocols and the general performance analysis. Finally, the possible perspectives are obtained by summarizing the existing methods.",0
"This paper presents a survey that summarizes how natural language processing techniques can be used in order to improve video localization performance. The authors provide a detailed analysis of different approaches that have been proposed in recent years in order to address different challenges encountered by state-of-the-art visual features extraction methods, such as scene classification and object detection tasks. They highlight the impact of natural language processing (NLP) techniques like semantic parsing, sentiment analysis, named entity recognition, and question answering in improving current systems. Finally, they present some future research directions towards a more general framework for NLP-based video analysis, where multiple modalities of input data could be combined seamlessly into a single pipeline using the power of deep learning models.",1
"Natural Human-Robot Interaction (HRI) is one of the key components for service robots to be able to work in human-centric environments. In such dynamic environments, the robot needs to understand the intention of the user to accomplish a task successfully. Towards addressing this point, we propose a software architecture that segments a target object from a crowded scene, indicated verbally by a human user. At the core of our system, we employ a multi-modal deep neural network for visual grounding. Unlike most grounding methods that tackle the challenge using pre-trained object detectors via a two-stepped process, we develop a single stage zero-shot model that is able to provide predictions in unseen data. We evaluate the performance of the proposed model on real RGB-D data collected from public scene datasets. Experimental results showed that the proposed model performs well in terms of accuracy and speed, while showcasing robustness to variation in the natural language input.",0
"Abstract: Robots equipped with cameras can observe humans performing tasks, but they often lack understanding of the context behind these actions. This problem becomes more challenging as robots need to generalize across multiple demonstrations from different users who may have diverse preferences or strategies. Our proposed solution utilizes deep neural networks that learn to ground visual observations into semantic natural language descriptions of human behavior. We present experimental results showing improved accuracy compared to prior work on both qualitative and quantitative metrics, including task success rates and user study evaluations by Amazon Mechanical Turk workers. Additionally, we demonstrate how our approach can enable more expressive natural language interaction between humans and robots, allowing them to engage in richer conversations during collaborative activities.",1
"Tracking by natural language specification is a new rising research topic that aims at locating the target object in the video sequence based on its language description. Compared with traditional bounding box (BBox) based tracking, this setting guides object tracking with high-level semantic information, addresses the ambiguity of BBox, and links local and global search organically together. Those benefits may bring more flexible, robust and accurate tracking performance in practical scenarios. However, existing natural language initialized trackers are developed and compared on benchmark datasets proposed for tracking-by-BBox, which can't reflect the true power of tracking-by-language. In this work, we propose a new benchmark specifically dedicated to the tracking-by-language, including a large scale dataset, strong and diverse baseline methods. Specifically, we collect 2k video sequences (contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the train/testing respectively. We densely annotate one sentence in English and corresponding bounding boxes of the target object for each video. We also introduce two new challenges into TNL2K for the object tracking task, i.e., adversarial samples and modality switch. A strong baseline method based on an adaptive local-global-search scheme is proposed for future works to compare. We believe this benchmark will greatly boost related researches on natural language guided tracking.",0
"In recent years there has been significant progress made in developing object tracking methods that use natural language inputs to guide their behavior. This approach promises more accurate and flexible tracking compared to traditional methods which rely on hardcoded rules or templates. However, designing efficient algorithms capable of using natural language inputs remains challenging due to the complexity of human languages. Moreover, developing effective benchmarks for evaluating such systems remains an open problem. This work presents new algorithms for integrating symbolic knowledge into deep learning based trackers, as well as a novel benchmark for testing object tracking systems under varying levels of semantic ambiguity. Our experimental results demonstrate substantial improvements over state-of-the-art methods both qualitatively and quantitatively, further justifying our claims towards better accuracy and flexibility in object tracking via natural language inputs. Overall, we believe this research paves the way for future advancements in artificial intelligence by providing robust solutions towards building intelligent agents capable of understanding complex tasks expressed naturally through language.",1
