"Deep Reinforcement Learning (RL) demonstrates excellent performance on tasks that can be solved by trained policy. It plays a dominant role among cutting-edge machine learning approaches using multi-layer Neural networks (NNs). At the same time, Deep RL suffers from high sensitivity to noisy, incomplete, and misleading input data. Following biological intuition, we involve Spiking Neural Networks (SNNs) to address some deficiencies of deep RL solutions. Previous studies in image classification domain demonstrated that standard NNs (with ReLU nonlinearity) trained using supervised learning can be converted to SNNs with negligible deterioration in performance. In this paper, we extend those conversion results to the domain of Q-Learning NNs trained using RL. We provide a proof of principle of the conversion of standard NN to SNN. In addition, we show that the SNN has improved robustness to occlusion in the input image. Finally, we introduce results with converting full-scale Deep Q-network to SNN, paving the way for future research to robust Deep RL applications.",0
"Cutting-edge machine learning approaches, particularly Deep Reinforcement Learning (RL), have shown remarkable performance in solving tasks that can be accomplished by trained policy. However, Deep RL is vulnerable to noisy, incomplete, and misleading input data. To address this issue, we incorporate Spiking Neural Networks (SNNs) based on biological intuition. Previous studies have shown that standard Neural Networks (NNs) trained using supervised learning can be converted to SNNs with minimal degradation in performance. In this study, we extend this approach to Q-Learning NNs trained using RL, demonstrating the potential for converting standard NN to SNN. Furthermore, our results show that SNNs have improved resistance to occlusion in input images. Finally, we present the conversion of full-scale Deep Q-network to SNN, opening up possibilities for robust applications of Deep RL in the future.",1
"Most e-commerce product feeds provide blended results of advertised products and recommended products to consumers. The underlying advertising and recommendation platforms share similar if not exactly the same set of candidate products. Consumers' behaviors on the advertised results constitute part of the recommendation model's training data and therefore can influence the recommended results. We refer to this process as Leverage. Considering this mechanism, we propose a novel perspective that advertisers can strategically bid through the advertising platform to optimize their recommended organic traffic. By analyzing the real-world data, we first explain the principles of Leverage mechanism, i.e., the dynamic models of Leverage. Then we introduce a novel Leverage optimization problem and formulate it with a Markov Decision Process. To deal with the sample complexity challenge in model-free reinforcement learning, we propose a novel Hybrid Training Leverage Bidding (HTLB) algorithm which combines the real-world samples and the emulator-generated samples to boost the learning speed and stability. Our offline experiments as well as the results from the online deployment demonstrate the superior performance of our approach.",0
"E-commerce product feeds typically display a mix of advertised and recommended products to consumers, with the advertising and recommendation platforms sharing a common set of products. The behavior of consumers when using the advertised products can influence the recommendations they receive, a process known as Leverage. With this in mind, we propose a new approach whereby advertisers can strategically bid on advertising platforms to optimize their organic traffic recommendations. Our research analyzes real-world data to explain the Leverage mechanism and introduce a novel Leverage optimization problem, formulated using a Markov Decision Process. To overcome the challenges of sample complexity in model-free reinforcement learning, we have developed a Hybrid Training Leverage Bidding (HTLB) algorithm that combines real-world and emulator-generated samples to enhance learning speed and stability. Our offline experiments and online deployment demonstrate the superior performance of our approach.",1
"We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.",0
"The initial part of our discussion covers a concise overview of reinforcement learning (RL), encompassing its triumphs, fundamentals, a case study, concerns, the ICML 2019 Workshop on RL for Real Life, utilization, educational resources, and prospects. Afterward, we delve into various RL implementations such as recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.",1
"This paper proposes a cascading failure mitigation strategy based on Reinforcement Learning (RL) method. Firstly, the principles of RL are introduced. Then, the Multi-Stage Cascading Failure (MSCF) problem is presented and its challenges are investigated. The problem is then tackled by the RL based on DC-OPF (Optimal Power Flow). Designs of the key elements of the RL framework (rewards, states, etc.) are also discussed in detail. Experiments on the IEEE 118-bus system by both shallow and deep neural networks demonstrate promising results in terms of reduced system collapse rates.",0
"The objective of this paper is to propose a strategy for mitigating cascading failures through the utilization of Reinforcement Learning (RL) approach. The initial section introduces the principles of RL, followed by an overview of the Multi-Stage Cascading Failure (MSCF) issue, along with an exploration of its challenges. The RL based on DC-OPF (Optimal Power Flow) is implemented to address the problem. The framework's crucial components, including rewards and states, are also thoroughly discussed. Experiments conducted on the IEEE 118-bus network using shallow and deep neural networks exhibit positive outcomes, particularly in terms of diminishing system collapse rates.",1
"In this paper, we show how novel transfer reinforcement learning techniques can be applied to the complex task of target driven navigation using the photorealistic AI2THOR simulator. Specifically, we build on the concept of Universal Successor Features with an A3C agent. We introduce the novel architectural contribution of a Successor Feature Dependant Policy (SFDP) and adopt the concept of Variational Information Bottlenecks to achieve state of the art performance. VUSFA, our final architecture, is a straightforward approach that can be implemented using our open source repository. Our approach is generalizable, showed greater stability in training, and outperformed recent approaches in terms of transfer learning ability.",0
"This article demonstrates how innovative transfer reinforcement learning techniques can be utilized to accomplish the challenging task of target-driven navigation in the photorealistic AI2THOR simulator. The study builds on the Universal Successor Features principle, with the addition of an A3C agent. A Successor Feature Dependent Policy (SFDP) is introduced as a new architectural component, and Variational Information Bottlenecks are employed to achieve state-of-the-art performance. The final architecture, VUSFA, is a simple technique that can be implemented using the authors' open-source repository. The approach is generalizable, demonstrated greater training stability, and surpassed earlier techniques in terms of transfer learning capabilities.",1
"This paper studies a recent proposal to use randomized value functions to drive exploration in reinforcement learning. These randomized value functions are generated by injecting random noise into the training data, making the approach compatible with many popular methods for estimating parameterized value functions. By providing a worst-case regret bound for tabular finite-horizon Markov decision processes, we show that planning with respect to these randomized value functions can induce provably efficient exploration.",0
"The aim of this study is to examine the proposal of utilizing randomized value functions for reinforcement learning exploration. The method involves injecting random noise into the training data to generate the value functions, which can be applied with various popular techniques for parameterized value function estimation. Our research establishes a worst-case regret bound for tabular finite-horizon Markov decision processes, demonstrating that adopting this approach can lead to efficient exploration by planning based on the randomized value functions.",1
"Recent advances in both machine learning and Internet-of-Things have attracted attention to automatic Activity Recognition, where users wear a device with sensors and their outputs are mapped to a predefined set of activities. However, few studies have considered the balance between wearable power consumption and activity recognition accuracy. This is particularly important when part of the computational load happens on the wearable device. In this paper, we present a new methodology to perform feature selection on the device based on Reinforcement Learning (RL) to find the optimum balance between power consumption and accuracy. To accelerate the learning speed, we extend the RL algorithm to address multiple sources of feedback, and use them to tailor the policy in conjunction with estimating the feedback accuracy. We evaluated our system on the SPHERE challenge dataset, a publicly available research dataset. The results show that our proposed method achieves a good trade-off between wearable power consumption and activity recognition accuracy.",0
"The use of machine learning and Internet-of-Things has recently gained attention in automatic Activity Recognition. This involves wearing a device with sensors that map outputs to specific activities. However, not many studies focus on balancing wearable power consumption and activity recognition accuracy, especially when computing is done on the wearable device. In this paper, we introduce a new approach to feature selection on the device using Reinforcement Learning (RL). Our method aims to find the best balance between accuracy and power consumption by incorporating multiple sources of feedback into the RL algorithm. We also estimate the feedback accuracy to better tailor the policy. We tested our system on the publicly available SPHERE challenge dataset and found that our approach achieves a favorable balance between wearable power consumption and activity recognition accuracy.",1
"This paper studies reinforcement learning (RL) under malicious falsification on cost signals and introduces a quantitative framework of attack models to understand the vulnerabilities of RL. Focusing on $Q$-learning, we show that $Q$-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals. We characterize the relation between the falsified cost and the $Q$-factors as well as the policy learned by the learning agent which provides fundamental limits for feasible offensive and defensive moves. We propose a robust region in terms of the cost within which the adversary can never achieve the targeted policy. We provide conditions on the falsified cost which can mislead the agent to learn an adversary's favored policy. A numerical case study of water reservoir control is provided to show the potential hazards of RL in learning-based control systems and corroborate the results.",0
"The goal of this study is to examine the impact of malicious falsification on cost signals in reinforcement learning (RL) and develop a quantitative framework of attack models. Specifically, we focus on $Q$-learning and demonstrate that even when cost signals are subject to stealthy attacks and bounded falsifications, $Q$-learning algorithms still converge. We analyze the relationship between the falsified cost and the $Q$-factors, as well as the policy learned by the agent, to understand the limitations of both offensive and defensive strategies. We propose a robust region of cost values that the adversary cannot manipulate to achieve their desired policy. Finally, we identify conditions under which the falsified cost can mislead the learning agent into adopting the adversary's preferred policy. A case study of water reservoir control serves as an example of the potential dangers of RL in learning-based control systems and supports our findings.",1
"Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.",0
"The process of inverse reinforcement learning (IRL) involves learning a reward function from demonstrations, which can lead to policy improvement and generalization. Although IRL has gained significant attention, there is a lack of research on identifying the minimum number of demonstrations necessary to teach a specific sequential decision-making task. We introduce a solution to this problem by framing it as a machine teaching challenge, aimed at discovering the fewest demonstrations required to define the reward equivalence class of the demonstrator. Our approach builds on prior work in algorithmic teaching for sequential decision-making tasks and involves a reduction to the set cover problem, which facilitates the development of an efficient approximation algorithm for identifying maximally informative demonstrations. We apply our machine teaching algorithm to two novel applications: estimating the minimal number of queries required for learning a policy using active IRL, and creating an innovative IRL algorithm capable of more efficient learning from informative demonstrations than traditional IRL methods.",1
"Counterfactual thinking describes a psychological phenomenon that people re-infer the possible results with different solutions about things that have already happened. It helps people to gain more experience from mistakes and thus to perform better in similar future tasks. This paper investigates the counterfactual thinking for agents to find optimal decision-making strategies in multi-agent reinforcement learning environments. In particular, we propose a multi-agent deep reinforcement learning model with a structure which mimics the human-psychological counterfactual thinking process to improve the competitive abilities for agents. To this end, our model generates several possible actions (intent actions) with a parallel policy structure and estimates the rewards and regrets for these intent actions based on its current understanding of the environment. Our model incorporates a scenario-based framework to link the estimated regrets with its inner policies. During the iterations, our model updates the parallel policies and the corresponding scenario-based regrets for agents simultaneously. To verify the effectiveness of our proposed model, we conduct extensive experiments on two different environments with real-world applications. Experimental results show that counterfactual thinking can actually benefit the agents to obtain more accumulative rewards from the environments with fair information by comparing to their opponents while keeping high performing efficiency.",0
"The psychological phenomenon of counterfactual thinking involves people imagining different outcomes for past events with alternative solutions. This type of thinking allows individuals to learn from their mistakes and perform better in future tasks. This study aims to explore the use of counterfactual thinking in improving decision-making strategies for agents in multi-agent reinforcement learning environments. A multi-agent deep reinforcement learning model is proposed, which mirrors the human counterfactual thinking process. The model generates multiple possible actions and estimates the rewards and regrets for these actions based on its understanding of the environment. A scenario-based framework is used to link the estimated regrets with the model's policies. The model updates its policies and corresponding regrets simultaneously during iterations. The effectiveness of the proposed model is tested through extensive experiments on two real-world environments. Results show that counterfactual thinking can enhance an agent's ability to accumulate rewards and perform efficiently while maintaining fair competition with opponents.",1
"We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint.",0
"Our approach demonstrates how machines can be trained to paint like human artists, who have the ability to create incredible paintings using just a few strokes. Utilizing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents acquire the skill to identify stroke position and color, and devise long-term plans to break down texture-heavy images into strokes. Our experiments reveal that exceptional visual outcomes can be attained with a stroke count in the hundreds, and that the training process does not necessitate human painter expertise or stroke tracking data. The code is readily available at https://github.com/hzwer/ICCV2019-LearningToPaint.",1
"Multi-agent systems have a wide range of applications in cooperative and competitive tasks. As the number of agents increases, nonstationarity gets more serious in multi-agent reinforcement learning (MARL), which brings great difficulties to the learning process. Besides, current mainstream algorithms configure each agent an independent network,so that the memory usage increases linearly with the number of agents which greatly slows down the interaction with the environment. Inspired by Generative Adversarial Networks (GAN), this paper proposes an iterative update method (IU) to stabilize the nonstationary environment. Further, we add first-person perspective and represent all agents by only one network which can change agents' policies from sequential compute to batch compute. Similar to continual lifelong learning, we realize the iterative update method in this unified representative network (IUUR). In this method, iterative update can greatly alleviate the nonstationarity of the environment, unified representation can speed up the interaction with environment and avoid the linear growth of memory usage. Besides, this method does not bother decentralized execution and distributed deployment. Experiments show that compared with MADDPG, our algorithm achieves state-of-the-art performance and saves wall-clock time by a large margin especially with more agents.",0
"The use of multi-agent systems is widespread in cooperative and competitive tasks. However, as the number of agents increases, nonstationarity becomes a more significant issue in multi-agent reinforcement learning (MARL), which makes learning more challenging. Additionally, current algorithms utilize individual networks for each agent, causing memory usage to increase linearly with the number of agents, thereby slowing down the interaction with the environment. To address these issues, this study proposes an iterative update method (IU) based on Generative Adversarial Networks (GAN) to stabilize the nonstationary environment. Furthermore, the study introduces a first-person perspective and employs a single network to represent all agents, allowing agents' policies to move from sequential to batch computation. This unified representative network (IUUR) uses the iterative update method, which effectively alleviates nonstationarity, speeds up interaction with the environment, and avoids the linear growth of memory usage. Additionally, this method does not require decentralized execution or distributed deployment. Experimental results demonstrate that our algorithm outperforms MADDPG and saves a significant amount of wall-clock time, particularly with a higher number of agents.",1
"Model-based Reinforcement Learning (MBRL) allows data-efficient learning which is required in real world applications such as robotics. However, despite the impressive data-efficiency, MBRL does not achieve the final performance of state-of-the-art Model-free Reinforcement Learning (MFRL) methods. We leverage the strengths of both realms and propose an approach that obtains high performance with a small amount of data. In particular, we combine MFRL and Model Predictive Control (MPC). While MFRL's strength in exploration allows us to train a better forward dynamics model for MPC, MPC improves the performance of the MFRL policy by sampling-based planning. The experimental results in standard continuous control benchmarks show that our approach can achieve MFRL`s level of performance while being as data-efficient as MBRL.",0
"In real world applications like robotics, data-efficient learning is necessary and Model-based Reinforcement Learning (MBRL) provides this. However, MBRL falls short of state-of-the-art Model-free Reinforcement Learning (MFRL) methods in terms of final performance. To overcome this, we propose a novel approach that combines the strengths of both MBRL and MFRL, resulting in high performance with minimal data. Our approach involves using MFRL for exploration and training a better forward dynamics model for Model Predictive Control (MPC), which in turn improves the performance of MFRL policy through sampling-based planning. Our experimental results in continuous control benchmarks demonstrate that our approach achieves the same level of performance as MFRL while being as data-efficient as MBRL.",1
"Generating image descriptions in different languages is essential to satisfy users worldwide. However, it is prohibitively expensive to collect large-scale paired image-caption dataset for every target language which is critical for training descent image captioning models. Previous works tackle the unpaired cross-lingual image captioning problem through a pivot language, which is with the help of paired image-caption data in the pivot language and pivot-to-target machine translation models. However, such language-pivoted approach suffers from inaccuracy brought by the pivot-to-target translation, including disfluency and visual irrelevancy errors. In this paper, we propose to generate cross-lingual image captions with self-supervised rewards in the reinforcement learning framework to alleviate these two types of errors. We employ self-supervision from mono-lingual corpus in the target language to provide fluency reward, and propose a multi-level visual semantic matching model to provide both sentence-level and concept-level visual relevancy rewards. We conduct extensive experiments for unpaired cross-lingual image captioning in both English and Chinese respectively on two widely used image caption corpora. The proposed approach achieves significant performance improvement over state-of-the-art methods.",0
"To cater to users worldwide, it is crucial to create image descriptions in multiple languages. Nonetheless, it is impractical to gather vast paired image-caption datasets for each language, which is necessary for training effective image captioning models. Prior research has addressed the issue of cross-lingual image captioning without paired data by using a pivot language as a reference. However, this approach is not without flaws, as it can result in inaccuracies due to errors in pivot-to-target machine translations. This paper introduces a new method for generating cross-lingual image captions that employs self-supervised rewards in the reinforcement learning framework. This approach helps reduce disfluency and visual irrelevancy errors by using fluency reward provided by self-supervision from mono-lingual corpus in the target language and a multi-level visual semantic matching model that provides both sentence-level and concept-level visual relevancy rewards. We evaluate our approach on two widely used image caption corpora in English and Chinese and find that it outperforms existing state-of-the-art methods.",1
"Data efficiency and robustness to task-irrelevant perturbations are long-standing challenges for deep reinforcement learning algorithms. Here we introduce a modular approach to addressing these challenges in a continuous control environment, without using hand-crafted or supervised information. Our Curious Object-Based seaRch Agent (COBRA) uses task-free intrinsically motivated exploration and unsupervised learning to build object-based models of its environment and action space. Subsequently, it can learn a variety of tasks through model-based search in very few steps and excel on structured hold-out tests of policy robustness.",0
"For deep reinforcement learning algorithms, achieving data efficiency and resilience against task-irrelevant disturbances has always been a daunting task. To tackle these issues in a continuous control environment, we have developed a modular approach that does not rely on hand-crafted or supervised information. Our approach, called Curious Object-Based seaRch Agent (COBRA), employs task-free intrinsically motivated exploration and unsupervised learning to construct object-based models of the environment and action space. Using this approach, COBRA can quickly learn a variety of tasks through model-based search and perform exceptionally well on structured hold-out tests that assess policy robustness.",1
"Embodied Question Answering (EQA) is a recently proposed task, where an agent is placed in a rich 3D environment and must act based solely on its egocentric input to answer a given question. The desired outcome is that the agent learns to combine capabilities such as scene understanding, navigation and language understanding in order to perform complex reasoning in the visual world. However, initial advancements combining standard vision and language methods with imitation and reinforcement learning algorithms have shown EQA might be too complex and challenging for these techniques. In order to investigate the feasibility of EQA-type tasks, we build the VideoNavQA dataset that contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the EQA task. We investigate several models, adapted from popular VQA methods, on this new benchmark. This establishes an initial understanding of how well VQA-style methods can perform within this novel EQA paradigm.",0
"A new task called Embodied Question Answering (EQA) has been proposed, which involves an agent navigating a complex 3D environment and answering questions based solely on its egocentric input. The aim is to enable the agent to combine scene understanding, navigation, and language understanding in order to perform complex reasoning in the visual world. However, early attempts using standard vision and language methods with imitation and reinforcement learning algorithms have proven to be too challenging for EQA. To explore the feasibility of EQA-type tasks, we have created the VideoNavQA dataset, which includes pairs of questions and videos generated in the House3D environment. The dataset evaluates question-answering performance while considering a wide range of questions and nearly-ideal navigation paths. We have tested several models adapted from popular VQA methods on this benchmark, providing an initial understanding of how VQA-style methods perform within the EQA paradigm.",1
"Although significant progress has been made in the field of automatic image captioning, it is still a challenging task. Previous works normally pay much attention to improving the quality of the generated captions but ignore the diversity of captions. In this paper, we combine determinantal point process (DPP) and reinforcement learning (RL) and propose a novel reinforcing DPP (R-DPP) approach to generate a set of captions with high quality and diversity for an image. We show that R-DPP performs better on accuracy and diversity than using noise as a control signal (GANs, VAEs). Moreover, R-DPP is able to preserve the modes of the learned distribution. Hence, beam search algorithm can be applied to generate a single accurate caption, which performs better than other RL-based models.",0
"Despite the advancements in automatic image captioning, generating diverse captions remains a difficult task. Previous studies have primarily focused on improving the quality of captions, neglecting diversity. This study presents a new approach, the Reinforcing Determinantal Point Process (R-DPP), that combines determinantal point process and reinforcement learning to generate a set of captions with both high quality and diversity. Our results show that R-DPP outperforms other methods, such as GANs and VAEs, in terms of accuracy and diversity. Moreover, R-DPP preserves the modes of the learned distribution, making it possible to use the beam search algorithm to generate a single accurate caption that outperforms other RL-based models.",1
"Recent developments in machine-learning algorithms have led to impressive performance increases in many traditional application scenarios of artificial intelligence research. In the area of deep reinforcement learning, deep learning functional architectures are combined with incremental learning schemes for sequential tasks that include interaction-based, but often delayed feedback. Despite their impressive successes, modern machine-learning approaches, including deep reinforcement learning, still perform weakly when compared to flexibly adaptive biological systems in certain naturally occurring scenarios. Such scenarios include transfers to environments different than the ones in which the training took place or environments that dynamically change, both of which are often mastered by biological systems through a capability that we here term ""fluid adaptivity"" to contrast it from the much slower adaptivity (""crystallized adaptivity"") of the prior learning from which the behavior emerged. In this article, we derive and discuss research strategies, based on analyzes of fluid adaptivity in biological systems and its neuronal modeling, that might aid in equipping future artificially intelligent systems with capabilities of fluid adaptivity more similar to those seen in some biologically intelligent systems. A key component of this research strategy is the dynamization of the problem space itself and the implementation of this dynamization by suitably designed flexibly interacting modules.",0
"The advances in machine-learning algorithms have resulted in remarkable performance improvements in many traditional applications of artificial intelligence research. Deep reinforcement learning employs deep learning functional architectures along with incremental learning schemes for sequential tasks that involve interaction-based but often delayed feedback. Despite their impressive achievements, modern machine-learning approaches, including deep reinforcement learning, are still inferior to biologically adaptive systems in certain natural scenarios. These scenarios include transfers to different environments or environments that undergo dynamic changes, which are readily mastered by biological systems through ""fluid adaptivity,"" in contrast to the slower ""crystallized adaptivity"" of prior learning. In this article, we propose research strategies based on the analysis of fluid adaptivity in biological systems and its neuronal modeling to equip future artificially intelligent systems with the fluid adaptivity capabilities seen in some biologically intelligent systems. A critical component of this research strategy involves dynamizing the problem space and implementing it through appropriately designed flexibly interacting modules.",1
"Deep reinforcement learning has learned to play many games well, but failed on others. To better characterize the modes and reasons of failure of deep reinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C) algorithm on four deceptive games, which are specially designed to provide challenges to game-playing agents. These games are implemented in the General Video Game AI framework, which allows us to compare the behavior of reinforcement learning-based agents with planning agents based on tree search. We find that several of these games reliably deceive deep reinforcement learners, and that the resulting behavior highlights the shortcomings of the learning algorithm. The particular ways in which agents fail differ from how planning-based agents fail, further illuminating the character of these algorithms. We propose an initial typology of deceptions which could help us better understand pitfalls and failure modes of (deep) reinforcement learning.",0
"Although deep reinforcement learning has demonstrated proficiency in playing numerous games, it has been unable to succeed in certain cases. In order to analyze the modes and reasons for these failures, we conducted a study using the widely utilized Asynchronous Actor-Critic (A2C) algorithm on four deliberately challenging games that were designed to test the abilities of game-playing agents. These games were built into the General Video Game AI framework, which allowed us to compare the decision-making of reinforcement learning-based agents with planning agents based on tree search. We discovered that deep reinforcement learners were easily deceived by several of these games, highlighting the weaknesses of the learning algorithm. The manner in which agents fail differs from that of planning-based agents, providing further insight into the nature of these algorithms. We have developed an initial typology of deceptions that may assist in comprehending the pitfalls and failure modes of (deep) reinforcement learning.",1
"Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence. However, as robots and humans come closer in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration will only take place through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent.",0
"With recent advancements in intelligent systems, deep learning, and artificial intelligence, robots are becoming more capable of automating and performing complex tasks. However, as robots and humans interact more closely, the issue of interpretability becomes more important. It is crucial for successful collaboration and interaction between robots and humans to have a mutual understanding of the environment and task at hand. However, this poses a challenge in deep learning systems. Our solution to this is a hierarchical deep reinforcement learning system called Dot-to-Dot. It consists of a low-level agent that efficiently handles the large actions/states space of a robotic system by following the directives of a high-level agent. The high-level agent forms an interpretable representation of the task and decision-making process for human operators. We tested this method on a MuJoCo-based model of the Fetch Robotics Manipulator and a Shadow Hand, and the results showed efficient learning of complex actions/states spaces by the low-level agent and an interpretable representation of the task and decision-making process learned by the high-level agent.",1
"In this paper, we propose a deep reinforcement learning (DRL) based mobility load balancing (MLB) algorithm along with a two-layer architecture to solve the large-scale load balancing problem for ultra-dense networks (UDNs). Our contribution is three-fold. First, this work proposes a two-layer architecture to solve the large-scale load balancing problem in a self-organized manner. The proposed architecture can alleviate the global traffic variations by dynamically grouping small cells into self-organized clusters according to their historical loads, and further adapt to local traffic variations through intra-cluster load balancing afterwards. Second, for the intra-cluster load balancing, this paper proposes an off-policy DRL-based MLB algorithm to autonomously learn the optimal MLB policy under an asynchronous parallel learning framework, without any prior knowledge assumed over the underlying UDN environments. Moreover, the algorithm enables joint exploration with multiple behavior policies, such that the traditional MLB methods can be used to guide the learning process thereby improving the learning efficiency and stability. Third, this work proposes an offline-evaluation based safeguard mechanism to ensure that the online system can always operate with the optimal and well-trained MLB policy, which not only stabilizes the online performance but also enables the exploration beyond current policies to make full use of machine learning in a safe way. Empirical results verify that the proposed framework outperforms the existing MLB methods in general UDN environments featured with irregular network topologies, coupled interferences, and random user movements, in terms of the load balancing performance.",0
"Our paper presents a two-layer architecture and a deep reinforcement learning (DRL) based mobility load balancing (MLB) algorithm to address the challenge of large-scale load balancing in ultra-dense networks (UDNs). Our contribution is three-fold. Firstly, we propose a self-organized clustering approach that groups small cells according to their historical loads to mitigate global traffic variations. Secondly, we introduce an off-policy DRL-based MLB algorithm to autonomously learn the optimal MLB policy for intra-cluster load balancing, which incorporates multiple behavior policies to enhance learning efficiency and stability. Thirdly, we propose an offline-evaluation based safeguard mechanism to ensure the online system always operates with the optimal and well-trained MLB policy, enabling safe exploration beyond current policies. Empirical results demonstrate that our proposed framework outperforms existing MLB methods in UDN environments with irregular network topologies, coupled interferences, and random user movements, in terms of load balancing performance.",1
"Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.",0
"The use of reinforcement learning in practical applications often involves agents learning from a set of pre-collected data with no opportunity for further data collection. This paper highlights the limitations of standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, due to errors introduced by extrapolation. These algorithms are ineffective in learning from data that is not correlated with the current policy distribution. To address this issue, a new class of off-policy algorithms called batch-constrained reinforcement learning is introduced. This algorithm restricts the action space to encourage the agent to behave similarly to an on-policy subset of the given data. The paper presents the first continuous control deep reinforcement learning algorithm that can effectively learn from any fixed batch data. The results show the effectiveness of this approach in several tasks.",1
"Policy gradient methods have demonstrated success in reinforcement learning tasks that have high-dimensional continuous state and action spaces. However, policy gradient methods are also notoriously sample inefficient. This can be attributed, at least in part, to the high variance in estimating the gradient of the task objective with Monte Carlo methods. Previous research has endeavored to contend with this problem by studying control variates (CVs) that can reduce the variance of estimates without introducing bias, including the early use of baselines, state dependent CVs, and the more recent state-action dependent CVs. In this work, we analyze the properties and drawbacks of previous CV techniques and, surprisingly, we find that these works have overlooked an important fact that Monte Carlo gradient estimates are generated by trajectories of states and actions. We show that ignoring the correlation across the trajectories can result in suboptimal variance reduction, and we propose a simple fix: a class of ""trajectory-wise"" CVs, that can further drive down the variance. We show that constructing trajectory-wise CVs can be done recursively and requires only learning state-action value functions like the previous CVs for policy gradient. We further prove that the proposed trajectory-wise CVs are optimal for variance reduction under reasonable assumptions.",0
"Reinforcement learning tasks with high-dimensional continuous state and action spaces have been successful using policy gradient methods. However, these methods are known for their inefficient use of samples, which is partly due to the high variance in estimating the task objective gradient with Monte Carlo methods. Previous research has tackled this issue by studying control variates (CVs) that can reduce variance without introducing bias. This includes the use of baselines, state-dependent CVs, and state-action dependent CVs. In this study, we analyze the drawbacks and properties of previous CV techniques. We found that previous works have overlooked an important aspect that Monte Carlo gradient estimates are generated by trajectories of states and actions. We propose a simple fix: a class of ""trajectory-wise"" CVs that can further reduce the variance. Constructing trajectory-wise CVs can be done recursively and requires only learning state-action value functions like the previous CVs for policy gradient. We prove that the proposed trajectory-wise CVs are optimal for variance reduction under reasonable assumptions.",1
"In artificial intelligence, we often specify tasks through a reward function. While this works well in some settings, many tasks are hard to specify this way. In deep reinforcement learning, for example, directly specifying a reward as a function of a high-dimensional observation is challenging. Instead, we present an interface for specifying tasks interactively using demonstrations. Our approach defines a set of increasingly complex policies. The interface allows the user to switch between these policies at fixed intervals to generate demonstrations of novel, more complex, tasks. We train new policies based on these demonstrations and repeat the process. We present a case study of our approach in the Lunar Lander domain, and show that this simple approach can quickly learn a successful landing policy and outperforms an existing comparison-based deep RL method.",0
"Tasks in artificial intelligence are often defined using a reward function, but this can be difficult for complex tasks like those in deep reinforcement learning. To address this, we developed an interface that allows users to specify tasks through interactive demonstrations. Our approach involves creating a series of progressively more complex policies that users can switch between to generate demonstrations of new tasks. We then train new policies based on these demonstrations and repeat the process. We tested our approach in the Lunar Lander domain and found that it quickly learned a successful landing policy, outperforming existing deep RL methods that rely on comparisons.",1
"We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images.Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, wetrain a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching (DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.",0
"The problem of fine-grained classification on an edge camera device with limited power is considered in this study. To conserve power and minimize communication bits, the edge device must interact with the cloud sparingly, with the cloud returning a classification label upon receiving the edge inputs. To address fine-grained classification, the researchers propose a foveated field-of-view with sequential fixation to model cloud-edge interactions. A novel deep reinforcement learning-based foveation model called DRIFT is introduced, which generates and recognizes mixed-acuity images. DRIFT's training only requires image-level category labels and encourages fixations to contain task-relevant information while maintaining data efficiency. The researchers train a foveation actor network using a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching (DDPGC3) algorithm and shape the reward to provide informative feedback after each fixation to guide RL training better. DRIFT's effectiveness is demonstrated by evaluating its performance on five fine-grained classification benchmark datasets, showing that it achieves state-of-the-art performance with over 3X reduction in transmitted pixels.",1
"Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains.",0
"The capacity for automatic data abstraction is crucial for both evaluating machine intelligence and facilitating summarization software. In the former, the question is whether a machine can comprehend enough about the input data to create a concise and meaningful abstraction. In the latter, this ability is utilized to save space or time by summarizing the core of the input data. This study investigates a general framework for learning to abstract sequential data using reinforcement learning in a goal-oriented manner. The unique ability to establish different abstraction goals enables the preservation of various aspects of the input data according to the intended purpose of the abstraction. Our reinforcement learning objective does not rely on human-defined examples of ideal abstraction. Crucially, our model processes the input sequence comprehensively without being restricted by the original input order. Our framework is also versatile, as we demonstrate its effectiveness in sketch, video, and text data domains with promising outcomes.",1
"Continuous reinforcement learning such as DDPG and A3C are widely used in robot control and autonomous driving. However, both methods have theoretical weaknesses. While DDPG cannot control noises in the control process, A3C does not satisfy the continuity conditions under the Gaussian policy. To address these concerns, we propose a new continues reinforcement learning method based on stochastic differential equations and we call it Incremental Reinforcement Learning (IRL). This method not only guarantees the continuity of actions within any time interval, but controls the variance of actions in the training process. In addition, our method does not assume Markov control in agents' action control and allows agents to predict scene changes for action selection. With our method, agents no longer passively adapt to the environment. Instead, they positively interact with the environment for maximum rewards.",0
"Robot control and autonomous driving commonly rely on continuous reinforcement learning, specifically DDPG and A3C. However, both approaches have theoretical limitations. DDPG struggles to manage noise during control, while A3C fails to meet continuity requirements with the Gaussian policy. To overcome these drawbacks, we present our new incremental reinforcement learning method, known as IRL. This approach employs stochastic differential equations to ensure action continuity and control action variance in training. Notably, IRL doesn't assume Markov control and enables agents to anticipate changes in the environment for optimal action selection. Rather than passively adapting, agents actively engage with the environment to maximize rewards.",1
"Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where function approximation must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed.   This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a ""simulator"" or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI)---a classical algorithm frequently studied in the linear setting---achieves $\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret, where $d$ is the ambient dimension of feature space, $H$ is the length of each episode, and $T$ is the total number of steps. Importantly, such regret is independent of the number of states and actions.",0
"The application of Modern Reinforcement Learning (RL) to practical problems with a large number of states often requires function approximation to approximate the value function or policy. However, this introduces challenges in computational and statistical efficiency, particularly in balancing exploration/exploitation. A key question remains about how to design efficient RL algorithms that incorporate function approximation. This paper presents a provable RL algorithm that achieves polynomial runtime and sample complexity in a linear setting without additional assumptions or a simulator. Specifically, an optimistic modification of Least-Squares Value Iteration (LSVI) achieves regret of $\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$, which is not dependent on the number of states or actions.",1
"Sequences play an important role in many applications and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This paper puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) techniques. AlphaSeq treats the sequence discovery problem as an episodic symbol-filling game, in which a player fills symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely-filled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov Decision Process (MDP), and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier CDMA systems. 2) AlphaSeq discovers new sequences that triple the signal-to-interference ratio -- benchmarked against the well-known Legendre sequence -- of a mismatched filter estimator in pulse compression radar systems.",0
"Sequences have a significant role in various applications and systems, and finding sequences with specific properties has been an intriguing pursuit. This research introduces a novel approach, AlphaSeq, to algorithmically discover desired sequences using deep reinforcement learning techniques. AlphaSeq treats the sequence discovery task as a symbol-filling game where the player sequentially fills vacant positions in a sequence set during an episode. At the end of each episode, a reward is given based on the sequence set's desirability. AlphaSeq models the game as a Markov Decision Process (MDP) and employs the DRL framework of AlphaGo to solve the MDP. As AlphaSeq progresses from a novice to an expert through many game-playing episodes, the discovered sequences improve progressively. AlphaSeq is particularly useful for problems with complex objectives that are not amenable to mathematical analysis. The capabilities of AlphaSeq are demonstrated in two applications: 1) AlphaSeq successfully rediscovered a set of ideal complementary codes that can zero-force all potential interferences in multi-carrier CDMA systems. 2) AlphaSeq discovered new sequences that triple the signal-to-interference ratio of a mismatched filter estimator in pulse compression radar systems, benchmarked against the well-known Legendre sequence.",1
"Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.",0
"Sparse rewards in the real world pose a challenge for most reinforcement learning algorithms. One solution is to allow the agent to generate its own rewards, resulting in more frequent and effective learning. Inspired by animals' curious behavior, our proposed method rewards novel observations with a bonus, which is added to the real task reward for combined learning. Our approach uses episodic memory to determine the novelty bonus by comparing current observations to those in memory based on the number of environment steps required to reach them. This overcomes prior issues of instant gratification and exploitation. We tested our method in 3D environments and found it outperformed the state-of-the-art curiosity method ICM in navigational tasks and allowed an ant to learn locomotion solely through first-person-view curiosity.",1
"We investigate a classification problem using multiple mobile agents capable of collecting (partial) pose-dependent observations of an unknown environment. The objective is to classify an image over a finite time horizon. We propose a network architecture on how agents should form a local belief, take local actions, and extract relevant features from their raw partial observations. Agents are allowed to exchange information with their neighboring agents to update their own beliefs. It is shown how reinforcement learning techniques can be utilized to achieve decentralized implementation of the classification problem by running a decentralized consensus protocol. Our experimental results on the MNIST handwritten digit dataset demonstrates the effectiveness of our proposed framework.",0
"Our study focuses on a classification challenge that involves using multiple mobile agents that can gather (partial) pose-dependent data about an unfamiliar environment. The goal is to classify an image within a limited timeframe. We present a network design that outlines how agents should create a local belief, carry out local actions, and extract pertinent features from their raw, incomplete observations. Agents can share information with nearby agents to update their own beliefs. We demonstrate how reinforcement learning approaches can be leveraged to achieve decentralized execution of the classification task using a decentralized consensus protocol. Our test results using the MNIST handwritten digit dataset validate the efficacy of our proposed framework.",1
"This paper considers the problem of image set-based face verification and identification. Unlike traditional single sample (an image or a video) setting, this situation assumes the availability of a set of heterogeneous collection of orderless images and videos. The samples can be taken at different check points, different identity documents $etc$. The importance of each image is usually considered either equal or based on a quality assessment of that image independent of other images and/or videos in that image set. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in a latent space. Specifically, we first propose a dependency-aware attention control (DAC) network, which uses actor-critic reinforcement learning for attention decision of each image to exploit the correlations among the unordered images. An off-policy experience replay is introduced to speed up the learning process. Moreover, the DAC is combined with a temporal model for videos using divide and conquer strategies. We also introduce a pose-guided representation (PGR) scheme that can further boost the performance at extreme poses. We propose a parameter-free PGR without the need for training as well as a novel metric learning-based PGR for pose alignment without the need for pose detection in testing stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets demonstrate that our method outperforms many state-of-art approaches on the set-based as well as video-based face recognition databases.",0
"In this paper, we examine the challenge of verifying and identifying faces based on sets of images. This differs from the traditional approach of using a single image or video, as it involves a collection of unordered images and videos that may have been taken at different times and places. Each image is typically given equal importance or evaluated independently of the others. The difficulty lies in modeling the relationship between these images. To address this issue, we propose using a Markov Decision Process in a latent space. Our approach involves a dependency-aware attention control network that uses reinforcement learning to determine the relevance of each image in the set. We also introduce a pose-guided representation scheme that improves performance in extreme poses. Our method outperforms many state-of-the-art approaches on various face recognition databases.",1
"We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications.",0
"Our proposal suggests using reinforcement learning (RL) to teach a proportional-derivative (PD) control based policy for the purpose of estimating and predicting 3D human pose from egocentric videos. This method can learn directly from unsegmented egocentric videos and motion capture data, which includes a variety of complex human motions, such as bending, hopping, crouching, and motion transitions. We also introduce a video-conditioned recurrent control technique to predict future motions of arbitrary length that are stable and physically valid. Moreover, to ensure our method runs as a single pass algorithm over the video data, we include a fail-safe mechanism based on the value function. Our experiments on controlled and in-the-wild data demonstrate that our approach surpasses previous methods in quantitative metrics and visual quality of the motions. Additionally, our method can transfer directly to real-world scenarios and run at 30 FPS, making it suitable for real-time applications.",1
"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",0
"To train deep reinforcement learning agents on complex behaviors in 3D virtual environments that have high degrees of aliasing, significant computational resources are needed. Minecraft is an example of such an environment. It is believed that interactive machine learning (IML) can help reduce the agent's susceptibility to aliasing by involving human teachers in the training process through demonstrations, critique, or action advice. However, the practicality of IML is limited by the number of human interactions required, and a balance must be struck between human teacher effort and agent performance. This study investigates the effectiveness of two reinforcement learning algorithms, Feedback Arbitration and Newtonian Action Advice, in reducing aliasing under visual aliasing conditions. The study also examines the potential cognitive load of different types of human action advice by varying their accuracy and frequency, and assesses the training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing.",1
"Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars.",0
"The study of Video Recognition has garnered significant attention and progress has been made in this field. Enhancing the accuracy and effectiveness of recognition can be achieved by implementing an appropriate frame sampling strategy. However, conventional solutions typically rely on pre-designed frame sampling strategies for recognition, which may lead to suboptimal performance, particularly in untrimmed videos due to the varying level of frame saliency. Our focus is on enhancing untrimmed video classification by implementing a learning-based frame sampling approach. We have formulated the frame sampling process as multiple parallel Markov decision processes, with the aim of gradually adjusting an initial sampling to select a frame/clip. We have developed a multi-agent reinforcement learning (MARL) framework to address these issues. Our MARL framework comprises a novel RNN-based context-aware observation network that models context information among nearby agents and historical states of a specific agent, a policy network that generates the probability distribution over a predefined action space at each step, and a classification network for reward calculation and final recognition. Our extensive experimental results demonstrate that our MARL-based approach outperforms conventional strategies using various 2D and 3D baseline methods. Our single RGB model achieves comparable performance to ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars datasets.",1
"Future robots should follow human social norms in order to be useful and accepted in human society. In this paper, we leverage already existing social knowledge in human societies by capturing it in our framework through the notion of social norms. We show how norms can be used to guide a reinforcement learning agent towards achieving normative behavior and apply the same set of norms over different domains. Thus, we are able to: (1) provide a way to intuitively encode social knowledge (through norms); (2) guide learning towards normative behaviors (through an automatic norm reward system); and (3) achieve a transfer of learning by abstracting policies; Finally, (4) the method is not dependent on a particular RL algorithm. We show how our approach can be seen as a means to achieve abstract representation and learn procedural knowledge based on the declarative semantics of norms and discuss possible implications of this in some areas of cognitive science.",0
"To ensure usefulness and acceptance in human society, future robots ought to conform to human social norms. Our framework captures this social knowledge by utilizing the concept of social norms, which can guide a reinforcement learning agent towards normative behavior and can be applied across different domains. Our approach allows for intuitive encoding of social knowledge, automatic norm reward systems to guide learning, and transfer of learning through policy abstraction. Additionally, our method is not reliant on a specific RL algorithm. We propose that our approach can enable abstract representation and procedural knowledge acquisition based on declarative semantics of norms, which could have implications in cognitive science.",1
"Language systems have been of great interest to the research community and have recently reached the mass market through various assistant platforms on the web. Reinforcement Learning methods that optimize dialogue policies have seen successes in past years and have recently been extended into methods that personalize the dialogue, e.g. take the personal context of users into account. These works, however, are limited to personalization to a single user with whom they require multiple interactions and do not generalize the usage of context across users. This work introduces a problem where a generalized usage of context is relevant and proposes two Reinforcement Learning (RL)-based approaches to this problem. The first approach uses a single learner and extends the traditional POMDP formulation of dialogue state with features that describe the user context. The second approach segments users by context and then employs a learner per context. We compare these approaches in a benchmark of existing non-RL and RL-based methods in three established and one novel application domain of financial product recommendation. We compare the influence of context and training experiences on performance and find that learning approaches generally outperform a handcrafted gold standard.",0
"The research community has shown much interest in language systems, which have become widely available through various web-based assistant platforms. Reinforcement Learning methods have been successful in optimizing dialogue policies, and more recently, in personalizing dialogue based on user context. However, current methods are limited to personalizing for a single user and do not generalize context usage across multiple users. This study proposes two Reinforcement Learning-based approaches to address this issue, including a single learner with user context features and multiple learners segmented by context. The study compares these approaches with existing non-RL and RL-based methods in financial product recommendation. The results show that learning approaches generally outperform handcrafted methods, and context and training experiences significantly influence performance.",1
"Control policies, trained using the Deep Reinforcement Learning, have been recently shown to be vulnerable to adversarial attacks introducing even very small perturbations to the policy input. The attacks proposed so far have been designed using heuristics, and build on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning techniques. Through numerical experiments, we demonstrate the efficiency of our attacks compared to existing attacks (usually based on Gradient methods). We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (this explains why Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties and the attacker can be modeled as a Partially Observable Markov Decision Process. We actually demonstrate that using Reinforcement Learning techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies.",0
"Recently, it has been discovered that control policies developed through Deep Reinforcement Learning are susceptible to adversarial attacks. Even the slightest modifications to the policy input can result in these attacks. Current adversarial attack techniques rely on heuristics and are based on existing methods used to deceive classifiers in supervised learning. In contrast, this study focuses on developing optimal attacks based on a well-defined objective, such as minimizing the average reward of the main agent. If the attacker has access to the policy, system dynamics, and rewards, referred to as a white-box attack, optimal attacks can be devised by solving a Markov Decision Process. However, for black-box attacks, where neither the policy nor the system is known, Reinforcement Learning techniques can be used to train optimal attacks. Through numerical experiments, it is shown that these attacks are more efficient than existing attacks, which are typically based on Gradient methods. The potential impact of attacks is also quantified, and it is found that the smoothness of the policy under attack is a key factor in its vulnerability. Additionally, it is shown that the system uncertainties and the attacker can be modeled as a Partially Observable Markov Decision Process from the main agent's perspective. Finally, it is demonstrated that using Reinforcement Learning techniques tailored to POMDP, such as Recurrent Neural Networks, leads to more resilient policies.",1
"We consider the problem of learning to behave optimally in a Markov Decision Process when a reward function is not specified, but instead we have access to a set of demonstrators of varying performance. We assume the demonstrators are classified into one of k ranks, and use ideas from ordinal regression to find a reward function that maximizes the margin between the different ranks. This approach is based on the idea that agents should not only learn how to behave from experts, but also how not to behave from non-experts. We show there are MDPs where important differences in the reward function would be hidden from existing algorithms by the behaviour of the expert. Our method is particularly useful for problems where we have access to a large set of agent behaviours with varying degrees of expertise (such as through GPS or cellphones). We highlight the differences between our approach and existing methods using a simple grid domain and demonstrate its efficacy on determining passenger-finding strategies for taxi drivers, using a large dataset of GPS trajectories.",0
"The problem we address is how to learn optimal behavior in a Markov Decision Process without a specified reward function, but with access to a group of demonstrators who exhibit different levels of performance. We assume that these demonstrators can be classified into k ranks and we use ideas from ordinal regression to determine a reward function that maximizes the margin between these ranks. Our approach is based on the concept that agents must learn not only from experts but also from non-experts how not to behave. We prove that some MDPs have important differences in the reward function which are hidden by expert behavior and that our method is especially useful for problems where there exists a large dataset of agent behaviors with varying levels of expertise, such as through GPS or cellphones. We compare our approach to existing methods using a simple grid domain and demonstrate its effectiveness in determining passenger-finding strategies for taxi drivers, using a large dataset of GPS trajectories.",1
"We describe an application of Wasserstein distance to Reinforcement Learning. The Wasserstein distance in question is between the distribution of mappings of trajectories of a policy into some metric space, and some other fixed distribution (which may, for example, come from another policy). Different policies induce different distributions, so given an underlying metric, the Wasserstein distance quantifies how different policies are. This can be used to learn multiple polices which are different in terms of such Wasserstein distances by using a Wasserstein regulariser. Changing the sign of the regularisation parameter, one can learn a policy for which its trajectory mapping distribution is attracted to a given fixed distribution.",0
"In this article, we explore the application of the Wasserstein distance to Reinforcement Learning. Specifically, we examine the distance between the distribution of trajectory mappings of a policy into a metric space and another fixed distribution (which may be derived from another policy). Since different policies create distinct distributions, the Wasserstein distance provides a measure of policy dissimilarity based on a given metric. To learn multiple policies with varying Wasserstein distances, we employ a Wasserstein regularizer. By adjusting the sign of the regularizer, it's possible to train a policy that attracts its trajectory mapping distribution to a specific fixed distribution.",1
"Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation.",0
"Reward mis-specification can lead to undesired behaviors in reinforcement learning agents, which is particularly challenging in multi-agent scenarios. Inverse reinforcement learning can automatically acquire suitable reward functions from expert demonstrations, but its extension to multi-agent settings is difficult due to the more complex notions of rational behaviors. To address this, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning that is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. Our algorithm is based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In experiments, we show that MA-AIRL can recover highly correlated reward functions with ground truth ones and outperforms prior methods in terms of policy imitation.",1
"In several reinforcement learning (RL) scenarios, mainly in security settings, there may be adversaries trying to interfere with the reward generating process. In this paper, we introduce Threatened Markov Decision Processes (TMDPs), which provide a framework to support a decision maker against a potential adversary in RL. Furthermore, we propose a level-$k$ thinking scheme resulting in a new learning framework to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries while the agent learns.",0
"The reward generating process in various reinforcement learning (RL) situations, particularly in security contexts, may face interference from adversaries. This paper presents Threatened Markov Decision Processes (TMDPs) as a framework for assisting decision makers in RL against potential adversaries. Additionally, a level-$k$ thinking scheme is suggested, which leads to a novel learning framework for handling TMDPs. Following the introduction of this framework and the derivation of theoretical outcomes, extensive experiments are conducted to provide relevant empirical evidence, demonstrating the advantages of considering adversaries during the agent's learning process.",1
"In many optimization problems in wireless communications, the expressions of objective function or constraints are hard or even impossible to derive, which makes the solutions difficult to find. In this paper, we propose a model-free learning framework to solve constrained optimization problems without the supervision of the optimal solution. Neural networks are used respectively for parameterizing the function to be optimized, parameterizing the Lagrange multiplier associated with instantaneous constraints, and approximating the unknown objective function or constraints. We provide learning algorithms to train all the neural networks simultaneously, and reveal the connections of the proposed framework with reinforcement learning. Numerical and simulation results validate the proposed framework and demonstrate the efficiency of model-free learning by taking power control problem as an example.",0
"The task of finding solutions to optimization problems in wireless communications can be challenging due to the difficulty or impossibility of deriving expressions for objective functions or constraints. This paper introduces a learning framework that does not require supervision of the optimal solution to solve constrained optimization problems. Neural networks are used to parameterize the function to be optimized, the Lagrange multiplier associated with instantaneous constraints, and to approximate the unknown objective function or constraints. The paper includes learning algorithms to train all the neural networks simultaneously and demonstrates the framework's connection to reinforcement learning. The proposed framework is validated through numerical and simulation results that showcase the effectiveness of model-free learning, with the power control problem serving as an example.",1
"Modern control theories such as systems engineering approaches try to solve nonlinear system problems by revelation of causal relationship or co-relationship among the components; most of those approaches focus on control of sophisticatedly modeled white-boxed systems. We suggest an application of actor-critic reinforcement learning approach to control a nonlinear, complex and black-boxed system. We demonstrated this approach on artificial green-house environment simulator all of whose control inputs have several side effects so human cannot figure out how to control this system easily. Our approach succeeded to maintain the circumstance at least 20 times longer than PID and Deep Q Learning.",0
"Contemporary control theories, like systems engineering techniques, aim to resolve nonlinear system issues by revealing causal or co-relationships among components. However, these methods primarily focus on regulating sophisticatedly modeled white-boxed systems. In contrast, we propose implementing an actor-critic reinforcement learning approach to manage a nonlinear, highly intricate, and black-boxed system. To demonstrate the efficacy of this approach, we employed it on an artificial greenhouse environment simulator with control inputs that have multiple side effects, making it challenging for humans to control. Our technique proved successful in sustaining the environment for a minimum of 20 times longer than both PID and Deep Q Learning methods.",1
"The sample inefficiency of standard deep reinforcement learning methods precludes their application to many real-world problems. Methods which leverage human demonstrations require fewer samples but have been researched less. As demonstrated in the computer vision and natural language processing communities, large-scale datasets have the capacity to facilitate research by serving as an experimental and benchmarking platform for new methods. However, existing datasets compatible with reinforcement learning simulators do not have sufficient scale, structure, and quality to enable the further development and evaluation of methods focused on using human examples. Therefore, we introduce a comprehensive, large-scale, simulator-paired dataset of human demonstrations: MineRL. The dataset consists of over 60 million automatically annotated state-action pairs across a variety of related tasks in Minecraft, a dynamic, 3D, open-world environment. We present a novel data collection scheme which allows for the ongoing introduction of new tasks and the gathering of complete state information suitable for a variety of methods. We demonstrate the hierarchality, diversity, and scale of the MineRL dataset. Further, we show the difficulty of the Minecraft domain along with the potential of MineRL in developing techniques to solve key research challenges within it.",0
"Standard deep reinforcement learning methods are not suitable for many real-world problems due to their sample inefficiency. Although methods that utilize human demonstrations require fewer samples, they have not been extensively studied. To facilitate research and serve as a benchmarking platform for new methods, large-scale datasets have been used in computer vision and natural language processing communities. However, existing datasets compatible with reinforcement learning simulators lack sufficient scale, structure, and quality to develop and evaluate methods that focus on human examples. Therefore, we introduce MineRL, a comprehensive, large-scale dataset of human demonstrations in Minecraft. With over 60 million automatically annotated state-action pairs across various tasks, MineRL offers a novel data collection scheme that allows for ongoing task introduction and complete state information gathering. We demonstrate the hierarchical, diverse, and scalable nature of MineRL and showcase its potential in developing techniques to solve key research challenges in the challenging Minecraft domain.",1
"The automatic generation of radiology reports given medical radiographs has significant potential to operationally and improve clinical patient care. A number of prior works have focused on this problem, employing advanced methods from computer vision and natural language generation to produce readable reports. However, these works often fail to account for the particular nuances of the radiology domain, and, in particular, the critical importance of clinical accuracy in the resulting generated reports. In this work, we present a domain-aware automatic chest X-ray radiology report generation system which first predicts what topics will be discussed in the report, then conditionally generates sentences corresponding to these topics. The resulting system is fine-tuned using reinforcement learning, considering both readability and clinical accuracy, as assessed by the proposed Clinically Coherent Reward. We verify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that our model offers marked improvements on both language generation metrics and CheXpert assessed accuracy over a variety of competitive baselines.",0
"The automatic creation of radiology reports from medical radiographs has the potential to significantly improve patient care and operational efficiency. While previous research has utilized advanced techniques from computer vision and natural language generation to generate readable reports, many fail to consider the unique nuances of the radiology field, particularly the importance of clinical accuracy in the final product. In this study, we present a domain-specific system for generating chest X-ray radiology reports that predicts report topics and generates corresponding sentences. Our system is fine-tuned using reinforcement learning, taking into account both readability and clinical accuracy as evaluated by the Clinically Coherent Reward. We evaluated our model on two datasets, Open-I and MIMIC-CXR, and found that it outperformed other models in both language generation and CheXpert accuracy.",1
"How to best explore in domains with sparse, delayed, and deceptive rewards is an important open problem for reinforcement learning (RL). This paper considers one such domain, the recently-proposed multi-agent benchmark of Pommerman. This domain is very challenging for RL --- past work has shown that model-free RL algorithms fail to achieve significant learning without artificially reducing the environment's complexity. In this paper, we illuminate reasons behind this failure by providing a thorough analysis on the hardness of random exploration in Pommerman. While model-free random exploration is typically futile, we develop a model-based automatic reasoning module that can be used for safer exploration by pruning actions that will surely lead the agent to death. We empirically demonstrate that this module can significantly improve learning.",0
"Reinforcement learning (RL) faces a significant challenge when exploring domains that offer sparse, delayed, and deceptive rewards. To address this issue, this paper focuses on the Pommerman multi-agent benchmark, which has proven to be particularly difficult for RL, with previous attempts at model-free RL failing to achieve significant learning without simplifying the environment. This paper explores why this failure occurs and identifies the challenges of random exploration in Pommerman. The authors propose a model-based automatic reasoning module that can improve the safety and effectiveness of exploration by pruning actions that are likely to lead to the agent's death. Empirical results demonstrate the effectiveness of this module in improving learning.",1
"In recent years, \emph{search story}, a combined display with other organic channels, has become a major source of user traffic on platforms such as e-commerce search platforms, news feed platforms and web and image search platforms. The recommended search story guides a user to identify her own preference and personal intent, which subsequently influences the user's real-time and long-term search behavior. %With such an increased importance of search stories, As search stories become increasingly important, in this work, we study the problem of personalized search story recommendation within a search engine, which aims to suggest a search story relevant to both a search keyword and an individual user's interest. To address the challenge of modeling both immediate and future values of recommended search stories (i.e., cross-channel effect), for which conventional supervised learning framework is not applicable, we resort to a Markov decision process and propose a deep reinforcement learning architecture trained by both imitation learning and reinforcement learning. We empirically demonstrate the effectiveness of our proposed approach through extensive experiments on real-world data sets from JD.com.",0
"Over the past few years, search story has emerged as a significant source of user traffic on various platforms such as e-commerce, news feeds, and web and image search platforms. The recommended search story helps users identify their preferences and intentions, thereby influencing their real-time and long-term search behavior. Given the growing importance of search stories, this study focuses on personalized search story recommendation within a search engine. The aim is to suggest a search story that aligns with both a user's interest and their search keyword. To overcome the challenge of modeling immediate and future values of recommended search stories, a Markov decision process and deep reinforcement learning architecture trained by imitation and reinforcement learning are proposed. Extensive experiments on real-world data sets from JD.com demonstrate the effectiveness of this approach.",1
"Deep reinforcement learning is prone to overfitting, and traditional benchmarks such as Atari 2600 benchmark can exacerbate this problem. The Obstacle Tower Challenge addresses this by using randomized environments and separate seeds for training, validation, and test runs. This paper examines various improvements and best practices to the PPO algorithm using the Obstacle Tower Challenge to empirically study their impact with regards to generalization. Our experiments show that the combination provides state-of-the-art performance on the Obstacle Tower Challenge.",0
"Overfitting is a common issue in deep reinforcement learning, which can be worsened by conventional benchmarks like the Atari 2600 benchmark. To tackle this problem, the Obstacle Tower Challenge utilizes randomized environments and distinct seeds for training, validation, and testing. This study explores different enhancements and optimal techniques for the PPO algorithm by analyzing their effectiveness in generalization through the Obstacle Tower Challenge. Our findings indicate that this approach yields exceptional performance on the Obstacle Tower Challenge.",1
"Deep reinforcement learning has achieved great successes in recent years, however, one main challenge is the sample inefficiency. In this paper, we focus on how to use action guidance by means of a non-expert demonstrator to improve sample efficiency in a domain with sparse, delayed, and possibly deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. We propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with a small number rollouts, can be integrated within asynchronous distributed deep reinforcement learning methods. Compared to a vanilla deep RL algorithm, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.",0
"While deep reinforcement learning has seen remarkable achievements lately, it still faces the issue of sample inefficiency. This paper focuses on enhancing sample efficiency in a domain that has sparse, delayed, and potentially misleading rewards - the Pommerman multi-agent benchmark - by utilizing action guidance from a non-expert demonstrator. We present a novel framework that incorporates a non-expert simulated demonstrator, such as Monte Carlo tree search with a limited number of rollouts, into asynchronous distributed deep reinforcement learning techniques. Our proposed approaches outperform vanilla deep RL algorithms in terms of faster learning and better policy convergence on a two-player mini version of the Pommerman game.",1
"In many real-world decision making problems, reaching an optimal decision requires taking into account a variable number of objects around the agent. Autonomous driving is a domain in which this is especially relevant, since the number of cars surrounding the agent varies considerably over time and affects the optimal action to be taken. Classical methods that process object lists can deal with this requirement. However, to take advantage of recent high-performing methods based on deep reinforcement learning in modular pipelines, special architectures are necessary. For these, a number of options exist, but a thorough comparison of the different possibilities is missing. In this paper, we elaborate limitations of fully-connected neural networks and other established approaches like convolutional and recurrent neural networks in the context of reinforcement learning problems that have to deal with variable sized inputs. We employ the structure of Deep Sets in off-policy reinforcement learning for high-level decision making, highlight their capabilities to alleviate these limitations, and show that Deep Sets not only yield the best overall performance but also offer better generalization to unseen situations than the other approaches.",0
"When making decisions in the real world, it's often important to consider multiple objects around the agent. This is especially true in autonomous driving, where the number of surrounding cars can change and impact the best course of action. While traditional methods can handle object lists, newer deep reinforcement learning methods require special architectures. There are several options available, but there hasn't been a comprehensive comparison. This paper examines the limitations of established neural networks in reinforcement learning problems with variable inputs. We use Deep Sets in off-policy reinforcement learning for high-level decision making, highlighting their ability to overcome these limitations. Our results show that Deep Sets perform the best overall and are better at generalizing to new situations than other approaches.",1
"Deep reinforcement learning has achieved great successes in recent years, but there are still open challenges, such as convergence to locally optimal policies and sample inefficiency. In this paper, we contribute a novel self-supervised auxiliary task, i.e., Terminal Prediction (TP), estimating temporal closeness to terminal states for episodic tasks. The intuition is to help representation learning by letting the agent predict how close it is to a terminal state, while learning its control policy. Although TP could be integrated with multiple algorithms, this paper focuses on Asynchronous Advantage Actor-Critic (A3C) and demonstrating the advantages of A3C-TP. Our extensive evaluation includes: a set of Atari games, the BipedalWalker domain, and a mini version of the recently proposed multi-agent Pommerman game. Our results on Atari games and the BipedalWalker domain suggest that A3C-TP outperforms standard A3C in most of the tested domains and in others it has similar performance. In Pommerman, our proposed method provides significant improvement both in learning efficiency and converging to better policies against different opponents.",0
"Despite the recent successes of deep reinforcement learning, there are still challenges to overcome, including issues with policies converging to locally optimal solutions and sample inefficiency. This paper introduces a new self-supervised auxiliary task called Terminal Prediction (TP), which estimates the proximity to terminal states for episodic tasks. The aim is to improve representation learning by allowing the agent to predict how close it is to a terminal state while learning its control policy. Although TP can be used with multiple algorithms, this paper focuses on demonstrating the advantages of A3C-TP, which is an integration of TP with Asynchronous Advantage Actor-Critic (A3C). An extensive evaluation was carried out on a variety of domains, including Atari games, the BipedalWalker domain, and a mini version of the multi-agent Pommerman game. The results show that A3C-TP outperforms standard A3C in most of the tested domains, and in others, it has similar performance. In Pommerman, the proposed method significantly improves learning efficiency and helps to converge to better policies against different opponents.",1
"Decision support systems (e.g., for ecological conservation) and autonomous systems (e.g., adaptive controllers in smart cities) start to be deployed in real applications. Although their operations often impact many users or stakeholders, no fairness consideration is generally taken into account in their design, which could lead to completely unfair outcomes for some users or stakeholders. To tackle this issue, we advocate for the use of social welfare functions that encode fairness and present this general novel problem in the context of (deep) reinforcement learning, although it could possibly be extended to other machine learning tasks.",0
"Real-world applications are beginning to utilize decision support systems, such as those employed in ecological conservation, and autonomous systems, such as adaptive controllers in smart cities. Unfortunately, these systems often affect numerous users or stakeholders, yet fairness is not typically factored into their design. This oversight could result in unjust outcomes for certain individuals or groups. To address this concern, we propose implementing social welfare functions that incorporate fairness considerations. Specifically, we explore this issue in the realm of (deep) reinforcement learning, though it may have potential applications in other machine learning tasks.",1
"Deep reinforcement learning (DRL) has achieved great success in various applications. However, recent studies show that machine learning models are vulnerable to adversarial attacks. DRL models have been attacked by adding perturbations to observations. While such observation based attack is only one aspect of potential attacks on DRL, other forms of attacks which are more practical require further analysis, such as manipulating environment dynamics. Therefore, we propose to understand the vulnerabilities of DRL from various perspectives and provide a thorough taxonomy of potential attacks. We conduct the first set of experiments on the unexplored parts within the taxonomy. In addition to current observation based attacks against DRL, we propose the first targeted attacks based on action space and environment dynamics. We also introduce the online sequential attacks based on temporal consistency information among frames. To better estimate gradient in black-box setting, we propose a sampling strategy and theoretically prove its efficiency and estimation error bound. We conduct extensive experiments to compare the effectiveness of different attacks with several baselines in various environments, including game playing, robotics control, and autonomous driving.",0
"DRL has been successful in many applications, but recent studies have revealed that adversarial attacks can harm machine learning models. DRL models have been attacked through observation-based perturbations, but other forms of attacks that are more practical need further analysis, such as altering environment dynamics. We propose to thoroughly understand DRL's vulnerabilities and provide a comprehensive taxonomy of potential attacks. Our experiments cover unexplored areas within the taxonomy, including targeted attacks based on action space and environment dynamics, as well as online sequential attacks based on temporal consistency. We also present a sampling strategy that can better estimate gradient in a black-box setting. Our extensive experiments compare different attacks with baselines in various environments, such as game playing, robotics control, and autonomous driving, to assess their effectiveness.",1
"Robust Markov Decision Processes (RMDPs) intend to ensure robustness with respect to changing or adversarial system behavior. In this framework, transitions are modeled as arbitrary elements of a known and properly structured uncertainty set and a robust optimal policy can be derived under the worst-case scenario. In this study, we address the issue of learning in RMDPs using a Bayesian approach. We introduce the Uncertainty Robust Bellman Equation (URBE) which encourages safe exploration for adapting the uncertainty set to new observations while preserving robustness. We propose a URBE-based algorithm, DQN-URBE, that scales this method to higher dimensional domains. Our experiments show that the derived URBE-based strategy leads to a better trade-off between less conservative solutions and robustness in the presence of model misspecification. In addition, we show that the DQN-URBE algorithm can adapt significantly faster to changing dynamics online compared to existing robust techniques with fixed uncertainty sets.",0
"The aim of Robust Markov Decision Processes (RMDPs) is to maintain stability when faced with changes or hostile behavior in the system. The model represents transitions as arbitrary elements of an established and structured uncertainty set, allowing for the derivation of a robust optimal policy for worst-case scenarios. This research explores the concept of learning in RMDPs, utilizing a Bayesian methodology. The Uncertainty Robust Bellman Equation (URBE) is introduced, encouraging safe exploration for adapting the uncertainty set to new observations while maintaining robustness. A URBE-based algorithm, DQN-URBE, is proposed, which can be scaled to higher dimensional domains. The experimental results suggest that the derived URBE-based strategy offers a better balance between less conservative solutions and robustness in the presence of model misspecification. Furthermore, the DQN-URBE algorithm has shown to adapt much faster to changing dynamics online, in comparison to existing robust techniques with fixed uncertainty sets.",1
"The detection of anatomical landmarks is a vital step for medical image analysis and applications for diagnosis, interpretation and guidance. Manual annotation of landmarks is a tedious process that requires domain-specific expertise and introduces inter-observer variability. This paper proposes a new detection approach for multiple landmarks based on multi-agent reinforcement learning. Our hypothesis is that the position of all anatomical landmarks is interdependent and non-random within the human anatomy, thus finding one landmark can help to deduce the location of others. Using a Deep Q-Network (DQN) architecture we construct an environment and agent with implicit inter-communication such that we can accommodate K agents acting and learning simultaneously, while they attempt to detect K different landmarks. During training the agents collaborate by sharing their accumulated knowledge for a collective gain. We compare our approach with state-of-the-art architectures and achieve significantly better accuracy by reducing the detection error by 50%, while requiring fewer computational resources and time to train compared to the naive approach of training K agents separately.",0
"The identification of anatomical landmarks plays a crucial role in medical image analysis and applications such as diagnosis, interpretation, and guidance. However, manually tagging landmarks is a laborious task that demands specialized knowledge and introduces discrepancies between different observers. This study introduces a new approach for detecting multiple landmarks by utilizing multi-agent reinforcement learning. The hypothesis is that the position of all anatomical landmarks is interdependent and non-random within the human body, meaning that locating one landmark can assist in identifying the location of others. To achieve this, we employ a Deep Q-Network (DQN) architecture to create an environment and agent capable of accommodating K agents working simultaneously to detect K different landmarks. During training, the agents collaborate and share their knowledge for collective gain. Compared to the current state-of-the-art approaches, our method yields significantly better accuracy by reducing the detection error by 50% and requires fewer computational resources and training time than the conventional approach of training K agents separately.",1
"Owe to the recent advancements in Artificial Intelligence especially deep learning, many data-driven decision support systems have been implemented to facilitate medical doctors in delivering personalized care. We focus on the deep reinforcement learning (DRL) models in this paper. DRL models have demonstrated human-level or even superior performance in the tasks of computer vision and game playings, such as Go and Atari game. However, the adoption of deep reinforcement learning techniques in clinical decision optimization is still rare. We present the first survey that summarizes reinforcement learning algorithms with Deep Neural Networks (DNN) on clinical decision support. We also discuss some case studies, where different DRL algorithms were applied to address various clinical challenges. We further compare and contrast the advantages and limitations of various DRL algorithms and present a preliminary guide on how to choose the appropriate DRL algorithm for particular clinical applications.",0
"Recent developments in Artificial Intelligence, particularly in deep learning, have led to the implementation of data-driven decision support systems that aid medical doctors in providing personalized care. This paper specifically examines deep reinforcement learning (DRL) models, which have shown human-level or superior performance in computer vision and game playing tasks like Atari games and Go. However, the use of DRL techniques in clinical decision optimization is still uncommon. This paper presents the first survey summarizing reinforcement learning algorithms that use Deep Neural Networks (DNN) for clinical decision support. Additionally, it discusses several case studies that apply different DRL algorithms to tackle various clinical challenges. The paper also compares and contrasts the advantages and limitations of different DRL algorithms and offers a preliminary guide on selecting an appropriate DRL algorithm for specific clinical applications.",1
"Reinforcement learning (RL) constitutes a promising solution for alleviating the problem of traffic congestion. In particular, deep RL algorithms have been shown to produce adaptive traffic signal controllers that outperform conventional systems. However, in order to be reliable in highly dynamic urban areas, such controllers need to be robust with the respect to a series of exogenous sources of uncertainty. In this paper, we develop an open-source callback-based framework for promoting the flexible evaluation of different deep RL configurations under a traffic simulation environment. With this framework, we investigate how deep RL-based adaptive traffic controllers perform under different scenarios, namely under demand surges caused by special events, capacity reductions from incidents and sensor failures. We extract several key insights for the development of robust deep RL algorithms for traffic control and propose concrete designs to mitigate the impact of the considered exogenous uncertainties.",0
"The use of reinforcement learning (RL) is a promising solution to address traffic congestion. Specifically, deep RL algorithms have demonstrated superior performance in developing adaptive traffic signal controllers compared to traditional systems. Nonetheless, to ensure reliability in highly dynamic urban areas, such controllers must be resistant to various external sources of uncertainty. This article presents an open-source callback-based framework to facilitate the flexible evaluation of different deep RL configurations under a traffic simulation environment. Using this framework, we examine the performance of deep RL-based adaptive traffic controllers in various scenarios, such as demand surges from special events, capacity reductions from incidents, and sensor failures. We derive several valuable insights for building robust deep RL algorithms for traffic control and suggest specific designs to minimize the impact of the aforementioned external uncertainties.",1
"Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.",0
"The objective of Automated Machine Learning (AutoML) is to discover the most efficient machine learning solutions automatically based on the machine learning problem. This technology can alleviate the tedious manual tuning process for data scientists and permit domain experts to utilize ready-made machine learning solutions without extensive expertise. This article evaluates the current advancements in AutoML in three categories: Automated Feature Engineering (AutoFE), Automated Model and Hyperparameter Learning (AutoMHL), and Automated Deep Learning (AutoDL). The paper highlights the cutting-edge techniques employed in these categories, such as Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. Additionally, it outlines commonly used AutoML frameworks and concludes with the existing challenges in AutoML.",1
"This paper augments the reward received by a reinforcement learning agent with potential functions in order to help the agent learn (possibly stochastic) optimal policies. We show that a potential-based reward shaping scheme is able to preserve optimality of stochastic policies, and demonstrate that the ability of an agent to learn an optimal policy is not affected when this scheme is augmented to soft Q-learning. We propose a method to impart potential based advice schemes to policy gradient algorithms. An algorithm that considers an advantage actor-critic architecture augmented with this scheme is proposed, and we give guarantees on its convergence. Finally, we evaluate our approach on a puddle-jump grid world with indistinguishable states, and the continuous state and action mountain car environment from classical control. Our results indicate that these schemes allow the agent to learn a stochastic optimal policy faster and obtain a higher average reward.",0
"In order to assist a reinforcement learning agent in learning optimal policies, this study incorporates potential functions to enhance the agent's reward. Through the use of a potential-based reward shaping scheme, we establish that the optimality of stochastic policies is maintained. Furthermore, we demonstrate that the addition of this scheme to soft Q-learning does not affect the agent's ability to learn an optimal policy. We also propose a technique for introducing potential-based advice schemes to policy gradient algorithms, and present an algorithm featuring an advantage actor-critic architecture augmented with this scheme, which we guarantee will converge. Our approach is evaluated in a puddle-jump grid world with indistinguishable states, as well as the continuous state and action mountain car environment from classical control. Our findings indicate that these schemes facilitate the agent in quickly learning a stochastic optimal policy and achieving a higher average reward.",1
"We introduce Arena, a toolkit for multi-agent reinforcement learning (MARL) research. In MARL, it usually requires customizing observations, rewards and actions for each agent, changing cooperative-competitive agent-interaction, and playing with/against a third-party agent, etc. We provide a novel modular design, called Interface, for manipulating such routines in essentially two ways: 1) Different interfaces can be concatenated and combined, which extends the OpenAI Gym Wrappers concept to MARL scenarios. 2) During MARL training or testing, interfaces can be embedded in either wrapped OpenAI Gym compatible Environments or raw environment compatible Agents. We offer off-the-shelf interfaces for several popular MARL platforms, including StarCraft II, Pommerman, ViZDoom, Soccer, etc. The interfaces effectively support self-play RL and cooperative-competitive hybrid MARL. Also, Arena can be conveniently extended to your own favorite MARL platform.",0
"Our toolkit, Arena, is designed for researching multi-agent reinforcement learning (MARL). In MARL, it's common to modify observations, rewards, and actions for each agent, adjust cooperative-competitive interactions, and incorporate a third-party agent. To simplify these processes, we've developed Interface, a modular design that can be used in two ways: 1) Different interfaces can be combined to expand the OpenAI Gym Wrappers concept to MARL scenarios. 2) Interfaces can be integrated into either wrapped OpenAI Gym compatible Environments or raw environment compatible Agents during MARL training or testing. We've also included ready-to-use interfaces for several popular MARL platforms, like StarCraft II, Pommerman, ViZDoom, and Soccer. These interfaces enable self-play RL and hybrid cooperative-competitive MARL. Additionally, Arena can be easily customized to suit your preferred MARL platform.",1
"In reinforcement learning algorithms, leveraging multiple views of the environment can improve the learning of complicated policies. In multi-view environments, due to the fact that the views may frequently suffer from partial observability, their level of importance are often different. In this paper, we propose a deep reinforcement learning method and an attention mechanism in a multi-view environment. Each view can provide various representative information about the environment. Through our attention mechanism, our method generates a single feature representation of environment given its multiple views. It learns a policy to dynamically attend to each view based on its importance in the decision-making process. Through experiments, we show that our method outperforms its state-of-the-art baselines on TORCS racing car simulator and three other complex 3D environments with obstacles. We also provide experimental results to evaluate the performance of our method on noisy conditions and partial observation settings.",0
"The learning of complex policies in reinforcement learning algorithms can be improved by utilizing multiple perspectives of the environment. However, the importance of each view in multi-view environments may differ due to partial observability. To address this, we introduce a novel approach that combines deep reinforcement learning and an attention mechanism in a multi-view environment. Our method generates a single feature representation by dynamically attending to each view based on its significance in decision-making. We demonstrate through experiments that our approach surpasses current state-of-the-art techniques on challenging 3D environments with obstacles, including the TORCS racing car simulator. We also evaluate the performance of our method in noisy and partially observable settings.",1
"Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.",0
"Episodic or trap-free environments are often assumed in known regret bounds for reinforcement learning. However, by occasionally allowing the algorithm to delegate an action to an external advisor, we have developed a regret bound without making either of these assumptions. This approach is known as DRL (delegative reinforcement learning) in the context of active one-shot model-based reinforcement learning. Our algorithm is a variant of Posterior Sampling Reinforcement Learning, with a subroutine that determines which actions should be delegated. It is important to note that the algorithm is not anytime, as the parameters must be adjusted based on the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states, and actions.",1
"Off-policy learning is more unstable compared to on-policy learning in reinforcement learning (RL). One reason for the instability of off-policy learning is a discrepancy between the target ($\pi$) and behavior (b) policy distributions. The discrepancy between $\pi$ and b distributions can be alleviated by employing a smooth variant of the importance sampling (IS), such as the relative importance sampling (RIS). RIS has parameter $\beta\in[0, 1]$ which controls smoothness. To cope with instability, we present the first relative importance sampling-off-policy actor-critic (RIS-Off-PAC) model-free algorithms in RL. In our method, the network yields a target policy (the actor), a value function (the critic) assessing the current policy ($\pi$) using samples drawn from behavior policy. We use action value generated from the behavior policy in reward function to train our algorithm rather than from the target policy. We also use deep neural networks to train both actor and critic. We evaluated our algorithm on a number of Open AI Gym benchmark problems and demonstrate better or comparable performance to several state-of-the-art RL baselines.",0
"Compared to on-policy learning in reinforcement learning (RL), off-policy learning is known to be more unstable. This instability is caused by a discrepancy between the target ($\pi$) and behavior (b) policy distributions. However, one solution to this problem is to use a smooth variant of importance sampling (IS), such as relative importance sampling (RIS), which has a parameter $\beta\in[0, 1]$ that controls smoothness. To address the instability issue, we propose the first model-free algorithm in RL called relative importance sampling-off-policy actor-critic (RIS-Off-PAC). Our approach utilizes a deep neural network to generate a target policy (the actor) and a value function (the critic) that assesses the current policy ($\pi$) using samples obtained from the behavior policy. Unlike other algorithms, we use action value generated from the behavior policy in the reward function to train our model. We evaluated our algorithm on several Open AI Gym benchmark problems and demonstrated better or comparable performance to other state-of-the-art RL baselines.",1
"Recently, reinforcement learning models have achieved great success, completing complex tasks such as mastering Go and other games with higher scores than human players. Many of these models collect considerable data on the tasks and improve accuracy by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks, respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using a large volume of past playing data. In this study, we propose a novel practical approach called reinforcement learning with convolutional reservoir computing (RCRC) model. The RCRC model has several desirable features: 1. it can extract visual and time-series features very fast because it uses random fixed-weight CNN and the reservoir computing model; 2. it does not require the training data to be stored because it extracts features without training and decides action with evolution strategy. Furthermore, the model achieves state of the art score in the popular reinforcement learning task. Incredibly, we find the random weight-fixed simple networks like only one dense layer network can also reach high score in the RL task.",0
"In recent times, reinforcement learning models have accomplished remarkable feats in completing complex tasks such as excelling in Go and other games with scores surpassing those of human players. These models accumulate substantial data on the tasks and enhance accuracy by utilizing convolutional neural networks (CNNs) and recurrent neural networks to extract visual and time-series features, respectively. However, these networks come with a high computational cost due to the requirement of extensive training via a vast amount of historical playing data. This study proposes a practical and groundbreaking approach called reinforcement learning with convolutional reservoir computing (RCRC) model. This model has various desirable features, such as fast extraction of visual and time-series features through the use of random fixed-weight CNN and the reservoir computing model. Moreover, it does not demand the training data to be stored as it extracts features without training and adopts an evolution strategy to decide the action. Additionally, the model achieves state of the art scores in the popular reinforcement learning task. Interestingly, the study reveals that random weight-fixed simple networks, such as a single dense layer network, can also attain high scores in the RL task.",1
"An optimal feedback controller for a given Markov decision process (MDP) can in principle be synthesized by value or policy iteration. However, if the system dynamics and the reward function are unknown, a learning agent must discover an optimal controller via direct interaction with the environment. Such interactive data gathering commonly leads to divergence towards dangerous or uninformative regions of the state space unless additional regularization measures are taken. Prior works proposed bounding the information loss measured by the Kullback-Leibler (KL) divergence at every policy improvement step to eliminate instability in the learning dynamics. In this paper, we consider a broader family of $f$-divergences, and more concretely $\alpha$-divergences, which inherit the beneficial property of providing the policy improvement step in closed form at the same time yielding a corresponding dual objective for policy evaluation. Such entropic proximal policy optimization view gives a unified perspective on compatible actor-critic architectures. In particular, common least-squares value function estimation coupled with advantage-weighted maximum likelihood policy improvement is shown to correspond to the Pearson $\chi^2$-divergence penalty. Other actor-critic pairs arise for various choices of the penalty-generating function $f$. On a concrete instantiation of our framework with the $\alpha$-divergence, we carry out asymptotic analysis of the solutions for different values of $\alpha$ and demonstrate the effects of the divergence function choice on common standard reinforcement learning problems.",0
"Value or policy iteration can be used to synthesize an optimal feedback controller for a given Markov decision process (MDP). However, when the system dynamics and reward function are unknown, a learning agent must interact with the environment to discover an optimal controller. This can lead to divergence towards dangerous or uninformative regions of the state space without regularization measures. Previous works have proposed bounding the information loss with the Kullback-Leibler (KL) divergence at every policy improvement step to eliminate instability. This paper expands on this by considering a broader family of $f$-divergences, specifically the $\alpha$-divergences, which provide a closed form policy improvement step and a corresponding dual objective for policy evaluation. This results in a unified perspective on compatible actor-critic architectures, with least-squares value function estimation and advantage-weighted maximum likelihood policy improvement corresponding to the Pearson $\chi^2$-divergence penalty. Different choices of the penalty-generating function $f$ generate other actor-critic pairs. The paper carries out asymptotic analysis of the solutions for different values of $\alpha$ and demonstrates the effects of divergence function choice on standard reinforcement learning problems.",1
"Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.",0
"The lack of reliable guarantees about machine-learning applications has slowed their adoption, despite significant progress in their development over the last decade. In this paper, we explore the repurposing of formal verification techniques from traditional software systems to verify reinforcement learning-enabled systems. Rather than modifying the complex neural network structure, our technique synthesizes simpler and more interpretable deterministic programs that can ensure desired safety properties are maintained, even in unpredictable environments. Our methodology uses a counterexample and syntax-guided inductive synthesis process to search for a deterministic program and inductive invariant over an infinite state transition system, with further specifications available to refine the search. The synthesized programs work with a neural network implementation to monitor and prevent unsafe actions, resulting in trustworthy reinforcement learning systems with minimal overhead. Our experimental results demonstrate the effectiveness of software-inspired formal verification techniques in a range of cyber-physical applications.",1
"We present a training pipeline for the autonomous driving task given the current camera image and vehicle speed as the input to produce the throttle, brake, and steering control output. The simulator Airsim's convenient weather and lighting API provides a sufficient diversity during training which can be very helpful to increase the trained policy's robustness. In order to not limit the possible policy's performance, we use a continuous and deterministic control policy setting. We utilize ResNet-34 as our actor and critic networks with some slight changes in the fully connected layers. Considering human's mastery of this task and the high-complexity nature of this task, we first use imitation learning to mimic the given human policy and leverage the trained policy and its weights to the reinforcement learning phase for which we use DDPG. This combination shows a considerable performance boost comparing to both pure imitation learning and pure DDPG for the autonomous driving task.",0
"Our study introduces a training pipeline that utilizes current camera image and vehicle speed to generate throttle, brake, and steering control output for autonomous driving. The inclusion of Airsim's weather and lighting API provides ample diversity during training, which enhances the trained policy's robustness. To ensure optimal performance, we employ a continuous and deterministic control policy setting and implement ResNet-34 as our actor and critic networks with minor modifications in the fully connected layers. Due to the intricate nature of this task and humans' expertise in it, we begin with imitation learning to replicate the human policy and then integrate the trained policy and its weights into the reinforcement learning phase using DDPG. This combination yields a significant performance improvement compared to pure imitation learning and pure DDPG for autonomous driving.",1
"During the development of autonomous systems such as driverless cars, it is important to characterize the scenarios that are most likely to result in failure. Adaptive Stress Testing (AST) provides a way to search for the most-likely failure scenario as a Markov decision process (MDP). Our previous work used a deep reinforcement learning (DRL) solver to identify likely failure scenarios. However, the solver's use of a feed-forward neural network with a discretized space of possible initial conditions poses two major problems. First, the system is not treated as a black box, in that it requires analyzing the internal state of the system, which leads to considerable implementation complexities. Second, in order to simulate realistic settings, a new instance of the solver needs to be run for each initial condition. Running a new solver for each initial condition not only significantly increases the computational complexity, but also disregards the underlying relationship between similar initial conditions. We provide a solution to both problems by employing a recurrent neural network that takes a set of initial conditions from a continuous space as input. This approach enables robust and efficient detection of failures because the solution generalizes across the entire space of initial conditions. By simulating an instance where an autonomous car drives while a pedestrian is crossing a road, we demonstrate the solver is now capable of finding solutions for problems that would have previously been intractable.",0
"In the development of self-driving vehicles, it is crucial to identify scenarios that are most prone to failure. One way to achieve this is by using Adaptive Stress Testing (AST), which utilizes a Markov decision process (MDP) to search for the most probable failure scenario. In our prior work, we employed a deep reinforcement learning (DRL) solver to identify potential failure scenarios. However, this approach had two significant drawbacks. First, the solver relied on a feed-forward neural network with a limited set of initial conditions, which complicated its implementation. Second, to simulate real-world settings, we had to run a new solver for each initial condition, which increased computational complexity and disregarded the connection between similar initial conditions. To address these issues, we introduced a recurrent neural network that can handle a continuous set of initial conditions as input. This solution offers a more efficient and robust method of identifying failures since the solution generalizes across the entire range of initial conditions. Our experiments, which included a pedestrian crossing a road while a self-driving car was moving, demonstrate the improved solver's ability to solve previously unsolvable problems.",1
On-policy reinforcement learning (RL) algorithms have high sample complexity while off-policy algorithms are difficult to tune. Merging the two holds the promise to develop efficient algorithms that generalize across diverse environments. It is however challenging in practice to find suitable hyper-parameters that govern this trade off. This paper develops a simple algorithm named P3O that interleaves off-policy updates with on-policy updates. P3O uses the effective sample size between the behavior policy and the target policy to control how far they can be from each other and does not introduce any additional hyper-parameters. Extensive experiments on the Atari-2600 and MuJoCo benchmark suites show that this simple technique is effective in reducing the sample complexity of state-of-the-art algorithms. Code to reproduce experiments in this paper is at https://github.com/rasoolfa/P3O.,0
"While on-policy reinforcement learning (RL) algorithms require a large number of samples, off-policy algorithms are challenging to adjust. However, combining the two could lead to the development of efficient algorithms that can perform well in various environments. Nevertheless, it is difficult to determine the appropriate hyper-parameters that balance these approaches. This study introduces a straightforward algorithm called P3O that alternates between off-policy and on-policy updates. P3O regulates the distance between the behavior policy and the target policy by using the effective sample size and does not require additional hyper-parameters. The results of extensive experiments on the Atari-2600 and MuJoCo benchmark suites indicate that this simple approach reduces the sample complexity of leading algorithms. The code to replicate the experiments is available at https://github.com/rasoolfa/P3O.",1
"In this paper, we propose a dual memory structure for reinforcement learning algorithms with replay memory. The dual memory consists of a main memory that stores various data and a cache memory that manages the data and trains the reinforcement learning agent efficiently. Experimental results show that the dual memory structure achieves higher training and test scores than the conventional single memory structure in three selected environments of OpenAI Gym. This implies that the dual memory structure enables better and more efficient training than the single memory structure.",0
"Our paper presents a proposal for a dual memory structure aimed at enhancing the performance of reinforcement learning algorithms featuring replay memory. This structure comprises a primary memory that stores diverse data and a cache memory that oversees the data and trains the reinforcement learning agent in an optimal manner. Based on our experiments, the dual memory structure outperforms the conventional single memory structure in three OpenAI Gym environments, as evidenced by the higher training and test scores attained. These findings signify that the dual memory structure facilitates superior and more effective training than the single memory structure.",1
"We study two time-scale linear stochastic approximation algorithms, which can be used to model well-known reinforcement learning algorithms such as GTD, GTD2, and TDC. We present finite-time performance bounds for the case where the learning rate is fixed. The key idea in obtaining these bounds is to use a Lyapunov function motivated by singular perturbation theory for linear differential equations. We use the bound to design an adaptive learning rate scheme which significantly improves the convergence rate over the known optimal polynomial decay rule in our experiments, and can be used to potentially improve the performance of any other schedule where the learning rate is changed at pre-determined time instants.",0
"The focus of our research is on two linear stochastic approximation algorithms that have the ability to model popular reinforcement learning algorithms like GTD, GTD2, and TDC. Our objective is to provide finite-time performance bounds in situations where the learning rate remains fixed. To achieve this, we employ a Lyapunov function based on the singular perturbation theory for linear differential equations. Our results enable us to create an adaptive learning rate scheme that surpasses the known optimal polynomial decay rule in terms of convergence rate. This scheme can also be used to enhance the performance of other schedules that change the learning rate at predetermined time instants.",1
"In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.",0
"The aim of this study is to examine the issue of overfitting in deep reinforcement learning. The conventional method of using the same environments for both training and testing in RL benchmarks provides limited information about an agent's capacity to generalize. To overcome this limitation, we have used procedurally generated environments to create distinct training and test sets. Furthermore, we have introduced an innovative environment named CoinRun, which serves as a benchmark for evaluating generalization in RL. Our findings demonstrate that agents tend to overfit even to large training sets. However, we have observed that deeper convolutional architectures and techniques commonly employed in supervised learning, such as L2 regularization, dropout, data augmentation, and batch normalization, enhance generalization.",1
"Despite the empirical success of the actor-critic algorithm, its theoretical understanding lags behind. In a broader context, actor-critic can be viewed as an online alternating update algorithm for bilevel optimization, whose convergence is known to be fragile. To understand the instability of actor-critic, we focus on its application to linear quadratic regulators, a simple yet fundamental setting of reinforcement learning. We establish a nonasymptotic convergence analysis of actor-critic in this setting. In particular, we prove that actor-critic finds a globally optimal pair of actor (policy) and critic (action-value function) at a linear rate of convergence. Our analysis may serve as a preliminary step towards a complete theoretical understanding of bilevel optimization with nonconvex subproblems, which is NP-hard in the worst case and is often solved using heuristics.",0
"Theoretical understanding of the actor-critic algorithm is lacking despite its empirical success. This algorithm can be seen as an online alternating update for bilevel optimization and is known to have fragile convergence. To investigate this instability, we examine its application to linear quadratic regulators in basic reinforcement learning. Our nonasymptotic convergence analysis of actor-critic in this context shows that it can find a globally optimal actor and critic pair at a linear convergence rate. This analysis is a preliminary step towards a complete theoretical understanding of nonconvex subproblems in bilevel optimization, which is an NP-hard problem often solved with heuristics.",1
"Reinforcement learning aims at searching the best policy model for decision making, and has been shown powerful for sequential recommendations. The training of the policy by reinforcement learning, however, is placed in an environment. In many real-world applications, however, the policy training in the real environment can cause an unbearable cost, due to the exploration in the environment. Environment reconstruction from the past data is thus an appealing way to release the power of reinforcement learning in these applications. The reconstruction of the environment is, basically, to extract the casual effect model from the data. However, real-world applications are often too complex to offer fully observable environment information. Therefore, quite possibly there are unobserved confounding variables lying behind the data. The hidden confounder can obstruct an effective reconstruction of the environment. In this paper, by treating the hidden confounder as a hidden policy, we propose a deconfounded multi-agent environment reconstruction (DEMER) approach in order to learn the environment together with the hidden confounder. DEMER adopts a multi-agent generative adversarial imitation learning framework. It proposes to introduce the confounder embedded policy, and use the compatible discriminator for training the policies. We then apply DEMER in an application of driver program recommendation. We firstly use an artificial driver program recommendation environment, abstracted from the real application, to verify and analyze the effectiveness of DEMER. We then test DEMER in the real application of Didi Chuxing. Experiment results show that DEMER can effectively reconstruct the hidden confounder, and thus can build the environment better. DEMER also derives a recommendation policy with a significantly improved performance in the test phase of the real application.",0
"Reinforcement learning is utilized to find the optimal policy model for decision-making, particularly in sequential recommendations. However, training the policy in the real environment can be costly due to the exploration required. One solution is to reconstruct the environment from past data, which involves extracting the causal effect model. However, real-world applications are often too complex to provide fully observable environment information, and unobserved confounding variables may exist. These hidden confounders can hinder effective environment reconstruction. To address this issue, we propose the Deconfounded Multi-Agent Environment Reconstruction (DEMER) approach, which treats the hidden confounder as a hidden policy. DEMER utilizes a multi-agent generative adversarial imitation learning framework, introducing the confounder-embedded policy and a compatible discriminator to train the policies. We apply DEMER to an application of driver program recommendation and test it in an artificial and real-world environment. The results show that DEMER effectively reconstructs the hidden confounder, leading to better environment building and significantly improved recommendation policy performance in the test phase of the real-world application.",1
"We aim to conduct a systematic mapping in the area of testing ML programs. We identify, analyze and classify the existing literature to provide an overview of the area. We followed well-established guidelines of systematic mapping to develop a systematic protocol to identify and review the existing literature. We formulate three sets of research questions, define inclusion and exclusion criteria and systematically identify themes for the classification of existing techniques. We also report the quality of the published works using established assessment criteria. we finally selected 37 papers out of 1654 based on our selection criteria up to January 2019. We analyze trends such as contribution facet, research facet, test approach, type of ML and the kind of testing with several other attributes. We also discuss the empirical evidence and reporting quality of selected papers. The data from the study is made publicly available for other researchers and practitioners. We present an overview of the area by answering several research questions. The area is growing rapidly, however, there is lack of enough empirical evidence to compare and assess the effectiveness of the techniques. More publicly available tools are required for use of practitioners and researchers. Further attention is needed on non-functional testing and testing of ML programs using reinforcement learning. We believe that this study can help researchers and practitioners to obtain an overview of the area and identify several sub-areas where more research is required",0
"Our objective is to systematically map the field of ML program testing by examining, categorizing, and analyzing existing literature. We adhered to established guidelines for systematic mapping and established a protocol to identify and evaluate relevant literature. We formulated three sets of research questions, defined criteria for inclusion and exclusion, and identified themes for classifying existing techniques. We evaluated the quality of published works using established assessment criteria and ultimately selected 37 papers out of 1654 based on our selection criteria up to January 2019. We analyzed trends such as contribution facet, research facet, test approach, type of ML, and testing methods, among other attributes. We also discussed the empirical evidence and reporting quality of the selected papers, and made the data from the study publicly available for other researchers and practitioners. Our study provides an overview of the area and answers several research questions. Although the field is growing rapidly, there is a lack of sufficient empirical evidence to compare and assess the effectiveness of techniques. More publicly available tools are needed for practitioners and researchers, and further attention is required on non-functional testing and testing using reinforcement learning. This study can assist researchers and practitioners in gaining an understanding of the area and identifying sub-areas that require further research.",1
"While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.",0
"Although model-based deep reinforcement learning (RL) shows potential for efficiency and generalization, the process of accurately learning dynamics models is often difficult and requires extensive interaction with the environment. Despite the existence of common principles such as classical mechanics that underlie a wide range of domains, current algorithms rarely utilize them. Conversely, humans are adept at acquiring and applying such dynamics priors to adapt easily to new environments. This study proposes an approach to extract task-agnostic dynamics priors from videos and integrate them into an RL agent. The method involves pre-training a frame predictor on task-agnostic physics videos to establish dynamics models for unseen target environments. The frame prediction design, SpatialNet, is tailored to capture localized physical phenomena and interactions. The approach enables faster policy learning and better convergence to policies, outperforming other methods across several environments. Additionally, the study shows that the incorporation of this prior facilitates more effective transfer between environments.",1
"Batch Reinforcement Learning (Batch RL) consists in training a policy using trajectories collected with another policy, called the behavioural policy. Safe policy improvement (SPI) provides guarantees with high probability that the trained policy performs better than the behavioural policy, also called baseline in this setting. Previous work shows that the SPI objective improves mean performance as compared to using the basic RL objective, which boils down to solving the MDP with maximum likelihood. Here, we build on that work and improve more precisely the SPI with Baseline Bootstrapping algorithm (SPIBB) by allowing the policy search over a wider set of policies. Instead of binarily classifying the state-action pairs into two sets (the \textit{uncertain} and the \textit{safe-to-train-on} ones), we adopt a softer strategy that controls the error in the value estimates by constraining the policy change according to the local model uncertainty. The method can take more risks on uncertain actions all the while remaining provably-safe, and is therefore less conservative than the state-of-the-art methods. We propose two algorithms (one optimal and one approximate) to solve this constrained optimization problem and empirically show a significant improvement over existing SPI algorithms both on finite MDPs and on infinite MDPs with a neural network function approximation.",0
"The concept of Batch Reinforcement Learning involves using trajectories acquired by a behavioural policy to train another policy. Safe policy improvement (SPI) ensures that the trained policy performs better than the behavioural policy, which is considered the baseline. Previous research indicates that utilizing the SPI objective leads to superior mean performance compared to the basic RL objective. However, our work builds upon this by introducing the Baseline Bootstrapping algorithm (SPIBB), which expands the search for policies that can be used. Rather than classifying state-action pairs into two distinct categories, our approach employs a more flexible strategy that considers the estimation error by restricting policy changes based on the local model uncertainty. This technique allows for more risk-taking on uncertain actions while still maintaining safety. We offer two algorithms, one optimal and one approximate, to address this constrained optimization issue and demonstrate significant improvements over existing SPI approaches in both finite and infinite MDPs that use neural network function approximation.",1
"Reinforcement learning usually makes use of numerical rewards, which have nice properties but also come with drawbacks and difficulties. Using rewards on an ordinal scale (ordinal rewards) is an alternative to numerical rewards that has received more attention in recent years. In this paper, a general approach to adapting reinforcement learning problems to the use of ordinal rewards is presented and motivated. We show how to convert common reinforcement learning algorithms to an ordinal variation by the example of Q-learning and introduce Ordinal Deep Q-Networks, which adapt deep reinforcement learning to ordinal rewards. Additionally, we run evaluations on problems provided by the OpenAI Gym framework, showing that our ordinal variants exhibit a performance that is comparable to the numerical variations for a number of problems. We also give first evidence that our ordinal variant is able to produce better results for problems with less engineered and simpler-to-design reward signals.",0
"Reinforcement learning often utilizes numerical rewards, which have advantages but also pose challenges. Recently, there has been a growing interest in using ordinal rewards as an alternative to numerical rewards. In this research, we propose a comprehensive strategy for adapting reinforcement learning tasks to the use of ordinal rewards. We demonstrate how to modify common reinforcement learning algorithms, such as Q-learning, to accommodate ordinal rewards and introduce Ordinal Deep Q-Networks, which incorporate deep reinforcement learning with ordinal rewards. Moreover, we evaluate our ordinal variants on problems from the OpenAI Gym framework and find that they perform similarly to the numerical variations in many cases. Furthermore, we provide initial evidence that our ordinal variant may achieve better results for problems with simpler and less engineered reward signals.",1
"In multi-task reinforcement learning there are two main challenges: at training time, the ability to learn different policies with a single model; at test time, inferring which of those policies applying without an external signal. In the case of continual reinforcement learning a third challenge arises: learning tasks sequentially without forgetting the previous ones. In this paper, we tackle these challenges by proposing DisCoRL, an approach combining state representation learning and policy distillation. We experiment on a sequence of three simulated 2D navigation tasks with a 3 wheel omni-directional robot. Moreover, we tested our approach's robustness by transferring the final policy into a real life setting. The policy can solve all tasks and automatically infer which one to run.",0
"Multi-task reinforcement learning presents two major hurdles: first, the need to acquire various policies using a single model during training, and second, the capability to determine which policy to execute without external cues during testing. In the case of continuous reinforcement learning, a third challenge emerges: the capacity to learn tasks in sequence while not forgetting previous ones. This article proposes DisCoRL, an approach that combines state representation learning and policy distillation to tackle these challenges. The technique is tested on three simulated 2D navigation tasks utilizing a 3-wheel omni-directional robot, and the final policy is transferred to a real-life environment to test its robustness. The policy can handle all tasks and make autonomous inferences about which task to execute.",1
"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",0
"The Rerouted Behavior Improvement (RBI) algorithm is proposed as a policy improvement method for Reinforcement Learning (RL) that takes into account the evaluation errors of the Q-function. These errors are common in RL when learning the Q-value from finite past experience data, and ignoring them can result in negative policy improvement. To minimize this penalty, RBI attenuates rapid policy changes of low probability actions that were less frequently sampled. This approach prevents catastrophic performance degradation and reduces regret when learning from a batch of past experience. Additionally, it increases data efficiency when the optimal action has high variance. RBI is evaluated in two tasks in the Atari Learning Environment, namely learning from observations of multiple behavior policies and iterative RL. Our results show that RBI is a safe learning approach and a general data-efficient learning algorithm that outperforms greedy policies and other constrained policy optimization algorithms. An anonymous Github repository of our RBI implementation can be found at https://github.com/eladsar/rbi.",1
"In this paper, we present a Bayesian view on model-based reinforcement learning. We use expert knowledge to impose structure on the transition model and present an efficient learning scheme based on variational inference. This scheme is applied to a heteroskedastic and bimodal benchmark problem on which we compare our results to NFQ and show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.",0
"Our paper adopts a Bayesian perspective in examining model-based reinforcement learning. We leverage expert knowledge to structure the transition model and propose a learning scheme based on variational inference that yields significant efficiency gains. We evaluate our approach on a challenging benchmark problem with heteroskedastic and bimodal characteristics, and compare our results to those of NFQ. Our approach not only enhances data efficiency but also provides human-readable insights on the dynamics underlying the problem.",1
"Deep reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a problem of generalizing the learned policy which makes the learning performance largely affected even by minor modifications of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be interpretable. To address these two challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to represent the policies in reinforcement learning by first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming that have demonstrated significant advantages in terms of interpretability and generalisability in supervised tasks. Extensive experiments conducted on cliff-walking and blocks manipulation tasks demonstrate that NLRL can induce interpretable policies achieving near-optimal performance while demonstrating good generalisability to environments of different initial states and problem sizes.",0
"Significant progress has been made in various tasks through the use of deep reinforcement learning (DRL). However, a major impediment to DRL algorithms is their inability to generalize learned policy, which negatively impacts learning performance even with minor adjustments to the training environment. Additionally, deep neural networks used in DRL result in learned policies that are difficult to interpret. To overcome these challenges, we propose a new approach called Neural Logic Reinforcement Learning (NLRL), which represents reinforcement learning policies using first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming, which have demonstrated superior interpretability and generalizability in supervised tasks. Our extensive experiments on cliff-walking and blocks manipulation tasks show that NLRL can induce interpretable policies that achieve near-optimal performance while also demonstrating good generalizability to different problem sizes and initial states.",1
"Recommendation problems with large numbers of discrete items, such as products, webpages, or videos, are ubiquitous in the technology industry. Deep neural networks are being increasingly used for these recommendation problems. These models use embeddings to represent discrete items as continuous vectors, and the vocabulary sizes and embedding dimensions, although heavily influence the model's accuracy, are often manually selected in a heuristical manner. We present Neural Input Search (NIS), a technique for learning the optimal vocabulary sizes and embedding dimensions for categorical features. The goal is to maximize prediction accuracy subject to a constraint on the total memory used by all embeddings. Moreover, we argue that the traditional Single-size Embedding (SE), which uses the same embedding dimension for all values of a feature, suffers from inefficient usage of model capacity and training data. We propose a novel type of embedding, namely Multi-size Embedding (ME), which allows the embedding dimension to vary for different values of the feature. During training we use reinforcement learning to find the optimal vocabulary size for each feature and embedding dimension for each value of the feature. In experiments on two common types of large scale recommendation problems, i.e. retrieval and ranking problems, NIS automatically found better vocabulary and embedding sizes that result in $6.8\%$ and $1.8\%$ relative improvements on Recall@1 and ROC-AUC over manually optimized ones.",0
"The technology industry frequently encounters recommendation problems involving a vast number of discrete items like products, webpages, and videos. These issues are now being addressed with deep neural networks that use embeddings to create continuous vector representations of the discrete items. However, the vocabulary sizes and embedding dimensions, which significantly impact the model's accuracy, are typically chosen through manual heuristics. We introduce Neural Input Search (NIS) as a method for determining the optimal vocabulary sizes and embedding dimensions for categorical features. Our objective is to obtain the highest prediction accuracy while limiting the total memory used by all embeddings. Furthermore, we argue that the conventional Single-size Embedding (SE) that employs the same embedding dimension for all feature values is ineffective in utilizing model capacity and training data. Instead, we propose Multi-size Embedding (ME), a new type of embedding that allows the embedding dimension to vary for different feature values. We use reinforcement learning during training to obtain the ideal vocabulary size for each feature and embedding dimension for each feature value. In experiments on two common types of large-scale recommendation problems, namely retrieval and ranking problems, NIS automatically identifies better vocabulary and embedding sizes, resulting in 6.8% and 1.8% relative improvements in Recall@1 and ROC-AUC, respectively, compared to manually optimized alternatives.",1
"Variance plays a crucial role in risk-sensitive reinforcement learning, and most risk measures can be analyzed via variance. In this paper, we consider two law-invariant risks as examples: mean-variance risk and exponential utility risk. With the aid of the state-augmentation transformation (SAT), we show that, the two risks can be estimated in Markov decision processes (MDPs) with a stochastic transition-based reward and a randomized policy. To relieve the enlarged state space, a novel definition of isotopic states is proposed for state lumping, considering the special structure of the transformed transition probability. In the numerical experiment, we illustrate state lumping in the SAT, errors from a naive reward simplification, and the validity of the SAT for the two risk estimations.",0
"The role of variance is vital in risk-sensitive reinforcement learning, and most risk measures can be analyzed by it. This article focuses on two examples of law-invariant risks: mean-variance risk and exponential utility risk. The authors utilize the state-augmentation transformation (SAT) to demonstrate that these risks can be estimated in Markov decision processes (MDPs) that involve stochastic transition-based rewards and randomized policies. To reduce the state space, a new definition of isotopic states is suggested for state lumping, taking into account the unique structure of the transformed transition probability. The authors conduct a numerical experiment to demonstrate state lumping in the SAT, errors arising from a naive reward simplification, and the SAT's applicability for estimating the two risks.",1
"We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",0
"The article presents NoisyNet, a deep reinforcement learning agent that incorporates parametric noise into its weights. By doing so, the agent's policy becomes stochastic, which helps facilitate efficient exploration. The noise parameters are learned using gradient descent, alongside the other network weights. Implementing NoisyNet is simple and does not require significant computational resources. The study demonstrates that using NoisyNet, rather than conventional exploration heuristics like entropy reward and $\epsilon$-greedy, leads to significantly higher scores in various Atari games. In some cases, the agent's performance improves from sub-human to super-human levels.",1
"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",0
"One major drawback of current methods in inverse reinforcement learning (IRL) is their limited capability to surpass the performance of the demonstrator. This is because IRL mainly focuses on finding a reward function that makes the demonstrator seem nearly optimal instead of deducing the underlying intentions of the demonstrator that may have been executed poorly in practice. This research introduces a new approach to reward learning from observation, known as Trajectory-ranked Reward EXtrapolation (T-REX), which goes beyond a set of ranked demonstrations to deduce high-quality reward functions from a potentially substandard set of demonstrations. When coupled with deep reinforcement learning, T-REX outperforms the most advanced imitation learning and IRL techniques on various Atari and MuJoCo benchmark tasks, achieving performance that is frequently more than double the best demonstration's performance. Additionally, the research shows that T-REX is resistant to ranking noise and can accurately extrapolate intention by simply observing a learner gradually improve at a task despite noise.",1
"Active Inference is a theory of action arising from neuroscience which casts action and planning as a bayesian inference problem to be solved by minimizing a single quantity - the variational free energy. Active Inference promises a unifying account of action and perception coupled with a biologically plausible process theory. Despite these potential advantages, current implementations of Active Inference can only handle small, discrete policy and state-spaces and typically require the environmental dynamics to be known. In this paper we propose a novel deep Active Inference algorithm which approximates key densities using deep neural networks as flexible function approximators, which enables Active Inference to scale to significantly larger and more complex tasks. We demonstrate our approach on a suite of OpenAIGym benchmark tasks and obtain performance comparable with common reinforcement learning baselines. Moreover, our algorithm shows similarities with maximum entropy reinforcement learning and the policy gradients algorithm, which reveals interesting connections between the Active Inference framework and reinforcement learning.",0
"The theory of Active Inference is rooted in neuroscience and views action and planning as a problem of bayesian inference. This theory aims to provide a comprehensive explanation of action and perception, while adhering to a biologically plausible process. However, the current implementation of Active Inference is limited to small, well-defined policies and states, and requires pre-knowledge of environmental dynamics. To address these limitations, we introduce a new deep Active Inference algorithm that uses deep neural networks to approximate crucial densities, making it possible to tackle larger and more complex problems. Our algorithm was tested on a set of OpenAIGym benchmarks and performed comparably with standard reinforcement learning techniques. Furthermore, our approach exhibits similarities with maximum entropy reinforcement learning and policy gradients, uncovering intriguing links between the Active Inference framework and reinforcement learning.",1
"Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -- e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation -- a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.",0
"Many deep reinforcement learning (RL) systems struggle to effectively learn from off-policy data, particularly if they are unable to explore the environment online. These limitations are especially problematic when using RL for real-world applications, where data collection can be costly, and models need to be tested offline before being deployed to interact with the environment (e.g., systems that learn from human interaction). To address these challenges, we introduce a novel class of off-policy batch RL algorithms that can learn offline from a fixed batch of human interaction data without exploring. Leveraging models pre-trained on data as a strong prior, we use KL-control to penalize divergence from the prior during RL training, and employ dropout-based uncertainty estimates to efficiently lower bound target Q-values as an alternative to Double Q-Learning. Our algorithms are tested on the difficult task of open-domain dialog generation, which features a 20,000-dimensional action space. We demonstrate the effectiveness of our Way Off-Policy algorithm by extracting multiple reward functions post-hoc from collected human interaction data and effectively learning from all of them. We also evaluate the real-world generalization of our systems by deploying them live to converse with humans in an open-domain setting and show significant improvements over prior methods in off-policy batch RL.",1
"Interest in derivative-free optimization (DFO) and ""evolutionary strategies"" (ES) has recently surged in the Reinforcement Learning (RL) community, with growing evidence that they can match state of the art methods for policy optimization problems in Robotics. However, it is well known that DFO methods suffer from prohibitively high sampling complexity. They can also be very sensitive to noisy rewards and stochastic dynamics. In this paper, we propose a new class of algorithms, called Robust Blackbox Optimization (RBO). Remarkably, even if up to $23\%$ of all the measurements are arbitrarily corrupted, RBO can provably recover gradients to high accuracy. RBO relies on learning gradient flows using robust regression methods to enable off-policy updates. On several MuJoCo robot control tasks, when all other RL approaches collapse in the presence of adversarial noise, RBO is able to train policies effectively. We also show that RBO can be applied to legged locomotion tasks including path tracking for quadruped robots.",0
"The Reinforcement Learning (RL) community has recently shown a growing interest in derivative-free optimization (DFO) and ""evolutionary strategies"" (ES), as they have been found to be equally effective as state-of-the-art methods for policy optimization problems in Robotics. However, the high sampling complexity associated with DFO methods and their sensitivity to noisy rewards and stochastic dynamics are well known. This paper introduces a new algorithmic class, Robust Blackbox Optimization (RBO), which is capable of recovering gradients to high accuracy, even when up to 23% of all measurements are arbitrarily corrupted. RBO uses robust regression methods to learn gradient flows and enables off-policy updates. In contrast to other RL approaches that collapse in the presence of adversarial noise, RBO can effectively train policies on several MuJoCo robot control tasks. Additionally, RBO can be applied to legged locomotion tasks, including path tracking for quadruped robots.",1
"Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.",0
"By integrating classical theoretical findings with the Deep Learning paradigm, Reinforcement Learning has made significant progress in various artificial intelligence tasks and spawned the field of Deep Reinforcement Learning (DRL). This article examines the most recent DRL algorithms, highlighting their theoretical backing, practical constraints, and observed empirical characteristics.",1
"At an early age, human infants are able to learn and build a model of the world very quickly by constantly observing and interacting with objects around them. One of the most fundamental intuitions human infants acquire is intuitive physics. Human infants learn and develop these models, which later serve as prior knowledge for further learning. Inspired by such behaviors exhibited by human infants, we introduce a graphical physics network integrated with deep reinforcement learning. Specifically, we introduce an intrinsic reward normalization method that allows our agent to efficiently choose actions that can improve its intuitive physics model the most.   Using a 3D physics engine, we show that our graphical physics network is able to infer object's positions and velocities very effectively, and our deep reinforcement learning network encourages an agent to improve its model by making it continuously interact with objects only using intrinsic motivation. We experiment our model in both stationary and non-stationary state problems and show benefits of our approach in terms of the number of different actions the agent performs and the accuracy of agent's intuition model.   Videos are at https://www.youtube.com/watch?v=pDbByp91r3M&t=2s",0
"At an early stage of development, human infants quickly learn about the world by observing and interacting with objects around them. Intuitive physics is one of the fundamental concepts that infants acquire and develop. These models serve as prior knowledge for future learning. Inspired by the learning behavior of human infants, we propose a graphical physics network with deep reinforcement learning. We introduce an intrinsic reward normalization method, allowing our agent to choose actions that can improve its intuitive physics model efficiently. Our graphical physics network, utilizing a 3D physics engine, effectively infers object positions and velocities. Our deep reinforcement learning network encourages the agent to improve its model by interacting with objects intrinsically. We conduct experiments on stationary and non-stationary state problems and demonstrate the benefits of our approach in terms of the accuracy of the agent's intuitive model and the number of actions performed. Videos of our model can be found at https://www.youtube.com/watch?v=pDbByp91r3M&t=2s.",1
"Flappy Bird, which has a very high popularity, has been trained in many algorithms. Some of these studies were trained from raw pixel values of game and some from specific attributes. In this study, the model was trained with raw game images, which had not been seen before. The trained model has learned as reinforcement when to make which decision. As an input to the model, the reward or penalty at the end of each step was returned and the training was completed. Flappy Bird game was trained with the Reinforcement Learning algorithm Deep Q-Network and Asynchronous Advantage Actor Critic (A3C) algorithms.",0
"Numerous algorithms have been utilized to train the extremely popular Flappy Bird game, including studies that utilize raw pixel values and specific attributes. This particular study trained the model using never-before-seen raw game images, allowing the model to learn through reinforcement when to make certain decisions. The training process involved inputting rewards or penalties at the end of each step, ultimately resulting in a fully trained model. The Reinforcement Learning algorithm Deep Q-Network and the Asynchronous Advantage Actor Critic (A3C) algorithms were both used to train Flappy Bird.",1
This paper considers a distributed reinforcement learning problem in which a network of multiple agents aim to cooperatively maximize the globally averaged return through communication with only local neighbors. A randomized communication-efficient multi-agent actor-critic algorithm is proposed for possibly unidirectional communication relationships depicted by a directed graph. It is shown that the algorithm can solve the problem for strongly connected graphs by allowing each agent to transmit only two scalar-valued variables at one time.,0
"The focus of this article is on a distributed reinforcement learning issue where numerous agents within a network work together to maximize the overall average return by communicating exclusively with local peers. The proposed solution to this potentially one-way communication issue, portrayed by a directed graph, is a randomized communication-efficient multi-agent actor-critic algorithm. This algorithm has the capability of resolving the problem for strongly connected graphs by enabling each agent to send only two scalar-valued variables at once.",1
"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",0
"Imitation Learning (IL) is a technique in machine learning that involves learning a policy from a dataset of demonstrations. It can be used to initiate learning before applying reinforcement learning (RL), or on its own to replicate human behavior in video games. However, current IL approaches only learn a single ""average"" policy from a dataset that may contain various types of behaviors. To address this limitation, we introduce a new approach called Behavioral Repertoire Imitation Learning (BRIL). BRIL learns a range of behaviors from a set of demonstrations by augmenting state-action pairs with behavioral descriptions. The result is a single neural network policy that can be fine-tuned to express different behaviors. We apply BRIL to train a policy on 7,777 human replays for build-order planning in StarCraft II. By using Principal Component Analysis (PCA) to construct a low-dimensional behavioral space, we demonstrate that the policy can be effectively modulated to express distinct behaviors. We also show that by using the UCB1 algorithm, we can adapt the policy's behavior between games and achieve better performance than the traditional IL approach.",1
"Temporal difference methods enable efficient estimation of value functions in reinforcement learning in an incremental fashion, and are of broader interest because they correspond learning as observed in biological systems. Standard value functions correspond to the expected value of a sum of discounted returns. While this formulation is often sufficient for many purposes, it would often be useful to be able to represent functions of the return as well. Unfortunately, most such functions cannot be estimated directly using TD methods. We propose a means of estimating functions of the return using its moments, which can be learned online using a modified TD algorithm. The moments of the return are then used as part of a Taylor expansion to approximate analytic functions of the return.",0
"Temporal difference methods are effective in estimating value functions incrementally in reinforcement learning and have broader applications due to their similarity to learning in biological systems. The standard value functions represent the expected value of discounted returns, which is generally adequate, but it would be beneficial to represent functions of the return as well. However, TD methods cannot directly estimate most of these functions. To solve this problem, we suggest estimating the moments of the return using a modified TD algorithm and using them in a Taylor expansion to approximate analytic functions of the return. This approach can be learned online.",1
"This paper targets the problem of image set-based face verification and identification. Unlike traditional single media (an image or video) setting, we encounter a set of heterogeneous contents containing orderless images and videos. The importance of each image is usually considered either equal or based on their independent quality assessment. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in the latent space. Specifically, we first present a dependency-aware attention control (DAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision of each image embedding to fully exploit the rich correlation cues among the unordered images. Moreover, we introduce its sample-efficient variant with off-policy experience replay to speed up the learning process. The pose-guided representation scheme can further boost the performance at the extremes of the pose variation.",0
"The objective of this paper is to tackle the issue of face verification and identification utilizing sets of images. This differs from the traditional approach of using single media, as the sets are composed of a variety of images and videos without a specific order. The significance of each image is considered equal or based on their individual quality assessment, making it challenging to model the relationship between them. To address this challenge, we propose a Markov Decision Process (MDP) in the latent space. Our solution involves a dependency-aware attention control (DAC) network that uses actor-critic reinforcement learning to make sequential attention decisions for each image embedding, exploiting the rich correlation cues among the unordered images. We also introduce a sample-efficient variant with off-policy experience replay to speed up the learning process. Additionally, our pose-guided representation scheme can enhance performance for extreme variations in pose.",1
"Composing previously mastered skills to solve novel tasks promises dramatic improvements in the data efficiency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we extend an important generalization of policy improvement to the maximum entropy framework and introduce an algorithm for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which addresses the failure cases of prior work and, in principle, recovers the optimal policy during transfer. This method works by explicitly learning the (discounted, future) divergence between base policies. We study this approach in the tabular case and on non-trivial continuous control problems with compositional structure and show that it outperforms or matches existing methods across all tasks considered.",0
"To increase the data efficiency of reinforcement learning, it is beneficial to apply previously learned skills to new tasks. However, recent studies have shown that combining action-value functions to create new behaviors can result in poor performance in certain situations. To address this issue, we introduced an algorithm that implements successor features in continuous action spaces and extended policy improvement to the maximum entropy framework. Additionally, we proposed a new approach that explicitly learns the divergence between base policies to recover optimal policies during transfer. Our method outperforms existing techniques in tabular cases and complex continuous control problems with compositional structures.",1
"Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents. We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature. In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. As hypothesized, the system with adaptive components performed better on many of the new tasks.",0
"Numerous deep reinforcement learning algorithms have inherent biases that influence the agent's objective and its interaction with the environment. These biases may include domain expertise and pre-set hyper-parameters. The use of such biases can affect the performance and generality of the algorithms, with stronger biases resulting in faster learning but weaker biases potentially leading to more general algorithms. However, inductive biases require substantial effort to acquire relevant domain knowledge or tune hyper-parameters effectively. This study revisits the domain-specific components that bias the objective and environmental interface of common deep reinforcement learning agents, examining whether performance declines when replaced with adaptive solutions from the literature. The results demonstrate that the adaptive components sometimes outperformed the original components, while the main benefit of having fewer domain-specific components was observed in the improved learning performance of the adaptive system on a different set of continuous control problems.",1
"Sharing knowledge between tasks is vital for efficient learning in a multi-task setting. However, most research so far has focused on the easier case where knowledge transfer is not harmful, i.e., where knowledge from one task cannot negatively impact the performance on another task. In contrast, we present an approach to multi-task deep reinforcement learning based on attention that does not require any a-priori assumptions about the relationships between tasks. Our attention network automatically groups task knowledge into sub-networks on a state level granularity. It thereby achieves positive knowledge transfer if possible, and avoids negative transfer in cases where tasks interfere. We test our algorithm against two state-of-the-art multi-task/transfer learning approaches and show comparable or superior performance while requiring fewer network parameters.",0
"Efficient learning in a multi-task environment depends on the exchange of knowledge between tasks. However, previous research has mainly focused on cases where knowledge transfer does not have a detrimental effect, meaning that knowledge from one task does not negatively affect the performance of another task. Conversely, we propose an attention-based approach to multi-task deep reinforcement learning that does not rely on any prior assumptions about the relationships between tasks. Our attention network automatically categorizes task knowledge into sub-networks at the state level, promoting positive knowledge transfer while avoiding negative transfer in instances where tasks interfere. We evaluate our algorithm against two state-of-the-art multi-task/transfer learning approaches and demonstrate comparable or superior performance while utilizing fewer network parameters.",1
"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",0
"Our approach involves applying the principles that have led to the success of Deep Q-Learning in order to address continuous action scenarios. To this end, we have developed an actor-critic, model-free algorithm that utilizes the deterministic policy gradient to operate in continuous action spaces. By employing the same learning algorithm, network architecture, and hyper-parameters, our method has been shown to effectively solve more than 20 simulated physics problems, including traditionally challenging tasks such as cartpole swing-up, dexterous manipulation, legged locomotion, and car driving. Moreover, our approach is capable of generating policies that are competitive with those produced by planning algorithms that have complete access to the dynamics of the domain and its derivatives. Additionally, we have demonstrated that our algorithm has the ability to learn policies end-to-end, directly from raw pixel inputs, for many of the tasks.",1
"Multiagent reinforcement learning (MARL) is commonly considered to suffer from non-stationary environments and exponentially increasing policy space. It would be even more challenging when rewards are sparse and delayed over long trajectories. In this paper, we study hierarchical deep MARL in cooperative multiagent problems with sparse and delayed reward. With temporal abstraction, we decompose the problem into a hierarchy of different time scales and investigate how agents can learn high-level coordination based on the independent skills learned at the low level. Three hierarchical deep MARL architectures are proposed to learn hierarchical policies under different MARL paradigms. Besides, we propose a new experience replay mechanism to alleviate the issue of the sparse transitions at the high level of abstraction and the non-stationarity of multiagent learning. We empirically demonstrate the effectiveness of our approaches in two domains with extremely sparse feedback: (1) a variety of Multiagent Trash Collection tasks, and (2) a challenging online mobile game, i.e., Fever Basketball Defense.",0
"MARL is known to face challenges such as non-stationary environments and exponentially increasing policy space. These difficulties become even more pronounced in situations where rewards are scarce and delayed over long periods. In this study, we explore hierarchical deep MARL in cooperative multiagent scenarios that involve sparse and delayed reward. Our approach involves decomposing the problem into a hierarchy of different time scales to enable high-level coordination based on the independent skills learned at the low level. We propose three hierarchical deep MARL architectures to learn hierarchical policies under different MARL paradigms. Moreover, we propose a new experience replay mechanism to mitigate the impact of sparse transitions at the high level of abstraction and the non-stationarity of multiagent learning. Our empirical results demonstrate the efficacy of our approaches in two domains with extremely sparse feedback: (1) various Multiagent Trash Collection tasks, and (2) a challenging online mobile game, Fever Basketball Defense.",1
"Dyna is an architecture for model-based reinforcement learning (RL), where simulated experience from a model is used to update policies or value functions. A key component of Dyna is search-control, the mechanism to generate the state and action from which the agent queries the model, which remains largely unexplored. In this work, we propose to generate such states by using the trajectory obtained from Hill Climbing (HC) the current estimate of the value function. This has the effect of propagating value from high-value regions and of preemptively updating value estimates of the regions that the agent is likely to visit next. We derive a noisy projected natural gradient algorithm for hill climbing, and highlight a connection to Langevin dynamics. We provide an empirical demonstration on four classical domains that our algorithm, HC-Dyna, can obtain significant sample efficiency improvements. We study the properties of different sampling distributions for search-control, and find that there appears to be a benefit specifically from using the samples generated by climbing on current value estimates from low-value to high-value region.",0
"Dyna is a model-based reinforcement learning architecture that updates policies or value functions using simulated experience from a model. However, the mechanism for generating the state and action from which the agent queries the model, known as search-control, has not been thoroughly explored. To address this, we propose using the trajectory obtained from Hill Climbing (HC) the current estimate of the value function to generate these states. This approach propagates value from high-value regions and preemptively updates value estimates of regions likely to be visited next. We introduce a noisy projected natural gradient algorithm for hill climbing, which has a connection to Langevin dynamics. Empirical results on four classical domains show that our HC-Dyna algorithm significantly improves sample efficiency. Additionally, we investigate different sampling distributions for search-control and find that using samples generated by climbing on current value estimates from low-value to high-value regions provides a benefit.",1
"Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .",0
"Although Deep RL has achieved remarkable success in learning control policies from raw pixels, these models lack generalization abilities. Our research proves that trained agents are unable to handle slight visual changes, and that fine-tuning, the typical transfer learning method, fails to adapt to those modifications. It is more effective to retrain the model from scratch than to attempt fine-tuning. By separating the visual transfer task from the control policy, we have improved sample efficiency and transfer behavior. This allows the agent trained on the source task to transfer well to target tasks. We utilize unaligned GANs to perform the visual mapping from the target to the source domain, resulting in a better control policy that can be further enhanced using imitation learning from imperfect demonstrations. Our approach has been successfully tested on synthetic visual variants of the Breakout game and transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. For a demonstration of our approach, please visit https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo.",1
"Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html.",0
"Model-based reinforcement learning (MBRL) has the potential to be much more efficient than model-free RL, but its research lacks standardization. Authors often create their own environments, and there are multiple closed-source or non-reproducible lines of research. It is unclear how existing MBRL algorithms compare to each other. To address this, we have gathered a collection of MBRL algorithms and designed over 18 benchmarking environments specifically for MBRL. We have tested these algorithms with consistent problem settings, including noisy environments. Our study not only evaluates performance but also identifies and unifies underlying algorithmic differences among MBRL algorithms. We outline three critical research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. To promote future MBRL research, we have made our benchmark open-source at http://www.cs.toronto.edu/~tingwuwang/mbrl.html.",1
"We study the problem of learning sequential decision-making policies in settings with multiple state-action representations. Such settings naturally arise in many domains, such as planning (e.g., multiple integer programming formulations) and various combinatorial optimization problems (e.g., those with both integer programming and graph-based formulations). Inspired by the classical co-training framework for classification, we study the problem of co-training for policy learning. We present sufficient conditions under which learning from two views can improve upon learning from a single view alone. Motivated by these theoretical insights, we present a meta-algorithm for co-training for sequential decision making. Our framework is compatible with both reinforcement learning and imitation learning. We validate the effectiveness of our approach across a wide range of tasks, including discrete/continuous control and combinatorial optimization.",0
"Our focus is on acquiring sequential decision-making policies in situations that have multiple state-action representations. These circumstances are common in various domains, such as planning, which may have several integer programming formulations, and combinatorial optimization problems that involve both graph-based and integer programming formulations. Our approach is inspired by the co-training framework used in classification. We aim to establish the optimal conditions for learning from two perspectives to improve the performance of learning from a single perspective. Our theoretical insights motivate the development of a co-training meta-algorithm for sequential decision-making that is compatible with both reinforcement and imitation learning. We demonstrate the efficacy of our method in a broad range of tasks, including discrete/continuous control and combinatorial optimization.",1
"Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training, allowing them to learn about every possible danger, but this is often impractical. This paper investigates safety and generalization from a limited number of training environments in deep reinforcement learning (RL). We find RL algorithms can fail dangerously on unseen test environments even when performing perfectly on training environments. Firstly, in a gridworld setting, we show that catastrophes can be significantly reduced with simple modifications, including ensemble model averaging and the use of a blocking classifier. In the more challenging CoinRun environment we find similar methods do not significantly reduce catastrophes. However, we do find that the uncertainty information from the ensemble is useful for predicting whether a catastrophe will occur within a few steps and hence whether human intervention should be requested.",0
"Prior to implementing autonomous agents in real-world situations, it is essential to ensure their safe performance in unfamiliar scenarios. While it would be ideal to expose agents to a wide range of situations during training to acquaint them with all possible risks, this is often impractical. This study examines safety and generalization in deep reinforcement learning (RL) with a limited number of training environments. Even when performing perfectly in training environments, RL algorithms can pose dangerous risks in unseen test environments. Our investigation indicates that by implementing simple modifications such as ensemble model averaging and the use of a blocking classifier, catastrophes can be significantly reduced in a gridworld setting. However, in the more challenging CoinRun environment, such techniques do not make a significant difference. Nevertheless, the ensemble's uncertainty information is beneficial in predicting the likelihood of a catastrophe within a few steps, and therefore, whether human intervention is necessary.",1
"Training a robotic arm to accomplish real-world tasks has been attracting increasing attention in both academia and industry. This work discusses the role of computer vision algorithms in this field. We focus on low-cost arms on which no sensors are equipped and thus all decisions are made upon visual recognition, e.g., real-time 3D pose estimation. This requires annotating a lot of training data, which is not only time-consuming but also laborious.   In this paper, we present an alternative solution, which uses a 3D model to create a large number of synthetic data, trains a vision model in this virtual domain, and applies it to real-world images after domain adaptation. To this end, we design a semi-supervised approach, which fully leverages the geometric constraints among keypoints. We apply an iterative algorithm for optimization. Without any annotations on real images, our algorithm generalizes well and produces satisfying results on 3D pose estimation, which is evaluated on two real-world datasets. We also construct a vision-based control system for task accomplishment, for which we train a reinforcement learning agent in a virtual environment and apply it to the real-world. Moreover, our approach, with merely a 3D model being required, has the potential to generalize to other types of multi-rigid-body dynamic systems.",0
"Recently, there has been an increasing interest in training robotic arms to perform real-world tasks, both in academia and industry. This article discusses the importance of computer vision algorithms in this field, particularly for low-cost arms without sensors that rely solely on visual recognition, such as real-time 3D pose estimation. However, annotating training data for this is not only time-consuming but also labor-intensive. To address this, the authors propose an alternative approach that uses a 3D model to generate synthetic data for training a vision model in a virtual environment. They then apply this model to real-world images using domain adaptation and a semi-supervised approach that takes into account geometric constraints among keypoints. The authors demonstrate the effectiveness of their method on 3D pose estimation and use it to construct a vision-based control system for task completion. Their approach, which only requires a 3D model, has the potential to generalize to other types of multi-rigid-body dynamic systems.",1
"There is a growing desire in the field of reinforcement learning (and machine learning in general) to move from black-box models toward more ""interpretable AI."" We improve interpretability of reinforcement learning by increasing the utility of decision tree policies learned via reinforcement learning. These policies consist of a decision tree over the state space, which requires fewer parameters to express than traditional policy representations. Existing methods for creating decision tree policies via reinforcement learning focus on accurately representing an action-value function during training, but this leads to much larger trees than would otherwise be required. To address this shortcoming, we propose a novel algorithm which only increases tree size when the estimated discounted future reward of the overall policy would increase by a sufficient amount. Through evaluation in a simulated environment, we show that its performance is comparable or superior to traditional tree-based approaches and that it yields a more succinct policy. Additionally, we discuss tuning parameters to control the tradeoff between optimizing for smaller tree size or for overall reward.",0
"The field of reinforcement learning (and machine learning in general) is increasingly interested in creating AI models that are more interpretable than black-box models. One way to achieve this is by enhancing the interpretability of decision tree policies learned through reinforcement learning. Decision tree policies are simpler than traditional policy representations, as they consist of a decision tree over the state space, and require fewer parameters to express. However, existing methods for creating these policies result in larger trees than necessary due to their focus on accurately representing an action-value function during training. To overcome this problem, we introduce a novel algorithm that enlarges the tree size only when the estimated discounted future reward of the overall policy is expected to increase significantly. We demonstrate through a simulated environment that our approach performs as well as or better than traditional tree-based techniques while yielding a more concise policy. Moreover, we explore tuning parameters to balance the tradeoff between optimizing for smaller tree size versus overall reward.",1
"In many real-world scenarios, an autonomous agent often encounters various tasks within a single complex environment. We propose to build a graph abstraction over the environment structure to accelerate the learning of these tasks. Here, nodes are important points of interest (pivotal states) and edges represent feasible traversals between them. Our approach has two stages. First, we jointly train a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic manner. Second, provided with the information from the world graph, a high-level Manager quickly finds solution to new tasks and expresses subgoals in reference to pivotal states to a low-level Worker. The Worker can then also leverage the graph to easily traverse to the pivotal states of interest, even across long distance, and explore non-locally. We perform a thorough ablation study to evaluate our approach on a suite of challenging maze tasks, demonstrating significant advantages from the proposed framework over baselines that lack world graph knowledge in terms of performance and efficiency.",0
"When an autonomous agent faces multiple tasks in a complex environment, it can be helpful to create a graph abstraction of the environment's structure to speed up task learning. This graph has nodes representing pivotal states and edges that show possible connections between them. Our two-stage approach starts by training a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic way. Then, using the world graph information, a high-level Manager can quickly find solutions to new tasks and communicate subgoals to a low-level Worker based on pivotal states. With the graph, the Worker can easily traverse to pivotal states of interest, even far away, and explore non-locally. We tested our approach on challenging maze tasks and found it significantly outperformed baselines lacking world graph knowledge in terms of efficiency and performance.",1
"We consider the exploration-exploitation trade-off in reinforcement learning and we show that an agent imbued with an epistemic-risk-seeking utility function is able to explore efficiently, as measured by regret. The parameter that controls how risk-seeking the agent is can be optimized to minimize regret, or annealed according to a schedule. We call the resulting algorithm K-learning and we show that the K-values that the agent maintains are optimistic for the expected optimal Q-values at each state-action pair. The utility function approach induces a natural Boltzmann exploration policy for which the 'temperature' parameter is equal to the risk-seeking parameter. This policy achieves a Bayesian regret bound of $\tilde O(L^{3/2} \sqrt{SAT})$, where L is the time horizon, S is the number of states, A is the number of actions, and T is the total number of elapsed time-steps. K-learning can be interpreted as mirror descent in the policy space, and it is similar to other well-known methods in the literature, including Q-learning, soft-Q-learning, and maximum entropy policy gradient. K-learning is simple to implement, as it only requires adding a bonus to the reward at each state-action and then solving a Bellman equation. We conclude with a numerical example demonstrating that K-learning is competitive with other state-of-the-art algorithms in practice.",0
"The exploration-exploitation trade-off in reinforcement learning is examined in this study, and it is demonstrated that an agent equipped with a utility function that seeks epistemic risk can effectively explore, as indicated by regret. The degree of risk-seeking behavior exhibited by the agent can be optimized to minimize regret or modified according to a predetermined schedule. The resultant algorithm, K-learning, maintains optimistic K-values for the anticipated optimal Q-values at every state-action pair. The utility function approach leads to a natural Boltzmann exploration policy, in which the temperature parameter is identical to the risk-seeking parameter. This policy achieves a Bayesian regret bound of approximately O(L^{3/2} sqrt{SAT}), where L represents the time horizon, S represents the number of states, A represents the number of actions, and T represents the total number of elapsed time-steps. K-learning can be interpreted as mirror descent in policy space and is comparable to other commonly used techniques in the literature such as Q-learning, soft-Q-learning, and maximum entropy policy gradient. K-learning is straightforward to implement by adding a bonus to the reward at each state-action and then solving a Bellman equation. Finally, a numerical example is presented, demonstrating that K-learning performs well in practice and is competitive with other advanced algorithms.",1
"Traditionally, machine learning algorithms rely on the assumption that all features of a given dataset are available for free. However, there are many concerns such as monetary data collection costs, patient discomfort in medical procedures, and privacy impacts of data collection that require careful consideration in any real-world health analytics system. An efficient solution would only acquire a subset of features based on the value it provides while considering acquisition costs. Moreover, datasets that provide feature costs are very limited, especially in healthcare. In this paper, we provide a health dataset as well as a method for assigning feature costs based on the total level of inconvenience asking for each feature entails. Furthermore, based on the suggested dataset, we provide a comparison of recent and state-of-the-art approaches to cost-sensitive feature acquisition and learning. Specifically, we analyze the performance of major sensitivity-based and reinforcement learning based methods in the literature on three different problems in the health domain, including diabetes, heart disease, and hypertension classification.",0
"In the conventional sense, machine learning algorithms assume that all features of a given dataset are readily available. However, there are numerous concerns, including the high cost of data collection, patient discomfort during medical procedures, and privacy implications of data collection, that require careful consideration in any health analytics system. To address this issue, an efficient solution would be to acquire only a subset of features based on their value, while considering acquisition costs. Nevertheless, datasets that provide feature costs are scarce, particularly in healthcare. This paper presents a health dataset and a method for assigning feature costs based on the total level of inconvenience associated with requesting each feature. Moreover, using the suggested dataset, a comparison of recent and state-of-the-art approaches to cost-sensitive feature acquisition and learning is provided. Specifically, the performance of major sensitivity-based and reinforcement learning-based methods in the literature is analyzed on three different health problems, including diabetes, heart disease, and hypertension classification.",1
"Current reinforcement learning methods fail if the reward function is imperfect, i.e. if the agent observes reward different from what it actually receives. We study this problem within the formalism of Corrupt Reward Markov Decision Processes (CRMDPs). We show that if the reward corruption in a CRMDP is sufficiently ""spiky"", the environment is solvable. We fully characterize the regret bound of a Spiky CRMDP, and introduce an algorithm that is able to detect its corrupt states. We show that this algorithm can be used to learn the optimal policy with any common reinforcement learning algorithm. Finally, we investigate our algorithm in a pair of simple gridworld environments, finding that our algorithm can detect the corrupt states and learn the optimal policy despite the corruption.",0
"Reinforcement learning methods are insufficient when there are flaws in the reward function, meaning the agent perceives a different reward than what it actually receives. Our research focuses on Corrupt Reward Markov Decision Processes (CRMDPs) to address this issue. We discovered that if the reward corruption in a CRMDP is ""spiky"" enough, the environment can be solved. We established the regret bound of a Spiky CRMDP and created an algorithm that can identify corrupt states. This algorithm can be utilized with any common reinforcement learning algorithm to learn the optimal policy. We tested our algorithm in basic gridworld environments and found that it can successfully identify corrupt states and learn the optimal policy despite the corruption.",1
"Sequence prediction models can be learned from example sequences with a variety of training algorithms. Maximum likelihood learning is simple and efficient, yet can suffer from compounding error at test time. Reinforcement learning such as policy gradient addresses the issue but can have prohibitively poor exploration efficiency. A rich set of other algorithms such as RAML, SPG, and data noising, have also been developed from different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently distinct algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of a reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, inspired from the framework, we present a new algorithm that dynamically interpolates among the family of algorithms for scheduled sequence model learning. Experiments on machine translation, text summarization, and game imitation learning demonstrate the superiority of the proposed algorithm.",0
"Various training algorithms can be utilized to learn sequence prediction models from example sequences. While maximum likelihood learning is uncomplicated and effective, it may encounter compounding errors during testing. Reinforcement learning, such as policy gradient, addresses the problem but may not explore efficiently. Other algorithms, including RAML, SPG, and data noising, have been developed from different perspectives. This paper establishes a formal connection between these algorithms by presenting a generalized entropy regularized policy optimization formulation. The study shows that all the apparently distinct algorithms can be reformed as special instances of the framework, with the only difference being the reward function configurations and a couple of hyperparameters. This unified interpretation provides a systematic view of the varying exploration and learning efficiency properties. Additionally, a new algorithm that dynamically interpolates among the family of algorithms for scheduled sequence model learning is presented, inspired by the framework. Experiments on machine translation, text summarization, and game imitation learning demonstrate the superiority of the proposed algorithm.",1
"In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to accelerate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data, value estimates, and state representations from restricted action spaces to the full task. We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.",0
"Random exploration may not be effective in achieving significant progress in learning for complex tasks, particularly those with extensive combinatorial action spaces. To expedite learning, we implement a curriculum that involves action spaces that gradually increase in complexity. Although we cannot control the environment, the agent can set an internal curriculum by initially limiting its action space. Our off-policy reinforcement learning approach allows for the estimation of optimal value functions for multiple action spaces simultaneously, with efficient transfer of data, value estimates, and state representations from restricted to full action spaces. We demonstrate the effectiveness of our approach in proof-of-concept control tasks and challenging StarCraft micromanagement tasks with large, multi-agent action spaces.",1
"Temporal point process is an expressive tool for modeling event sequences over time. In this paper, we take a reinforcement learning view whereby the observed sequences are assumed to be generated from a mixture of latent policies. The purpose is to cluster the sequences with different temporal patterns into the underlying policies while learning each of the policy model. The flexibility of our model lies in: i) all the components are networks including the policy network for modeling the intensity function of temporal point process; ii) to handle varying-length event sequences, we resort to inverse reinforcement learning by decomposing the observed sequence into states (RNN hidden embedding of history) and actions (time interval to next event) in order to learn the reward function, thus achieving better performance or increasing efficiency compared to existing methods using rewards over the entire sequence such as log-likelihood or Wasserstein distance. We adopt an expectation-maximization framework with the E-step estimating the cluster labels for each sequence, and the M-step aiming to learn the respective policy. Extensive experiments show the efficacy of our method against state-of-the-arts.",0
"In this paper, we utilize temporal point process to model event sequences over time. Our approach is based on reinforcement learning, where we assume that observed sequences are generated from a mixture of latent policies. Our goal is to cluster sequences with diverse temporal patterns into underlying policies while simultaneously learning each policy model. The key feature of our model is that all components, including the policy network for modeling the intensity function of temporal point process, are networks. To accommodate varying-length event sequences, we utilize inverse reinforcement learning by breaking down the observed sequence into states (RNN hidden embedding of history) and actions (time interval to next event) in order to learn the reward function. This approach leads to better performance and increased efficiency compared to existing methods that use rewards over the entire sequence, such as log-likelihood or Wasserstein distance. We employ an expectation-maximization framework with the E-step to estimate the cluster labels for each sequence, and the M-step to learn the respective policy. Our extensive experiments demonstrate the effectiveness of our method compared to state-of-the-art techniques.",1
"A simple, flexible approach to creating expressive priors in Gaussian process (GP) models makes new kernels from a combination of basic kernels, e.g. summing a periodic and linear kernel can capture seasonal variation with a long term trend. Despite a well-studied link between GPs and Bayesian neural networks (BNNs), the BNN analogue of this has not yet been explored. This paper derives BNN architectures mirroring such kernel combinations. Furthermore, it shows how BNNs can produce periodic kernels, which are often useful in this context. These ideas provide a principled approach to designing BNNs that incorporate prior knowledge about a function. We showcase the practical value of these ideas with illustrative experiments in supervised and reinforcement learning settings.",0
"A Gaussian process (GP) model can utilize a straightforward and adaptable technique for generating expressive priors by forming novel kernels from basic kernels, such as combining a linear kernel with a periodic kernel to capture long-term trends and seasonal variations. Although there exists a well-established connection between GPs and Bayesian neural networks (BNNs), the BNN equivalent of this approach has yet to be explored. This study presents BNN architectures that mirror such kernel combinations and demonstrates how BNNs can produce periodic kernels, which are frequently advantageous in this situation. These concepts establish a systematic method for designing BNNs that integrate previous knowledge of a function. To demonstrate the practical significance of these concepts, we conduct illustrative experiments in supervised and reinforcement learning settings.",1
"The security of Deep Reinforcement Learning (Deep RL) algorithms deployed in real life applications are of a primary concern. In particular, the robustness of RL agents in cyber-physical systems against adversarial attacks are especially vital since the cost of a malevolent intrusions can be extremely high. Studies have shown Deep Neural Networks (DNN), which forms the core decision-making unit in most modern RL algorithms, are easily subjected to adversarial attacks. Hence, it is imperative that RL agents deployed in real-life applications have the capability to detect and mitigate adversarial attacks in an online fashion. An example of such a framework is the Meta-Learned Advantage Hierarchy (MLAH) agent that utilizes a meta-learning framework to learn policies robustly online. Since the mechanism of this framework are still not fully explored, we conducted multiple experiments to better understand the framework's capabilities and limitations. Our results shows that the MLAH agent exhibits interesting coping behaviors when subjected to different adversarial attacks to maintain a nominal reward. Additionally, the framework exhibits a hierarchical coping capability, based on the adaptability of the Master policy and sub-policies themselves. From empirical results, we also observed that as the interval of adversarial attacks increase, the MLAH agent can maintain a higher distribution of rewards, though at the cost of higher instabilities.",0
"Ensuring the security of Deep Reinforcement Learning (Deep RL) algorithms used in real-life applications is crucial, particularly in cyber-physical systems where adversarial attacks can be very costly. Studies reveal that Deep Neural Networks (DNNs), which are the core decision-making units in modern RL algorithms, are highly vulnerable to such attacks. Therefore, it is essential for RL agents in real-life applications to be able to detect and mitigate adversarial attacks online. The Meta-Learned Advantage Hierarchy (MLAH) agent is an example of such a framework that uses a meta-learning approach to learn robust policies online. We conducted several experiments to explore the capabilities and limitations of the MLAH agent, and our findings show that it exhibits interesting coping behaviors when faced with various adversarial attacks to maintain nominal rewards. Furthermore, the framework has a hierarchical coping ability, which depends on both the adaptability of the Master policy and sub-policies. Our empirical results also demonstrate that as the interval of adversarial attacks increases, the MLAH agent can maintain a higher distribution of rewards, but at the cost of greater instability.",1
"Advances in renewable energy generation and introduction of the government targets to improve energy efficiency gave rise to a concept of a Zero Energy Building (ZEB). A ZEB is a building whose net energy usage over a year is zero, i.e., its energy use is not larger than its overall renewables generation. A collection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses the problem of energy sharing in such a community. This is different from previously addressed energy sharing between buildings as our focus is on the improvement of community energy status, while traditionally research focused on reducing losses due to transmission and storage, or achieving economic gains. We model this problem in a multi-agent environment and propose a Deep Reinforcement Learning (DRL) based solution. Each building is represented by an intelligent agent that learns over time the appropriate behaviour to share energy. We have evaluated the proposed solution in a multi-agent simulation built using osBrain. Results indicate that with time agents learn to collaborate and learn a policy comparable to the optimal policy, which in turn improves the ZEC's energy status. Buildings with no renewables preferred to request energy from their neighbours rather than from the supply grid.",0
"The emergence of the Zero Energy Building (ZEB) concept was driven by advancements in renewable energy generation and the government's push to enhance energy efficiency. ZEBs are buildings that consume only as much energy as they generate through renewables over a year, and a group of such structures constitutes a Zero Energy Community (ZEC). In this paper, we tackle the issue of energy sharing within a ZEC. Unlike previous research that focused on minimizing transmission and storage losses or achieving economic gains, our focus is on improving the community's energy status. We propose a Deep Reinforcement Learning (DRL) based solution, where each building is represented by an intelligent agent that learns how to share energy with other agents in a multi-agent environment. Our evaluation, conducted using osBrain, reveals that the agents gradually learn to collaborate and adopt a policy that enhances the ZEC's energy status, with non-renewable buildings preferring to obtain energy from their neighbors rather than the supply grid.",1
"Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model. Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO).   In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning.",0
"The process of hyperparameter tuning is an essential component of achieving optimal performance in machine learning models. Traditionally, hyperparameters are optimized by training models using a grid of potential values and selecting the option that performs best on a validation sample (known as grid search). More recently, a surrogate model approach has been introduced, which predicts the validation loss for specific hyperparameters, models, and datasets. This method sequentially selects the next hyperparameter to test using an acquisition function based on the expected value and uncertainty of the surrogate model (known as sequential model-based Bayesian optimization, SMBO). In this study, we propose a reinforcement learning approach to model hyperparameter optimization as a sequential decision problem. This method eliminates the need for a heuristic acquisition function like SMBO, as the model learns which hyperparameters to test based on their potential for reducing validation loss. Our experiments using 50 datasets demonstrate that our method outperforms existing approaches to hyperparameter learning.",1
"Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedbacks. In particular, we introduce an online user-agent interacting environment simulator, which can pre-train and evaluate model parameters offline before applying the model online. Moreover, we validate the importance of list-wise recommendations during the interactions between users and agent, and develop a novel approach to incorporate them into the proposed framework LIRD for list-wide recommendations. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.",0
"The problem of information overload can be addressed by recommenders that suggest personalized items or services to users. However, most traditional systems have fixed strategies for making recommendations. This paper proposes a recommender system that continuously improves its strategies through interactions with users. The system models these interactions as a Markov Decision Process and uses Reinforcement Learning to learn optimal strategies by recommending trial-and-error items and receiving feedback from users. The proposed framework includes an online user-agent simulator for offline pre-training and evaluation of model parameters. The system also incorporates list-wise recommendations and introduces a novel approach for list-wide recommendations. The proposed framework is validated on a real-world e-commerce dataset, demonstrating its effectiveness.",1
"Standard computer vision systems assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is a major challenge in itself. We address the problem of learning to look around: how can an agent learn to acquire informative visual observations? We propose a reinforcement learning solution, where the agent is rewarded for reducing its uncertainty about the unobserved portions of its environment. Specifically, the agent is trained to select a short sequence of glimpses after which it must infer the appearance of its full environment. To address the challenge of sparse rewards, we further introduce sidekick policy learning, which exploits the asymmetry in observability between training and test time. The proposed methods learn observation policies that not only perform the completion task for which they are trained, but also generalize to exhibit useful ""look-around"" behavior for a range of active perception tasks.",0
"Computer vision systems typically rely on well-captured inputs, such as photographs taken by a skilled photographer, but capturing good observations independently is a significant challenge. Our focus is on solving the problem of learning how to observe effectively. We suggest a reinforcement learning approach where the agent is incentivized to minimize its uncertainty about unobserved areas in its environment. The agent is trained to choose a brief sequence of glimpses that will allow it to deduce the appearance of its surroundings. We address the difficulty of insufficient rewards by introducing sidekick policy learning, which utilizes the asymmetry in observability between training and testing. Our methods teach observation policies that not only meet the completion task but also demonstrate valuable ""look-around"" behavior for a variety of active perception tasks.",1
"We consider the finite horizon continuous reinforcement learning problem. Our contribution is three-fold. First,we give a tractable algorithm based on optimistic value iteration for the problem. Next,we give a lower bound on regret of order $\Omega(T^{2/3})$ for any algorithm discretizes the state space, improving the previous regret bound of $\Omega(T^{1/2})$ of Ortner and Ryabko \cite{contrl} for the same problem. Next,under the assumption that the rewards and transitions are H\""{o}lder Continuous we show that the upper bound on the discretization error is $const.Ln^{-\alpha}T$. Finally,we give some simple experiments to validate our propositions.",0
"The problem we are addressing is the finite horizon continuous reinforcement learning problem. Our contribution consists of three parts. Firstly, we provide a feasible algorithm utilizing optimistic value iteration to solve the problem. Secondly, we improve the regret bound for any algorithm that discretizes the state space from Ortner and Ryabko's previous bound of $\Omega(T^{1/2})$ to a new, lower bound of $\Omega(T^{2/3})$. Thirdly, assuming that the rewards and transitions are H\""{o}lder Continuous, we demonstrate that the discretization error's upper bound is $const.Ln^{-\alpha}T$. Lastly, we conduct some straightforward experiments to support our arguments.",1
"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",0
"As reinforcement learning (RL) continues to tackle more complex tasks, the interest in AI safety and machine ethics also grows. This paper proposes an extension to Deep Q-Networks (DQNs), called Empathic DQN, as a contribution to these fields. Empathic DQN is inspired by empathy and the golden rule, aiming to prevent negative side effects resulting from goal-directed behavior of a learning agent in a setting where it coexists with other independent agents. Empathic DQN combines the self-centered value with the estimated value of other agents by imagining the value of it being in the other's situation. Two gridworld environments show the approach's potential to decrease collateral harms, and although extending it to complex environments is challenging, this first step showcases the potential of bridging machine ethics and RL to provide useful priors for norm-abiding RL agents.",1
"We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.",0
"A new viewpoint is proposed for representation learning in reinforcement learning by analyzing the geometric characteristics of the value function space. This perspective is utilized to offer formal proof of the value functions' effectiveness as auxiliary tasks. Our approach involves modifying the representation to minimize the linear approximation of the value function for all stationary policies in a given environment. We prove that this optimization can be achieved by accurately predicting a specific type of value function called adversarial value functions (AVFs). Additionally, we demonstrate that using value functions as auxiliary tasks is a relaxation of our formulation that reduces expected error, with AVFs being a natural option. We also identify a correlation between AVFs and proto-value functions (Mahadevan, 2005). Finally, using the four-room domain, we conduct a series of experiments that highlight the features of AVFs and their importance as auxiliary tasks.",1
"Hierarchy in reinforcement learning agents allows for control at multiple time scales yielding improved sample efficiency, the ability to deal with long time horizons and transferability of sub-policies to tasks outside the training distribution. It is often implemented as a master policy providing goals to a sub-policy. Ideally, we would like the goal-spaces to be learned, however, properties of optimal goal spaces still remain unknown and consequently there is no method yet to learn optimal goal spaces. Motivated by this, we systematically analyze how various modifications to the ground-truth goal-space affect learning in hierarchical models with the aim of identifying important properties of optimal goal spaces. Our results show that, while rotation of ground-truth goal spaces and noise had no effect, having additional unnecessary factors significantly impaired learning in hierarchical models.",0
"Reinforcement learning agents that implement hierarchy allow for control at multiple time scales, resulting in improved sample efficiency, the ability to handle long-term goals, and the transferability of sub-policies to tasks outside of the training distribution. Typically, a master policy is utilized to provide goals to a sub-policy. Ideally, we would like to learn the goal-spaces, but since the properties of optimal goal spaces are still unknown, there is no current method to do so. To address this issue, we conducted a systematic analysis of how modifications to the ground-truth goal-space affect learning in hierarchical models. Our findings indicate that rotating the ground-truth goal spaces and adding noise had no effect, but including unnecessary factors significantly impaired learning in hierarchical models.",1
"Model-based reinforcement learning has the potential to be more sample efficient than model-free approaches. However, existing model-based methods are vulnerable to model bias, which leads to poor generalization and asymptotic performance compared to model-free counterparts. In addition, they are typically based on the model predictive control (MPC) framework, which not only is computationally inefficient at decision time but also does not enable policy transfer due to the lack of an explicit policy representation. In this paper, we propose a novel uncertainty-aware model-based policy optimization framework which solves those issues. In this framework, the agent simultaneously learns an uncertainty-aware dynamics model and optimizes the policy according to these learned models. In the optimization step, the policy gradient is computed by automatic differentiation through the models. With respect to sample efficiency alone, our approach shows promising results on challenging continuous control benchmarks with competitive asymptotic performance and significantly lower sample complexity than state-of-the-art baselines.",0
"The potential for greater sample efficiency exists with model-based reinforcement learning in comparison to model-free approaches. However, current model-based methods are susceptible to model bias, leading to inferior generalization and asymptotic performance when compared to model-free counterparts. Moreover, these methods typically rely on the computationally inefficient model predictive control (MPC) framework and lack an explicit policy representation, which hinders policy transfer. In this study, we introduce a new uncertainty-aware model-based policy optimization framework that addresses these limitations. Our framework allows the agent to learn an uncertainty-aware dynamics model while simultaneously optimizing the policy based on these models. We compute the policy gradient using automatic differentiation through the models during the optimization step. Our approach demonstrates promising results on challenging continuous control benchmarks in terms of sample efficiency alone, exhibiting competitive asymptotic performance and significantly lower sample complexity than current state-of-the-art baselines.",1
"Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.",0
"Agents using reinforcement learning in complex and varied environments can benefit from structured behavior decomposition. This is typically accomplished through hierarchical reinforcement learning, which breaks down policies into lower-level primitives or options and a higher-level meta-policy that chooses appropriate behaviors for specific situations. However, the meta-policy must still make appropriate decisions in all states. This study proposes a policy design that breaks down into primitives like hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide whether to act in the current state independently. An information-theoretic mechanism is used to enable this decentralized decision-making, with each primitive determining the amount of information needed to make a decision. The primitive requiring the most information acts in the world. The primitives are incentivized to use as little information as possible, leading to natural competition and specialization. This policy architecture is experimentally shown to improve generalization compared to both flat and hierarchical policies.",1
"Reinforcement Learning, a machine learning framework for training an autonomous agent based on rewards, has shown outstanding results in various domains. However, it is known that learning a good policy is difficult in a domain where rewards are rare. We propose a method, optimistic proximal policy optimization (OPPO) to alleviate this difficulty. OPPO considers the uncertainty of the estimated total return and optimistically evaluates the policy based on that amount. We show that OPPO outperforms the existing methods in a tabular task.",0
"Various domains have witnessed the exceptional performance of Reinforcement Learning, which is a machine learning framework that trains an independent agent on rewards. Nonetheless, acquiring a good policy is challenging in a domain that lacks rewards. Therefore, we suggest a solution called optimistic proximal policy optimization (OPPO) that can ease this hurdle. OPPO assesses the policy based on the estimated total return's uncertainty and optimistically evaluates it. Our study indicates that OPPO surpasses the current methods in a tabular task.",1
"Motivated by the study of $Q$-learning algorithms in reinforcement learning, we study a class of stochastic approximation procedures based on operators that satisfy monotonicity and quasi-contractivity conditions with respect to an underlying cone. We prove a general sandwich relation on the iterate error at each time, and use it to derive non-asymptotic bounds on the error in terms of a cone-induced gauge norm. These results are derived within a deterministic framework, requiring no assumptions on the noise. We illustrate these general bounds in application to synchronous $Q$-learning for discounted Markov decision processes with discrete state-action spaces, in particular by deriving non-asymptotic bounds on the $\ell_\infty$-norm for a range of stepsizes. These results are the sharpest known to date, and we show via simulation that the dependence of our bounds cannot be improved in a worst-case sense. These results show that relative to a model-based $Q$-iteration, the $\ell_\infty$-based sample complexity of $Q$-learning is suboptimal in terms of the discount factor $\gamma$.",0
"Our focus is on a category of stochastic approximation methods that build on operators satisfying monotonicity and quasi-contractivity conditions related to a base cone. Our interest in investigating these methods stems from their association with $Q$-learning algorithms utilized in reinforcement learning. We offer a general sandwich relation on the error at each iteration, which we use to deduce non-asymptotic error bounds in relation to a gauge norm generated by the underlying cone. Our approach is deterministic, without any assumptions about the noise. We apply the derived results to synchronous $Q$-learning for discounted Markov decision processes with discrete state-action spaces. Specifically, we establish non-asymptotic bounds on the $\ell_\infty$-norm for diverse step sizes, which are the most precise to date. Through simulations, we demonstrate that our bounds cannot be surpassed in the worst-case scenario. Our findings reveal that, compared to model-based $Q$-iteration, the $\ell_\infty$-based sample complexity of $Q$-learning is not optimal regarding the discount factor $\gamma$.",1
"When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.",0
"In order for agents to effectively navigate a complex environment, they must establish and maintain beliefs about the relevant factors within that environment. Our proposal outlines an efficient method for training expressive generative models within these complex environments. Through our research, we have demonstrated that a predictive algorithm utilizing an expressive generative model can establish stable belief-states in dynamic 3D environments with rich visual content. Our experiments indicate that the learned representation captures the environment's layout and the agent's position and orientation. In comparison to strong model-free baseline agents, our model substantially enhances data-efficiency on various reinforcement learning (RL) tasks. We have discovered that predicting multiple future steps (overshooting) with an expressive generative model is essential for stable representations to emerge. Employing expressive generative models in RL can be computationally intensive, but we have devised a method to reduce this computational burden, enabling the creation of agents that are just as competitive as model-free baselines.",1
"In Reinforcement Learning we look for meaning in the flow of input/output information. If we do not find meaning, the information flow is not more than noise to us. Before we are able to find meaning, we should first learn how to discover and identify objects. What is an object? In this article we will demonstrate that an object is an event-driven model. These models are a generalization of action-driven models. In Markov Decision Process we have an action-driven model which changes its state at each step. The advantage of event-driven models is their greater sustainability as they change their states only upon the occurrence of particular events. These events may occur very rarely, therefore the state of the event-driven model is much more predictable.",0
"The focus of Reinforcement Learning is on extracting significance from the input/output data flow. Without any significance, the flow of data is insignificant noise. In order to derive meaning, it is essential to acquire the skill of object recognition. But what exactly constitutes an object? This article proposes that an object is an event-driven model, which is a broader concept than an action-driven model. Typically, in a Markov Decision Process, an action-driven model changes its state at every step. However, the advantage of event-driven models is that they only change their state upon the occurrence of specific events, which may be rare. Therefore, the state of an event-driven model is much more predictable.",1
"Although reinforcement learning has made great strides recently, a continuing limitation is that it requires an extremely high number of interactions with the environment. In this paper, we explore the effectiveness of reusing experience from the experience replay buffer in the Deep Q-Learning algorithm. We test the effectiveness of applying learning update steps multiple times per environmental step in the VizDoom environment and show first, this requires a change in the learning rate, and second that it does not improve the performance of the agent. Furthermore, we show that updating less frequently is effective up to a ratio of 4:1, after which performance degrades significantly. These results quantitatively confirm the widespread practice of performing learning updates every 4th environmental step.",0
"While reinforcement learning has progressed significantly, a major drawback remains: it necessitates a great many interactions with the environment. This study examines the usefulness of reusing experience from the experience replay buffer in the Deep Q-Learning algorithm. We evaluate the effectiveness of executing learning updates multiple times per environmental step in the VizDoom environment and demonstrate that this necessitates altering the learning rate and does not enhance the agent's performance. We also demonstrate that updating less often is beneficial up to a 4:1 ratio, beyond which performance deteriorates considerably. These findings support the common approach of performing learning updates every fourth environmental step.",1
"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",0
"Our objective is to have agents optimize the appropriate reward function, even though it is challenging to specify. Inverse Reinforcement Learning (IRL) is useful for inferring reward functions from demonstrations, but it assumes that the expert is imperfectly optimal. Conversely, actual individuals often have systematic biases, such as risk-aversion and myopia. One option is to identify these biases and take them into account during learning. However, in the era of deep learning, researchers suggest a data-driven approach instead of mathematical models of human behavior with specific assumptions. We tested this approach by learning the demonstrator's planning algorithm that they utilized to generate demonstrations, as a differentiable planner, instead of relying on assumptions about the specific bias the demonstrator has when planning. Our exploration showed mixed results. While learning the planner can lead to better reward inference than relying on the wrong assumption, the benefit is outweighed by the loss incurred by going from an exact to a differentiable planner. This indicates that agents require a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. The code is available at https://tinyurl.com/learningbiases.",1
"Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",0
"The process of using end-to-end reinforcement learning for controlling actual robots using sight poses various difficulties, particularly regarding the effectiveness of sample collection. However, state representation learning can aid in the acquisition of a concise, effective, and applicable representation of states, which hastens policy learning, decreases the quantity of necessary samples, and simplifies interpretation. We assess multiple state representation learning techniques in goal-oriented robotics tasks and introduce a novel unsupervised model that combines the strengths of several approaches by stacking representations. This method records all relevant characteristics, performs as well as or better than end-to-end learning with superior sample efficiency, and is resilient to changes in hyper-parameters.",1
"Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.",0
"Learning control tasks through model-based reinforcement learning (RL) has been proven to be data efficient. However, it is challenging to implement this approach in domains with complex observations such as images. Our paper proposes a method for learning representations that are suitable for iterative model-based policy improvement, even when dealing with complex dynamics and image observations. These representations are optimized to infer simple dynamics and cost models from data generated by the current policy. This enables the use of a model-based RL method based on the linear-quadratic regulator (LQR) for systems with image observations. Our approach is evaluated on various robotics tasks, including manipulation with a real-world robotic arm from images. Our results show that our method significantly outperforms other model-based RL methods while being more efficient than model-free RL.",1
"We consider the problem of imitation learning from expert demonstrations in partially observable Markov decision processes (POMDPs). Belief representations, which characterize the distribution over the latent states in a POMDP, have been modeled using recurrent neural networks and probabilistic latent variable models, and shown to be effective for reinforcement learning in POMDPs. In this work, we investigate the belief representation learning problem for generative adversarial imitation learning in POMDPs. Instead of training the belief module and the policy separately as suggested in prior work, we learn the belief module jointly with the policy, using a task-aware imitation loss to ensure that the representation is more aligned with the policy's objective. To improve robustness of representation, we introduce several informative belief regularization techniques, including multi-step prediction of dynamics and action-sequences. Evaluated on various partially observable continuous-control locomotion tasks, our belief-module imitation learning approach (BMIL) substantially outperforms several baselines, including the original GAIL algorithm and the task-agnostic belief learning algorithm. Extensive ablation analysis indicates the effectiveness of task-aware belief learning and belief regularization.",0
"The focus of our study is on how to learn from expert demonstrations in partially observable Markov decision processes (POMDPs) using imitation learning. Recurrent neural networks and probabilistic latent variable models have been utilized to model belief representations, which describe the distribution over latent states in POMDPs, and have proven effective for reinforcement learning in such systems. We explore the problem of belief representation learning for generative adversarial imitation learning in POMDPs. Instead of training the belief module and policy separately, as suggested in previous research, we jointly learn both modules using a task-specific imitation loss to ensure alignment with the policy's objective. Additionally, we introduce informative belief regularization techniques, such as multi-step prediction of dynamics and action-sequences, to enhance the robustness of the representation. Our approach, called the belief-module imitation learning approach (BMIL), performs significantly better than several baselines including the original GAIL algorithm and the task-agnostic belief learning algorithm when evaluated on various partially observable continuous-control locomotion tasks. Our extensive ablation analysis demonstrates the effectiveness of task-aware belief learning and belief regularization.",1
"Animals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent's location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional successor features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible.",0
"To optimize their interactions with the environment, animals must develop strategies based on incomplete and unreliable sensory information. Relevant information, such as location or the presence of a predator, is often inferred from available sensory data. Successor representations (SR) offer a balance between model-based and model-free reinforcement learning, allowing for quick adaptation to changes in reward or goals. Recent studies suggest that SR may align with neural responses, but it is unclear how they are learned in noisy environments. In this study, we propose a neural model utilizing distributional successor features that can efficiently compute value functions in partially observed environments, enabling reinforcement learning in noisy environments where direct policy learning is difficult. The model builds on the distributed distributional code for uncertainty representation and computation.",1
"Rewards and punishments in different forms are pervasive and present in a wide variety of decision-making scenarios. By observing the outcome of a sufficient number of repeated trials, one would gradually learn the value and usefulness of a particular policy or strategy. However, in a given environment, the outcomes resulting from different trials are subject to chance influence and variations. In learning about the usefulness of a given policy, significant costs are involved in systematically undertaking the sequential trials; therefore, in most learning episodes, one would wish to keep the cost within bounds by adopting learning stopping rules. In this paper, we examine the deployment of different stopping strategies in given learning environments which vary from highly stringent for mission critical operations to highly tolerant for non-mission critical operations, and emphasis is placed on the former with particular application to aviation safety. In policy evaluation, two sequential phases of learning are identified, and we describe the outcomes variations using a probabilistic model, with closedform expressions obtained for the key measures of performance. Decision rules that map the trial observations to policy choices are also formulated. In addition, simulation experiments are performed, which corroborate the validity of the theoretical results.",0
"Different forms of rewards and punishments are present in many decision-making scenarios. One can gradually learn the value and usefulness of a policy or strategy by observing the outcome of repeated trials. However, chance influence and variations can affect the outcomes in a given environment. Undertaking sequential trials to learn about a policy's usefulness incurs significant costs, so it is necessary to adopt learning stopping rules to keep the cost within bounds. This paper examines the deployment of different stopping strategies in various learning environments, with emphasis on mission-critical operations such as aviation safety. We identify two sequential phases of learning in policy evaluation and describe outcome variations using a probabilistic model. We also formulate decision rules that map trial observations to policy choices and perform simulation experiments to validate our theoretical results.",1
"End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.",0
"This paper examines the effectiveness of recurrent neural networks (RNNs) as reinforcement learning agents in structured prediction scenarios, such as dialogue. These agents learn both a state representation and a policy simultaneously. The study investigates the quality of the representations learned by RNN-based agents trained with both policy gradient and value-based methods. The results of the experiments reveal that RNNs trained with policy gradient often struggle to learn a state representation that leads to an optimal policy in situations where the same action should be taken at different states. The reason for this issue is state aliasing, where multiple distinct states are combined in the representation space, resulting in the same optimal action. The paper provides insights into this phenomenon through experiments on a simple maze setting and a more complex text-based game, and recommends best practices for training RNNs with reinforcement learning.",1
"We propose a novel framework for multi-task reinforcement learning (MTRL). Using a variational inference formulation, we learn policies that generalize across both changing dynamics and goals. The resulting policies are parametrized by shared parameters that allow for transfer between different dynamics and goal conditions, and by task-specific latent-space embeddings that allow for specialization to particular tasks. We show how the latent-spaces enable generalization to unseen dynamics and goals conditions. Additionally, policies equipped with such embeddings serve as a space of skills (or options) for hierarchical reinforcement learning. Since we can change task dynamics and goals independently, we name our framework Disentangled Skill Embeddings (DSE).",0
"Our proposed framework for multi-task reinforcement learning (MTRL) involves a unique approach. By utilizing a variational inference formulation, we can develop policies that can adapt to varying dynamics and goals. These policies rely on shared parameters for transferability between different dynamics and goals, as well as task-specific latent-space embeddings for specialization in specific tasks. The latent-spaces allow for generalization to unknown dynamics and goals, and the policies equipped with such embeddings can function as a skill or option space for hierarchical reinforcement learning. Our framework is named Disentangled Skill Embeddings (DSE), as we can alter task dynamics and goals independently.",1
"Deep reinforcement learning has made significant progress in the field of continuous control, such as physical control and autonomous driving. However, it is challenging for a reinforcement model to learn a policy for each task sequentially due to catastrophic forgetting. Specifically, the model would forget knowledge it learned in the past when trained on a new task. We consider this challenge from two perspectives: i) acquiring task-specific skills is difficult since task information and rewards are not highly related; ii) learning knowledge from previous experience is difficult in continuous control domains. In this paper, we introduce an end-to-end framework namely Continual Diversity Adversarial Network (CDAN). We first develop an unsupervised diversity exploration method to learn task-specific skills using an unsupervised objective. Then, we propose an adversarial self-correction mechanism to learn knowledge by exploiting past experience. The two learning procedures are presumably reciprocal. To evaluate the proposed method, we propose a new continuous reinforcement learning environment named Continual Ant Maze (CAM) and a new metric termed Normalized Shorten Distance (NSD). The experimental results confirm the effectiveness of diversity exploration and self-correction. It is worthwhile noting that our final result outperforms baseline by 18.35% in terms of NSD, and 0.61 according to the average reward.",0
"The field of continuous control, such as physical control and autonomous driving, has seen significant progress with the use of deep reinforcement learning. However, catastrophic forgetting presents a challenge for reinforcement models when learning policies for tasks sequentially. This occurs when the model forgets past knowledge when trained on a new task. Two perspectives are considered for this challenge: the difficulty of acquiring task-specific skills due to the lack of relation between task information and rewards, and the difficulty of learning from previous experience in continuous control domains. To address this, we propose the Continual Diversity Adversarial Network (CDAN) framework, which includes an unsupervised diversity exploration method to learn task-specific skills and an adversarial self-correction mechanism to learn from past experience. We evaluate our approach using a new continuous reinforcement learning environment called Continual Ant Maze (CAM) and a new metric called Normalized Shorten Distance (NSD). Our experimental results demonstrate the effectiveness of diversity exploration and self-correction, with our final result outperforming the baseline by 18.35% in terms of NSD and 0.61 in average reward.",1
"Exploration in sparse reward reinforcement learning remains an open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. A video of our experimental results can be found at https://youtu.be/b0MbY3lUlEI.",0
"Sparse reward reinforcement learning exploration remains a challenge that has yet to be fully addressed. To complement the limited external reward, many advanced techniques use intrinsic motivation to provide additional feedback during exploration. However, adding these signals as bonus rewards results in a mixed policy that neither explores nor fulfills tasks effectively. Instead, our approach focuses on learning separate intrinsic and extrinsic task policies and scheduling between them to expedite exploration and enhance learning stability. Additionally, we introduce a new type of intrinsic reward called successor feature control (SFC) that is not task-specific but general. SFC considers complete trajectory statistics and differs from prior methods that only use local information for intrinsic motivation assessment. We evaluate our proposed scheduled intrinsic drive (SID) agent using pure visual input in three different environments: VizDoom, DeepMind Lab, and DeepMind Control Suite. The results demonstrate a significant increase in exploration efficiency with SFC and the hierarchical use of intrinsic drives. An experimental video of our findings is available at https://youtu.be/b0MbY3lUlEI.",1
"Real Time Strategy (RTS) games require macro strategies as well as micro strategies to obtain satisfactory performance since it has large state space, action space, and hidden information. This paper presents a novel hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a sub-genre of RTS games. The novelty of this work are: (1) proposing a hierarchical framework, where agents execute macro strategies by imitation learning and carry out micromanipulations through reinforcement learning, (2) developing a simple self-learning method to get better sample efficiency for training, and (3) designing a dense reward function for multi-agent cooperation in the absence of game engine or Application Programming Interface (API). Finally, various experiments have been performed to validate the superior performance of the proposed method over other state-of-the-art reinforcement learning algorithms. Agent successfully learns to combat and defeat bronze-level built-in AI with 100% win rate, and experiments show that our method can create a competitive multi-agent for a kind of mobile MOBA game {\it King of Glory} in 5v5 mode.",0
"In order to achieve satisfactory results in Real Time Strategy (RTS) games, both macro and micro strategies are required due to the vast state space, action space, and hidden information involved. This study introduces a new hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a subset of RTS games. The model proposes a hierarchical framework where agents execute macro strategies through imitation learning and micromanipulations through reinforcement learning. Additionally, a simple self-learning method is presented to improve sample efficiency during training, and a dense reward function for multi-agent cooperation is designed in the absence of a game engine or Application Programming Interface (API). The effectiveness of the proposed method is validated through various experiments, highlighting its superiority over other reinforcement learning algorithms. The agent successfully learns to defeat the bronze-level built-in AI with a 100% win rate, and the experiments demonstrate that the method can create a competitive multi-agent for the mobile MOBA game ""King of Glory"" in 5v5 mode.",1
"We present Placeto, a reinforcement learning (RL) approach to efficiently find device placements for distributed neural network training. Unlike prior approaches that only find a device placement for a specific computation graph, Placeto can learn generalizable device placement policies that can be applied to any graph. We propose two key ideas in our approach: (1) we represent the policy as performing iterative placement improvements, rather than outputting a placement in one shot; (2) we use graph embeddings to capture relevant information about the structure of the computation graph, without relying on node labels for indexing. These ideas allow Placeto to train efficiently and generalize to unseen graphs. Our experiments show that Placeto requires up to 6.1x fewer training steps to find placements that are on par with or better than the best placements found by prior approaches. Moreover, Placeto is able to learn a generalizable placement policy for any given family of graphs, which can then be used without any retraining to predict optimized placements for unseen graphs from the same family. This eliminates the large overhead incurred by prior RL approaches whose lack of generalizability necessitates re-training from scratch every time a new graph is to be placed.",0
"Introducing Placeto, an RL-based technique for efficiently locating device placements for distributed neural network training. Unlike previous methods that determine a device placement for a specific computation graph, Placeto can learn placement policies that are generalizable and applicable to any graph. Our approach involves two main concepts: (1) representing the policy as a series of iterative placement enhancements rather than a one-shot output and (2) using graph embeddings to capture relevant information regarding the computation graph's structure without relying on node labels for indexing. These ideas allow Placeto to train effectively and generalize to new graphs. Our experiments reveal that Placeto requires up to 6.1x fewer training steps to find placements that match or exceed the best placements found by previous techniques. Furthermore, Placeto can learn a generalizable placement policy for any given graph family, which can be used without retraining to predict optimized placements for new graphs from the same family. This eliminates the significant overhead associated with prior RL approaches that require retraining from scratch each time a new graph is placed because of their lack of generalizability.",1
"Global routing has been a historically challenging problem in electronic circuit design, where the challenge is to connect a large and arbitrary number of circuit components with wires without violating the design rules for the printed circuit boards or integrated circuits. Similar routing problems also exist in the design of complex hydraulic systems, pipe systems and logistic networks. Existing solutions typically consist of greedy algorithms and hard-coded heuristics. As such, existing approaches suffer from a lack of model flexibility and non-optimum solutions. As an alternative approach, this work presents a deep reinforcement learning method for solving the global routing problem in a simulated environment. At the heart of the proposed method is deep reinforcement learning that enables an agent to produce an optimal policy for routing based on the variety of problems it is presented with leveraging the conjoint optimization mechanism of deep reinforcement learning. Conjoint optimization mechanism is explained and demonstrated in details; the best network structure and the parameters of the learned model are explored. Based on the fine-tuned model, routing solutions and rewards are presented and analyzed. The results indicate that the approach can outperform the benchmark method of a sequential A* method, suggesting a promising potential for deep reinforcement learning for global routing and other routing or path planning problems in general. Another major contribution of this work is the development of a global routing problem sets generator with the ability to generate parameterized global routing problem sets with different size and constraints, enabling evaluation of different routing algorithms and the generation of training datasets for future data-driven routing approaches.",0
"The task of connecting a large number of electronic circuit components with wires while adhering to design rules has always been a difficult problem in circuit design. The same problem exists in the design of complex hydraulic and pipe systems, as well as logistics networks. Existing solutions to this problem usually employ greedy algorithms and fixed heuristics, which limit their flexibility and can lead to suboptimal solutions. In this paper, we propose a new approach using deep reinforcement learning to solve the global routing problem in a simulated environment. Our method utilizes the conjoint optimization mechanism of deep reinforcement learning to create an optimal routing policy, allowing the agent to handle a variety of routing problems. We explain and demonstrate the conjoint optimization mechanism and explore the best network structure and parameters of the learned model. Using the fine-tuned model, we present and analyze routing solutions and rewards, showing that our approach outperforms the benchmark sequential A* method, indicating potential for deep reinforcement learning in routing and path planning problems in general. We also develop a global routing problem sets generator, which can generate parameterized problem sets of different sizes and constraints for evaluation of different routing algorithms and the generation of training datasets for future data-driven routing approaches.",1
"Interference among concurrent transmissions in a wireless network is a key factor limiting the system performance. One way to alleviate this problem is to manage the radio resources in order to maximize either the average or the worst-case performance. However, joint consideration of both metrics is often neglected as they are competing in nature. In this article, a mechanism for radio resource management using multi-agent deep reinforcement learning (RL) is proposed, which strikes the right trade-off between maximizing the average and the $5^{th}$ percentile user throughput. Each transmitter in the network is equipped with a deep RL agent, receiving partial observations from the network (e.g., channel quality, interference level, etc.) and deciding whether to be active or inactive at each scheduling interval for given radio resources, a process referred to as link scheduling. Based on the actions of all agents, the network emits a reward to the agents, indicating how good their joint decisions were. The proposed framework enables the agents to make decisions in a distributed manner, and the reward is designed in such a way that the agents strive to guarantee a minimum performance, leading to a fair resource allocation among all users across the network. Simulation results demonstrate the superiority of our approach compared to decentralized baselines in terms of average and $5^{th}$ percentile user throughput, while achieving performance close to that of a centralized exhaustive search approach. Moreover, the proposed framework is robust to mismatches between training and testing scenarios. In particular, it is shown that an agent trained on a network with low transmitter density maintains its performance and outperforms the baselines when deployed in a network with a higher transmitter density.",0
"The performance of a wireless network is often limited by interference caused by concurrent transmissions. To address this issue, radio resources can be managed to maximize either the average or worst-case performance. However, balancing these metrics is challenging as they are in competition with one another. This article proposes a new approach for radio resource management using multi-agent deep reinforcement learning (RL) to strike a balance between maximizing average and $5^{th}$ percentile user throughput. Each transmitter in the network is equipped with a deep RL agent that receives partial observations and decides whether to be active or inactive at each scheduling interval. The network rewards the agents based on their joint decisions, encouraging them to work together to guarantee a minimum performance and ensure fair resource allocation. The proposed framework enables distributed decision-making, and simulation results show that it outperforms decentralized baselines in terms of average and $5^{th}$ percentile user throughput while remaining robust to changes in network density.",1
"Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in terms of both sample efficiency and asymptotic performance. Despite their initial successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released in https://github.com/WilsonWangTHU/POPLIN.",0
"The efficiency and performance of model-based reinforcement learning (MBRL) with model-predictive control or online planning has been established for locomotion control tasks. However, the current planning methods employ a random generation of candidate sequences in the action space, limiting their efficacy in complex high-dimensional environments. In this paper, we introduce a new MBRL algorithm, namely model-based policy planning (POPLIN), which combines policy networks with online planning to solve for action planning at each time-step using neural networks. We conduct experiments with optimization with respect to both the action sequences and the parameters of the policy network and demonstrate that POPLIN outperforms other state-of-the-art algorithms such as PETS, TD3 and SAC in the MuJoCo benchmarking environments, with a 3x increase in sample efficiency. We attribute this success to the smoother optimization surface in parameter space and the effective application of the distilled policy network during test time for certain environments such as Cheetah. Our code is publicly available at https://github.com/WilsonWangTHU/POPLIN.",1
"Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning, however, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of $emphatic$ $weightings$. We develop a new actor-critic algorithm$\unicode{x2014}$called Actor Critic with Emphatic weightings (ACE)$\unicode{x2014}$that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods$\unicode{x2014}$particularly OffPAC and DPG$\unicode{x2014}$converge to the wrong solution whereas ACE finds the optimal solution.",0
"In reinforcement learning, Policy gradient methods are widely used for control, especially in the continuous action setting. For the on-policy setting, numerous algorithms have been proposed that are theoretically sound because of the policy gradient theorem, which provides a simplified form for the gradient. However, in off-policy learning, where the behavior policy may not be trying to learn and follow the optimal policy for the given task, the existence of such a theorem has been hard to come by. In this study, we have solved this open problem by presenting the first off-policy policy gradient theorem, which utilizes emphatic weightings. We have developed a new actor-critic algorithm, named Actor Critic with Emphatic weightings (ACE), that approximates the simplified gradients provided by the theorem. We have demonstrated, through a simple counterexample, that previous off-policy policy gradient methods, specifically OffPAC and DPG, converge to the incorrect solution, whereas ACE finds the optimal solution.",1
"Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rule-based replay strategy, which may be sub-optimal. In this work, we consider learning a replay policy to optimize the cumulative reward. Replay learning is challenging because the replay memory is noisy and large, and the cumulative reward is unstable. To address these issues, we propose a novel experience replay optimization (ERO) framework which alternately updates two policies: the agent policy, and the replay policy. The agent is updated to maximize the cumulative reward based on the replayed data, while the replay policy is updated to provide the agent with the most useful experiences. The conducted experiments on various continuous control tasks demonstrate the effectiveness of ERO, empirically showing promise in experience replay learning to improve the performance of off-policy reinforcement learning algorithms.",0
"The technique of experience replay allows reinforcement learning agents to store and reuse past experiences, similar to how humans recall memories to handle current situations. However, current off-policy algorithms either replay experiences uniformly or use a rule-based approach, which may not be optimal. This study proposes a novel approach called experience replay optimization (ERO) to learn a replay policy that maximizes the cumulative reward. The challenge lies in dealing with a noisy and large replay memory, as well as unstable cumulative rewards. The ERO framework addresses these issues by updating two policies alternately: the agent policy and the replay policy. The agent policy maximizes the cumulative reward based on the replayed data, while the replay policy provides the agent with the most useful experiences. Experimental results on various continuous control tasks show that ERO is effective in improving the performance of off-policy reinforcement learning algorithms through experience replay learning.",1
"Estimates of predictive uncertainty are important for accurate model-based planning and reinforcement learning. However, predictive uncertainties---especially ones derived from modern deep learning systems---can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based reinforcement learning and argues that good uncertainties must be calibrated, i.e. their probabilities should match empirical frequencies of predicted events. We describe a simple way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently improves planning, sample complexity, and exploration. On the \textsc{HalfCheetah} MuJoCo task, our system achieves state-of-the-art performance using 50\% fewer samples than the current leading approach. Our findings suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.",0
"Accurate model-based planning and reinforcement learning require reliable estimates of predictive uncertainty. However, modern deep learning systems may produce inaccurate predictive uncertainties, which hinder performance. This study examines the necessary uncertainties for model-based reinforcement learning and argues that good uncertainties must be calibrated to match empirical frequencies of predicted events. We propose a simple method to enhance any model-based reinforcement learning agent with a calibrated model, and we demonstrate that doing so consistently enhances planning, sample complexity, and exploration. Our system achieves state-of-the-art performance on the \textsc{HalfCheetah} MuJoCo task with 50\% fewer samples than the current leading approach. Our results suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.",1
"Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",0
"The potential of deep reinforcement learning (deep RL) lies in its ability to automate the acquisition of intricate controllers that can directly link sensory inputs to low-level actions. For robotic locomotion, deep RL could facilitate the learning of locomotion skills with minimal engineering and no explicit model of the robot dynamics. However, applying deep RL to real-world robotic tasks is exceptionally challenging due to the poor sample complexity and sensitivity to hyperparameters. While tuning hyperparameters in simulated domains is easy, it can be costly on physical systems, such as legged robots, which can be damaged during trial-and-error learning. In this study, we present a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We demonstrate the effectiveness of our approach by teaching a real-world Minitaur robot to walk stably from scratch in about two hours without relying on any model or simulation. Furthermore, our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. The project website contains videos of training and the learned policy.",1
"Imitation Learning describes the problem of recovering an expert policy from demonstrations. While inverse reinforcement learning approaches are known to be very sample-efficient in terms of expert demonstrations, they usually require problem-dependent reward functions or a (task-)specific reward-function regularization. In this paper, we show a natural connection between inverse reinforcement learning approaches and Optimal Transport, that enables more general reward functions with desirable properties (e.g., smoothness). Based on our observation, we propose a novel approach called Wasserstein Adversarial Imitation Learning. Our approach considers the Kantorovich potentials as a reward function and further leverages regularized optimal transport to enable large-scale applications. In several robotic experiments, our approach outperforms the baselines in terms of average cumulative rewards and shows a significant improvement in sample-efficiency, by requiring just one expert demonstration.",0
"The problem of recovering an expert policy from demonstrations is referred to as Imitation Learning. Although inverse reinforcement learning methods are known for their efficiency in using expert demonstrations, they typically require a task-specific reward function or regularization. This paper establishes a connection between inverse reinforcement learning methods and Optimal Transport, allowing for reward functions with desirable properties, such as smoothness. Our proposed method, Wasserstein Adversarial Imitation Learning, utilizes Kantorovich potentials as a reward function and applies regularized optimal transport for use in large-scale applications. Through several experiments involving robots, our approach outperforms the baselines in terms of average cumulative rewards and demonstrates significant improvements in sample-efficiency, requiring only one expert demonstration.",1
"We consider the core reinforcement-learning problem of on-policy value function approximation from a batch of trajectory data, and focus on various issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation. The two methods are known to achieve complementary bias-variance trade-off properties, with TD tending to achieve lower variance but potentially higher bias. In this paper, we argue that the larger bias of TD can be a result of the amplification of local approximation errors. We address this by proposing an algorithm that adaptively switches between TD and MC in each state, thus mitigating the propagation of errors. Our method is based on learned confidence intervals that detect biases of TD estimates. We demonstrate in a variety of policy evaluation tasks that this simple adaptive algorithm performs competitively with the best approach in hindsight, suggesting that learned confidence intervals are a powerful technique for adapting policy evaluation to use TD or MC returns in a data-driven way.",0
"The main focus of our study is the reinforcement-learning problem of approximating on-policy value functions from a set of trajectory data. We examine the issues associated with Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation, which are known to have complementary bias-variance trade-off properties. TD has lower variance but potentially higher bias compared to MC. Our research indicates that TD's higher bias can be attributed to the amplification of local approximation errors. To mitigate this, we propose an algorithm that switches between TD and MC in each state, utilizing learned confidence intervals to detect TD estimate biases. Our results demonstrate that this adaptive algorithm performs competitively with the best approach in hindsight, indicating that learned confidence intervals can be a valuable tool for adapting policy evaluation to use TD or MC returns in a data-driven manner.",1
"We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.",0
"Our proposal aims to establish a unified approach to achieve coordination and communication in Multi-Agent Reinforcement Learning (MARL) by rewarding agents for their ability to causally influence the actions of other agents. This assessment is carried out through counterfactual reasoning, where an agent considers alternative actions it could have taken and evaluates their impact on the behavior of other agents. Actions that cause significant changes to the behavior of other agents are deemed influential and rewarded accordingly. We demonstrate that this approach is equivalent to rewarding agents for having high mutual information between their actions. Our experiments reveal that this influence-based approach leads to improved coordination and communication in challenging social dilemma environments, resulting in remarkable learning curves for deep RL agents and meaningful communication protocols. The influence rewards can be computed in a decentralized manner by enabling agents to learn a model of other agents using deep neural networks. This is in contrast to previous works on emergent communication in the MARL setting, which were unable to learn diverse policies in a decentralized fashion and had to rely on centralized training. Therefore, this influence-based reward system creates new possibilities for research in this area.",1
"Efficient exploration is necessary to achieve good sample efficiency for reinforcement learning in general. From small, tabular settings such as gridworlds to large, continuous and sparse reward settings such as robotic object manipulation tasks, exploration through adding an uncertainty bonus to the reward function has been shown to be effective when the uncertainty is able to accurately drive exploration towards promising states. However reward bonuses can still be inefficient since they are non-stationary, which means that we must wait for function approximators to catch up and converge again when uncertainties change. We propose the idea of directed exploration, that is learning a goal-conditioned policy where goals are simply other states, and using that to directly try to reach states with large uncertainty. The goal-conditioned policy is independent of uncertainty and is thus stationary. We show in our experiments how directed exploration is more efficient at exploration and more robust to how the uncertainty is computed than adding bonuses to rewards.",0
"To achieve good sample efficiency in reinforcement learning, efficient exploration is crucial. From small tabular settings like gridworlds to larger, continuous and sparse reward settings like robotic object manipulation tasks, exploration by adding an uncertainty bonus to the reward function has been found effective in driving exploration towards promising states. However, using reward bonuses can still be inefficient as they are non-stationary, requiring function approximators to catch up and converge again when uncertainties change. To overcome this, we propose the concept of directed exploration - learning a goal-conditioned policy where goals are other states, and using it to directly reach states with high uncertainty. The goal-conditioned policy is independent of uncertainty and therefore stationary. Our experiments demonstrate that directed exploration is more efficient in exploration and more robust in how uncertainty is computed than adding bonuses to rewards.",1
"In this work, we describe practical lessons we have learned from successfully using contextual bandits (CBs) to improve key business metrics of the Microsoft Virtual Agent for customer support. While our current use cases focus on single step einforcement learning (RL) and mostly in the domain of natural language processing and information retrieval we believe many of our findings are generally applicable. Through this article, we highlight certain issues that RL practitioners may encounter in similar types of applications as well as offer practical solutions to these challenges.",0
"Our work details the practical insights we gained by employing contextual bandits (CBs) to enhance the vital business metrics of the Microsoft Virtual Agent for customer support. Although our current usage scenarios mainly concentrate on natural language processing and information retrieval single step reinforcement learning (RL), we are convinced that our discoveries have broader relevance. Our article addresses specific problems that RL experts may face in comparable applications and provides effective solutions to overcome these hurdles.",1
"In real-world applications of reinforcement learning (RL), noise from inherent stochasticity of environments is inevitable. However, current policy evaluation algorithms, which plays a key role in many RL algorithms, are either prone to noise or inefficient. To solve this issue, we introduce a novel policy evaluation algorithm, which we call Gap-increasing RetrAce Policy Evaluation (GRAPE). It leverages two recent ideas: (1) gap-increasing value update operators in advantage learning for noise-tolerance and (2) off-policy eligibility trace in Retrace algorithm for efficient learning. We provide detailed theoretical analysis of the new algorithm that shows its efficiency and noise-tolerance inherited from Retrace and advantage learning. Furthermore, our analysis shows that GRAPE's learning is significantly efficient than that of a simple learning-rate-based approach while keeping the same level of noise-tolerance. We applied GRAPE to control problems and obtained experimental results supporting our theoretical analysis.",0
"In reinforcement learning (RL) applications, noise caused by stochastic environments is unavoidable, but current policy evaluation algorithms, which are essential to RL algorithms, are either inefficient or susceptible to noise. To address this issue, we have developed a new policy evaluation algorithm, called Gap-increasing RetrAce Policy Evaluation (GRAPE), which incorporates two recent concepts: gap-increasing value update operators for noise tolerance in advantage learning, and off-policy eligibility trace in Retrace algorithm for efficient learning. We have conducted a detailed theoretical analysis of GRAPE, which demonstrates its efficiency and noise tolerance inherited from Retrace and advantage learning. Our analysis also shows that GRAPE's learning is significantly more efficient than a simple learning-rate-based approach while maintaining the same level of noise tolerance. We have tested GRAPE on control problems and obtained experimental results that support our theoretical analysis.",1
"The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of a wide variety of automated methods for neural architecture search. The choice of the network architecture has proven to be critical, and many advances in deep learning spring from its immediate improvements. However, deep learning techniques are computationally intensive and their application requires a high level of domain knowledge. Therefore, even partial automation of this process helps to make deep learning more accessible to both researchers and practitioners. With this survey, we provide a formalism which unifies and categorizes the landscape of existing methods along with a detailed analysis that compares and contrasts the different approaches. We achieve this via a comprehensive discussion of the commonly adopted architecture search spaces and architecture optimization algorithms based on principles of reinforcement learning and evolutionary algorithms along with approaches that incorporate surrogate and one-shot models. Additionally, we address the new research directions which include constrained and multi-objective architecture search as well as automated data augmentation, optimizer and activation function search.",0
"The increasing interest in automating machine and deep learning has resulted in the creation of various automated techniques for neural architecture search. Selecting the right network architecture is crucial, and enhancements in deep learning mostly stem from its immediate improvements. However, implementing deep learning methods is computationally demanding and requires extensive domain knowledge. As a result, even partial automation of this process can make deep learning more accessible to researchers and professionals. Through our survey, we present a framework that unites and categorizes the existing methods, accompanied by an extensive analysis that compares and contrasts the various approaches. We accomplish this by comprehensively discussing the commonly used architecture search spaces and optimization algorithms based on principles of reinforcement learning and evolutionary algorithms, incorporating surrogate and one-shot models. Additionally, we explore new research directions, such as constrained and multi-objective architecture search, automated data augmentation, optimizer, and activation function search.",1
"Imitation from observation is the framework of learning tasks by observing demonstrated state-only trajectories. Recently, adversarial approaches have achieved significant performance improvements over other methods for imitating complex behaviors. However, these adversarial imitation algorithms often require many demonstration examples and learning iterations to produce a policy that is successful at imitating a demonstrator's behavior. This high sample complexity often prohibits these algorithms from being deployed on physical robots. In this paper, we propose an algorithm that addresses the sample inefficiency problem by utilizing ideas from trajectory centric reinforcement learning algorithms. We test our algorithm and conduct experiments using an imitation task on a physical robot arm and its simulated version in Gazebo and will show the improvement in learning rate and efficiency.",0
"The basis of learning tasks through observation is to imitate what is seen. Lately, adversarial methods have outperformed other techniques in replicating complex behaviors. However, the drawback is that these approaches often require a large number of examples and iterations to develop a successful policy. Consequently, these algorithms are not practical for deployment on physical robots due to their high sample complexity. In this article, we propose a solution to this issue by incorporating trajectory-centric reinforcement learning techniques. Our algorithm is tested through an imitation task on a physical robot arm and its simulated version in Gazebo. We demonstrate the effectiveness of our approach by showing improvements in learning rate and efficiency.",1
"We aim to jointly optimize antenna tilt angle, and vertical and horizontal half-power beamwidths of the macrocells in a heterogeneous cellular network (HetNet). The interactions between the cells, most notably due to their coupled interference render this optimization prohibitively complex. Utilizing a single agent reinforcement learning (RL) algorithm for this optimization becomes quite suboptimum despite its scalability, whereas multi-agent RL algorithms yield better solutions at the expense of scalability. Hence, we propose a compromise algorithm between these two. Specifically, a multi-agent mean field RL algorithm is first utilized in the offline phase so as to transfer information as features for the second (online) phase single agent RL algorithm, which employs a deep neural network to learn users locations. This two-step approach is a practical solution for real deployments, which should automatically adapt to environmental changes in the network. Our results illustrate that the proposed algorithm approaches the performance of the multi-agent RL, which requires millions of trials, with hundreds of online trials, assuming relatively low environmental dynamics, and performs much better than a single agent RL. Furthermore, the proposed algorithm is compact and implementable, and empirically appears to provide a performance guarantee regardless of the amount of environmental dynamics.",0
"Our goal is to optimize the antenna tilt angle, as well as the vertical and horizontal half-power beamwidths of macrocells in a heterogeneous cellular network (HetNet). However, this optimization is complicated by the interactions between cells, particularly the coupled interference. While a single agent reinforcement learning (RL) algorithm is scalable, it is not optimal, and multi-agent RL algorithms are better but less scalable. We propose a compromise algorithm that uses a multi-agent mean field RL algorithm in the offline phase to transfer information as features to the second (online) phase single agent RL algorithm. The latter uses a deep neural network to learn users' locations. This two-step approach is adaptable to environmental changes in the network and is a practical solution for real deployments. Our results show that the proposed algorithm performs nearly as well as the multi-agent RL with millions of trials, with only hundreds of online trials, assuming low environmental dynamics. It also performs better than a single agent RL. The proposed algorithm is compact, implementable, and provides a performance guarantee regardless of the amount of environmental dynamics.",1
"This paper presents novel mixed-type Bayesian optimization (BO) algorithms to accelerate the optimization of a target objective function by exploiting correlated auxiliary information of binary type that can be more cheaply obtained, such as in policy search for reinforcement learning and hyperparameter tuning of machine learning models with early stopping. To achieve this, we first propose a mixed-type multi-output Gaussian process (MOGP) to jointly model the continuous target function and binary auxiliary functions. Then, we propose information-based acquisition functions such as mixed-type entropy search (MT-ES) and mixed-type predictive ES (MT-PES) for mixed-type BO based on the MOGP predictive belief of the target and auxiliary functions. The exact acquisition functions of MT-ES and MT-PES cannot be computed in closed form and need to be approximated. We derive an efficient approximation of MT-PES via a novel mixed-type random features approximation of the MOGP model whose cross-correlation structure between the target and auxiliary functions can be exploited for improving the belief of the global target maximizer using observations from evaluating these functions. We propose new practical constraints to relate the global target maximizer to the binary auxiliary functions. We empirically evaluate the performance of MT-ES and MT-PES with synthetic and real-world experiments.",0
"In this article, we introduce innovative Bayesian optimization (BO) algorithms that combine binary auxiliary information with a target objective function to improve optimization speed. This approach is particularly useful in scenarios where the auxiliary information is easier and cheaper to obtain, such as policy search for reinforcement learning and hyperparameter tuning of machine learning models with early stopping. Our methodology involves using a mixed-type multi-output Gaussian process (MOGP) to jointly model the target function and binary auxiliary functions, followed by the development of information-based acquisition functions like mixed-type entropy search (MT-ES) and mixed-type predictive ES (MT-PES) for mixed-type BO. While the exact acquisition functions for MT-ES and MT-PES cannot be computed, we derive an efficient approximation of MT-PES via a mixed-type random features approximation of the MOGP model. Furthermore, we introduce practical constraints to establish a relationship between the global target maximizer and binary auxiliary functions. Finally, we assess the performance of our algorithms using both synthetic and real-world experiments.",1
"We propose a lifelong learning architecture, the Neural Computer Agent (NCA), where a Reinforcement Learning agent is paired with a predictive model of the environment learned by a Differentiable Neural Computer (DNC). The agent and DNC model are trained in conjunction iteratively. The agent improves its policy in simulations generated by the DNC model and rolls out the policy to the live environment, collecting experiences in new portions or tasks of the environment for further learning. Experiments in two synthetic environments show that DNC models can continually learn from pixels alone to simulate new tasks as they are encountered by the agent, while the agents can be successfully trained to solve the tasks using Proximal Policy Optimization entirely in simulations.",0
"Our proposal is to use the Neural Computer Agent (NCA) as a lifelong learning architecture. This involves pairing a Reinforcement Learning agent with a predictive model of the environment, which is learned by a Differentiable Neural Computer (DNC). The agent and DNC model are trained together in a continuous manner. The agent improves its policy by simulating the DNC model and then applies this policy to the live environment, collecting experiences from new tasks for further learning. Our experiments in two synthetic environments show that DNC models can learn continually from pixels alone and simulate new tasks as they are encountered by the agent. The agents are trained successfully to solve the tasks in simulations using Proximal Policy Optimization.",1
"We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is \textit{agnostic} to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries, and can adapt in \textit{continuously} changing environments. In our \textit{policy consolidation} model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent's policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.",0
"Our proposed solution to address the problem of catastrophic forgetting in deep reinforcement learning is flexible enough to adapt to changes in the distribution of experiences and does not require prior knowledge of task boundaries. Additionally, it can function effectively in environments that undergo continuous changes. Our policy consolidation model involves the policy network interacting with a cascade of hidden networks that remember the agent's policy at various timescales and improve its learning ability without forgetting previous knowledge. Through our experiments on several continuous control tasks, we have observed that our model outperforms baselines in single-task, alternating two-task, and multi-agent competitive self-play settings.",1
"Curriculum learning is often employed in deep reinforcement learning to let the agent progress more quickly towards better behaviors. Numerical methods for curriculum learning in the literature provides only initial heuristic solutions, with little to no guarantee on their quality. We define a new gray-box function that, including a suitable scheduling problem, can be effectively used to reformulate the curriculum learning problem. We propose different efficient numerical methods to address this gray-box reformulation. Preliminary numerical results on a benchmark task in the curriculum learning literature show the viability of the proposed approach.",0
"To facilitate faster progress towards improved behaviors, deep reinforcement learning frequently makes use of curriculum learning. However, the existing literature only offers preliminary heuristic solutions with no assurance of their effectiveness. To address this, we introduce a grey-box function that can be utilized to effectively reformulate the problem of curriculum learning, incorporating an appropriate scheduling problem. We propose multiple efficient numerical methods to tackle this grey-box reformulation. Initial numerical testing on a benchmark task from the curriculum learning literature demonstrates the feasibility of our proposed technique.",1
"Stochastic approximation (SA) is a key method used in statistical learning. Recently, its non-asymptotic convergence analysis has been considered in many papers. However, most of the prior analyses are made under restrictive assumptions such as unbiased gradient estimates and convex objective function, which significantly limit their applications to sophisticated tasks such as online and reinforcement learning. These restrictions are all essentially relaxed in this work. In particular, we analyze a general SA scheme to minimize a non-convex, smooth objective function. We consider update procedure whose drift term depends on a state-dependent Markov chain and the mean field is not necessarily of gradient type, covering approximate second-order method and allowing asymptotic bias for the one-step updates. We illustrate these settings with the online EM algorithm and the policy-gradient method for average reward maximization in reinforcement learning.",0
"The technique of stochastic approximation (SA) is a crucial tool in statistical learning, and has recently been the subject of analysis in multiple studies to determine its non-asymptotic convergence. However, previous analyses have been limited by strict assumptions such as unbiased gradient estimates and convex objective functions, making them unsuitable for complex tasks like online and reinforcement learning. This paper relaxes these restrictions and presents a general SA approach that can minimize a non-convex, smooth objective function. The update procedure is designed with a state-dependent Markov chain drift term, and the mean field is not necessarily of gradient type, thus allowing for approximate second-order methods and asymptotic bias for one-step updates. The online EM algorithm and the policy-gradient method for average reward maximization in reinforcement learning are used to demonstrate these concepts.",1
"Heuristic algorithms such as simulated annealing, Concorde, and METIS are effective and widely used approaches to find solutions to combinatorial optimization problems. However, they are limited by the high sample complexity required to reach a reasonable solution from a cold-start. In this paper, we introduce a novel framework to generate better initial solutions for heuristic algorithms using reinforcement learning (RL), named RLHO. We augment the ability of heuristic algorithms to greedily improve upon an existing initial solution generated by RL, and demonstrate novel results where RL is able to leverage the performance of heuristics as a learning signal to generate better initialization.   We apply this framework to Proximal Policy Optimization (PPO) and Simulated Annealing (SA). We conduct a series of experiments on the well-known NP-complete bin packing problem, and show that the RLHO method outperforms our baselines. We show that on the bin packing problem, RL can learn to help heuristics perform even better, allowing us to combine the best parts of both approaches.",0
"Combinatorial optimization problems are commonly solved using heuristic algorithms like Concorde, simulated annealing, and METIS. However, these algorithms require a high sample complexity to reach a reasonable solution from a cold-start, limiting their effectiveness. To address this issue, we propose a new framework called RLHO that uses reinforcement learning (RL) to generate better initial solutions for heuristic algorithms. By augmenting the ability of heuristic algorithms to improve upon initial solutions generated by RL, we demonstrate how RL can leverage the performance of heuristics as a learning signal to produce better initialization. Our experiments on the bin packing problem show that RLHO outperforms our baselines, and that the combination of RL and heuristics results in even better performance. Specifically, we apply this framework to Proximal Policy Optimization (PPO) and Simulated Annealing (SA) to achieve superior results.",1
"Human ability at solving complex tasks is helped by priors on object and event semantics of their environment. This paper investigates the use of similar prior knowledge for transfer learning in Reinforcement Learning agents. In particular, the paper proposes to use a first-order-logic language grounded in deep neural networks to represent facts about objects and their semantics in the real world. Facts are provided as background knowledge a priori to learning a policy for how to act in the world. The priors are injected with the conventional input in a single agent architecture. As proof-of-concept, the paper tests the system in simple experiments that show the importance of symbolic abstraction and flexible fact derivation. The paper shows that the proposed system can learn to take advantage of both the symbolic layer and the image layer in a single decision selection module.",0
"This paper explores how humans' ability to solve complex tasks is aided by their prior knowledge of object and event semantics in their environment. The authors investigate the potential of utilizing similar prior knowledge for transfer learning in Reinforcement Learning agents. They propose using a first-order-logic language that is grounded in deep neural networks to represent facts about real-world objects and their semantics. These facts are provided as background knowledge prior to learning how to navigate the world. The priors are integrated with conventional input in a single agent architecture. To demonstrate the effectiveness of this approach, the authors conduct simple experiments that highlight the importance of symbolic abstraction and flexible fact derivation. The results indicate that the proposed system can effectively utilize both the symbolic and image layers in a single decision selection module.",1
"We develop a framework for interacting with uncertain environments in reinforcement learning (RL) by leveraging preferences in the form of utility functions. We claim that there is value in considering different risk measures during learning. In this framework, the preference for risk can be tuned by variation of the parameter $\beta$ and the resulting behavior can be risk-averse, risk-neutral or risk-taking depending on the parameter choice. We evaluate our framework for learning problems with model uncertainty. We measure and control for \emph{epistemic} risk using dynamic programming (DP) and policy gradient-based algorithms. The risk-averse behavior is then compared with the behavior of the optimal risk-neutral policy in environments with epistemic risk.",0
"Using utility functions, we have developed a framework for reinforcement learning (RL) that can effectively interact with uncertain environments. Our approach involves considering different risk measures during the learning process. By adjusting the parameter $\beta$, we can tune the level of risk preference and achieve behavior that is either risk-averse, risk-neutral or risk-taking. We have applied our framework to learning problems that involve model uncertainty and have measured and controlled for \emph{epistemic} risk using dynamic programming (DP) and policy gradient-based algorithms. Our results show that the risk-averse behavior is comparable to that of the optimal risk-neutral policy in environments with epistemic risk.",1
"Real-world applications require RL algorithms to act safely. During learning process, it is likely that the agent executes sub-optimal actions that may lead to unsafe/poor states of the system. Exploration is particularly brittle in high-dimensional state/action space due to increased number of low-performing actions. In this work, we consider risk-averse exploration in approximate RL setting. To ensure safety during learning, we propose the distributionally robust policy iteration scheme that provides lower bound guarantee on state-values. Our approach induces a dynamic level of risk to prevent poor decisions and yet preserves the convergence to the optimal policy. Our formulation results in a efficient algorithm that accounts for a simple re-weighting of policy actions in the standard policy iteration scheme. We extend our approach to continuous state/action space and present a practical algorithm, distributionally robust soft actor-critic, that implements a different exploration strategy: it acts conservatively at short-term and it explores optimistically in a long-run. We provide promising experimental results on continuous control tasks.",0
"RL algorithms used in real-world applications must prioritize safety. While learning, the agent may perform sub-optimal actions that lead to unsafe or poor system states. Exploration is especially difficult in high-dimensional state/action spaces because of the large number of low-performing actions. This study introduces risk-averse exploration in the approximate RL setting. To ensure safety during learning, the distributionally robust policy iteration scheme with a lower bound guarantee on state-values is proposed. This approach balances risk dynamically to prevent poor decisions while maintaining convergence to the optimal policy. The algorithm is efficient and involves a simple re-weighting of policy actions in the standard policy iteration scheme. The approach is extended to continuous state/action spaces in the distributionally robust soft actor-critic algorithm. This algorithm explores conservatively in the short-term and optimistically in the long-run. The experimental results on continuous control tasks are promising.",1
"Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. This work is inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, prompting us to ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts models early in training, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? Our experiments confirm these findings across a range of architectures and datasets.",0
"Many machine learning algorithms rely on importance-weighted risk minimization to achieve optimal results in areas such as causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. Although the impact of importance weighting on low-capacity misspecified models is well-established, little is known about its effects on over-parameterized deep neural networks. This study was motivated by recent theoretical findings showing that deep linear networks optimized by SGD learn weight-agnostic solutions on (linearly) separable data. The researchers investigated the effect of importance weighting on realistic deep networks, which are separable in many practical datasets. They discovered that while importance weighting has an impact on models early in training, its effect diminishes over successive epochs. The study also found that L2 regularization and batch normalization, but not dropout, restore some of the impact of importance weighting. However, these techniques express the effect via the wrong abstraction, leaving practitioners unsure of how much L2 regularization to use to achieve the desired weighting effect. The experiments conducted in this study confirm these findings across a range of architectures and datasets.",1
"Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.",0
"The issue of efficient exploration in Reinforcement Learning remains unresolved, and typically, agents are rewarded for coming across new situations by chance. However, this paper proposes a solution to this problem with an active exploration algorithm called Model-Based Active eXploration (MAX). MAX utilizes a group of forward models to plan and observe novel events, optimizing the agent's behavior based on a novelty measure derived from the Bayesian approach to exploration. This measure is estimated by measuring the disagreement between the futures predicted by the ensemble members. Our empirical findings indicate that in semi-random discrete environments where targeted exploration is necessary for progress, MAX is at least ten times more efficient than robust baseline methods. Additionally, MAX can scale to high-dimensional continuous environments, where it creates task-agnostic models that can be applied to any downstream task.",1
"In a discounted reward Markov Decision Process (MDP), the objective is to find the optimal value function, i.e., the value function corresponding to an optimal policy. This problem reduces to solving a functional equation known as the Bellman equation and a fixed point iteration scheme known as the value iteration is utilized to obtain the solution. In literature, a successive over-relaxation based value iteration scheme is proposed to speed-up the computation of the optimal value function. The speed-up is achieved by constructing a modified Bellman equation that ensures faster convergence to the optimal value function. However, in many practical applications, the model information is not known and we resort to Reinforcement Learning (RL) algorithms to obtain optimal policy and value function. One such popular algorithm is Q-learning. In this paper, we propose Successive Over-Relaxation (SOR) Q-learning. We first derive a modified fixed point iteration for SOR Q-values and utilize stochastic approximation to derive a learning algorithm to compute the optimal value function and an optimal policy. We then prove the almost sure convergence of the SOR Q-learning to SOR Q-values. Finally, through numerical experiments, we show that SOR Q-learning is faster compared to the standard Q-learning algorithm.",0
"The goal of a Markov Decision Process (MDP) with discounted rewards is to determine the optimum value function, which corresponds to the best policy. This involves solving the Bellman equation using value iteration. Researchers have developed a modified version of value iteration called Successive Over-Relaxation (SOR) to speed up the computation of the optimal value function. However, in practical applications, the model is often unknown, and Reinforcement Learning (RL) algorithms, such as Q-learning, are used instead. This paper introduces a new algorithm called SOR Q-learning, which uses a modified fixed point iteration and stochastic approximation to determine the optimal value function and policy. The paper proves that SOR Q-learning almost surely converges to SOR Q-values and provides numerical experiments demonstrating its faster performance compared to standard Q-learning.",1
"We present SmartChoices, an approach to making machine learning (ML) a first class citizen in programming languages which we see as one way to lower the entrance cost to applying ML to problems in new domains. There is a growing divide in approaches to building systems: on the one hand, programming leverages human experts to define a system while on the other hand behavior is learned from data in machine learning. We propose to hybridize these two by providing a 3-call API which we expose through an object called SmartChoice. We describe the SmartChoices-interface, how it can be used in programming with minimal code changes, and demonstrate that it is an easy to use but still powerful tool by demonstrating improvements over not using ML at all on three algorithmic problems: binary search, QuickSort, and caches. In these three examples, we replace the commonly used heuristics with an ML model entirely encapsulated within a SmartChoice and thus requiring minimal code changes. As opposed to previous work applying ML to algorithmic problems, our proposed approach does not require to drop existing implementations but seamlessly integrates into the standard software development workflow and gives full control to the software developer over how ML methods are applied. Our implementation relies on standard Reinforcement Learning (RL) methods. To learn faster, we use the heuristic function, which they are replacing, as an initial function. We show how this initial function can be used to speed up and stabilize learning while providing a safety net that prevents performance to become substantially worse -- allowing for a safe deployment in critical applications in real life.",0
"Our approach, called SmartChoices, aims to integrate machine learning (ML) into programming languages to make it more accessible for problem-solving in new domains. Currently, there is a divide in system-building approaches between programming, which relies on human input, and ML, which learns from data. We propose a hybrid solution by introducing the SmartChoice object and a 3-call API that allows for minimal code changes. This tool can be easily used to improve the performance of algorithms, such as binary search, QuickSort, and caches, by replacing commonly used heuristics with an encapsulated ML model within SmartChoice. Unlike previous work, our approach does not require the abandonment of existing implementations and can be seamlessly integrated into the standard software development workflow. Our implementation uses standard Reinforcement Learning (RL) methods and leverages the heuristic function as an initial function to speed up and stabilize the learning process. This approach ensures a safe deployment in critical applications without compromising performance.",1
"We model human decision-making behaviors in a risk-taking task using inverse reinforcement learning (IRL) for the purposes of understanding real human decision making under risk. To the best of our knowledge, this is the first work applying IRL to reveal the implicit reward function in human risk-taking decision making and to interpret risk-prone and risk-averse decision-making policies. We hypothesize that the state history (e.g. rewards and decisions in previous trials) are related to the human reward function, which leads to risk-averse and risk-prone decisions. We design features that reflect these factors in the reward function of IRL and learn the corresponding weight that is interpretable as the importance of features. The results confirm the sub-optimal risk-related decisions of human-driven by the personalized reward function. In particular, the risk-prone person tends to decide based on the current pump number, while the risk-averse person relies on burst information from the previous trial and the average end status. Our results demonstrate that IRL is an effective tool to model human decision-making behavior, as well as to help interpret the human psychological process in risk decision-making.",0
"We utilized inverse reinforcement learning (IRL) to simulate human decision-making behaviors in a risk-taking task in order to gain insight into actual human decision-making under risk. This is the first study that applies IRL to reveal the implicit reward function in human risk-taking decision making and to interpret risk-averse and risk-prone decision-making policies. Our hypothesis is that the state history, such as previous rewards and decisions, is linked to the human reward function, leading to risk-averse and risk-prone decisions. We developed features that reflect these factors in the reward function of IRL and determined the corresponding weight, which is interpretable as the importance of features. Our findings confirm that human-driven sub-optimal risk-related decisions are influenced by personalized reward functions. Specifically, risk-prone individuals tend to base decisions on the current pump number, while risk-averse individuals rely on burst information from the previous trial and the average end status. Our results demonstrate that IRL is an effective tool for modeling human decision-making behavior and to help interpret the human psychological process in risk decision-making.",1
"Curriculum learning in reinforcement learning is used to shape exploration by presenting the agent with increasingly complex tasks. The idea of curriculum learning has been largely applied in both animal training and pedagogy. In reinforcement learning, all previous task sequencing methods have shaped exploration with the objective of reducing the time to reach a given performance level. We propose novel uses of curriculum learning, which arise from choosing different objective functions. Furthermore, we define a general optimization framework for task sequencing and evaluate the performance of popular metaheuristic search methods on several tasks. We show that curriculum learning can be successfully used to: improve the initial performance, take fewer suboptimal actions during exploration, and discover better policies.",0
"The concept of curriculum learning in reinforcement learning involves gradually introducing the agent to more complex tasks in order to guide its exploration. This approach has been utilized in animal training and pedagogy, and previous methods for task sequencing in reinforcement learning have aimed to minimize the time required to achieve a certain level of performance. Our proposal suggests alternative uses of curriculum learning by varying the objective functions, and we also introduce a general optimization framework for task sequencing. Through evaluating metaheuristic search methods on various tasks, we demonstrate the efficacy of curriculum learning in enhancing initial performance, reducing suboptimal actions during exploration, and uncovering superior policies.",1
"Curriculum learning has been successfully used in reinforcement learning to accelerate the learning process, through knowledge transfer between tasks of increasing complexity. Critical tasks, in which suboptimal exploratory actions must be minimized, can benefit from curriculum learning, and its ability to shape exploration through transfer. We propose a task sequencing algorithm maximizing the cumulative return, that is, the return obtained by the agent across all the learning episodes. By maximizing the cumulative return, the agent not only aims at achieving high rewards as fast as possible, but also at doing so while limiting suboptimal actions. We experimentally compare our task sequencing algorithm to several popular metaheuristic algorithms for combinatorial optimization, and show that it achieves significantly better performance on the problem of cumulative return maximization. Furthermore, we validate our algorithm on a critical task, optimizing a home controller for a micro energy grid.",0
"The process of learning through transfer of knowledge between tasks of increasing complexity, known as curriculum learning, has proven successful in reinforcement learning for faster learning. Curriculum learning can be particularly useful for critical tasks where minimizing suboptimal exploratory actions is crucial, as it can shape exploration through transfer. Our proposal is an algorithm for sequencing tasks that maximizes the cumulative return - the return obtained by the agent across all learning episodes. This algorithm aims to not only achieve high rewards quickly, but also limit suboptimal actions. To test our algorithm's effectiveness, we compared it to several popular metaheuristic algorithms for combinatorial optimization and found that it outperformed them in maximizing the cumulative return. Additionally, we validated our algorithm in a real-world application - optimizing a home controller for a micro energy grid.",1
"Continuous action policy search is currently the focus of intensive research, driven both by the recent success of deep reinforcement learning algorithms and the emergence of competitors based on evolutionary algorithms. In this paper, we present a broad survey of policy search methods, providing a unified perspective on very different approaches, including also Bayesian Optimization and directed exploration methods. The main message of this overview is in the relationship between the families of methods, but we also outline some factors underlying sample efficiency properties of the various approaches.",0
"Intense research is currently centered on continuous action policy search, fueled by the success of deep reinforcement learning algorithms and the emergence of evolutionary algorithm-based competitors. This article presents a comprehensive review of policy search methods, offering a unified perspective on diverse approaches, including Bayesian Optimization and directed exploration methods. The primary objective of this overview is to highlight the connection between the different techniques, while also outlining the factors that contribute to the sample efficiency properties of each approach.",1
"Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon $H$. In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit to learn a low-dimensional representation of the probability transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ${O}\big(H^2d\log T\sqrt{T}\big)$ where $d$ is the number of features. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ${O}\big(H^2\widetilde{d}\log T\sqrt{T}\big)$, where $\widetilde{d}$ is the effective dimension of the kernel space. To our best knowledge, for RL using features or kernels, our results are the first regret bounds that are near-optimal in time $T$ and dimension $d$ (or $\widetilde{d}$) and polynomial in the planning horizon $H$.",0
"When the state-action space is large, exploration in reinforcement learning (RL) is hindered by the curse of dimensionality. To overcome this, value and policy functions can be parameterized using given features. However, existing methods have either no theoretical guarantee or experience exponential regret in the planning horizon H. This paper introduces the MatrixRL algorithm, which uses ideas from linear bandit to learn a low-dimensional representation of the probability transition model while balancing the exploitation-exploration tradeoff. MatrixRL achieves a regret bound of O(H^2dlogTsqrt(T)), where d is the number of features. A kernelized version of MatrixRL is also presented, which works with an arbitrary kernel Hilbert space without explicit features. This version has a regret bound of O(H^2tilde{d}logTsqrt(T)), where tilde{d} is the effective dimension of the kernel space. These results are the first regret bounds that are near-optimal in time T and dimension d (or tilde{d}) and polynomial in planning horizon H for RL using features or kernels.",1
"Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI .",0
"Sparse reward signals pose a challenge for reinforcement learning algorithms, as random exploration methods may rely on chance to reach rewarding states. To address this, recent research has explored intrinsic motivation techniques such as generative models, predictive forward models, and novelty discrimination. Our proposed method, EMI, constructs an embedding representation of states and actions that does not require generative decoding of the full observation. Instead, it extracts predictive signals to guide exploration through forward prediction in the representation space. Our experiments demonstrate competitive results in challenging locomotion tasks with continuous control and image-based exploration tasks with discrete actions on Atari. The source code for EMI is available at https://github.com/snu-mllab/EMI.",1
"Many AI problems, in robotics and other domains, are goal-directed, essentially seeking a trajectory leading to some goal state. In such problems, the way we choose to represent a trajectory underlies algorithms for trajectory prediction and optimization. Interestingly, most all prior work in imitation and reinforcement learning builds on a sequential trajectory representation -- calculating the next state in the trajectory given its predecessors. We propose a different perspective: a goal-conditioned trajectory can be represented by first selecting an intermediate state between start and goal, partitioning the trajectory into two. Then, recursively, predicting intermediate points on each sub-segment, until a complete trajectory is obtained. We call this representation a sub-goal tree, and building on it, we develop new methods for trajectory prediction, learning, and optimization. We show that in a supervised learning setting, sub-goal trees better account for trajectory variability, and can predict trajectories exponentially faster at test time by leveraging a concurrent computation. Then, for optimization, we derive a new dynamic programming equation for sub-goal trees, and use it to develop new planning and reinforcement learning algorithms. These algorithms, which are not based on the standard Bellman equation, naturally account for hierarchical sub-goal structure in a task. Empirical results on motion planning domains show that the sub-goal tree framework significantly improves both accuracy and prediction time.",0
"In various fields, including robotics, many AI problems involve reaching a goal state by following a trajectory. The accuracy of trajectory prediction and optimization algorithms depends on how we represent the trajectory. Previous work in imitation and reinforcement learning has mainly used a sequential representation, where the next state in the trajectory is calculated based on its predecessors. Our approach is different: we propose representing a goal-conditioned trajectory by selecting an intermediate state between the start and goal, dividing the trajectory into two segments, and recursively predicting intermediate points on each sub-segment until a complete trajectory is obtained. This representation is called a sub-goal tree and allows for new methods in trajectory prediction, learning, and optimization. We show that using sub-goal trees in supervised learning results in better trajectory variability and faster prediction at test time by using concurrent computation. In optimization, we derive a new dynamic programming equation for sub-goal trees, which allows us to develop new planning and reinforcement learning algorithms that naturally consider hierarchical sub-goal structures in a task. Empirical results in motion planning domains demonstrate that the sub-goal tree framework significantly improves both accuracy and prediction time.",1
"We examine the question of when and how parametric models are most useful in reinforcement learning. In particular, we look at commonalities and differences between parametric models and experience replay. Replay-based learning algorithms share important traits with model-based approaches, including the ability to plan: to use more computation without additional data to improve predictions and behaviour. We discuss when to expect benefits from either approach, and interpret prior work in this context. We hypothesise that, under suitable conditions, replay-based algorithms should be competitive to or better than model-based algorithms if the model is used only to generate fictional transitions from observed states for an update rule that is otherwise model-free. We validated this hypothesis on Atari 2600 video games. The replay-based algorithm attained state-of-the-art data efficiency, improving over prior results with parametric models.",0
"Our focus is on determining the optimal circumstances and methods for implementing parametric models in reinforcement learning. Our analysis specifically evaluates the similarities and differences between parametric models and experience replay. Although replay-based learning algorithms possess qualities similar to those of model-based approaches, such as the capacity to plan, we examine when each approach is most advantageous and interpret previous research in this regard. Our hypothesis is that if the model is solely utilized to generate hypothetical transitions from observed states for a model-free update rule, replay-based algorithms will be as good as or better than model-based algorithms under certain conditions. Our hypothesis was tested on Atari 2600 video games, where the replay-based algorithm achieved the best data efficiency, surpassing previous results with parametric models.",1
"Value-based reinforcement-learning algorithms provide state-of-the-art results in model-free discrete-action settings, and tend to outperform actor-critic algorithms. We argue that actor-critic algorithms are limited by their need for an on-policy critic. We propose Bootstrapped Dual Policy Iteration (BDPI), a novel model-free reinforcement-learning algorithm for continuous states and discrete actions, with an actor and several off-policy critics. Off-policy critics are compatible with experience replay, ensuring high sample-efficiency, without the need for off-policy corrections. The actor, by slowly imitating the average greedy policy of the critics, leads to high-quality and state-specific exploration, which we compare to Thompson sampling. Because the actor and critics are fully decoupled, BDPI is remarkably stable, and unusually robust to its hyper-parameters. BDPI is significantly more sample-efficient than Bootstrapped DQN, PPO, and ACKTR, on discrete, continuous and pixel-based tasks. Source code: https://github.com/vub-ai-lab/bdpi.",0
"In model-free discrete-action settings, value-based reinforcement-learning algorithms are currently the best performing and surpass actor-critic algorithms. However, the latter is limited due to its reliance on an on-policy critic. To address this issue, we introduce a new model-free reinforcement-learning algorithm called Bootstrapped Dual Policy Iteration (BDPI) for continuous states and discrete actions. BDPI consists of an actor and multiple off-policy critics that are compatible with experience replay, thus ensuring high sample-efficiency without the need for off-policy corrections. By slowly imitating the average greedy policy of the critics, the actor promotes high-quality and state-specific exploration, similar to Thompson sampling. BDPI is remarkably stable and robust to hyper-parameters due to the complete decoupling of the actor and critics. It is also significantly more sample-efficient than Bootstrapped DQN, PPO, and ACKTR in discrete, continuous, and pixel-based tasks. The source code for BDPI can be found at https://github.com/vub-ai-lab/bdpi.",1
"The standard reinforcement learning (RL) formulation considers the expectation of the (discounted) cumulative reward. This is limiting in applications where we are concerned with not only the expected performance, but also the distribution of the performance. In this paper, we introduce micro-objective reinforcement learning --- an alternative RL formalism that overcomes this issue. In this new formulation, a RL task is specified by a set of micro-objectives, which are constructs that specify the desirability or undesirability of events. In addition, micro-objectives allow prior knowledge in the form of temporal abstraction to be incorporated into the global RL objective. The generality of this formalism, and its relations to single/multi-objective RL, and hierarchical RL are discussed.",0
"The traditional approach to reinforcement learning (RL) focuses solely on the expected cumulative reward, which can be restrictive in contexts where performance distribution is also a concern. To address this limitation, we propose a new RL framework called micro-objective reinforcement learning. This approach defines RL tasks based on micro-objectives, which represent the desirability or undesirability of events. Moreover, micro-objectives enable the integration of prior knowledge in the form of temporal abstraction into the overall RL objective. We explore the versatility of this formalism and its connections to single/multi-objective RL and hierarchical RL.",1
"We introduce a unified probabilistic framework for solving sequential decision making problems ranging from Bayesian optimisation to contextual bandits and reinforcement learning. This is accomplished by a probabilistic model-based approach that explains observed data while capturing predictive uncertainty during the decision making process. Crucially, this probabilistic model is chosen to be a Meta-Learning system that allows learning from a distribution of related problems, allowing data efficient adaptation to a target task. As a suitable instantiation of this framework, we explore the use of Neural processes due to statistical and computational desiderata. We apply our framework to a broad range of problem domains, such as control problems, recommender systems and adversarial attacks on RL agents, demonstrating an efficient and general black-box learning approach.",0
"A unified probabilistic framework is presented in this study to address sequential decision making problems, including Bayesian optimization, contextual bandits, and reinforcement learning. This framework utilizes a probabilistic model-based approach that not only explains observed data but also accounts for predictive uncertainty during the decision-making process. To enable data-efficient adaptation to a target task, a Meta-Learning system is chosen as the probabilistic model, which can learn from a distribution of related problems. Neural processes are employed as a suitable instantiation of this framework due to statistical and computational requirements. The framework is applied to various problem domains, such as control problems, recommender systems, and adversarial attacks on RL agents, illustrating an effective and general black-box learning approach.",1
"Multi-agent learning is a promising method to simulate aggregate competitive behaviour in finance. Learning expert agents' reward functions through their external demonstrations is hence particularly relevant for subsequent design of realistic agent-based simulations. Inverse Reinforcement Learning (IRL) aims at acquiring such reward functions through inference, allowing to generalize the resulting policy to states not observed in the past. This paper investigates whether IRL can infer such rewards from agents within real financial stochastic environments: limit order books (LOB). We introduce a simple one-level LOB, where the interactions of a number of stochastic agents and an expert trading agent are modelled as a Markov decision process. We consider two cases for the expert's reward: either a simple linear function of state features; or a complex, more realistic non-linear function. Given the expert agent's demonstrations, we attempt to discover their strategy by modelling their latent reward function using linear and Gaussian process (GP) regressors from previous literature, and our own approach through Bayesian neural networks (BNN). While the three methods can learn the linear case, only the GP-based and our proposed BNN methods are able to discover the non-linear reward case. Our BNN IRL algorithm outperforms the other two approaches as the number of samples increases. These results illustrate that complex behaviours, induced by non-linear reward functions amid agent-based stochastic scenarios, can be deduced through inference, encouraging the use of inverse reinforcement learning for opponent-modelling in multi-agent systems.",0
"The use of multi-agent learning is a promising approach to simulate competitive behaviour in finance. In order to design realistic agent-based simulations, it is important to learn the expert agents' reward functions through their external demonstrations. Inverse Reinforcement Learning (IRL) is a method that aims to acquire these reward functions through inference, allowing for generalization of resulting policies to unobserved states. This research examines whether IRL can infer such rewards from agents in real financial stochastic environments, specifically in limit order books (LOB). A simple one-level LOB is introduced, where the interactions of stochastic agents and an expert trading agent are modeled as a Markov decision process. Two cases for the expert's reward are considered: a simple linear function of state features, or a more realistic non-linear function. The expert agent's strategy is discovered through modeling their latent reward function using linear and Gaussian process (GP) regressors from previous literature, and a Bayesian neural network (BNN) approach. While all three methods can learn the linear case, only the GP-based and BNN methods can discover the non-linear reward case. The BNN IRL algorithm outperforms the other two approaches as the number of samples increases. These results demonstrate that IRL can deduce complex behaviours induced by non-linear reward functions in agent-based stochastic scenarios, encouraging its use for opponent-modelling in multi-agent systems.",1
"Recent efforts in Machine Learning (ML) interpretability have focused on creating methods for explaining black-box ML models. However, these methods rely on the assumption that simple approximations, such as linear models or decision-trees, are inherently human-interpretable, which has not been empirically tested. Additionally, past efforts have focused exclusively on comprehension, neglecting to explore the trust component necessary to convince non-technical experts, such as clinicians, to utilize ML models in practice. In this paper, we posit that reinforcement learning (RL) can be used to learn what is interpretable to different users and, consequently, build their trust in ML models. To validate this idea, we first train a neural network to provide risk assessments for heart failure patients. We then design a RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving a diverse set of clinicians from multiple institutions in three different countries. Our results demonstrate that ML experts cannot accurately predict which system outputs will maximize clinicians' confidence in the underlying neural network model, and suggest additional findings that have broad implications to the future of research into ML interpretability and the use of ML in medicine.",0
"The recent focus in Machine Learning (ML) interpretability has been on developing methods to explain the workings of black-box ML models. However, these methods assume that simple approximations such as decision-trees or linear models are easily understandable, which has not been empirically tested. Furthermore, previous efforts have ignored the importance of building trust among non-technical experts such as clinicians, who need to be convinced to use ML models in practice. In this article, we propose that reinforcement learning (RL) can be used to determine what is interpretable to different users and thereby establish their trust in ML models. To test this idea, we first train a neural network to assess the risk of heart failure patients. We then develop an RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving clinicians from different institutions in three countries, and our findings reveal that ML experts are unable to predict which system outputs will enhance clinicians' confidence in the underlying neural network model. Our results have important implications for future research into ML interpretability and the use of ML in medicine.",1
"The utility of learning a dynamics/world model of the environment in reinforcement learning has been shown in a many ways. When using neural networks, however, these models suffer catastrophic forgetting when learned in a lifelong or continual fashion. Current solutions to the continual learning problem require experience to be segmented and labeled as discrete tasks, however, in continuous experience it is generally unclear what a sufficient segmentation of tasks would be. Here we propose a method to continually learn these internal world models through the interleaving of internally generated episodes of past experiences (i.e., pseudo-rehearsal). We show this method can sequentially learn unsupervised temporal prediction, without task labels, in a disparate set of Atari games. Empirically, this interleaving of the internally generated rollouts with the external environment's observations leads to a consistent reduction in temporal prediction loss compared to non-interleaved learning and is preserved over repeated random exposures to various tasks. Similarly, using a network distillation approach, we show that modern policy gradient based reinforcement learning algorithms can use this internal model to continually learn to optimize reward based on the world model's representation of the environment.",0
"Numerous studies have demonstrated the usefulness of acquiring a dynamics or world model of the environment in reinforcement learning. Unfortunately, when utilizing neural networks, these models tend to experience catastrophic forgetting when acquired in a continuous or lifelong manner. Currently, the most popular solutions to the continual learning problem require experience to be divided into discrete tasks, which can be challenging in situations where experience is continuous. Our proposed solution involves interleaving internally generated memories of past experiences (i.e., pseudo-rehearsal) to continually acquire these internal world models. We demonstrate that this method can learn unsupervised temporal prediction in a diverse range of Atari games without task labels. Furthermore, by combining this method with network distillation, we show that modern policy gradient-based reinforcement learning algorithms can continually optimize reward based on the world model's representation of the environment. Our results indicate that interleaving these internally generated rollouts with external observations can consistently reduce temporal prediction loss compared to non-interleaved learning and is maintained even after multiple random exposures to various tasks.",1
"We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL is the first provably efficient algorithm in ILFO setting, which learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results, which typically only consider tabular reinforcement learning settings or settings that require access to a near-optimal reset distribution. We also investigate the extension of FAIL in a model-based setting. Finally we demonstrate the efficacy of FAIL on multiple OpenAI Gym control tasks.",0
"Our focus is on Imitation Learning (IL) through Observations alone (ILFO) in large-scale MDPs. Commonly, IL algorithms require an expert to provide actions directly to the learner, but our approach is different: the expert only provides observation sequences. We have developed Forward Adversarial Imitation Learning (FAIL), a new model-free algorithm for ILFO. FAIL learns time-dependent policies by minimizing an Integral Probability Metric between the expert's policy and the learner's observation distributions. It is the first algorithm to be proven efficient in the ILFO setting, learning near-optimal policies with a number of samples that is polynomial in relevant parameters and independent of the number of unique observations. This extends the domain of provably sample-efficient learning algorithms beyond existing results that consider only tabular reinforcement learning or settings requiring access to a near-optimal reset distribution. We have also explored the extension of FAIL in a model-based setting and demonstrated its effectiveness in multiple OpenAI Gym control tasks.",1
"Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.",0
"The latest advancements in deep reinforcement learning are focused on developing decision-making agents capable of performing effectively in complex domains. One particular technique that has garnered increased attention is multi-agent reinforcement learning, where multiple agents learn simultaneously to coordinate their actions. In such scenarios, the dynamic decision-making policies of the agents pose additional learning challenges. This research paper examines recent efforts to address the non-stationarity issue in multi-agent deep reinforcement learning. The discussed approaches involve adjustments to the training process, such as centralized training, learning the opponent's policy, meta-learning, communication, and decentralized learning. The study concludes with a list of unresolved issues and potential research avenues.",1
"We focus on the problem of teaching a robot to solve tasks presented sequentially, i.e., in a continual learning scenario. The robot should be able to solve all tasks it has encountered, without forgetting past tasks. We provide preliminary work on applying Reinforcement Learning to such setting, on 2D navigation tasks for a 3 wheel omni-directional robot. Our approach takes advantage of state representation learning and policy distillation. Policies are trained using learned features as input, rather than raw observations, allowing better sample efficiency. Policy distillation is used to combine multiple policies into a single one that solves all encountered tasks.",0
"Our primary concern is teaching a robot to tackle tasks that are presented one after another, without forgetting previous ones, in a continual learning environment. Our focus in this work is on 2D navigation tasks for a 3-wheel omni-directional robot, and we explore the application of Reinforcement Learning for this purpose. To optimize sample efficiency, we employ state representation learning and policy distillation, where policies are trained using learned features instead of raw observations. We combine multiple policies into a single solution for all tasks using policy distillation.",1
"Model-based Reinforcement Learning approaches have the promise of being sample efficient. Much of the progress in learning dynamics models in RL has been made by learning models via supervised learning. But traditional model-based approaches lead to `compounding errors' when the model is unrolled step by step. Essentially, the state transitions that the learner predicts (by unrolling the model for multiple steps) and the state transitions that the learner experiences (by acting in the environment) may not be consistent. There is enough evidence that humans build a model of the environment, not only by observing the environment but also by interacting with the environment. Interaction with the environment allows humans to carry out experiments: taking actions that help uncover true causal relationships which can be used for building better dynamics models. Analogously, we would expect such interactions to be helpful for a learning agent while learning to model the environment dynamics. In this paper, we build upon this intuition by using an auxiliary cost function to ensure consistency between what the agent observes (by acting in the real world) and what it imagines (by acting in the `learned' world). We consider several tasks - Mujoco based control tasks and Atari games - and show that the proposed approach helps to train powerful policies and better dynamics models.",0
"The potential for sample efficiency exists in Model-based Reinforcement Learning approaches. Progress in learning dynamics models has been made through supervised learning, but traditional approaches can result in errors compounding over time. This is due to inconsistencies between the predicted state transitions and the actual state transitions experienced by the learner. Humans are able to build a model of their environment through interaction, which enables them to conduct experiments and discover causal relationships for better model building. Similarly, interaction with the environment can aid a learning agent in modeling its dynamics. This paper uses an auxiliary cost function to ensure consistency between what the agent observes in the real world and what it imagines in the learned world. Through testing on Mujoco based control tasks and Atari games, the proposed approach is shown to effectively train policies and improve dynamics models.",1
"Reinforcement learning has seen great advancements in the past five years. The successful introduction of deep learning in place of more traditional methods allowed reinforcement learning to scale to very complex domains achieving super-human performance in environments like the game of Go or numerous video games. Despite great successes in multiple domains, these new methods suffer from their own issues that make them often inapplicable to the real world problems. Extreme lack of data efficiency, together with huge variance and difficulty in enforcing safety constraints, is one of the three most prominent issues in the field. Usually, millions of data points sampled from the environment are necessary for these algorithms to converge to acceptable policies.   This thesis proposes novel Generative Adversarial Imaginative Reinforcement Learning algorithm. It takes advantage of the recent introduction of highly effective generative adversarial models, and Markov property that underpins reinforcement learning setting, to model dynamics of the real environment within the internal imagination module. Rollouts from the imagination are then used to artificially simulate the real environment in a standard reinforcement learning process to avoid, often expensive and dangerous, trial and error in the real environment. Experimental results show that the proposed algorithm more economically utilises experience from the real environment than the current state-of-the-art Rainbow DQN algorithm, and thus makes an important step towards sample efficient deep reinforcement learning.",0
"In the last five years, reinforcement learning has made significant progress. The replacement of traditional methods with deep learning has allowed the scaling of reinforcement learning to complex domains, achieving super-human performance in environments such as the game of Go and various video games. However, these new methods have their own issues that limit their applicability to real-world problems, including a lack of data efficiency, high variance, and difficulty in enforcing safety constraints. Typically, millions of data points are required for these algorithms to converge to acceptable policies. To address these issues, this thesis proposes a novel Generative Adversarial Imaginative Reinforcement Learning algorithm that leverages highly effective generative adversarial models and the Markov property to model the dynamics of the real environment within the internal imagination module. The algorithm uses rollouts from the imagination to simulate the real environment in a standard reinforcement learning process, reducing the need for expensive and dangerous trial and error in the real environment. Experimental results demonstrate that the proposed algorithm utilizes experience from the real environment more efficiently than the current state-of-the-art Rainbow DQN algorithm, representing a significant step towards sample-efficient deep reinforcement learning.",1
"Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/",0
"Sensorimotor learning has long struggled with efficient exploration, particularly in stochastic environments. While noise-free, non-stochastic domains like video games and simulation have made progress, they do not scale well to real robotics setups. To address this issue, we propose a new approach inspired by active learning literature, which involves training an ensemble of dynamics models and maximizing the disagreement between them to encourage self-supervised exploration without external rewards. Our method also optimizes the agent's policy in a differentiable manner using the disagreement objective, resulting in sample-efficient exploration. We validate our approach across various benchmark environments, including stochastic-Atari, Mujoco, and Unity, and demonstrate its effectiveness on a real robot learning to interact with objects from scratch. Project videos and code can be found at https://pathak22.github.io/exploration-by-disagreement/.",1
"We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.",0
"We suggest CAVIA for meta-learning, which is a straightforward modification of MAML. It is less susceptible to meta-overfitting, easier to parallelize, and more straightforward to understand. CAVIA divides the model parameters into two parts: context parameters, which act as extra input to the model and are adjusted for each task, and shared parameters, which are meta-trained and shared across tasks. During testing, only the context parameters are modified, leading to a low-dimensional task representation. We demonstrate through experiments that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Additionally, our experiments reveal flaws in current benchmarks, as some cases require only a small amount of adaptation.",1
"Soft Actor-Critic (SAC) is an off-policy actor-critic deep reinforcement learning (DRL) algorithm based on maximum entropy reinforcement learning. By combining off-policy updates with an actor-critic formulation, SAC achieves state-of-the-art performance on a range of continuous-action benchmark tasks, outperforming prior on-policy and off-policy methods. The off-policy method employed by SAC samples data uniformly from past experience when performing parameter updates. We propose Emphasizing Recent Experience (ERE), a simple but powerful off-policy sampling technique, which emphasizes recently observed data while not forgetting the past. The ERE algorithm samples more aggressively from recent experience, and also orders the updates to ensure that updates from old data do not overwrite updates from new data. We compare vanilla SAC and SAC+ERE, and show that ERE is more sample efficient than vanilla SAC for continuous-action Mujoco tasks. We also consider combining SAC with Priority Experience Replay (PER), a scheme originally proposed for deep Q-learning which prioritizes the data based on temporal-difference (TD) error. We show that SAC+PER can marginally improve the sample efficiency performance of SAC, but much less so than SAC+ERE. Finally, we propose an algorithm which integrates ERE and PER and show that this hybrid algorithm can give the best results for some of the Mujoco tasks.",0
"The Soft Actor-Critic (SAC) algorithm is a deep reinforcement learning (DRL) technique that utilizes maximum entropy reinforcement learning and combines off-policy updates with an actor-critic formulation to achieve superior performance on continuous-action benchmark tasks. SAC samples data uniformly from past experience during parameter updates, but a new off-policy sampling technique called Emphasizing Recent Experience (ERE) has been proposed to improve sample efficiency. ERE prioritizes recently observed data while still considering past experience and orders updates to avoid overwriting new data with old data. SAC+ERE outperforms vanilla SAC on continuous-action Mujoco tasks. Priority Experience Replay (PER), a data prioritization scheme, is also considered for SAC. SAC+PER marginally improves sample efficiency, but not as much as SAC+ERE. A hybrid algorithm that integrates ERE and PER gives the best results for some Mujoco tasks.",1
"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",0
"In order for Reinforcement Learning (RL) to achieve success in real-world tasks, it is necessary to effectively utilize the hierarchical, relational, and compositional structure of the environment, and learn how to transfer it to the task at hand. Recent developments in language representation learning enable the creation of models that can learn about the world through text corpora and incorporate that knowledge into decision-making problems downstream. Therefore, we propose that the integration of natural language understanding into RL should be explored more closely. We examine the current state of the field, including research on instruction-following, text games, and learning from textual domain knowledge. Ultimately, we recommend the creation of new environments and further exploration into the potential uses of recent Natural Language Processing (NLP) techniques in these tasks.",1
"Network slicing promises to provision diversified services with distinct requirements in one infrastructure. Deep reinforcement learning (e.g., deep $\mathcal{Q}$-learning, DQL) is assumed to be an appropriate algorithm to solve the demand-aware inter-slice resource management issue in network slicing by regarding the varying demands and the allocated bandwidth as the environment state and the action, respectively. However, allocating bandwidth in a finer resolution usually implies larger action space, and unfortunately DQL fails to quickly converge in this case. In this paper, we introduce discrete normalized advantage functions (DNAF) into DQL, by separating the $\mathcal{Q}$-value function as a state-value function term and an advantage term and exploiting a deterministic policy gradient descent (DPGD) algorithm to avoid the unnecessary calculation of $\mathcal{Q}$-value for every state-action pair. Furthermore, as DPGD only works in continuous action space, we embed a k-nearest neighbor algorithm into DQL to quickly find a valid action in the discrete space nearest to the DPGD output. Finally, we verify the faster convergence of the DNAF-based DQL through extensive simulations.",0
"Network slicing offers the ability to provide various services with distinct requirements on a single infrastructure. To address the issue of managing inter-slice resources based on demand, deep reinforcement learning (DRL) algorithms, such as deep $\mathcal{Q}$-learning (DQL), have been proposed. However, finer bandwidth allocation leads to a larger action space, which can impede the convergence of DQL. To address this issue, we propose integrating discrete normalized advantage functions (DNAF) into DQL. This involves separating the $\mathcal{Q}$-value function into a state-value function term and an advantage term, and using a deterministic policy gradient descent (DPGD) algorithm to avoid unnecessary calculations. To handle the discrete action space, we incorporate a k-nearest neighbor algorithm into DQL. Our simulations demonstrate that the DNAF-based DQL approach achieves faster convergence than traditional DQL.",1
"Off-policy evaluation (OPE) in both contextual bandits and reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. The problem's importance has attracted many proposed solutions, including importance sampling (IS), self-normalized IS (SNIS), and doubly robust (DR) estimates. DR and its variants ensure semiparametric local efficiency if Q-functions are well-specified, but if they are not they can be worse than both IS and SNIS. It also does not enjoy SNIS's inherent stability and boundedness. We propose new estimators for OPE based on empirical likelihood that are always more efficient than IS, SNIS, and DR and satisfy the same stability and boundedness properties as SNIS. On the way, we categorize various properties and classify existing estimators by them. Besides the theoretical guarantees, empirical studies suggest the new estimators provide advantages.",0
"Off-policy evaluation (OPE) is a technique used in both contextual bandits and reinforcement learning that enables the evaluation of new decision policies without the need for exploration, which can be costly or impractical. Proposed solutions to this problem include importance sampling (IS), self-normalized IS (SNIS), and doubly robust (DR) estimates. While DR and its variations can ensure semiparametric local efficiency if Q-functions are well-specified, they may be worse than IS and SNIS if they are not. Moreover, DR lacks the inherent stability and boundedness of SNIS. This paper proposes new OPE estimators based on empirical likelihood that are always more efficient than IS, SNIS, and DR, and possess the same stability and boundedness properties as SNIS. The paper also categorizes various properties and classifies existing estimators based on them. Empirical studies suggest that the new estimators provide benefits in addition to theoretical guarantees.",1
"Access to parallel and distributed computation has enabled researchers and developers to improve algorithms and performance in many applications. Recent research has focused on next generation special purpose systems with multiple kinds of coprocessors, known as heterogeneous system-on-chips (SoC). In this paper, we introduce a method to intelligently schedule--and learn to schedule--a stream of tasks to available processing elements in such a system. We use deep reinforcement learning enabling complex sequential decision making and empirically show that our reinforcement learning system provides for a viable, better alternative to conventional scheduling heuristics with respect to minimizing execution time.",0
"The availability of parallel and distributed computation has facilitated the enhancement of algorithms and performance in various applications. The latest studies have concentrated on the development of specialized systems for the future that contain several types of coprocessors, referred to as heterogeneous system-on-chips (SoC). This article presents a technique for intelligently scheduling a sequence of tasks to available processing elements in such a system, which can also learn to schedule. Our approach utilizes deep reinforcement learning to enable intricate sequential decision-making and we demonstrate through empirical evidence that our reinforcement learning system offers a suitable and superior alternative to traditional scheduling heuristics in terms of minimizing execution time.",1
"Dealing with sparse rewards is a longstanding challenge in reinforcement learning. The recent use of hindsight methods have achieved success on a variety of sparse-reward tasks, but they fail on complex tasks such as stacking multiple blocks with a robot arm in simulation. Curiosity-driven exploration using the prediction error of a learned dynamics model as an intrinsic reward has been shown to be effective for exploring a number of sparse-reward environments. We present a method that combines hindsight with curiosity-driven exploration and curriculum learning in order to solve the challenging sparse-reward block stacking task. We are the first to stack more than two blocks using only sparse reward without human demonstrations.",0
"For a long time, tackling sparse rewards has been a difficult task in reinforcement learning. While hindsight methods have proven to be successful in handling sparse-reward tasks, they do not work well for complex tasks like stacking several blocks with a robot arm in simulation. To explore sparse-reward environments, curiosity-driven exploration that utilizes the prediction error of a learned dynamics model as an intrinsic reward has been effective. To solve the challenging sparse-reward block stacking task, we have developed a method that combines hindsight with curiosity-driven exploration and curriculum learning. Our approach has enabled us to stack more than two blocks using only sparse reward, without any human demonstrations.",1
"Exploration and adaptation to new tasks in a transfer learning setup is a central challenge in reinforcement learning. In this work, we build on the idea of modeling a distribution over policies in a Bayesian deep reinforcement learning setup to propose a transfer strategy. Recent works have shown to induce diversity in the learned policies by maximizing the entropy of a distribution of policies (Bachman et al., 2018; Garnelo et al., 2018) and thus, we postulate that our proposed approach leads to faster exploration resulting in improved transfer learning. We support our hypothesis by demonstrating favorable experimental results on a variety of settings on fully-observable GridWorld and partially observable MiniGrid (Chevalier-Boisvert et al., 2018) environments.",0
"A main obstacle in reinforcement learning is the ability to explore and adjust to novel tasks within a transfer learning framework. In this study, we expand on the concept of developing a probability distribution over policies in a Bayesian deep reinforcement learning framework to introduce a transfer approach. Recent research has demonstrated that increasing the entropy of a policy distribution (Bachman et al., 2018; Garnelo et al., 2018) can enhance diversity in the learned policies and consequently facilitate quicker exploration, leading to better transfer learning outcomes. Our proposed technique is supported by our favorable experimental outcomes in various fully-observable GridWorld and partially observable MiniGrid (Chevalier-Boisvert et al., 2018) environments.",1
"Imagine a patient in critical condition. What and when should be measured to forecast detrimental events, especially under the budget constraints? We answer this question by deep reinforcement learning (RL) that jointly minimizes the measurement cost and maximizes predictive gain, by scheduling strategically-timed measurements. We learn our policy to be dynamically dependent on the patient's health history. To scale our framework to exponentially large action space, we distribute our reward in a sequential setting that makes the learning easier. In our simulation, our policy outperforms heuristic-based scheduling with higher predictive gain and lower cost. In a real-world ICU mortality prediction task (MIMIC3), our policies reduce the total number of measurements by $31\%$ or improve predictive gain by a factor of $3$ as compared to physicians, under the off-policy policy evaluation.",0
"The use of deep reinforcement learning (RL) can help determine what and when to measure a critically ill patient in order to accurately predict negative events, all while considering budget constraints. Our approach involves scheduling strategically-timed measurements that minimize cost and maximize predictive gain, with the policy being dynamically dependent on the patient's health history. To handle the exponentially large action space, we adopted a sequential setting to distribute the reward, which makes learning easier. Our simulation results show that our policy outperforms heuristic-based scheduling in terms of predictive gain and cost. In a real-world ICU mortality prediction task (MIMIC3), our policies were shown to reduce the total number of measurements by 31% or improve predictive gain by a factor of 3 compared to physicians, as evaluated off-policy.",1
"Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with \emph{exploration} by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. Then, we qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.",0
"The utilization of entropy regularization is a common technique to enhance policy optimization in reinforcement learning. Its primary purpose is to facilitate exploration by encouraging the selection of policies with greater stochasticity. This study examines this assertion by creating new visualizations of the optimization landscape through randomly perturbing the loss function. The research demonstrates that optimizing policy is challenging due to the objective function's geometry, even when the exact gradient is available. Furthermore, the study qualitatively shows that in specific environments, a higher entropy policy can smooth out the optimization landscape, linking local optima and enabling the use of larger learning rates. This research introduces novel tools for comprehending the optimization landscape, asserts policy entropy as a regularizer, and emphasizes the difficulty of developing policy optimization algorithms that are universally applicable.",1
"This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. Our approach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows paradigm: it bootstraps the trained policy with the baseline when the uncertainty is high. Our first algorithm, $\Pi_b$-SPIBB, comes with SPI theoretical guarantees. We also implement a variant, $\Pi_{\leq b}$-SPIBB, that is even more efficient in practice. We apply our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the superiority of SPIBB with respect to existing algorithms, not only in safety but also in mean performance. Finally, we implement a model-free version of SPIBB and show its benefits on a navigation task with deep RL implementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network representation able to train efficiently and reliably from batch data, without any interaction with the environment.",0
"The focus of this paper is on Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL). The aim is to train a policy from a fixed dataset without direct access to the true environment, which performs at least as well as the baseline policy used to collect the data. The approach used, called SPI with Baseline Bootstrapping (SPIBB), is based on the knows-what-it-knows paradigm and bootstraps the trained policy with the baseline when uncertainty is high. The first algorithm, called $\Pi_b$-SPIBB, comes with theoretical guarantees, and a more efficient variant, $\Pi_{\leq b}$-SPIBB, has also been implemented. The algorithms have been tested on a stochastic gridworld domain and randomly generated MDPs, demonstrating superiority in safety and mean performance compared to existing algorithms. Finally, a model-free version of SPIBB called SPIBB-DQN has been implemented, which is the first RL algorithm relying on a neural network representation to train efficiently and reliably from batch data without any interaction with the environment.",1
"Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.",0
"The combination of model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is a challenging task due to the imperfect dynamics model, which can degrade the learning algorithm's performance. The dynamics model is almost always imperfect in complex environments, leading to a key challenge of combining model-based approaches with model-free learning to avoid performance degradation. To address this issue, we propose a novel model-based technique called stochastic ensemble value expansion (STEVE). STEVE dynamically interpolates between model rollouts of various horizon lengths for each individual example, ensuring that the model is only utilized when there are no significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks, with a ten-fold increase in sample efficiency. Unlike previous model-based approaches, our technique does not suffer from performance degradation in complex environments.",1
"We consider the problem of imitation learning from a finite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent generative adversarial methods based on matching the policy distribution between the expert and the agent could be unstable during training. We propose a new framework for imitation learning by estimating the support of the expert policy to compute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comparable or better performance than the state of the art under different reinforcement learning algorithms.",0
"Our focus is on the challenge of imitating the behavior of an expert using a limited number of their trajectories, without reinforcement signals. The traditional method of first determining the expert's reward function through inverse reinforcement learning and then using reinforcement learning can be computationally expensive and indirect. Recently, generative adversarial approaches that match the policy distribution of the expert and agent have proven unstable during training. To overcome these issues, we propose a new imitation learning framework that estimates the expert policy's support to determine a fixed reward function. This approach allows us to reframe imitation learning within the standard reinforcement learning setting. Our approach has been tested on discrete and continuous domains, and we have achieved comparable or better performance than the current state-of-the-art under various reinforcement learning algorithms.",1
"Human behavior expression and experience are inherently multi-modal, and characterized by vast individual and contextual heterogeneity. To achieve meaningful human-computer and human-robot interactions, multi-modal models of the users states (e.g., engagement) are therefore needed. Most of the existing works that try to build classifiers for the users states assume that the data to train the models are fully labeled. Nevertheless, data labeling is costly and tedious, and also prone to subjective interpretations by the human coders. This is even more pronounced when the data are multi-modal (e.g., some users are more expressive with their facial expressions, some with their voice). Thus, building models that can accurately estimate the users states during an interaction is challenging. To tackle this, we propose a novel multi-modal active learning (AL) approach that uses the notion of deep reinforcement learning (RL) to find an optimal policy for active selection of the users data, needed to train the target (modality-specific) models. We investigate different strategies for multi-modal data fusion, and show that the proposed model-level fusion coupled with RL outperforms the feature-level and modality-specific models, and the naive AL strategies such as random sampling, and the standard heuristics such as uncertainty sampling. We show the benefits of this approach on the task of engagement estimation from real-world child-robot interactions during an autism therapy. Importantly, we show that the proposed multi-modal AL approach can be used to efficiently personalize the engagement classifiers to the target user using a small amount of actively selected users data.",0
"To enable effective interactions between humans and computers or robots, it is necessary to develop multi-modal models that can accurately determine the state of the user, such as their engagement level. However, creating such models is challenging because human behavior is complex and can vary greatly depending on the individual and the context. Additionally, existing approaches typically require fully labeled data to train the models, which is expensive and subjective. To address these issues, we propose a novel multi-modal active learning (AL) technique that leverages deep reinforcement learning (RL) to identify the optimal data needed to train modality-specific models. Our approach outperforms other AL strategies, such as random sampling and uncertainty sampling, and allows for efficient personalization of engagement classifiers using a small amount of actively selected data. We demonstrate the effectiveness of our approach on real-world child-robot interactions during autism therapy.",1
"We consider the setup of stochastic multi-armed bandits in the case when reward distributions are piecewise i.i.d. and bounded with unknown changepoints. We focus on the case when changes happen simultaneously on all arms, and in stark contrast with the existing literature, we target gap-dependent (as opposed to only gap-independent) regret bounds involving the magnitude of changes $(\Delta^{chg}_{i,g})$ and optimality-gaps ($\Delta^{opt}_{i,g}$). Diverging from previous works, we assume the more realistic scenario that there can be undetectable changepoint gaps and under a different set of assumptions, we show that as long as the compounded delayed detection for each changepoint is bounded there is no need for forced exploration to actively detect changepoints. We introduce two adaptations of UCB-strategies that employ scan-statistics in order to actively detect the changepoints, without knowing in advance the changepoints and also the mean before and after any change. Our first method \UCBLCPD does not know the number of changepoints $G$ or time horizon $T$ and achieves the first time-uniform concentration bound for this setting using the Laplace method of integration. The second strategy \ImpCPD makes use of the knowledge of $T$ to achieve the order optimal regret bound of $\min\big\lbrace O(\sum\limits_{i=1}^{K} \sum\limits_{g=1}^{G}\frac{\log(T/H_{1,g})}{\Delta^{opt}_{i,g}}), O(\sqrt{GT})\big\rbrace$, (where $H_{1,g}$ is the problem complexity) thereby closing an important gap with respect to the lower bound in a specific challenging setting. Our theoretical findings are supported by numerical experiments on synthetic and real-life datasets.",0
"In this study, we examine stochastic multi-armed bandits with piecewise i.i.d. reward distributions that are bounded with unknown changepoints. Specifically, we focus on the scenario where changes occur simultaneously across all arms and aim to derive gap-dependent regret bounds that consider the magnitude of changes and optimality-gaps. Unlike previous works, we consider the possibility of undetectable changepoint gaps and demonstrate that forced exploration is unnecessary if the compounded delayed detection for each changepoint is limited. We propose two adaptations of UCB-strategies that leverage scan-statistics to detect changepoints without prior knowledge of the number of changepoints or time horizon. Our first method, \UCBLCPD, achieves the first time-uniform concentration bound for this setting using the Laplace method of integration, while our second strategy, \ImpCPD, uses knowledge of the time horizon to achieve the optimal regret bound. Our theoretical findings are supported by experiments on both synthetic and real-life datasets.",1
"Figures, such as bar charts, pie charts, and line plots, are widely used to convey important information in a concise format. They are usually human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning where the goal is to automatically generate a natural language description of the figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. First, we introduce a new dataset for figure captioning, FigCAP, based on FigureQA. Second, we propose two novel attention mechanisms. To achieve accurate generation of labels in figures, we propose Label Maps Attention. To model the relations between figure labels, we propose Relation Maps Attention. Third, we use sequence-level training with reinforcement learning in order to directly optimizes evaluation metrics, which alleviates the exposure bias issue and further improves the models in generating long captions. Extensive experiments show that the proposed method outperforms the baselines, thus demonstrating a significant potential for the automatic captioning of vast repositories of figures.",0
"The use of figures, such as bar charts, pie charts, and line plots, is prevalent in presenting crucial information in a succinct manner. However, these figures are easily understood by humans but pose a challenge for computers to automatically process. This study focuses on the problem of generating a natural language description of a figure, known as figure captioning. This task has not received much attention and remains difficult. Firstly, a new dataset, FigCAP, is introduced for figure captioning based on FigureQA. Secondly, two attention mechanisms are proposed to accurately generate labels in figures: Label Maps Attention and Relation Maps Attention to model the relationships between figure labels. Thirdly, to overcome exposure bias issues and improve the models in generating long captions, sequence-level training with reinforcement learning is employed to directly optimize evaluation metrics. Extensive experiments reveal that the proposed method outperforms the baselines, suggesting huge potential for the automatic captioning of extensive repositories of figures.",1
"The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.",0
"The Arcade Learning Environment (ALE) has become a popular tool for evaluating reinforcement learning agents due to the fact that Atari games showcase the competence we expect from intelligent agents without bias towards any particular solution approach. However, the ALE presents two major challenges: the representation learning problem and the behavioural learning problem. While the latter is often the focus of research, the former adds significant computational expense. To address this, we introduce MinAtar, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus primarily on the behavioural challenges. MinAtar includes analogues of five Atari games, each with a 10x10xn binary state representation, where n corresponds to game-specific objects. By simplifying the representation learning problem, we can perform experiments with significantly less computational expense and allocate the saved compute time to perform step-size parameter sweeps and more runs. This allows for improved reproducibility and more confident conclusions. We hope that MinAtar can provide researchers with a platform to investigate behavioural challenges similar to those in the ALE.",1
"We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite partially observable Markov Decision Processes (POMDPs). We see this as a useful procedure for off-policy ""debugging"" in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the RL and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic. This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.",0
"The paragraph describes a method for detecting instances where a reinforcement learned policy would have led to a different outcome compared to the observed policy. The approach involves using structural causal models to generate counterfactual trajectories in partially observable Markov Decision Processes. This can be useful in ""debugging"" high-risk situations such as healthcare, where specific episodes can be identified based on the expected difference in reward between the policies. This approach can help domain experts review these episodes and make informed decisions. The effectiveness of this method is demonstrated using a synthetic sepsis management environment.",1
"Deep Reinforcement Learning (DRL) algorithms for continuous action spaces are known to be brittle toward hyperparameters as well as \cut{being}sample inefficient. Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework which offers greater stability and empirical gains. The choice of policy distribution, a factored Gaussian, is motivated by \cut{chosen due}its easy re-parametrization rather than its modeling power. We introduce Normalizing Flow policies within the SAC framework that learn more expressive classes of policies than simple factored Gaussians. \cut{We also present a series of stabilization tricks that enable effective training of these policies in the RL setting.}We show empirically on continuous grid world tasks that our approach increases stability and is better suited to difficult exploration in sparse reward settings.",0
"Continuous action space Deep Reinforcement Learning (DRL) algorithms are fragile when it comes to hyperparameters and inefficient in terms of sampling. To address these issues, Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework that provides greater stability and empirical gains. The choice of policy distribution is a factored Gaussian, which is easy to re-parametrize. However, we have introduced Normalizing Flow policies within the SAC framework to learn more expressive classes of policies. Empirical evidence from continuous grid world tasks shows that our approach improves stability and is better suited for difficult exploration in sparse reward settings.",1
"Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.",0
"The agent in many RL tasks receives high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this, we introduce the concept of a DeepMDP, which is a model with a parameterized latent space. It is trained using two losses: predicting rewards and predicting the distribution over next latent states. By optimizing these objectives, we ensure that the DeepMDP is a good model of the environment and that the latent space is a good representation of the state space. We compare our results to previous work in the bisimulation literature and use various metrics. Our experimental results show that a trained DeepMDP can recover the latent structure of high-dimensional observations in a synthetic environment. Additionally, we demonstrate that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to significant improvements in performance compared to model-free RL.",1
"Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",0
"The goal of self-supervised learning is to learn representations from data without manual supervision. However, current efforts neglect the importance of scalability in self-supervision, as this approach does not require manual labels. This study revisits this principle and scales two popular self-supervised approaches to analyze their effectiveness with 100 million images. The researchers demonstrate that by scaling on various dimensions, including data size and problem 'hardness,' it is possible to match or exceed the performance of supervised pre-training for tasks such as object detection, surface normal estimation (3D), and visual navigation using reinforcement learning. Scaling these methods also offers valuable insight into the limitations of current self-supervised techniques and evaluations. The researchers conclude that current self-supervised methods fail to fully exploit the advantages of large-scale data and do not appear to learn effective high-level semantic representations. They also present a comprehensive benchmark across nine different datasets and tasks, emphasizing the importance of comparable evaluation settings. The code for this study is available at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",1
"Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (`where' vs. `what'). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.",0
"Our soft attention model for the reinforcement learning domain is influenced by recent progress in attention models for image captioning and question answering. The model employs a top-down attention mechanism that is soft, creating a bottleneck in the agent, which helps it concentrate on task-relevant information by sequentially interrogating its environment view. The attention mechanism's output provides a clear understanding of the information used by the agent to select its actions, making it more straightforward to interpret than conventional models. We examine different tactics that agents learn and demonstrate that a few strategies emerge consistently across various games. We also prove that the model learns to query separately about space and content (‘where’ vs. ‘what’). We showcase that an agent utilizing this mechanism can achieve state-of-the-art performance on ATARI tasks while remaining transparent.",1
"Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \underline{c}lustered \underline{r}einforcement \underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on a continuous control task and several \emph{Atari 2600} games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.",0
"Reinforcement learning (RL) faces the challenging task of designing an effective exploration strategy, particularly when dealing with large state spaces or sparse rewards. In such cases, the agent must seek out novel or high-quality areas within the environment. However, current methods often fail to adequately utilize information about neighboring areas to guide the agent's exploration. To address this issue, we propose a new RL framework called Clustered Reinforcement Learning (CRL). CRL employs clustering to group collected states into clusters, and then offers the agent a bonus reward that reflects both the novelty and quality of neighboring clusters to guide exploration. Our experiments on a continuous control task and several Atari 2600 games demonstrate that CRL outperforms existing methods and achieves superior performance in most cases.",1
"Recent breakthroughs in AI for multi-agent games like Go, Poker, and Dota, have seen great strides in recent years. Yet none of these games address the real-life challenge of cooperation in the presence of unknown and uncertain teammates. This challenge is a key game mechanism in hidden role games. Here we develop the DeepRole algorithm, a multi-agent reinforcement learning agent that we test on The Resistance: Avalon, the most popular hidden role game. DeepRole combines counterfactual regret minimization (CFR) with deep value networks trained through self-play. Our algorithm integrates deductive reasoning into vector-form CFR to reason about joint beliefs and deduce partially observable actions. We augment deep value networks with constraints that yield interpretable representations of win probabilities. These innovations enable DeepRole to scale to the full Avalon game. Empirical game-theoretic methods show that DeepRole outperforms other hand-crafted and learned agents in five-player Avalon. DeepRole played with and against human players on the web in hybrid human-agent teams. We find that DeepRole outperforms human players as both a cooperator and a competitor.",0
"Great advancements in AI for multi-agent games such as Go, Poker and Dota have been made in recent years. However, these games fail to address the real-life challenge of cooperation in the presence of unknown and uncertain teammates, which is a crucial game mechanism in hidden role games. To tackle this issue, we have developed the DeepRole algorithm. It is a multi-agent reinforcement learning agent that we have tested on The Resistance: Avalon, the most popular hidden role game. We have combined counterfactual regret minimization (CFR) with deep value networks that are trained through self-play. Our algorithm incorporates deductive reasoning into vector-form CFR, which enables it to reason about joint beliefs and deduce partially observable actions. We have augmented deep value networks with constraints that yield interpretable representations of win probabilities. These innovations allow DeepRole to scale to the full Avalon game. Empirical game-theoretic methods demonstrate that DeepRole outperforms other hand-crafted and learned agents in five-player Avalon. DeepRole has played with and against human players on the web in hybrid human-agent teams. We have found that DeepRole surpasses human players as both a cooperator and a competitor.",1
"We study the problem of inverse reinforcement learning (IRL) with the added twist that the learner is assisted by a helpful teacher. More formally, we tackle the following algorithmic question: How could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process? We present an interactive teaching framework where a teacher adaptively chooses the next demonstration based on learner's current policy. In particular, we design teaching algorithms for two concrete settings: an omniscient setting where a teacher has full knowledge about the learner's dynamics and a blackbox setting where the teacher has minimal knowledge. Then, we study a sequential variant of the popular MCE-IRL learner and prove convergence guarantees of our teaching algorithm in the omniscient setting. Extensive experiments with a car driving simulator environment show that the learning progress can be speeded up drastically as compared to an uninformative teacher.",0
"Our focus is on inverse reinforcement learning (IRL), but with the added aspect of a helpful teacher aiding the learner. Specifically, we aim to answer the question of how a teacher can provide demonstrations that are informative and accelerate the IRL learning process. To achieve this, we introduce an interactive teaching framework where the teacher selects the next demonstration based on the learner's current policy. We present teaching algorithms for two scenarios: the omniscient setting where the teacher has complete knowledge of the learner's dynamics, and the blackbox setting where the teacher has minimal knowledge. Further, we explore a sequential variant of the MCE-IRL learner and demonstrate convergence guarantees of our teaching algorithm in the omniscient setting. Our experiments in a car driving simulator environment show that informative teaching can significantly enhance learning progress when compared to uninformative teaching.",1
"We present a method to generate directed acyclic graphs (DAGs) using deep reinforcement learning, specifically deep Q-learning. Generating graphs with specified structures is an important and challenging task in various application fields, however most current graph generation methods produce graphs with undirected edges. We demonstrate that this method is capable of generating DAGs with topology and node types satisfying specified criteria in highly sparse reward environments.",0
"Our study proposes a technique that employs deep Q-learning, a form of deep reinforcement learning, to create directed acyclic graphs (DAGs). The task of generating graphs with predetermined structures is a significant and difficult undertaking in several application domains. Nonetheless, existing graph generation approaches generally yield graphs with undirected edges. Our approach illustrates that it can create DAGs with topology and node types that meet specific requirements in conditions where rewards are highly sparse.",1
"Despite significant recent advances in deep neural networks, training them remains a challenge due to the highly non-convex nature of the objective function. State-of-the-art methods rely on error backpropagation, which suffers from several well-known issues, such as vanishing and exploding gradients, inability to handle non-differentiable nonlinearities and to parallelize weight-updates across layers, and biological implausibility. These limitations continue to motivate exploration of alternative training algorithms, including several recently proposed auxiliary-variable methods which break the complex nested objective function into local subproblems. However, those techniques are mainly offline (batch), which limits their applicability to extremely large datasets, as well as to online, continual or reinforcement learning. The main contribution of our work is a novel online (stochastic/mini-batch) alternating minimization (AM) approach for training deep neural networks, together with the first theoretical convergence guarantees for AM in stochastic settings and promising empirical results on a variety of architectures and datasets.",0
"Despite significant progress in deep neural networks, training them remains a challenge due to the highly non-convex nature of the objective function. The current state-of-the-art method, error backpropagation, has well-known limitations such as vanishing and exploding gradients, inability to handle non-differentiable nonlinearities, and biological implausibility. As a result, alternative training algorithms have been proposed, including auxiliary-variable methods that break the objective function into local subproblems. However, these methods are mainly suitable for batch training and limit their applicability to large datasets and online learning. Our work proposes a novel online alternating minimization approach for training deep neural networks, which includes theoretical convergence guarantees in stochastic settings and promising results on various architectures and datasets.",1
"Model-based reinforcement learning (MBRL) has been proposed as a promising alternative solution to tackle the high sampling cost challenge in the canonical reinforcement learning (RL), by leveraging a learned model to generate synthesized data for policy training purpose. The MBRL framework, nevertheless, is inherently limited by the convoluted process of jointly learning control policy and configuring hyper-parameters (e.g., global/local models, real and synthesized data, etc). The training process could be tedious and prohibitively costly. In this research, we propose an ""reinforcement on reinforcement"" (RoR) architecture to decompose the convoluted tasks into two layers of reinforcement learning. The inner layer is the canonical model-based RL training process environment (TPE), which learns the control policy for the underlying system and exposes interfaces to access states, actions and rewards. The outer layer presents an RL agent, called as AI trainer, to learn an optimal hyper-parameter configuration for the inner TPE. This decomposition approach provides a desirable flexibility to implement different trainer designs, called as ""train the trainer"". In our research, we propose and optimize two alternative trainer designs: 1) a uni-head trainer and 2) a multi-head trainer. Our proposed RoR framework is evaluated for five tasks in the OpenAI gym (i.e., Pendulum, Mountain Car, Reacher, Half Cheetah and Swimmer). Compared to three other baseline algorithms, our proposed Train-the-Trainer algorithm has a competitive performance in auto-tuning capability, with upto 56% expected sampling cost saving without knowing the best parameter setting in advance. The proposed trainer framework can be easily extended to other cases in which the hyper-parameter tuning is costly.",0
"To address the issue of high sampling costs in traditional reinforcement learning (RL), model-based reinforcement learning (MBRL) has been proposed as a viable solution that leverages a learned model to generate synthesized data for policy training. However, the MBRL framework is limited by the complexity of jointly learning control policies and configuring hyper-parameters. This can make the training process tedious and prohibitively expensive. To overcome these challenges, we propose a ""reinforcement on reinforcement"" (RoR) architecture that decomposes the training process into two layers of reinforcement learning. The inner layer involves the canonical model-based RL training process environment (TPE), which learns the control policy for the underlying system and provides access to states, actions, and rewards. The outer layer comprises an RL agent, known as the AI trainer, which learns an optimal hyper-parameter configuration for the inner TPE. This approach offers flexibility to implement different trainer designs, also known as ""train the trainer"". We optimized two alternative trainer designs, namely a uni-head trainer and a multi-head trainer, and evaluated our proposed RoR framework for five tasks in the OpenAI gym. Our Train-the-Trainer algorithm outperformed three other baseline algorithms in terms of auto-tuning capability, achieving up to 56% expected sampling cost savings without prior knowledge of the best parameter settings. This proposed trainer framework can be easily extended to other scenarios that involve costly hyper-parameter tuning.",1
"In this thesis, we draw inspiration from both classical system identification and modern machine learning in order to solve estimation problems for real-world, physical systems. The main approach to estimation and learning adopted is optimization based. Concepts such as regularization will be utilized for encoding of prior knowledge and basis-function expansions will be used to add nonlinear modeling power while keeping data requirements practical. The thesis covers a wide range of applications, many inspired by applications within robotics, but also extending outside this already wide field. Usage of the proposed methods and algorithms are in many cases illustrated in the real-world applications that motivated the research. Topics covered include dynamics modeling and estimation, model-based reinforcement learning, spectral estimation, friction modeling and state estimation and calibration in robotic machining. In the work on modeling and identification of dynamics, we develop regularization strategies that allow us to incorporate prior domain knowledge into flexible, overparameterized models. We make use of classical control theory to gain insight into training and regularization while using flexible tools from modern deep learning. A particular focus of the work is to allow use of modern methods in scenarios where gathering data is associated with a high cost. In the robotics-inspired parts of the thesis, we develop methods that are practically motivated and ensure that they are implementable also outside the research setting. We demonstrate this by performing experiments in realistic settings and providing open-source implementations of all proposed methods and algorithms.",0
"This thesis combines classical system identification and modern machine learning to address estimation problems for physical systems in real-world scenarios, using an optimization-based approach. Regularization is utilized to incorporate prior knowledge, and basis-function expansions allow for nonlinear modeling while maintaining practical data requirements. The applications covered are wide-ranging, including robotics and beyond, with real-world examples provided to illustrate the proposed methods and algorithms. The thesis explores topics such as dynamics modeling and estimation, model-based reinforcement learning, spectral estimation, friction modeling, and state estimation and calibration in robotic machining. The goal is to develop regularization strategies that enable the incorporation of domain knowledge into flexible models, leveraging classical control theory and modern deep learning tools. A key focus is on making these methods practical and cost-effective, even in scenarios where data gathering is challenging. The robotics-inspired components of the thesis prioritize implementability, with experiments conducted in realistic settings and open-source implementations provided for all proposed methods and algorithms.",1
"We present an approach to make molecular optimization more efficient. We infer a hypergraph replacement grammar from the ChEMBL database, count the frequencies of particular rules being used to expand particular nonterminals in other rules, and use these as conditional priors for the policy model. Simulating random molecules from the resulting probabilistic grammar, we show that conditional priors result in a molecular distribution closer to the training set than using equal rule probabilities or unconditional priors. We then treat molecular optimization as a reinforcement learning problem, using a novel modification of the policy gradient algorithm - batch-advantage: using individual rewards minus the batch average reward to weight the log probability loss. The reinforcement learning agent is tasked with building molecules using this grammar, with the goal of maximizing benchmark scores available from the literature. To do so, the agent has policies both to choose the next node in the graph to expand and to select the next grammar rule to apply. The policies are implemented using the Transformer architecture with the partially expanded graph as the input at each step. We show that using the empirical priors as the starting point for a policy eliminates the need for pre-training, and allows us to reach optima faster. We achieve competitive performance on common benchmarks from the literature, such as penalized logP and QED, with only hundreds of training steps on a budget GPU instance.",0
"Our proposed method aims to enhance molecular optimization by using a hypergraph replacement grammar generated from the ChEMBL database. We compute the frequencies of specific rules used to expand nonterminals in other rules, which serve as conditional priors for the policy model. By simulating random molecules from the resultant probabilistic grammar, we demonstrate that the conditional priors result in a molecular distribution that is more similar to the training set compared to using equal rule probabilities or unconditional priors. We treat molecular optimization as a reinforcement learning problem by using a novel modification of the policy gradient algorithm called batch-advantage, which utilizes individual rewards minus the batch average reward to weigh the log probability loss. Our reinforcement learning agent is tasked with constructing molecules using this grammar while maximizing benchmark scores found in the literature. The agent has policies for selecting the next node and grammar rule to apply, implemented using the Transformer architecture with the partially expanded graph as input at each step. We show that using empirical priors as a starting point for the policy eliminates the need for pre-training and leads to faster reaching of optima. We achieve competitive performance on commonly used benchmarks, such as penalized logP and QED, with only a few hundred training steps on a budget GPU instance.",1
"We present RL-VAE, a graph-to-graph variational autoencoder that uses reinforcement learning to decode molecular graphs from latent embeddings. Methods have been described previously for graph-to-graph autoencoding, but these approaches require sophisticated decoders that increase the complexity of training and evaluation (such as requiring parallel encoders and decoders or non-trivial graph matching). Here, we repurpose a simple graph generator to enable efficient decoding and generation of molecular graphs.",0
"RL-VAE, an autoencoder that uses reinforcement learning to decode molecular graphs from latent embeddings, is introduced in this study. While previous methods have outlined graph-to-graph autoencoding, they entail complex decoders that complicate training and evaluation by requiring parallel encoders and decoders or non-trivial graph matching. Instead, we have adapted a basic graph generator to achieve efficient decoding and generation of molecular graphs.",1
We propose reinforcement learning on simple networks consisting of random connections of spiking neurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable parameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear dynamics that transform the inputs into rich high-dimensional representations based on past context. The random input representations can be efficiently interpreted by an output (or readout) layer with trainable parameters. Systematic initialization of the random connections and training of the readout layer using Q-learning algorithm enable such small random spiking networks to learn optimally and achieve the same learning efficiency as humans on complex reinforcement learning tasks like Atari games. The spike-based approach using small random recurrent networks provides a computationally efficient alternative to state-of-the-art deep reinforcement learning networks with several layers of trainable parameters. The low-complexity spiking networks can lead to improved energy efficiency in event-driven neuromorphic hardware for complex reinforcement learning tasks.,0
"Our proposal is to use reinforcement learning on uncomplicated networks that are composed of spiking neurons that are randomly connected, both feed-forward and recurrent. These networks require very few trainable parameters, yet they can learn complex tasks. The recurrent spiking networks that are sparsely and randomly interconnected have highly non-linear dynamics that can transform inputs into high-dimensional representations that are rich in past context. The output or readout layer, which can be trained with parameters, can efficiently interpret the random input representations. By systematically initializing random connections and using the Q-learning algorithm to train the readout layer, these small random spiking networks can learn optimally and achieve the same efficiency as humans on challenging reinforcement learning tasks, such as Atari games. Using spike-based approaches on small, random recurrent networks provides a computationally efficient alternative to the deep reinforcement learning networks with multiple trainable parameter layers that are currently available. These low-complexity spiking networks could lead to improved energy efficiency in event-driven neuromorphic hardware for complex reinforcement learning tasks.",1
"Polynomial inequalities lie at the heart of many mathematical disciplines. In this paper, we consider the fundamental computational task of automatically searching for proofs of polynomial inequalities. We adopt the framework of semi-algebraic proof systems that manipulate polynomial inequalities via elementary inference rules that infer new inequalities from the premises. These proof systems are known to be very powerful, but searching for proofs remains a major difficulty. In this work, we introduce a machine learning based method to search for a dynamic proof within these proof systems. We propose a deep reinforcement learning framework that learns an embedding of the polynomials and guides the choice of inference rules, taking the inherent symmetries of the problem as an inductive bias. We compare our approach with powerful and widely-studied linear programming hierarchies based on static proof systems, and show that our method reduces the size of the linear program by several orders of magnitude while also improving performance. These results hence pave the way towards augmenting powerful and well-studied semi-algebraic proof systems with machine learning guiding strategies for enhancing the expressivity of such proof systems.",0
"The significance of polynomial inequalities in various mathematical fields is undeniable. This study focuses on the crucial task of automatically searching for polynomial inequality proofs. Semi-algebraic proof systems are used in this study, which utilize elementary inference rules to deduce new inequalities from premises. Although these proof systems are potent, finding proofs remains a challenge. Therefore, this research proposes a machine learning-based approach to search for dynamic proof within the semi-algebraic proof systems. The deep reinforcement learning framework is adopted to learn polynomial embedding and guide the choice of inference rules based on the problem's inherent symmetries. The results of this approach are compared with linear programming hierarchies based on static proof systems, and it is demonstrated that this method significantly reduces the size of the linear program while improving performance. This study sets the stage for augmenting powerful semi-algebraic proof systems with machine learning techniques to enhance their expressivity.",1
"Visual navigation in complex environments is inefficient with traditional reactive policy or general-purposed recurrent policy. To address the long-term memory issue, this paper proposes a graph attention memory (GAM) architecture consisting of memory construction module, graph attention module and control module. The memory construction module builds the topological graph based on supervised learning by taking the exploration prior. Then, guided attention features are extracted with the graph attention module. Finally, the deep reinforcement learning based control module makes decisions based on visual observations and guided attention features. Detailed convergence analysis of GAM is presented in this paper. We evaluate GAM-based navigation system in two complex 3D environments. Experimental results show that the GAM-based navigation system significantly improves learning efficiency and outperforms all baselines in average success rate.",0
"Conventional reactive or general-purpose recurrent policies are not effective for navigating complex environments visually. To overcome the issue of long-term memory, the proposed solution in this study is a graph attention memory (GAM) architecture, which comprises of a memory construction module, a graph attention module, and a control module. The memory construction module builds a topological graph based on supervised learning using the exploration prior. The graph attention module then extracts guided attention features. Finally, the deep reinforcement learning-based control module makes decisions based on visual observations and guided attention features. The paper provides a detailed analysis of the convergence of the GAM. The GAM-based navigation system is evaluated in two complex 3D environments, and experimental results demonstrate that it considerably improves learning efficiency and outperforms all baselines in average success rate.",1
"The posterior variance of Gaussian processes is a valuable measure of the learning error which is exploited in various applications such as safe reinforcement learning and control design. However, suitable analysis of the posterior variance which captures its behavior for finite and infinite number of training data is missing. This paper derives a novel bound for the posterior variance function which requires only local information because it depends only on the number of training samples in the proximity of a considered test point. Furthermore, we prove sufficient conditions which ensure the convergence of the posterior variance to zero. Finally, we demonstrate that the extension of our bound to an average learning bound outperforms existing approaches.",0
"The use of posterior variance in Gaussian processes is important for assessing learning error and is applied in various fields like safe reinforcement learning and control design. Unfortunately, a comprehensive analysis of the posterior variance that considers its behavior for both finite and infinite training data is absent. To address this, we present a new bound for the posterior variance function that only requires local information, specifically the number of training samples near a test point. Additionally, we establish certain conditions that ensure the posterior variance converges to zero. Lastly, we show that our bound, when extended to an average learning bound, performs better than existing methods.",1
"Autonomous multiple tasks learning is a fundamental capability to develop versatile artificial agents that can act in complex environments. In real-world scenarios, tasks may be interrelated (or ""hierarchical"") so that a robot has to first learn to achieve some of them to set the preconditions for learning other ones. Even though different strategies have been used in robotics to tackle the acquisition of interrelated tasks, in particular within the developmental robotics framework, autonomous learning in this kind of scenarios is still an open question. Building on previous research in the framework of intrinsically motivated open-ended learning, in this work we describe how this question can be addressed working on the level of task selection, in particular considering the multiple interrelated tasks scenario as an MDP where the system is trying to maximise its competence over all the tasks.",0
"Developing versatile artificial agents that can operate in complex environments requires the ability to learn multiple tasks autonomously. In real-world situations, tasks may be hierarchical, meaning that a robot must first learn to accomplish some tasks before it can learn others. While various strategies have been employed in robotics to address the acquisition of interrelated tasks, this remains an open question in the field of autonomous learning. This study builds on previous research in intrinsically motivated open-ended learning and proposes a solution for this challenge. The approach focuses on task selection and treats the multiple interrelated tasks scenario as a Markov Decision Process (MDP) where the system aims to maximize its competency across all tasks.",1
"Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.",0
"Regularization is a common technique used in (deep) reinforcement learning algorithms to improve their success rates. Typically, this involves using entropy or Kullback-Leibler divergence. However, we propose a new theory that extends this approach in two ways. Firstly, we consider a wider range of regularizers, and secondly, we explore the modified policy iteration approach, which encompasses both policy iteration and value iteration. Our theory is based on the regularized Bellman operator and the Legendre-Fenchel transform, which are fundamental concepts in convex optimization. Using this approach, we can analyze the error propagation of various algorithmic schemes, including classical methods such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic, and Dynamic Policy Programming. This theory also has connections to proximal convex optimization, specifically Mirror Descent.",1
"Truckload brokerages, a $100 billion/year industry in the U.S., plays the critical role of matching shippers with carriers, often to move loads several days into the future. Brokerages not only have to find companies that will agree to move a load, the brokerage often has to find a price that both the shipper and carrier will agree to. The price not only varies by shipper and carrier, but also by the traffic lanes and other variables such as commodity type. Brokerages have to learn about shipper and carrier response functions by offering a price and observing whether each accepts the quote. We propose a knowledge gradient policy with bootstrap aggregation for high-dimensional contextual settings to guide price experimentation by maximizing the value of information. The learning policy is tested using a carefully calibrated fleet simulator that includes a stochastic lookahead policy that simulates fleet movements, as well as the stochastic modeling of driver assignments and the carrier's load commitment policies with advance booking.",0
"In the United States, the truckload brokerage industry is worth $100 billion annually and plays a crucial role in connecting shippers with carriers for loads that are often transported several days in advance. The brokers not only have to find companies willing to transport the load, but also negotiate a price that satisfies both the shipper and carrier. This price can vary depending on factors such as traffic lanes and commodity type. To determine the appropriate price, brokers must learn about the response functions of both shippers and carriers by offering a quote and observing their acceptance. To maximize the value of information, we suggest using a knowledge gradient policy with bootstrap aggregation for high-dimensional contextual settings to guide price experimentation. This learning policy is tested using a fleet simulator that includes stochastic lookahead policy, driver assignments, and advance booking policies of carriers.",1
"While multi-agent interactions can be naturally modeled as a graph, the environment has traditionally been considered as a black box. We propose to create a shared agent-entity graph, where agents and environmental entities form vertices, and edges exist between the vertices which can communicate with each other. Agents learn to cooperate by exchanging messages along the edges of this graph. Our proposed multi-agent reinforcement learning framework is invariant to the number of agents or entities present in the system as well as permutation invariance, both of which are desirable properties for any multi-agent system representation. We present state-of-the-art results on coverage, formation and line control tasks for multi-agent teams in a fully decentralized framework and further show that the learned policies quickly transfer to scenarios with different team sizes along with strong zero-shot generalization performance. This is an important step towards developing multi-agent teams which can be realistically deployed in the real world without assuming complete prior knowledge or instantaneous communication at unbounded distances.",0
"Traditionally, the environment in multi-agent interactions has been treated as a black box, even though it can be easily represented as a graph. In order to foster cooperation between agents, we suggest creating a shared agent-entity graph where vertices represent agents and environmental entities, and edges enable communication between them. Through message exchange along these edges, agents learn to work together. Our proposed multi-agent reinforcement learning framework is applicable to any number of agents or entities and is permutation invariant, which are both highly desirable qualities in a multi-agent system representation. We demonstrate outstanding results on coverage, formation, and line control tasks for fully decentralized multi-agent teams. Furthermore, we show that learned policies can be easily applied to scenarios with varying team sizes and exhibit strong zero-shot generalization capabilities. This represents a major step towards deploying multi-agent teams in the real world without requiring prior knowledge or instantaneous communication over unlimited distances.",1
"Despite significant progress, deep reinforcement learning (RL) suffers from data-inefficiency and limited generalization. Recent efforts apply meta-learning to learn a meta-learner from a set of RL tasks such that a novel but related task could be solved quickly. Though specific in some ways, different tasks in meta-RL are generally similar at a high level. However, most meta-RL methods do not explicitly and adequately model the specific and shared information among different tasks, which limits their ability to learn training tasks and to generalize to novel tasks. In this paper, we propose to capture the shared information on the one hand and meta-learn how to quickly abstract the specific information about a task on the other hand. Methodologically, we train an SGD meta-learner to quickly optimize a task encoder for each task, which generates a task embedding based on past experience. Meanwhile, we learn a policy which is shared across all tasks and conditioned on task embeddings. Empirical results on four simulated tasks demonstrate that our method has better learning capacity on both training and novel tasks and attains up to 3 to 4 times higher returns compared to baselines.",0
"Although deep reinforcement learning (RL) has made significant progress, it still faces challenges related to limited generalization and data-inefficiency. To address these issues, recent studies have introduced meta-learning, which involves training a meta-learner on a set of RL tasks to enable quick solving of novel, related tasks. However, most meta-RL methods fail to effectively model the specific and shared information among different tasks, which could hinder their ability to learn and generalize to new tasks. This paper proposes a solution that captures both the shared information and specific task information. Specifically, the paper trains an SGD meta-learner to optimize a task encoder for each task, creating a task embedding based on past experiences, and a shared policy that is conditioned on task embeddings. The paper presents empirical results from four simulated tasks, demonstrating that their method has better learning capacity and achieves up to 3 to 4 times higher returns compared to baselines.",1
"One problem in the application of reinforcement learning to real-world problems is the curse of dimensionality on the action space. Macro actions, a sequence of primitive actions, have been studied to diminish the dimensionality of the action space with regard to the time axis. However, previous studies relied on humans defining macro actions or assumed macro actions as repetitions of the same primitive actions. We present Factorized Macro Action Reinforcement Learning (FaMARL) which autonomously learns disentangled factor representation of a sequence of actions to generate macro actions that can be directly applied to general reinforcement learning algorithms. FaMARL exhibits higher scores than other reinforcement learning algorithms on environments that require an extensive amount of search.",0
"The challenge of applying reinforcement learning to real-world problems is the issue of the action space becoming too complex. Researchers have explored the use of macro actions, which involve a series of basic actions, to simplify the action space with respect to time. However, previous studies have relied on human input to define macro actions or have assumed that macro actions are simply repeated basic actions. To address this limitation, our team has developed Factorized Macro Action Reinforcement Learning (FaMARL), which can independently learn a disentangled factor representation of a sequence of actions to generate macro actions that can be used in general reinforcement learning algorithms. FaMARL has shown superior results compared to other reinforcement learning algorithms in environments that require extensive search.",1
"Despite the numerous advances, reinforcement learning remains away from widespread acceptance for autonomous controller design as compared to classical methods due to lack of ability to effectively tackle the reality gap. The reliance on absolute or deterministic reward as a metric for optimization process renders reinforcement learning highly susceptible to changes in problem dynamics. We introduce a novel framework that effectively quantizes the uncertainty of the design space and induces robustness in controllers by switching to a reliability-based optimization routine. The data efficiency of the method is maintained to match reward based optimization methods by employing a model-based approach. We prove the stability of learned neuro-controllers in both static and dynamic environments on classical reinforcement learning tasks such as Cart Pole balancing and Inverted Pendulum.",0
"Despite significant advancements, reinforcement learning has not gained as much popularity as classical methods for autonomous controller design, mainly because it struggles to overcome the reality gap. The use of absolute or deterministic reward as a metric in the optimization process makes reinforcement learning vulnerable to changes in problem dynamics. To address this, we propose a new framework that quantifies the uncertainty of the design space and enhances controller robustness through a reliability-based optimization routine. Our approach maintains the data efficiency of reward-based optimization methods by utilizing a model-based approach. We demonstrate the stability of learned neuro-controllers in both static and dynamic environments on standard reinforcement learning tasks, such as Cart Pole balancing and Inverted Pendulum.",1
"This paper proposes a novel scheme for the watermarking of Deep Reinforcement Learning (DRL) policies. This scheme provides a mechanism for the integration of a unique identifier within the policy in the form of its response to a designated sequence of state transitions, while incurring minimal impact on the nominal performance of the policy. The applications of this watermarking scheme include detection of unauthorized replications of proprietary policies, as well as enabling the graceful interruption or termination of DRL activities by authorized entities. We demonstrate the feasibility of our proposal via experimental evaluation of watermarking a DQN policy trained in the Cartpole environment.",0
"A new method for watermarking Deep Reinforcement Learning (DRL) policies is presented in this paper. The proposed approach involves embedding a distinct identifier into the policy using a designated sequence of state transitions. This watermarking technique has minimal impact on the policy's performance and allows for detecting unauthorized copies of proprietary policies and authorized termination of DRL activities. To validate the feasibility of the proposed scheme, we conducted experiments by watermarking a DQN policy trained in the Cartpole environment.",1
"This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.",0
"The purpose of this study is to examine a group of attacks that focus on the confidentiality aspect of security within Deep Reinforcement Learning (DRL) policies. Previous research has shown that supervised machine learning models, such as classifiers, are susceptible to model extraction attacks due to the attacker's ability to repeatedly query the model for labels, which can be used to create a labeled dataset to replicate the original model. The research presented here demonstrates how imitation learning techniques can be used to launch model extraction attacks on DRL agents and provides proof-of-concept attacks that can be used for black-box attacks against the integrity of DRL policies. Additionally, potential solutions for mitigation techniques are discussed.",1
"This paper investigates the resilience and robustness of Deep Reinforcement Learning (DRL) policies to adversarial perturbations in the state space. We first present an approach for the disentanglement of vulnerabilities caused by representation learning of DRL agents from those that stem from the sensitivity of the DRL policies to distributional shifts in state transitions. Building on this approach, we propose two RL-based techniques for quantitative benchmarking of adversarial resilience and robustness in DRL policies against perturbations of state transitions. We demonstrate the feasibility of our proposals through experimental evaluation of resilience and robustness in DQN, A2C, and PPO2 policies trained in the Cartpole environment.",0
"The objective of this research is to examine the ability of Deep Reinforcement Learning (DRL) policies to withstand adversarial perturbations in the state space. Initially, we introduce an approach to differentiate between vulnerabilities that arise from the representation learning of DRL agents and those that result from the sensitivity of DRL policies to distributional shifts in state transitions. Based on this method, we suggest two RL-based techniques to quantitatively evaluate the adversarial resilience and robustness of DRL policies against state transition perturbations. Our proposals are tested through experiments on DQN, A2C, and PPO2 policies trained in the Cartpole environment, demonstrating their feasibility.",1
"Active learning from demonstration allows a robot to query a human for specific types of input to achieve efficient learning. Existing work has explored a variety of active query strategies; however, to our knowledge, none of these strategies directly minimize the performance risk of the policy the robot is learning. Utilizing recent advances in performance bounds for inverse reinforcement learning, we propose a risk-aware active inverse reinforcement learning algorithm that focuses active queries on areas of the state space with the potential for large generalization error. We show that risk-aware active learning outperforms standard active IRL approaches on gridworld, simulated driving, and table setting tasks, while also providing a performance-based stopping criterion that allows a robot to know when it has received enough demonstrations to safely perform a task.",0
"The process of active learning from demonstration enables a robot to seek specific types of input from a human to learn more efficiently. Although various strategies for active queries have been explored, none have been found to directly reduce the risk of the policy being learned. In light of recent advances in performance bounds for inverse reinforcement learning, we propose a new algorithm for risk-aware active inverse reinforcement learning that focuses on areas of the state space that carry the potential for high generalization error. Our study demonstrates that risk-aware active learning outperforms standard active IRL approaches across different tasks, including gridworld, simulated driving, and table setting tasks. Moreover, it provides a performance-based stopping criterion that enables a robot to determine when it has received sufficient demos to safely carry out a task.",1
"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features. Two types of reconstructors are subsequently proposed to employ the backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder. Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together. The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.",0
"This paper addresses the issue of describing the visual aspects of a video sequence using natural language. Instead of solely relying on video content cues to create a language description, we propose a novel encoder-decoder-reconstructor architecture called RecNet. This architecture utilizes both forward and backward flows for video captioning. The encoder-decoder component produces a sentence description based on the encoded video semantic features, while two types of reconstructors use the backward flow to reproduce the video features from local and global perspectives. To achieve a comprehensive reconstruction of the video features, we fuse the two types of reconstructors. The RecNet is trained end-to-end by casting the generation loss and reconstruction loss into the optimization process. Moreover, we fine-tune the RecNet using CIDEr optimization via reinforcement learning, which significantly improves the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor consistently enhances the performance of video captioning.",1
"While off-policy temporal difference (TD) methods have widely been used in reinforcement learning due to their efficiency and simple implementation, their Bayesian counterparts have not been utilized as frequently. One reason is that the non-linear max operation in the Bellman optimality equation makes it difficult to define conjugate distributions over the value functions. In this paper, we introduce a novel Bayesian approach to off-policy TD methods, called as ADFQ, which updates beliefs on state-action values, Q, through an online Bayesian inference method known as Assumed Density Filtering. We formulate an efficient closed-form solution for the value update by approximately estimating analytic parameters of the posterior of the Q-beliefs. Uncertainty measures in the beliefs not only are used in exploration but also provide a natural regularization for the value update considering all next available actions. ADFQ converges to Q-learning as the uncertainty measures of the Q-beliefs decrease and improves common drawbacks of other Bayesian RL algorithms such as computational complexity. We extend ADFQ with a neural network. Our empirical results demonstrate that ADFQ outperforms comparable algorithms on various Atari 2600 games, with drastic improvements in highly stochastic domains or domains with a large action space.",0
"Off-policy temporal difference (TD) methods are commonly used in reinforcement learning due to their simplicity and efficiency. However, their Bayesian counterparts are not as frequently used, partly due to the difficulty in defining conjugate distributions over the value functions caused by the non-linear max operation in the Bellman optimality equation. This paper introduces a novel Bayesian approach called ADFQ, which updates beliefs on state-action values using Assumed Density Filtering. By approximately estimating analytic parameters of the posterior of the Q-beliefs, we formulate an efficient closed-form solution for the value update. Uncertainty measures in the beliefs are used for exploration and regularization for the value update, considering all next available actions. ADFQ converges to Q-learning as the uncertainty measures decrease and improves on the computational complexity of other Bayesian RL algorithms. ADFQ is extended with a neural network, and empirical results show that it outperforms comparable algorithms on various Atari 2600 games, particularly in highly stochastic domains or domains with large action spaces.",1
"Recent results in Reinforcement Learning (RL) have shown that agents with limited training environments are susceptible to a large amount of overfitting across many domains. A key challenge for RL generalization is to quantitatively explain the effects of changing parameters on testing performance. Such parameters include architecture, regularization, and RL-dependent variables such as discount factor and action stochasticity. We provide empirical results that show complex and interdependent relationships between hyperparameters and generalization. We further show that several empirical metrics such as gradient cosine similarity and trajectory-dependent metrics serve to provide intuition towards these results.",0
"In the field of Reinforcement Learning (RL), recent findings reveal that agents who are trained in constrained environments are prone to experiencing significant levels of overfitting in various domains. One of the major obstacles faced by RL generalization is the ability to provide a quantitative explanation of how adjustments to parameters affect testing performance. These parameters comprise of elements such as architecture, regularization, and RL-dependent variables (i.e., discount factor and action stochasticity). Our research presents empirical evidence that demonstrates the intricate and interconnected relationships between hyperparameters and generalization. Furthermore, we demonstrate how various empirical metrics, including gradient cosine similarity and trajectory-dependent metrics, can help provide insight into these results.",1
"Inverse reinforcement learning (IRL) is the problem of finding a reward function that generates a given optimal policy for a given Markov Decision Process. This paper looks at an algorithmic-independent geometric analysis of the IRL problem with finite states and actions. A L1-regularized Support Vector Machine formulation of the IRL problem motivated by the geometric analysis is then proposed with the basic objective of the inverse reinforcement problem in mind: to find a reward function that generates a specified optimal policy. The paper further analyzes the proposed formulation of inverse reinforcement learning with $n$ states and $k$ actions, and shows a sample complexity of $O(n^2 \log (nk))$ for recovering a reward function that generates a policy that satisfies Bellman's optimality condition with respect to the true transition probabilities.",0
"The paper explores Inverse Reinforcement Learning (IRL), which involves identifying a reward function that produces an optimal policy for a given Markov Decision Process. It proposes a geometric analysis of the IRL problem for finite states and actions, which is independent of the algorithm. The paper then introduces a L1-regularized Support Vector Machine formulation of the IRL problem, with the aim of finding a reward function that generates a particular optimal policy. The formulation is analyzed in the context of $n$ states and $k$ actions, and it is demonstrated that a sample complexity of $O(n^2 \log (nk))$ can retrieve a reward function that satisfies Bellman's optimality condition based on the true transition probabilities.",1
"Several recent papers have examined generalization in reinforcement learning (RL), by proposing new environments or ways to add noise to existing environments, then benchmarking algorithms and model architectures on those environments. We discuss subtle conceptual properties of RL benchmarks that are not required in supervised learning (SL), and also properties that an RL benchmark should possess. Chief among them is one we call the principle of unchanged optimality: there should exist a single $\pi$ that is optimal across all train and test tasks. In this work, we argue why this principle is important, and ways it can be broken or satisfied due to subtle choices in state representation or model architecture. We conclude by discussing challenges and future lines of research in theoretically analyzing generalization benchmarks.",0
"In recent studies, researchers have explored generalization in reinforcement learning (RL) by introducing new environments or techniques to introduce noise to existing environments. They then assess the performance of algorithms and model architectures on these environments. In this paper, we delve into the nuanced conceptual characteristics of RL benchmarks that are not necessary in supervised learning (SL). We also highlight the essential properties that an RL benchmark should possess, including the principle of unchanged optimality, which necessitates the existence of a single optimal $\pi$ across all training and testing tasks. We explain why this principle is critical and how it can be realized or violated due to subtle choices in state representation or model architecture. Finally, we discuss the obstacles facing researchers in theoretically analyzing generalization benchmarks and potential avenues for future research.",1
"In this paper, we propose Rogue-Gym, a simple and classic style roguelike game built for evaluating generalization in reinforcement learning (RL). Combined with the recent progress of deep neural networks, RL has successfully trained human-level agents without human knowledge in many games such as those for Atari 2600. However, it has been pointed out that agents trained with RL methods often overfit the training environment, and they work poorly in slightly different environments. To investigate this problem, some research environments with procedural content generation have been proposed. Following these studies, we propose the use of roguelikes as a benchmark for evaluating the generalization ability of RL agents. In our Rogue-Gym, agents need to explore dungeons that are structured differently each time they start a new game. Thanks to the very diverse structures of the dungeons, we believe that the generalization benchmark of Rogue-Gym is sufficiently fair. In our experiments, we evaluate a standard reinforcement learning method, PPO, with and without enhancements for generalization. The results show that some enhancements believed to be effective fail to mitigate the overfitting in Rogue-Gym, although others slightly improve the generalization ability.",0
"The main idea of our paper is to introduce Rogue-Gym, a straightforward and traditional roguelike game, as a means for assessing the generalization capabilities of reinforcement learning (RL). While deep neural networks have enabled RL to train agents that can perform at a human-level in various games such as those for Atari 2600, there is a common issue of RL-trained agents overfitting to their training environments, leading to poor performance in slightly different environments. To address this problem, some studies have proposed research environments that involve procedural content generation. In line with these studies, we suggest that roguelikes can serve as an effective benchmark for evaluating the generalization ability of RL agents. Our Rogue-Gym game requires agents to navigate through randomly structured dungeons each time they play, which ensures a fair and diverse evaluation of their generalization capabilities. We conducted experiments using a standard RL method, PPO, with and without enhancements for generalization, and found that while some enhancements failed to mitigate overfitting in Rogue-Gym, others were able to slightly improve the generalization ability of the agents.",1
"We study how to set channel numbers in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot solution, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods.   Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs). Code and models will be available at: https://github.com/JiahuiYu/slimmable_networks",0
"Our research focuses on optimizing channel numbers in neural networks to enhance accuracy while working within limited resources, such as FLOPs, latency, memory footprint, or model size. We have developed a solution called AutoSlim, which is a simple and one-shot approach that eliminates the need to train multiple network samples and conduct reinforcement learning searches. Instead, we train a single slimmable network that approximates the network accuracy of different channel configurations. We then evaluate the trained model and slim the layer with the least accuracy drop. This process enables us to obtain optimized channel configurations under different resource constraints. We conducted experiments using MobileNet v1, MobileNet v2, ResNet-50, and RL-searched MNasNet for ImageNet classification. Our approach demonstrated significant improvements over default channel configurations, as well as other channel pruning and neural architecture search methods. Notably, our AutoSlim-MobileNet-v2 achieved 74.2% top-1 accuracy at 305M FLOPs, which was 2.4% better than default MobileNet-v2 (301M FLOPs) and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 achieved 1.3% better accuracy than MobileNet-v1 (569M FLOPs) at 570M FLOPs, without depthwise convolutions. The code and models for our approach will be available at https://github.com/JiahuiYu/slimmable_networks.",1
"Standard reinforcement learning methods aim to master one way of solving a task whereas there may exist multiple near-optimal policies. Being able to identify this collection of near-optimal policies can allow a domain expert to efficiently explore the space of reasonable solutions. Unfortunately, existing approaches that quantify uncertainty over policies are not ultimately relevant to finding policies with qualitatively distinct behaviors. In this work, we formalize the difference between policies as a difference between the distribution of trajectories induced by each policy, which encourages diversity with respect to both state visitation and action choices. We derive a gradient-based optimization technique that can be combined with existing policy gradient methods to now identify diverse collections of well-performing policies. We demonstrate our approach on benchmarks and a healthcare task.",0
"The conventional reinforcement learning techniques focus on mastering a single way of solving a task, but there may be multiple policies that are almost perfect. The identification of these near-optimal policies can aid a domain expert in efficiently exploring practical solutions. However, current methods that measure uncertainty over policies are not suitable for identifying policies with distinct behaviors. This study presents a formalized approach that distinguishes policies based on the distribution of trajectories induced by each policy, promoting diversity in both state visitation and action choices. The researchers developed a gradient-based optimization method that can be integrated with existing policy gradient methods to identify varied sets of well-performing policies. The efficacy of this approach was demonstrated on benchmarks and a healthcare task.",1
"Experience reuse is key to sample-efficient reinforcement learning. One of the critical issues is how the experience is represented and stored. Previously, the experience can be stored in the forms of features, individual models, and the average model, each lying at a different granularity. However, new tasks may require experience across multiple granularities. In this paper, we propose the policy residual representation (PRR) network, which can extract and store multiple levels of experience. PRR network is trained on a set of tasks with a multi-level architecture, where a module in each level corresponds to a subset of the tasks. Therefore, the PRR network represents the experience in a spectrum-like way. When training on a new task, PRR can provide different levels of experience for accelerating the learning. We experiment with the PRR network on a set of grid world navigation tasks, locomotion tasks, and fighting tasks in a video game. The results show that the PRR network leads to better reuse of experience and thus outperforms some state-of-the-art approaches.",0
"Efficient reinforcement learning relies on experience reuse, and the way experience is represented and stored is crucial. Traditionally, experience has been stored as features, individual models, or average models, each at different levels of detail. However, some tasks require experience across multiple levels. In this study, we introduce the policy residual representation (PRR) network, which can extract and store multiple levels of experience. The PRR network is trained on a multi-level architecture, where each module corresponds to a subset of tasks, allowing for a spectrum-like representation of experience. When training on a new task, PRR can provide different levels of experience to speed up learning. We tested the PRR network on grid world navigation, locomotion, and fighting tasks in a video game, and found that it outperforms some state-of-the-art approaches due to its ability to reuse experience efficiently.",1
"Effective medical test suggestions benefit both patients and physicians to conserve time and improve diagnosis accuracy. In this work, we show that an agent can learn to suggest effective medical tests. We formulate the problem as a stage-wise Markov decision process and propose a reinforcement learning method to train the agent. We introduce a new representation of multiple action policy along with the training method of the proposed representation. Furthermore, a new exploration scheme is proposed to accelerate the learning of disease distributions. Our experimental results demonstrate that the accuracy of disease diagnosis can be significantly improved with good medical test suggestions.",0
"The provision of useful medical test recommendations is advantageous for patients and doctors as it saves time and enhances the accuracy of diagnoses. Our research demonstrates that it is possible for an agent to learn to suggest effective medical tests. To achieve this, we use a stage-wise Markov decision process and a reinforcement learning approach to train the agent. We develop a fresh representation of multiple action policies, along with a new training method for our proposed representation. Additionally, we introduce an exploration scheme to expedite the learning of disease distributions. Our experiments reveal that good medical test suggestions can considerably enhance the precision of disease diagnoses.",1
"Most practical recommender systems focus on estimating immediate user engagement without considering the long-term effects of recommendations on user behavior. Reinforcement learning (RL) methods offer the potential to optimize recommendations for long-term user engagement. However, since users are often presented with slates of multiple items - which may have interacting effects on user choice - methods are required to deal with the combinatorics of the RL action space. In this work, we address the challenge of making slate-based recommendations to optimize long-term value using RL. Our contributions are three-fold. (i) We develop SLATEQ, a decomposition of value-based temporal-difference and Q-learning that renders RL tractable with slates. Under mild assumptions on user choice behavior, we show that the long-term value (LTV) of a slate can be decomposed into a tractable function of its component item-wise LTVs. (ii) We outline a methodology that leverages existing myopic learning-based recommenders to quickly develop a recommender that handles LTV. (iii) We demonstrate our methods in simulation, and validate the scalability of decomposed TD-learning using SLATEQ in live experiments on YouTube.",0
"Practical recommender systems typically prioritize estimating immediate user engagement, disregarding the potential long-term effects of recommendations on user behavior. Reinforcement learning (RL) methods present an opportunity to optimize recommendations for long-term user engagement, but the complexity of presenting users with multiple items requires techniques to handle the combinatorial nature of the RL action space. In this study, we address the challenge of using RL to make slate-based recommendations that optimize long-term value. Our contributions include the development of SLATEQ, a decomposition of value-based temporal-difference and Q-learning that makes RL tractable with slates, and a methodology for leveraging existing myopic learning-based recommenders to quickly develop a recommender that handles long-term value. We demonstrate the effectiveness of our methods in simulation and validate their scalability using live experiments on YouTube.",1
"Many potential applications of reinforcement learning in the real world involve interacting with other agents whose numbers vary over time. We propose new neural policy architectures for these multi-agent problems. In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we follow the spirit of recent work on the power of relational inductive biases in deep networks by learning multi-agent relationships at the policy level via an attentional architecture. In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents' state information when selecting their next action. The structure of our architectures allow them to be applied on environments with varying numbers of agents. We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and significantly outperforming it when scaling to large numbers of agents.",0
"The utilization of reinforcement learning in real-world situations often involves interacting with varying numbers of other agents. Our proposed solution involves the development of new neural policy architectures that cater to these multi-agent problems. Instead of training individual, separate policies for each agent and then incorporating cooperation through additional inter-policy mechanisms, we adopt the approach of recent research on the effectiveness of relational inductive biases in deep networks. Using an attentional architecture, we learn multi-agent relationships at the policy level, allowing all agents to share the same policy. However, each agent applies the policy independently in their own context to aggregate other agents' state information when selecting their next action. This architecture can be applied to environments with varying numbers of agents. We demonstrate the efficiency of our architecture by solving a multi-agent autonomous vehicle coordination problem and obtaining superior results compared to a full-knowledge, fully-centralized reference solution. Furthermore, our architecture outperforms the reference solution when scaling to larger numbers of agents.",1
"Recent advances in deep reinforcement learning algorithms have shown great potential and success for solving many challenging real-world problems, including Go game and robotic applications. Usually, these algorithms need a carefully designed reward function to guide training in each time step. However, in real world, it is non-trivial to design such a reward function, and the only signal available is usually obtained at the end of a trajectory, also known as the episodic reward or return. In this work, we introduce a new algorithm for temporal credit assignment, which learns to decompose the episodic return back to each time-step in the trajectory using deep neural networks. With this learned reward signal, the learning efficiency can be substantially improved for episodic reinforcement learning. In particular, we find that expressive language models such as the Transformer can be adopted for learning the importance and the dependency of states in the trajectory, therefore providing high-quality and interpretable learned reward signals. We have performed extensive experiments on a set of MuJoCo continuous locomotive control tasks with only episodic returns and demonstrated the effectiveness of our algorithm.",0
"Advanced deep reinforcement learning algorithms have demonstrated success in solving difficult real-world problems, such as Go games and robotics. However, these algorithms typically require a carefully designed reward function to guide training at each time step. In reality, it is challenging to create such a function, and the only available signal is often obtained at the end of a trajectory. This is known as the episodic reward or return. In this study, a new algorithm for temporal credit assignment is presented, which uses deep neural networks to decompose the episodic return into each time step in the trajectory. This learned reward signal improves the efficiency of episodic reinforcement learning. The Transformer language model is used to learn the importance and dependency of states in the trajectory, resulting in high-quality and interpretable learned reward signals. Experiments on a set of MuJoCo continuous locomotive control tasks with only episodic returns demonstrate the effectiveness of the algorithm.",1
"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",0
"Although recent reinforcement learning (RL) methods have demonstrated impressive results in complex domains like Atari games, they frequently suffer from low sample efficiency. To minimize the time required to interact with the environment, reward shaping is commonly used, which involves designing reward functions that provide the agent with intermediate rewards for making progress towards the objective. However, creating suitable shaping rewards is challenging and time-consuming. Our research addresses this issue by utilizing natural language instructions to conduct reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that translates free-form natural language instructions to intermediate rewards based on the agent's actions. These language-based rewards can be effortlessly integrated into any standard RL algorithm. We tested our approach on Montezuma's Revenge from the Atari Learning Environment, a well-known benchmark in RL. Our experiments on 15 distinct tasks demonstrate that, with the same number of interactions with the environment, language-based rewards resulted in successful task completion 60% more frequently on average than learning without language.",1
"The goal of computational color constancy is to preserve the perceptive colors of objects under different lighting conditions by removing the effect of color casts caused by the scene's illumination. With the rapid development of deep learning based techniques, significant progress has been made in image semantic segmentation. In this work, we exploit the semantic information together with the color and spatial information of the input image in order to remove color casts. We train a convolutional neural network (CNN) model that learns to estimate the illuminant color and gamma correction parameters based on the semantic information of the given image. Experimental results show that feeding the CNN with the semantic information leads to a significant improvement in the results by reducing the error by more than 40%.",0
"The aim of computational color constancy is to eliminate color casts caused by the lighting of a scene, in order to maintain the perceived colors of objects in different conditions. Image semantic segmentation has advanced considerably due to the rapid development of deep learning techniques. This study employs semantic information, as well as color and spatial information, from input images to eliminate color casts. A CNN model is trained to estimate illuminant color and gamma correction parameters based on the semantic information of the image. The experimental findings indicate that supplying the CNN with semantic information results in a significant reduction in error of over 40%.",1
"Designing new molecules with a set of predefined properties is a core problem in modern drug discovery and development. There is a growing need for de-novo design methods that would address this problem. We present MolecularRNN, the graph recurrent generative model for molecular structures. Our model generates diverse realistic molecular graphs after likelihood pretraining on a big database of molecules. We perform an analysis of our pretrained models on large-scale generated datasets of 1 million samples. Further, the model is tuned with policy gradient algorithm, provided a critic that estimates the reward for the property of interest. We show a significant distribution shift to the desired range for lipophilicity, drug-likeness, and melting point outperforming state-of-the-art works. With the use of rejection sampling based on valency constraints, our model yields 100% validity. Moreover, we show that invalid molecules provide a rich signal to the model through the use of structure penalty in our reinforcement learning pipeline.",0
"A fundamental issue in modern drug discovery and development is devising new molecules that have specific properties. To tackle this issue, there is a growing demand for de-novo design methods. Our solution to this problem is MolecularRNN, which is a graph recurrent generative model for molecular structures. By pretraining our model on a vast database of molecules, we are able to produce a diverse range of realistic molecular graphs. We conducted an analysis of our pretrained models on large-scale generated datasets of 1 million samples. Furthermore, we fine-tuned our model using a policy gradient algorithm and a critic to estimate the reward for the targeted property. Our results show that our model outperforms the current state-of-the-art works in achieving a significant distribution shift to the desired range for lipophilicity, drug-likeness, and melting point. We achieved 100% validity by using rejection sampling based on valency constraints. Additionally, we discovered that invalid molecules offer valuable insights to the model through the use of a structure penalty in our reinforcement learning pipeline.",1
"We study data poisoning attacks in the online setting where training items arrive sequentially, and the attacker may perturb the current item to manipulate online learning. Importantly, the attacker has no knowledge of future training items nor the data generating distribution. We formulate online data poisoning attack as a stochastic optimal control problem, and solve it with model predictive control and deep reinforcement learning. We also upper bound the suboptimality suffered by the attacker for not knowing the data generating distribution. Experiments validate our control approach in generating near-optimal attacks on both supervised and unsupervised learning tasks.",0
"Our focus is on analyzing data poisoning attacks in the online environment, where training items are received in a sequential manner. The attacker is able to manipulate the current item to influence online learning and has no insight into future training items or the data generation distribution. We present an approach that frames the online data poisoning attack as a stochastic optimal control problem, utilizing model predictive control and deep reinforcement learning for resolution. We also provide an upper bound for the suboptimality that the attacker would experience due to a lack of understanding of the data generating distribution. Our experiments demonstrate the effectiveness of our control method in generating nearly optimal attacks for both supervised and unsupervised learning tasks.",1
"Model-based reinforcement learning is an appealing framework for creating agents that learn, plan, and act in sequential environments. Model-based algorithms typically involve learning a transition model that takes a state and an action and outputs the next state---a one-step model. This model can be composed with itself to enable predicting multiple steps into the future, but one-step prediction errors can get magnified, leading to unacceptable inaccuracy. This compounding-error problem plagues planning and undermines model-based reinforcement learning. In this paper, we address the compounding-error problem by introducing a multi-step model that directly outputs the outcome of executing a sequence of actions. Novel theoretical and empirical results indicate that the multi-step model is more conducive to efficient value-function estimation, and it yields better action selection compared to the one-step model. These results make a strong case for using multi-step models in the context of model-based reinforcement learning.",0
"The framework of model-based reinforcement learning is attractive for developing agents that can learn, plan, and act in sequential environments. Generally, model-based algorithms involve learning a one-step transition model that takes a state and action as input and produces the next state. However, using this model for predicting several steps into the future can lead to compounding prediction errors, resulting in unacceptable inaccuracies that undermine the model-based reinforcement learning process. To address this issue, this paper proposes a multi-step model that directly predicts the outcome of executing a sequence of actions. Theoretical and empirical evidence suggests that this multi-step model is more effective for efficient value-function estimation and action selection than the one-step model. These findings support the use of multi-step models in the context of model-based reinforcement learning.",1
"AI agents are being developed to support high stakes decision-making processes from driving cars to prescribing drugs, making it increasingly important for human users to understand their behavior. Policy summarization methods aim to convey strengths and weaknesses of such agents by demonstrating their behavior in a subset of informative states. Some policy summarization methods extract a summary that optimizes the ability to reconstruct the agent's policy under the assumption that users will deploy inverse reinforcement learning. In this paper, we explore the use of different models for extracting summaries. We introduce an imitation learning-based approach to policy summarization; we demonstrate through computational simulations that a mismatch between the model used to extract a summary and the model used to reconstruct the policy results in worse reconstruction quality; and we demonstrate through a human-subject study that people use different models to reconstruct policies in different contexts, and that matching the summary extraction model to these can improve performance. Together, our results suggest that it is important to carefully consider user models in policy summarization.",0
"The development of AI agents is aimed at supporting critical decision-making processes such as driving cars and prescribing drugs. Therefore, it is essential for human users to comprehend their behavior. Policy summarization methods have been introduced to exhibit the strengths and weaknesses of such agents by showcasing their behavior in a specific set of informative states. Some methods extract a summary that enhances the ability to reconstruct the agent's policy under the assumption that users will deploy inverse reinforcement learning. In this study, we explore a variety of models for extracting summaries and present an imitation learning-based approach to policy summarization. Our computational simulations demonstrate that a mismatch between the model used to extract a summary and the model used to reconstruct the policy results in poor reconstruction quality. Additionally, our human-subject study shows that people use different models to reconstruct policies in different contexts, and matching the summary extraction model to these can improve performance. Our findings suggest that user models must be carefully considered in policy summarization.",1
"A key impediment to reinforcement learning (RL) in real applications with limited, batch data is defining a reward function that reflects what we implicitly know about reasonable behaviour for a task and allows for robust off-policy evaluation. In this work, we develop a method to identify an admissible set of reward functions for policies that (a) do not diverge too far from past behaviour, and (b) can be evaluated with high confidence, given only a collection of past trajectories. Together, these ensure that we propose policies that we trust to be implemented in high-risk settings. We demonstrate our approach to reward design on synthetic domains as well as in a critical care context, for a reward that consolidates clinical objectives to learn a policy for weaning patients from mechanical ventilation.",0
"One of the main challenges of applying reinforcement learning (RL) to real-world scenarios with limited, batch data is creating a reward function that accurately represents our implicit understanding of reasonable behavior for a given task while also enabling reliable off-policy evaluation. In this study, we introduce a technique for identifying a set of acceptable reward functions for policies that (a) stay consistent with past behavior and (b) can be confidently evaluated using only previous trajectories. This ensures that the policies we propose can be trusted in high-risk situations. We showcase our approach to reward design on both artificial domains and in a critical care context, where we develop a reward that combines clinical objectives to teach a policy for weaning patients off mechanical ventilation.",1
"In this paper we propose a hybrid architecture of actor-critic algorithms for reinforcement learning in parameterized action space, which consists of multiple parallel sub-actor networks to decompose the structured action space into simpler action spaces along with a critic network to guide the training of all sub-actor networks. While this paper is mainly focused on parameterized action space, the proposed architecture, which we call hybrid actor-critic, can be extended for more general action spaces which has a hierarchical structure. We present an instance of the hybrid actor-critic architecture based on proximal policy optimization (PPO), which we refer to as hybrid proximal policy optimization (H-PPO). Our experiments test H-PPO on a collection of tasks with parameterized action space, where H-PPO demonstrates superior performance over previous methods of parameterized action reinforcement learning.",0
"The aim of this paper is to introduce a hybrid actor-critic architecture for reinforcement learning in parameterized action space. The architecture includes multiple parallel sub-actor networks that break down the structured action space into simpler action spaces. Additionally, a critic network is used to guide the training of all sub-actor networks. Though the focus is on parameterized action space, the proposed architecture, known as hybrid actor-critic, can be adapted for more general action spaces that have a hierarchical structure. We present an example of the hybrid actor-critic architecture, based on proximal policy optimization (PPO), which we call hybrid proximal policy optimization (H-PPO). Our experiments involve testing H-PPO on a variety of tasks with parameterized action space, and it outperforms previous methods of parameterized action reinforcement learning.",1
"Deep learning has enabled traditional reinforcement learning methods to deal with high-dimensional problems. However, one of the disadvantages of deep reinforcement learning methods is the limited exploration capacity of learning agents. In this paper, we introduce an approach that integrates human strategies to increase the exploration capacity of multiple deep reinforcement learning agents. We also report the development of our own multi-agent environment called Multiple Tank Defence to simulate the proposed approach. The results show the significant performance improvement of multiple agents that have learned cooperatively with human strategies. This implies that there is a critical need for human intellect teamed with machines to solve complex problems. In addition, the success of this simulation indicates that our multi-agent environment can be used as a testbed platform to develop and validate other multi-agent control algorithms.",0
"The utilization of deep learning has allowed conventional reinforcement learning techniques to handle problems with high dimensions. Nevertheless, a drawback of deep reinforcement learning methods is that the learning agents have limited exploration capacity. In this study, we present an approach that incorporates human strategies to enhance the exploration capacity of multiple deep reinforcement learning agents. Furthermore, we created our own multi-agent environment, called Multiple Tank Defence, to simulate the proposed approach. Our findings reveal that the performance of multiple agents that have learned cooperatively with human strategies has significantly improved. This suggests that the collaboration of human intelligence and machines is crucial in solving intricate problems. Additionally, the triumph of this simulation indicates that our multi-agent environment can serve as a platform for testing and validating other multi-agent control algorithms.",1
"We study the sample complexity of model-based reinforcement learning (henceforth RL) in general contextual decision processes that require strategic exploration to find a near-optimal policy. We design new algorithms for RL with a generic model class and analyze their statistical properties. Our algorithms have sample complexity governed by a new structural parameter called the witness rank, which we show to be small in several settings of interest, including factored MDPs. We also show that the witness rank is never larger than the recently proposed Bellman rank parameter governing the sample complexity of the model-free algorithm OLIVE (Jiang et al., 2017), the only other provably sample-efficient algorithm for global exploration at this level of generality. Focusing on the special case of factored MDPs, we prove an exponential lower bound for a general class of model-free approaches, including OLIVE, which, when combined with our algorithmic results, demonstrates exponential separation between model-based and model-free RL in some rich-observation settings.",0
"Our research delves into the sample complexity of model-based reinforcement learning (abbreviated as RL) in contextual decision processes that require strategic exploration to achieve a policy that is nearly optimal. We have created fresh algorithms for RL that work with a general model class and have assessed their statistical properties. Our algorithms' sample complexity is regulated by a novel structural parameter called the witness rank, which we have found to be small in various settings of interest, such as factored MDPs. We have also proved that the witness rank is never greater than the recently proposed Bellman rank parameter that governs the sample complexity of the model-free OLIVE algorithm (Jiang et al., 2017), the only other algorithm that has been verified to be sample-efficient for global exploration at this level of generality. By concentrating on factored MDPs' particular case, we have shown an exponential lower bound for a general group of model-free approaches, including OLIVE. This, coupled with our algorithmic findings, demonstrates that there is an exponential gap between model-based and model-free RL in certain rich-observation settings.",1
"We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within $\varepsilon$ of the optimal LQR controller, each step of policy evaluation requires at most $(n+d)^3/\varepsilon^2$ samples, where $n$ is the dimension of the state vector and $d$ is the dimension of the input vector. On the other hand, only $\log(1/\varepsilon)$ policy improvement steps suffice, resulting in an overall sample complexity of $(n+d)^3 \varepsilon^{-2} \log(1/\varepsilon)$. We furthermore build on our analysis and construct a simple adaptive procedure based on $\varepsilon$-greedy exploration which relies on approximate PI as a sub-routine and obtains $T^{2/3}$ regret, improving upon a recent result of Abbasi-Yadkori et al.",0
"In this study, we focus on the sample complexity of approximating the policy iteration (PI) for the Linear Quadratic Regulator (LQR). Our research builds on recent work that uses LQR as a testbed to understand the limitations of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis identifies the conflict between policy improvement and policy evaluation, indicating that policy evaluation has a greater impact on sample complexity. Specifically, we demonstrate that to achieve a controller within $\varepsilon$ of the optimal LQR controller, each policy evaluation step requires no more than $(n+d)^3/\varepsilon^2$ samples, where $n$ represents the state vector's dimension and $d$ the input vector's dimension. On the other hand, only $\log(1/\varepsilon)$ policy improvement steps are necessary, resulting in a total sample complexity of $(n+d)^3 \varepsilon^{-2} \log(1/\varepsilon)$. We also develop a straightforward adaptive procedure based on $\varepsilon$-greedy exploration, which uses approximated PI as a subroutine, leading to $T^{2/3}$ regret. This procedure improves on a recent result of Abbasi-Yadkori et al.",1
"Although reinforcement learning (RL) can provide reliable solutions in many settings, practitioners are often wary of the discrepancies between the RL solution and their status quo procedures. Therefore, they may be reluctant to adapt to the novel way of executing tasks proposed by RL. On the other hand, many real-world problems require relatively small adjustments from the status quo policies to achieve improved performance. Therefore, we propose a student-teacher RL mechanism in which the RL (the ""student"") learns to maximize its reward, subject to a constraint that bounds the difference between the RL policy and the ""teacher"" policy. The teacher can be another RL policy (e.g., trained under a slightly different setting), the status quo policy, or any other exogenous policy. We formulate this problem using a stochastic optimization model and solve it using a primal-dual policy gradient algorithm. We prove that the policy is asymptotically optimal. However, a naive implementation suffers from high variance and convergence to a stochastic optimal policy. With a few practical adjustments to address these issues, our numerical experiments confirm the effectiveness of our proposed method in multiple GridWorld scenarios.",0
"Although reinforcement learning (RL) is a dependable solution in many instances, practitioners may hesitate to implement it due to discrepancies with their existing procedures. However, some real-world problems only require minor modifications to existing policies for improved performance. To address this, we suggest a student-teacher RL mechanism, where the RL (the ""student"") maximizes its reward while adhering to a constraint that limits differences from the ""teacher"" policy. The teacher can be another RL policy, the current policy, or an external policy. We use a stochastic optimization model and a primal-dual policy gradient algorithm to solve this problem, which we show to be asymptotically optimal. However, a basic implementation may suffer from high variance and stochastic optimal policy convergence. We address these issues with practical adjustments and demonstrate the efficacy of our approach in multiple GridWorld scenarios through numerical experiments.",1
"Achieving faster execution with shorter compilation time can enable further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently, simulated annealing and genetic algorithms. Our work takes a unique approach by formulating compiler optimizations for neural networks as a reinforcement learning problem, whose solution takes fewer steps to converge. This solution, dubbed ReLeASE, comes with a sampling algorithm that leverages clustering to focus the costly samples (real hardware measurements) on representative points, subsuming an entire subspace. Our adaptive sampling not only reduces the number of samples, but also improves the quality of samples for better exploration in shorter time. As such, experimentation with real hardware shows that reinforcement learning with adaptive sampling provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%. Further experiments also confirm that our adaptive sampling can even improve AutoTVM's simulated annealing by 4.00x.",0
"Enabling greater diversity and innovation in neural networks can be achieved by achieving faster execution and shorter compilation times. However, current methods of executing neural networks rely on hand-optimized libraries, traditional compilation heuristics, or more recently, simulated annealing and genetic algorithms. Our approach is unique in that it formulates compiler optimizations for neural networks as a reinforcement learning problem, which converges in fewer steps. Our solution, called ReLeASE, includes a sampling algorithm that leverages clustering to focus expensive samples on representative points, covering an entire subspace. Our adaptive sampling reduces the number of samples and improves their quality for better exploration in less time. Experiments with real hardware show that reinforcement learning with adaptive sampling provides a 4.45x speed up in optimization time over AutoTVM, while improving inference time of modern deep networks by 5.6%. Further experiments confirm that adaptive sampling can even improve AutoTVM's simulated annealing by 4.00x.",1
"Latent-state environments with long horizons, such as those faced by recommender systems, pose significant challenges for reinforcement learning (RL). In this work, we identify and analyze several key hurdles for RL in such environments, including belief state error and small action advantage. We develop a general principle of advantage amplification that can overcome these hurdles through the use of temporal abstraction. We propose several aggregation methods and prove they induce amplification in certain settings. We also bound the loss in optimality incurred by our methods in environments where latent state evolves slowly and demonstrate their performance empirically in a stylized user-modeling task.",0
"Reinforcement learning (RL) faces significant challenges in latent-state environments with long horizons, like those encountered by recommender systems. This study examines several obstacles, including belief state error and small action advantage, that hinder RL in such environments. A general principle of advantage amplification is introduced, which can leverage temporal abstraction to overcome these hurdles. We suggest various aggregation methods that induce amplification in specific settings, and we limit the loss in optimality incurred by our strategies in environments where latent state changes slowly. Finally, we demonstrate the effectiveness of our techniques empirically in a user-modeling task.",1
"Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumptions of traditional supervised learning theory do not apply. We focus on the special class of reparameterizable RL problems, where the trajectory distribution can be decomposed using the reparametrization trick. For this problem class, estimating the expected return is efficient and the trajectory can be computed deterministically given peripheral random variables, which enables us to study reparametrizable RL using supervised learning and transfer learning theory. Through these relationships, we derive guarantees on the gap between the expected and empirical return for both intrinsic and external errors, based on Rademacher complexity as well as the PAC-Bayes bound. Our bound suggests the generalization capability of reparameterizable RL is related to multiple factors including ""smoothness"" of the environment transition, reward and agent policy function class. We also empirically verify the relationship between the generalization gap and these factors through simulations.",0
"The challenge of comprehending generalization in reinforcement learning (RL) is significant because traditional supervised learning theory assumptions do not apply. Our focus is on reparameterizable RL problems, a unique class where the trajectory distribution can be decomposed using the reparametrization trick. For this class, estimating the expected return is efficient, and the trajectory is deterministically computed given peripheral random variables. This enables us to study reparametrizable RL using supervised learning and transfer learning theory. Using these relationships, we derive guarantees on the gap between the expected and empirical return for intrinsic and external errors. Rademacher complexity and the PAC-Bayes bound are used to create the bound. Our bound suggests that the generalization capability of reparameterizable RL is related to factors such as the ""smoothness"" of the environment transition, reward, and agent policy function class. We also verify this relationship through simulations.",1
"We revisit the stochastic variance-reduced policy gradient (SVRPG) method proposed by Papini et al. (2018) for reinforcement learning. We provide an improved convergence analysis of SVRPG and show that it can find an $\epsilon$-approximate stationary point of the performance function within $O(1/\epsilon^{5/3})$ trajectories. This sample complexity improves upon the best known result $O(1/\epsilon^2)$ by a factor of $O(1/\epsilon^{1/3})$. At the core of our analysis is (i) a tighter upper bound for the variance of importance sampling weights, where we prove that the variance can be controlled by the parameter distance between different policies; and (ii) a fine-grained analysis of the epoch length and batch size parameters such that we can significantly reduce the number of trajectories required in each iteration of SVRPG. We also empirically demonstrate the effectiveness of our theoretical claims of batch sizes on reinforcement learning benchmark tasks.",0
"The stochastic variance-reduced policy gradient (SVRPG) method proposed by Papini et al. (2018) is revisited in the context of reinforcement learning. An improved convergence analysis of SVRPG is provided, demonstrating its ability to find an $\epsilon$-approximate stationary point of the performance function within $O(1/\epsilon^{5/3})$ trajectories, which is a significant improvement over the previous best known result of $O(1/\epsilon^2)$ by a factor of $O(1/\epsilon^{1/3})$. Our analysis centers around (i) establishing a tighter upper bound for the variance of importance sampling weights, which can be controlled by the parameter distance between different policies; and (ii) conducting a fine-grained analysis of the epoch length and batch size parameters to reduce the number of trajectories required in each iteration of SVRPG. Additionally, we provide empirical evidence supporting the effectiveness of our theoretical claims on reinforcement learning benchmark tasks.",1
"A universal rule-based self-learning approach using deep reinforcement learning (DRL) is proposed for the first time to solve nonlinear ordinary differential equations and partial differential equations. The solver consists of a deep neural network-structured actor that outputs candidate solutions, and a critic derived only from physical rules (governing equations and boundary and initial conditions). Solutions in discretized time are treated as multiple tasks sharing the same governing equation, and the current step parameters provide an ideal initialization for the next owing to the temporal continuity of the solutions, which shows a transfer learning characteristic and indicates that the DRL solver has captured the intrinsic nature of the equation. The approach is verified through solving the Schr\""odinger, Navier-Stokes, Burgers', Van der Pol, and Lorenz equations and an equation of motion. The results indicate that the approach gives solutions with high accuracy, and the solution process promises to get faster.",0
"For the first time, a self-learning approach using deep reinforcement learning (DRL) based on universal rules has been proposed to solve nonlinear ordinary differential equations and partial differential equations. The solver consists of an actor with a deep neural network structure that generates candidate solutions, and a critic that is derived solely from physical rules, including governing equations and boundary and initial conditions. Multiple tasks with the same governing equation are treated as solutions in discretized time, and the current step parameters serve as an ideal initialization for the next step due to the temporal continuity of the solutions, indicating a transfer learning characteristic that demonstrates the DRL solver has captured the intrinsic nature of the equation. The approach has been validated by solving various equations, including the Schr\""odinger, Navier-Stokes, Burgers', Van der Pol, and Lorenz equations, as well as an equation of motion. The results demonstrate that the approach provides highly accurate solutions, and the solution process is expected to become faster.",1
"System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference, Variational Autoencoders and Concrete relaxations, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of learned dynamics showcased on various simulated tasks.",0
"The identification of complex and nonlinear systems is a significant challenge for both model predictive control and model-based reinforcement learning. Despite their intricacy, these systems can be well approximated by linear dynamical systems if they are divided into appropriate subsequences. This approach not only enables us to find accurate approximations of the dynamics but also provides a deeper understanding of the underlying system. By utilizing Bayesian inference, Variational Autoencoders, and Concrete relaxations, we demonstrate how to acquire a more comprehensive and meaningful state space that includes joint constraints and wall collisions in a maze from partial and high-dimensional observations. This representation results in improved accuracy of the learned dynamics, as evidenced by various simulated tasks.",1
"In importance sampling (IS)-based reinforcement learning algorithms such as Proximal Policy Optimization (PPO), IS weights are typically clipped to avoid large variance in learning. However, policy update from clipped statistics induces large bias in tasks with high action dimensions, and bias from clipping makes it difficult to reuse old samples with large IS weights. In this paper, we consider PPO, a representative on-policy algorithm, and propose its improvement by dimension-wise IS weight clipping which separately clips the IS weight of each action dimension to avoid large bias and adaptively controls the IS weight to bound policy update from the current policy. This new technique enables efficient learning for high action-dimensional tasks and reusing of old samples like in off-policy learning to increase the sample efficiency. Numerical results show that the proposed new algorithm outperforms PPO and other RL algorithms in various Open AI Gym tasks.",0
"The reinforcement learning algorithms that rely on importance sampling (IS), such as Proximal Policy Optimization (PPO), usually clip IS weights to prevent excessive variance during learning. However, using clipped statistics for policy updates can generate significant bias in tasks with high action dimensions, rendering it difficult to reuse older samples with large IS weights. This study examines PPO, a typical on-policy algorithm, and proposes an enhancement that involves dimension-wise clipping of IS weights. This method clips each action dimension's IS weight separately to minimize bias and adaptively controls the IS weight to restrict policy updates from the existing policy. This approach enables efficient learning in high action-dimensional tasks and facilitates the reuse of old samples, similar to that in off-policy learning, to improve sample efficiency. The experimental outcomes indicate that the proposed algorithm surpasses other RL algorithms and PPO in various Open AI Gym tasks.",1
"Though reinforcement learning has greatly benefited from the incorporation of neural networks, the inability to verify the correctness of such systems limits their use. Current work in explainable deep learning focuses on explaining only a single decision in terms of input features, making it unsuitable for explaining a sequence of decisions. To address this need, we introduce Abstracted Policy Graphs, which are Markov chains of abstract states. This representation concisely summarizes a policy so that individual decisions can be explained in the context of expected future transitions. Additionally, we propose a method to generate these Abstracted Policy Graphs for deterministic policies given a learned value function and a set of observed transitions, potentially off-policy transitions used during training. Since no restrictions are placed on how the value function is generated, our method is compatible with many existing reinforcement learning methods. We prove that the worst-case time complexity of our method is quadratic in the number of features and linear in the number of provided transitions, $O(|F|^2 |tr\_samples|)$. By applying our method to a family of domains, we show that our method scales well in practice and produces Abstracted Policy Graphs which reliably capture relationships within these domains.",0
"The use of neural networks in reinforcement learning has been beneficial, but the inability to verify their accuracy limits their effectiveness. Current work in explainable deep learning only explains individual decisions, which is not sufficient for explaining a sequence of decisions. To address this issue, we introduce Abstracted Policy Graphs, which are Markov chains of abstract states that provide a concise overview of a policy. Our method generates these graphs for deterministic policies using a learned value function and observed transitions. This approach is compatible with various reinforcement learning methods and has a worst-case time complexity of $O(|F|^2 |tr\_samples|)$. Our method has been tested in different domains and produces reliable Abstracted Policy Graphs that capture the relationships within these domains.",1
"It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration. However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent's behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent's motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency. These two components appear to balance reward for matching a specific instance of behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task -- the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with $10$ degrees of freedom (DoF) and 3D with $38$ DoF.",0
"Achieving the goal of having a reinforcement learning (RL) based agent learn behavior through observation alone is desirable, but it poses a challenge within the RL framework to establish rewards that support this objective. To tackle this issue, we propose the use of Siamese networks that can gauge the distance between the agent's and observed behaviors. This approach enables the provision of a reward signal to an RL agent based on the distance between the desired motion and its actual motion. To minimize this distance, we employ an RNN-based comparator model, which computes distances in space and time between motion clips. We discovered that incorporating multi-task data and an extra image encoding loss can help ensure temporal consistency and balance rewards for matching specific instances of behavior versus general behavior. Our focus is on the one-shot learning setting, where only one demonstration is available for a given task, making this problem particularly challenging. We demonstrate our method on humanoid agents in both 2D and 3D with 10 and 38 degrees of freedom, respectively.",1
"We consider a new family of operators for reinforcement learning with the goal of alleviating the negative effects and becoming more robust to approximation or estimation errors. Various theoretical results are established, which include showing on a sample path basis that our family of operators preserve optimality and increase the action gap. Our empirical results illustrate the strong benefits of our family of operators, significantly outperforming the classical Bellman operator and recently proposed operators.",0
"Our objective is to develop a fresh set of operators for reinforcement learning that can mitigate negative consequences and enhance resistance to errors in approximation or estimation. Several theoretical outcomes are demonstrated, such as proving that our operators maintain optimality and widen the action gap on a sample path basis. Our practical findings demonstrate the substantial advantages of our operators, surpassing those of the traditional Bellman operator and recently introduced operators.",1
"The convergence of many reinforcement learning (RL) algorithms with linear function approximation has been investigated extensively but most proofs assume that these methods converge to a unique solution. In this paper, we provide a complete characterization of non-uniqueness issues for a large class of reinforcement learning algorithms, simultaneously unifying many counter-examples to convergence in a theoretical framework. We achieve this by proving a new condition on features that can determine whether the convergence assumptions are valid or non-uniqueness holds. We consider a general class of RL methods, which we call natural algorithms, whose solutions are characterized as the fixed point of a projected Bellman equation (when it exists); notably, bootstrapped temporal difference-based methods such as $TD(\lambda)$ and $GTD(\lambda)$ are natural algorithms. Our main result proves that natural algorithms converge to the correct solution if and only if all the value functions in the approximation space satisfy a certain shape. This implies that natural algorithms are, in general, inherently prone to converge to the wrong solution for most feature choices even if the value function can be represented exactly. Given our results, we show that state aggregation based features are a safe choice for natural algorithms and we also provide a condition for finding convergent algorithms under other feature constructions.",0
"Numerous studies have examined the convergence of several reinforcement learning (RL) algorithms with linear function approximation. However, these studies generally assume that the methods converge to a unique solution. This paper presents a comprehensive analysis of non-uniqueness issues for a broad range of reinforcement learning algorithms. By proving a new condition on features, we can determine whether convergence assumptions are valid or non-uniqueness holds. Our research focuses on a general class of RL methods called natural algorithms, whose solutions are characterized as the fixed point of a projected Bellman equation. Our main finding is that natural algorithms converge to the correct solution only if all the value functions in the approximation space conform to a specific shape. This means that natural algorithms are generally susceptible to converging to the wrong solution for most feature choices, even if the value function can be represented exactly. However, we demonstrate that state aggregation based features are a safe choice for natural algorithms, and we provide a condition for discovering convergent algorithms under other feature constructions.",1
"A fundamental issue in reinforcement learning algorithms is the balance between exploration of the environment and exploitation of information already obtained by the agent. Especially, exploration has played a critical role for both efficiency and efficacy of the learning process. However, Existing works for exploration involve task-agnostic design, that is performing well in one environment, but be ill-suited to another. To the purpose of learning an effective and efficient exploration policy in an automated manner. We formalized a feasible metric for measuring the utility of exploration based on counterfactual ideology. Based on that, We proposed an end-to-end algorithm to learn exploration policy by meta-learning. We demonstrate that our method achieves good results compared to previous works in the high-dimensional control tasks in MuJoCo simulator.",0
"The challenge in reinforcement learning algorithms lies in finding the right balance between exploration and exploitation. Exploration is crucial for an efficient and effective learning process, but current exploration methods are not adaptable to different environments. To address this issue, we developed a metric to measure the usefulness of exploration using counterfactual ideology. Our end-to-end algorithm for meta-learning exploration policy demonstrates good results in high-dimensional control tasks in the MuJoCo simulator, outperforming previous methods. Our aim is to automate the process of learning an effective and efficient exploration policy.",1
"Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.",0
"Multi-agent scenarios pose unique challenges that go beyond those encountered in single-agent settings, making reinforcement learning crucial for real-world applications. In this study, we introduce an actor-critic algorithm that trains decentralized policies in multi-agent settings. This algorithm employs centrally computed critics that utilize an attention mechanism to select pertinent information for each agent during each timestep. Our approach is more effective and scalable in complex multi-agent environments than recent methods. It can be applied to various learning problems, including cooperative and adversarial settings with individualized rewards, and does not assume any specific action space for the agents. Thus, it is highly flexible and adaptable to most multi-agent learning challenges.",1
"A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of a cost signal constrained to lie below an - adjustable - threshold. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is a fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving.",0
"The Budgeted Markov Decision Process (BMDP) is a modified version of the Markov Decision Process that is suitable for applications with safety constraints. It incorporates a risk concept in the form of a cost signal, which must remain below a customizable threshold. Previously, BMDPs could only be solved when the state spaces were finite and the dynamics were known. However, this study expands the current knowledge to encompass continuous space environments with unknown dynamics. The authors demonstrate that the resolution of a BMDP is a fixed point of a new Budgeted Bellman Optimality operator. This discovery enables the creation of expanded versions of Deep Reinforcement Learning algorithms, which are suitable for handling large-scale BMDPs. The authors verify the effectiveness of their approach using two simulated applications: spoken dialogue and autonomous driving.",1
"The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.",0
"Exploration during learning can cause significant variations in the performance of reinforcement learning algorithms. Unfortunately, current algorithms offer little insight into the quality of their policies, making them less useful in high-stakes situations such as healthcare. To address this accountability shortfall, we suggest that algorithms produce policy certificates. These certificates establish the policy's sub-optimality and return in the next episode, enabling individuals to intervene if the certified quality is inadequate. We introduce two new algorithms with certificates and a theoretical analysis framework that guarantees the quality of their policies and certificates. In tabular MDPs, we demonstrate that computing certificates can even increase the sample-efficiency of optimism-based exploration. As a result, our algorithm is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and it matches (and in some situations slightly improves upon) current minimax regret bounds.",1
"Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO), which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. Our experiments show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling, but are key to learning good policies.",0
"The potential benefits of adjusting environment variables are overlooked by policy gradient methods. These variables are unobservable state features that are randomly determined in a physical setting but can be controlled in a simulator. Neglecting these variables can result in slow learning or suboptimal policies, particularly when they heavily impact the transition dynamics. Our paper proposes a solution called fingerprint policy optimisation (FPO), which aims to find an optimal policy across the distribution of environment variables. FPO uses Bayesian optimisation to select the most effective distribution of environment variables that maximises the policy gradient method's improvement at each iteration. To enable practical implementation of BO, we introduce two low-dimensional fingerprints of the current policy that are easy to compute. Our experiments show that FPO can efficiently learn policies that can withstand significant rare events, which are crucial for developing effective policies but are unlikely to be observed through random sampling.",1
"Value functions are crucial for model-free Reinforcement Learning (RL) to obtain a policy implicitly or guide the policy updates. Value estimation heavily depends on the stochasticity of environmental dynamics and the quality of reward signals. In this paper, we propose a two-step understanding of value estimation from the perspective of future prediction, through decomposing the value function into a reward-independent future dynamics part and a policy-independent trajectory return part. We then derive a practical deep RL algorithm from the above decomposition, consisting of a convolutional trajectory representation model, a conditional variational dynamics model to predict the expected representation of future trajectory and a convex trajectory return model that maps a trajectory representation to its return. Our algorithm is evaluated in MuJoCo continuous control tasks and shows superior results under both common settings and delayed reward settings.",0
"To achieve a policy or guide policy updates in model-free Reinforcement Learning (RL), value functions play a crucial role. The accuracy of value estimation depends heavily on the stochasticity of environmental dynamics and the quality of reward signals. This paper proposes a two-step approach to understanding value estimation in terms of future prediction. The value function is broken down into a reward-independent future dynamics component and a policy-independent trajectory return component. The proposed deep RL algorithm is based on this decomposition, which includes a convolutional trajectory representation model, a conditional variational dynamics model to predict the expected representation of future trajectory, and a convex trajectory return model that maps a trajectory representation to its return. In MuJoCo continuous control tasks, our algorithm outperforms other methods in both common settings and delayed reward settings.",1
"Exploration is an extremely challenging problem in reinforcement learning, especially in high dimensional state and action spaces and when only sparse rewards are available. Effective representations can indicate which components of the state are task relevant and thus reduce the dimensionality of the space to explore. In this work, we take a representation learning viewpoint on exploration, utilizing prior experience to learn effective latent representations, which can subsequently indicate which regions to explore. Prior experience on separate but related tasks help learn representations of the state which are effective at predicting instantaneous rewards. These learned representations can then be used with an entropy-based exploration method to effectively perform exploration in high dimensional spaces by effectively lowering the dimensionality of the search space. We show the benefits of this representation for meta-exploration in a simulated object pushing environment.",0
"Reinforcement learning faces a significant challenge in exploration, particularly when dealing with high-dimensional state and action spaces and limited rewards. The use of efficient representations can help identify task-relevant components of the state and reduce the exploration space. This study adopts a representation learning approach to exploration, leveraging prior experience to develop latent representations that guide the exploration process. By using prior experience from related tasks, effective state representations can be learned, which can predict instantaneous rewards. These learned representations can be combined with an entropy-based exploration strategy to effectively explore high-dimensional spaces by reducing the search space's dimensionality. The study demonstrates the effectiveness of this representation in meta-exploration using a simulated object pushing environment.",1
"Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in ""grid-worlds"" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.",0
"The performance of Reinforcement Learning agents is anticipated to improve over time, usually through an assurance of an algorithm's asymptotic behavior, based on certain assumptions about the environment. We introduce a policy algorithm that guarantees a value close to the optimal value with a probability of 1 in all computable probabilistic environments, as long as the agent has a finite horizon. This is referred to as strong asymptotic optimality, which was previously uncertain for policies in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is designed to prioritize exploration as it expects exploratory actions to reduce its uncertainty about the environment. This approach, called inquisitiveness, can be applied generally, and is manageable in more specific environment classes. To evaluate Inq's performance, we conducted experiments in ""grid-worlds"", comparing it to other weakly asymptotically optimal agents.",1
"Deep neural networks have achieved impressive results on a wide variety of tasks. However, quantifying uncertainty in the network's output is a challenging task. Bayesian models offer a mathematical framework to reason about model uncertainty. Variational methods have been used for approximating intractable integrals that arise in Bayesian inference for neural networks. In this report, we review the major variational inference concepts pertinent to Bayesian neural networks and compare various approximation methods used in literature. We also talk about the applications of variational bayes in Reinforcement learning and continual learning.",0
"Although deep neural networks have performed remarkably well in diverse tasks, determining the degree of uncertainty in the networks' output is difficult. Bayesian models provide a mathematical structure for addressing model uncertainty. To tackle the arduous integrals that arise in Bayesian inference for neural networks, variational methods have been employed. Our report examines the essential variational inference ideas that relate to Bayesian neural networks and compares the different approximation techniques found in literature. Additionally, we discuss how variational Bayesian methods are used in Reinforcement learning and continual learning.",1
"We examine the problem of adversarial reinforcement learning for multi-agent domains including a rule-based agent. Rule-based algorithms are required in safety-critical applications for them to work properly in a wide range of situations. Hence, every effort is made to find failure scenarios during the development phase. However, as the software becomes complicated, finding failure cases becomes difficult. Especially in multi-agent domains, such as autonomous driving environments, it is much harder to find useful failure scenarios that help us improve the algorithm. We propose a method for efficiently finding failure scenarios; this method trains the adversarial agents using multi-agent reinforcement learning such that the tested rule-based agent fails. We demonstrate the effectiveness of our proposed method using a simple environment and autonomous driving simulator.",0
"Our focus is on adversarial reinforcement learning for multi-agent domains, which includes a rule-based agent. Rule-based algorithms are crucial for safety-critical applications, as they must function effectively in diverse situations. During development, extensive efforts are made to identify possible failure scenarios. However, as software complexity increases, identifying failure cases becomes more challenging, especially in multi-agent domains like autonomous driving environments. To address this issue, we propose an effective method to identify failure scenarios by training adversarial agents using multi-agent reinforcement learning. Our approach aims to make the tested rule-based agent fail. We demonstrate the effectiveness of this approach using a simple environment and an autonomous driving simulator.",1
"Joint replacement is the most common inpatient surgical treatment in the US. We investigate the clinical pathway optimization for knee replacement, which is a sequential decision process from onset to recovery. Based on episodic claims from previous cases, we view the pathway optimization as an intelligence crowdsourcing problem and learn the optimal decision policy from data by imitating the best expert at every intermediate state. We develop a reinforcement learning-based pipeline that uses value iteration, state compression and aggregation learning, kernel representation and cross validation to predict the best treatment policy. It also provides forecast of the clinical pathway under the optimized policy. Empirical validation shows that the optimized policy reduces the overall cost by 7 percent and reduces the excessive cost premium by 33 percent.",0
"In the US, joint replacement is the most commonly performed inpatient surgical procedure. Our research focuses on improving the clinical pathway for knee replacement, which involves a series of decisions from the initial onset of the condition to the recovery phase. By analyzing past cases, we approach this optimization problem as a form of collective intelligence, and use data to learn the best decision-making strategy from the most successful experts at each stage. Our solution involves a reinforcement learning-based process that includes value iteration, state compression, aggregation learning, kernel representation, and cross-validation to determine the optimal treatment plan. It also provides a forecast of the clinical pathway based on the optimized policy. Our empirical analysis shows that this approach can reduce overall costs by 7 percent and decrease excessive cost premiums by 33 percent.",1
"We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.",0
"Our research introduces a new problem in RL, where the agent must generalize to an unknown environment characterized by a subtask graph, which outlines a set of subtasks and their dependencies. Unlike current hierarchical multitask RL methods that dictate the agent's actions at a high level, our problem only provides information about the subtasks and their relationships, requiring the agent to use advanced reasoning to determine the optimal subtask to perform. To tackle this challenge, we propose the Neural Subtask Graph Solver (NSGS), which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a new non-parametric gradient-based policy called graph reward propagation. This pre-trains our NSGS agent and fine-tunes it using the actor-critic method. Our experimental results on two 2D visual domains demonstrate that our agent performs sophisticated reasoning to identify the best approach to execute the subtask graph and can generalize well to unknown subtask graphs. Moreover, we compare our agent with the Monte-Carlo tree search (MCTS) method, and our approach is significantly more efficient. Finally, we show that the performance of NSGS can be enhanced by combining it with MCTS.",1
"In many finite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temporal discounting is often applied to optimize over a shorter effective planning horizon. This comes at the risk of potentially biasing the optimization target away from the undiscounted goal. In settings where this bias is unacceptable - where the system must optimize for longer horizons at higher discounts - the target of the value function approximator may increase in variance leading to difficulties in learning. We present an extension of temporal difference (TD) learning, which we call TD($\Delta$), that breaks down a value function into a series of components based on the differences between value functions with smaller discount factors. The separation of a longer horizon value function into these components has useful properties in scalability and performance. We discuss these properties and show theoretic and empirical improvements over standard TD learning in certain settings.",0
"In finite horizon episodic reinforcement learning (RL) scenarios, optimizing for the undiscounted return is often preferred, such as in Atari where the objective is to accumulate points while surviving for a long time. However, it can be challenging to learn with this target, and temporal discounting is frequently employed to optimize over a shorter planning horizon. This approach poses a risk of biasing the optimization target away from the undiscounted goal, making it unsuitable for settings that require optimization over longer horizons with higher discounts. In such situations, increasing the target of the value function approximator may lead to variance and learning difficulties. To address this issue, we propose TD($\Delta$), an extension of temporal difference (TD) learning that breaks down a value function into several components based on the differences between value functions with smaller discount factors. This technique improves scalability and performance, as demonstrated through theoretical and empirical analyses in specific scenarios.",1
"Recent advances in reinforcement learning have proved that given an environment we can learn to perform a task in that environment if we have access to some form of a reward function (dense, sparse or derived from IRL). But most of the algorithms focus on learning a single best policy to perform a given set of tasks. In this paper, we focus on an algorithm that learns to not just perform a task but different ways to perform the same task. As we know when the environment is complex enough there always exists multiple ways to perform a task. We show that using the concept of information maximization it is possible to learn latent codes for discovering multiple ways to perform any given task in an environment.",0
"Recent developments in reinforcement learning have demonstrated that we can acquire the ability to execute a task in an environment if we possess access to some form of a reward function, such as dense, sparse, or derived from IRL. However, most of the algorithms concentrate on discovering a single optimal policy to execute a specific set of tasks. This article instead focuses on an algorithm that learns not only how to execute a task but also various approaches to accomplish the same task. We are aware that in a complex environment, there are always numerous ways to perform a task. We demonstrate that by employing the principle of information maximization, we can learn latent codes that identify multiple ways to perform any given task in an environment.",1
"Reinforcement learning (RL) has had many successes in both ""deep"" and ""shallow"" settings. In both cases, significant hyperparameter tuning is often required to achieve good performance. Furthermore, when nonlinear function approximation is used, non-stationarity in the state representation can lead to learning instability. A variety of techniques exist to combat this --- most notably large experience replay buffers or the use of multiple parallel actors. These techniques come at the cost of moving away from the online RL problem as it is traditionally formulated (i.e., a single agent learning online without maintaining a large database of training examples). Meta-learning can potentially help with both these issues by tuning hyperparameters online and allowing the algorithm to more robustly adjust to non-stationarity in a problem. This paper applies meta-gradient descent to derive a set of step-size tuning algorithms specifically for online RL control with eligibility traces. Our novel technique, Metatrace, makes use of an eligibility trace analogous to methods like $TD(\lambda)$. We explore tuning both a single scalar step-size and a separate step-size for each learned parameter. We evaluate Metatrace first for control with linear function approximation in the classic mountain car problem and then in a noisy, non-stationary version. Finally, we apply Metatrace for control with nonlinear function approximation in 5 games in the Arcade Learning Environment where we explore how it impacts learning speed and robustness to initial step-size choice. Results show that the meta-step-size parameter of Metatrace is easy to set, Metatrace can speed learning, and Metatrace can allow an RL algorithm to deal with non-stationarity in the learning task.",0
"Both ""deep"" and ""shallow"" applications of reinforcement learning (RL) have been successful, but they often require significant hyperparameter tuning for optimal performance. The use of nonlinear function approximation can lead to learning instability due to non-stationarity in the state representation, which can be mitigated through techniques such as large experience replay buffers or multiple parallel actors. However, these approaches deviate from the traditional online RL problem of a single agent learning without a large database of training examples. Meta-learning can potentially address these issues by tuning hyperparameters online and allowing for more robust adjustments to non-stationarity. This paper introduces Metatrace, a novel technique that uses meta-gradient descent to derive step-size tuning algorithms specifically for online RL control with eligibility traces. The approach is tested on both linear and nonlinear function approximation tasks, including the classic mountain car problem and 5 games in the Arcade Learning Environment. Results show that Metatrace is effective at speeding up learning and improving robustness to non-stationarity, and the meta-step-size parameter is easy to set.",1
"Automatic machine learning is an important problem in the forefront of machine learning. The strongest AutoML systems are based on neural networks, evolutionary algorithms, and Bayesian optimization. Recently AlphaD3M reached state-of-the-art results with an order of magnitude speedup using reinforcement learning with self-play. In this work we extend AlphaD3M by using a pipeline grammar and a pre-trained model which generalizes from many different datasets and similar tasks. Our results demonstrate improved performance compared with our earlier work and existing methods on AutoML benchmark datasets for classification and regression tasks. In the spirit of reproducible research we make our data, models, and code publicly available.",0
"The issue of automatic machine learning is currently at the forefront of machine learning research. The most powerful AutoML systems use neural networks, evolutionary algorithms, and Bayesian optimization. AlphaD3M has recently achieved groundbreaking results through the application of reinforcement learning with self-play, resulting in a significant speed increase. Our study builds upon AlphaD3M's findings by incorporating a pipeline grammar and a pre-trained model, which can generalize from multiple datasets and similar tasks. Our results show improved performance compared to our previous research and existing AutoML methods on classification and regression benchmarks. To promote reproducible research, we have made our data, models, and code available to the public.",1
"Recent reinforcement learning algorithms, though achieving impressive results in various fields, suffer from brittle training effects such as regression in results and high sensitivity to initialization and parameters. We claim that some of the brittleness stems from variance differences, i.e. when different environment areas - states and/or actions - have different rewards variance. This causes two problems: First, the ""Boring Areas Trap"" in algorithms such as Q-learning, where moving between areas depends on the current area variance, and getting out of a boring area is hard due to its low variance. Second, the ""Manipulative Consultant"" problem, when value-estimation functions used in DQN and Actor-Critic algorithms influence the agent to prefer boring areas, regardless of the mean rewards return, as they maximize estimation precision rather than rewards. This sheds a new light on how exploration contribute to training, as it helps with both challenges. Cognitive experiments in humans showed that noised reward signals may paradoxically improve performance. We explain this using the two mentioned problems, claiming that both humans and algorithms may share similar challenges. Inspired by this result, we propose the Adaptive Symmetric Reward Noising (ASRN), by which we mean adding Gaussian noise to rewards according to their states' estimated variance, thus avoiding the two problems while not affecting the environment's mean rewards behavior. We conduct our experiments in a Multi Armed Bandit problem with variance differences. We demonstrate that a Q-learning algorithm shows the brittleness effect in this problem, and that the ASRN scheme can dramatically improve the results. We show that ASRN helps a DQN algorithm training process reach better results in an end to end autonomous driving task using the AirSim driving simulator.",0
"Despite achieving impressive outcomes in diverse domains, recent reinforcement learning algorithms suffer from training fragility, including regression in outcomes and high sensitivity to initialization and parameters. Our assertion is that some of this brittleness arises from differences in variance, where distinct environmental areas, i.e., states and/or actions, possess varying rewards variance. This leads to two issues: firstly, the ""Boring Areas Trap"" in algorithms like Q-learning, where transitioning between regions depends on the present area variance, making it challenging to leave a dull area due to its low variance. Secondly, the ""Manipulative Consultant"" problem, where value-estimation functions utilized in DQN and Actor-Critic algorithms influence the agent to prefer boring areas, disregarding the mean rewards return, as they optimize estimation precision rather than rewards. This presents a novel perspective on how exploration contributes to training, as it addresses both challenges. Cognitive studies in humans have demonstrated that noisy reward signals paradoxically enhance performance. The two issues mentioned above explain this, suggesting that humans and algorithms may face similar issues. Our solution to this problem is the Adaptive Symmetric Reward Noising (ASRN), which involves adding Gaussian noise to rewards based on the estimated variance of their states, thus avoiding the two issues while preserving the mean rewards behavior of the environment. We conduct experiments on a Multi Armed Bandit problem with variance differences, demonstrating that the Q-learning algorithm displays the brittleness effect, while the ASRN scheme significantly improves results. Additionally, we show that ASRN aids the training process of a DQN algorithm to achieve better outcomes in an end-to-end autonomous driving task utilizing the AirSim driving simulator.",1
"The optimal predictor for a linear dynamical system (with hidden state and Gaussian noise) takes the form of an autoregressive linear filter, namely the Kalman filter. However, a fundamental problem in reinforcement learning and control theory is to make optimal predictions in an unknown dynamical system. To this end, we take the approach of directly learning an autoregressive filter for time-series prediction under unknown dynamics. Our analysis differs from previous statistical analyses in that we regress not only on the inputs to the dynamical system, but also the outputs, which is essential to dealing with process noise. The main challenge is to estimate the filter under worst case input (in $\mathcal H_\infty$ norm), for which we use an $L^\infty$-based objective rather than ordinary least-squares. For learning an autoregressive model, our algorithm has optimal sample complexity in terms of the rollout length, which does not seem to be attained by naive least-squares.",0
"The Kalman filter is the best predictor for a linear dynamical system with hidden state and Gaussian noise. However, there is a vital issue in reinforcement learning and control theory, which is making accurate predictions in an unknown dynamical system. Therefore, we directly learn an autoregressive filter for time-series prediction without knowing the dynamics. Our approach differs from previous statistical analyses as we regress not only on the inputs but also on the outputs, which is crucial for handling process noise. The significant challenge is to estimate the filter under worst-case input, which we accomplish using an $L^\infty$-based objective instead of ordinary least-squares. Our algorithm has the optimal sample complexity for learning an autoregressive model in terms of the rollout length, which is not achievable through naive least-squares.",1
"Despite recent innovations in network architectures and loss functions, training RNNs to learn long-term dependencies remains difficult due to challenges with gradient-based optimisation methods. Inspired by the success of Deep Neuroevolution in reinforcement learning (Such et al. 2017), we explore the use of gradient-free population-based global optimisation (PBO) techniques -- training RNNs to capture long-term dependencies in time-series data. Testing evolution strategies (ES) and particle swarm optimisation (PSO) on an application in volatility forecasting, we demonstrate that PBO methods lead to performance improvements in general, with ES exhibiting the most consistent results across a variety of architectures.",0
"Training RNNs to capture long-term dependencies in time-series data remains a difficult task, despite advancements in network architectures and loss functions. This is due to challenges with gradient-based optimization methods. To address this issue, we draw inspiration from the success of Deep Neuroevolution in reinforcement learning and explore the use of gradient-free population-based global optimization (PBO) techniques. Through testing evolution strategies (ES) and particle swarm optimization (PSO) on an application in volatility forecasting, we show that PBO methods generally lead to performance improvements. Among the two methods, ES consistently delivers the most promising results across a range of architectures.",1
"Recent work has explored the problem of autonomous navigation by imitating a teacher and learning an end-to-end policy, which directly predicts controls from raw images. However, these approaches tend to be sensitive to mistakes by the teacher and do not scale well to other environments or vehicles. To this end, we propose Observational Imitation Learning (OIL), a novel imitation learning variant that supports online training and automatic selection of optimal behavior by observing multiple imperfect teachers. We apply our proposed methodology to the challenging problems of autonomous driving and UAV racing. For both tasks, we utilize the Sim4CV simulator that enables the generation of large amounts of synthetic training data and also allows for online learning and evaluation. We train a perception network to predict waypoints from raw image data and use OIL to train another network to predict controls from these waypoints. Extensive experiments demonstrate that our trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines and even humans in simulation. The project website is available at https://sites.google.com/kaust.edu.sa/oil/ and a video at https://youtu.be/_rhq8a0qgeg",0
"Recently, researchers have been investigating autonomous navigation through imitation learning, which involves learning an end-to-end policy that predicts controls from raw images by imitating a teacher. However, these approaches are often influenced by errors made by the teacher and do not effectively adapt to different environments or vehicles. As a solution, Observational Imitation Learning (OIL) has been proposed, which is a new variant of imitation learning that facilitates online training and automatic selection of optimal behavior by observing multiple imperfect teachers. The proposed methodology has been applied to the challenging tasks of autonomous driving and UAV racing, utilizing the Sim4CV simulator for generating large amounts of synthetic training data and enabling online learning and evaluation. A perception network has been trained to predict waypoints from raw image data, and OIL has been used to train another network to predict controls from these waypoints. Extensive experiments demonstrate that the trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines, and even humans in simulation. The project website and a video are available at https://sites.google.com/kaust.edu.sa/oil/ and https://youtu.be/_rhq8a0qgeg, respectively.",1
"Despite recent successes in Reinforcement Learning, value-based methods often suffer from high variance hindering performance. In this paper, we illustrate this in a continuous control setting where state of the art methods perform poorly whenever sensor noise is introduced. To overcome this issue, we introduce Recurrent Value Functions (RVFs) as an alternative to estimate the value function of a state. We propose to estimate the value function of the current state using the value function of past states visited along the trajectory. Due to the nature of their formulation, RVFs have a natural way of learning an emphasis function that selectively emphasizes important states. First, we establish RVF's asymptotic convergence properties in tabular settings. We then demonstrate their robustness on a partially observable domain and continuous control tasks. Finally, we provide a qualitative interpretation of the learned emphasis function.",0
"Despite recent progress in Reinforcement Learning, value-based methods often face performance challenges due to high variance. This paper showcases this issue in a continuous control scenario where state of the art methods struggle when sensor noise is present. To combat this problem, we introduce Recurrent Value Functions (RVFs) as an alternative approach to estimate the state's value function. Our proposal involves utilizing the value function of past states visited along the trajectory to estimate the value function of the current state. RVFs have a distinct feature that enables them to learn an emphasis function that selectively highlights important states. We establish RVF's asymptotic convergence abilities in tabular settings and demonstrate their resilience on a partially observable domain and continuous control tasks. Finally, we provide a qualitative interpretation of the learned emphasis function.",1
"In the space of only a few years, deep generative modeling has revolutionized how we think of artificial creativity, yielding autonomous systems which produce original images, music, and text. Inspired by these successes, researchers are now applying deep generative modeling techniques to the generation and optimization of molecules - in our review we found 45 papers on the subject published in the past two years. These works point to a future where such systems will be used to generate lead molecules, greatly reducing resources spent downstream synthesizing and characterizing bad leads in the lab. In this review we survey the increasingly complex landscape of models and representation schemes that have been proposed. The four classes of techniques we describe are recursive neural networks, autoencoders, generative adversarial networks, and reinforcement learning. After first discussing some of the mathematical fundamentals of each technique, we draw high level connections and comparisons with other techniques and expose the pros and cons of each. Several important high level themes emerge as a result of this work, including the shift away from the SMILES string representation of molecules towards more sophisticated representations such as graph grammars and 3D representations, the importance of reward function design, the need for better standards for benchmarking and testing, and the benefits of adversarial training and reinforcement learning over maximum likelihood based training.",0
"Over the past few years, deep generative modeling has transformed our understanding of artificial creativity, leading to the development of self-sufficient systems that create original music, text, and images. As a result of these advancements, researchers are now utilizing deep generative modeling techniques to generate and optimize molecules. In our analysis, we discovered 45 papers published in the past two years on this subject. These works suggest that such systems will be utilized to generate lead molecules, minimizing the resources used in synthesizing and characterizing bad leads in the lab. Our review explores the increasingly intricate landscape of models and representation schemes proposed, including recursive neural networks, autoencoders, generative adversarial networks, and reinforcement learning. We begin by describing the mathematical fundamentals of each technique before drawing connections and comparisons with other techniques and outlining their advantages and disadvantages. Our work highlights several key themes, such as the shift away from SMILES string representation of molecules to more advanced representations like graph grammars and 3D representations, the significance of reward function design, the need for better benchmarking and testing standards, and the benefits of adversarial training and reinforcement learning over maximum likelihood based training.",1
"Phishing is the simplest form of cybercrime with the objective of baiting people into giving away delicate information such as individually recognizable data, banking and credit card details, or even credentials and passwords. This type of simple yet most effective cyber-attack is usually launched through emails, phone calls, or instant messages. The credential or private data stolen are then used to get access to critical records of the victims and can result in extensive fraud and monetary loss. Hence, sending malicious messages to victims is a stepping stone of the phishing procedure. A \textit{phisher} usually setups a deceptive website, where the victims are conned into entering credentials and sensitive information. It is therefore important to detect these types of malicious websites before causing any harmful damages to victims. Inspired by the evolving nature of the phishing websites, this paper introduces a novel approach based on deep reinforcement learning to model and detect malicious URLs. The proposed model is capable of adapting to the dynamic behavior of the phishing websites and thus learn the features associated with phishing website detection.",0
"Cybercrime can take many forms, but phishing is one of the most straightforward and effective methods. Its aim is to trick people into revealing sensitive information, such as bank details, passwords, or personal information. This is usually done through emails, phone calls, or instant messages. Once the attacker has obtained this information, they can gain access to the victim's data and potentially commit fraud or steal money. In order to launch a successful phishing attack, the attacker will often create a fake website designed to look legitimate, where the victim is prompted to enter their details. Detecting these sites before they can cause harm is crucial. This paper proposes a novel approach, using deep reinforcement learning, to model and detect malicious URLs. By adapting to the dynamic nature of phishing websites, the model can learn to recognize the features associated with phishing and prevent potential damage to victims.",1
"Legged locomotion is a challenging task for learning algorithms, especially when the task requires a diverse set of primitive behaviors. To solve these problems, we introduce a hierarchical framework to automatically decompose complex locomotion tasks. A high-level policy issues commands in a latent space and also selects for how long the low-level policy will execute the latent command. Concurrently, the low-level policy uses the latent command and only the robot's on-board sensors to control the robot's actuators. Our approach allows the high-level policy to run at a lower frequency than the low-level one. We test our framework on a path-following task for a dynamic quadruped robot and we show that steering behaviors automatically emerge in the latent command space as low-level skills are needed for this task. We then show efficient adaptation of the trained policy to a different task by transfer of the trained low-level policy. Finally, we validate the policies on a real quadruped robot. To the best of our knowledge, this is the first application of end-to-end hierarchical learning to a real robotic locomotion task.",0
"Learning algorithms face a significant challenge when it comes to legged locomotion, particularly when the task involves a varied set of basic behaviors. To tackle this issue, we have developed a hierarchical framework that can break down complex locomotion tasks automatically. A high-level policy gives commands in a latent space and determines how long the low-level policy should execute these commands. Meanwhile, the low-level policy uses these commands and onboard sensors to control the robot's actuators. Our method enables the high-level policy to operate at a lower frequency than the low-level policy. We have tested this framework on a dynamic quadruped robot in a path-following task, and have found that steering behaviors emerge automatically in the latent command space as low-level skills are required for this task. We have also shown that the trained policy can adapt efficiently to a different task through the transfer of the trained low-level policy. Finally, we have validated the policies on a real quadruped robot. This is the first known instance of end-to-end hierarchical learning being applied to a real robotic locomotion task.",1
"Branch-and-bound (BnB) algorithms are widely used to solve combinatorial problems, and the performance crucially depends on its branching heuristic.In this work, we consider a typical problem of maximum common subgraph (MCS), and propose a branching heuristic inspired from reinforcement learning with a goal of reaching a tree leaf as early as possible to greatly reduce the search tree size.Extensive experiments show that our method is beneficial and outperforms current best BnB algorithm for the MCS.",0
"The use of branch-and-bound (BnB) algorithms is prevalent in solving combinatorial problems, and their efficiency is largely determined by the chosen branching heuristic. Our study is focused on the maximum common subgraph (MCS) problem and introduces a reinforcement learning-inspired branching heuristic aimed at achieving quick access to a tree leaf, leading to a significant reduction in search tree size. Our experiments demonstrate the effectiveness of our approach, which surpasses the current leading BnB algorithm for MCS.",1
"N-discount optimality was introduced as a hierarchical form of policy- and value-function optimality, with Blackwell optimality lying at the top level of the hierarchy Veinott (1969); Blackwell (1962). We formalize notions of myopic discount factors, value functions and policies in terms of Blackwell optimality in MDPs, and we provide a novel concept of regret, called Blackwell regret, which measures the regret compared to a Blackwell optimal policy. Our main analysis focuses on long horizon MDPs with sparse rewards. We show that selecting the discount factor under which zero Blackwell regret can be achieved becomes arbitrarily hard. Moreover, even with oracle knowledge of such a discount factor that can realize a Blackwell regret-free value function, an $\epsilon$-Blackwell optimal value function may not even be gain optimal. Difficulties associated with this class of problems is discussed, and the notion of a policy gap is defined as the difference in expected return between a given policy and any other policy that differs at that state; we prove certain properties related to this gap. Finally, we provide experimental results that further support our theoretical results.",0
"The concept of N-discount optimality is a form of policy and value-function optimality that is hierarchical, with Blackwell optimality at the top level of the hierarchy, according to Veinott (1969) and Blackwell (1962). We define myopic discount factors, value functions, and policies in terms of Blackwell optimality in MDPs, and introduce the concept of Blackwell regret, which measures the regret compared to a Blackwell optimal policy. Our focus is on long horizon MDPs with sparse rewards, and we demonstrate that selecting the discount factor to achieve zero Blackwell regret can be extremely challenging. Even with knowledge of a discount factor that can produce a Blackwell regret-free value function, an $\epsilon$-Blackwell optimal value function may not be the most gain optimal. We explore the difficulties associated with this class of problems and define the policy gap as the difference in expected return between a given policy and any other policy that differs at that state. We present certain properties related to this gap and provide experimental results to support our theoretical findings.",1
"Recent advances in deep reinforcement learning have achieved human-level performance on a variety of real-world applications. However, the current algorithms still suffer from poor gradient estimation with excessive variance, resulting in unstable training and poor sample efficiency. In our paper, we proposed an innovative optimization strategy by utilizing stochastic variance reduced gradient (SVRG) techniques. With extensive experiments on Atari domain, our method outperforms the deep q-learning baselines on 18 out of 20 games.",0
"Deep reinforcement learning has made significant progress in achieving human-level performance in several real-world applications. Nonetheless, the current algorithms encounter difficulties in estimating gradients, leading to unstable training and low sample efficiency. Our study introduces a novel optimization approach utilizing stochastic variance reduced gradient (SVRG) techniques. Through extensive experiments in Atari domain, our method surpasses the deep q-learning baselines in 18 out of 20 games.",1
"The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent's effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.",0
"The field of reinforcement learning (RL) is facing complex domains that pose challenges due to their combinatory nature. A successful RL agent must be capable of planning effectively. Prior research has relied on an explicit environment model, coupled with a planning algorithm like tree search. Recently, a new method has emerged that teaches agents how to plan by offering a structure for planning through an inductive bias in the function approximator. This bias is created by using a tree-structured neural network, which is trained using an end-to-end model-free RL algorithm. Our paper goes even further, demonstrating through empirical evidence that a standard neural network without specialized structure, such as convolutional networks and LSTMs, can learn to mimic a model-based planner. We assess the agent's ability to plan by examining its capacity to generalize across a combinatory and irreversible state space, its efficiency in utilizing data, and its ability to allocate additional thinking time. Our findings reveal that our agent possesses many features that one would anticipate in a planning algorithm. Moreover, it surpasses the current state-of-the-art in complex combinatorial domains like Sokoban and outperforms other model-free approaches with strong inductive biases towards planning.",1
"Experience replay (ER) is a fundamental component of off-policy deep reinforcement learning (RL). ER recalls experiences from past iterations to compute gradient estimates for the current policy, increasing data-efficiency. However, the accuracy of such updates may deteriorate when the policy diverges from past behaviors and can undermine the performance of ER. Many algorithms mitigate this issue by tuning hyper-parameters to slow down policy changes. An alternative is to actively enforce the similarity between policy and the experiences in the replay memory. We introduce Remember and Forget Experience Replay (ReF-ER), a novel method that can enhance RL algorithms with parameterized policies. ReF-ER (1) skips gradients computed from experiences that are too unlikely with the current policy and (2) regulates policy changes within a trust region of the replayed behaviors. We couple ReF-ER with Q-learning, deterministic policy gradient and off-policy gradient methods. We find that ReF-ER consistently improves the performance of continuous-action, off-policy RL on fully observable benchmarks and partially observable flow control problems.",0
"Off-policy deep reinforcement learning relies on experience replay (ER) as a crucial element. This involves recalling past experiences to estimate gradients for the current policy, resulting in greater data efficiency. However, the accuracy of these updates may suffer when the policy diverges from past behaviors, adversely affecting the effectiveness of ER. Some algorithms address this issue by adjusting hyper-parameters to slow policy changes, while others enforce similarity between the policy and the experiences in the replay memory. Our novel method, Remember and Forget Experience Replay (ReF-ER), enhances RL algorithms with parameterized policies. ReF-ER skips gradients computed from experiences that are too unlikely with the current policy and regulates policy changes within a trust region of the replayed behaviors. We have coupled ReF-ER with Q-learning, deterministic policy gradient, and off-policy gradient methods and observed consistent improvement in the performance of continuous-action, off-policy RL on fully observable benchmarks and partially observable flow control problems.",1
"Imitation by observation is an approach for learning from expert demonstrations that lack action information, such as videos. Recent approaches to this problem can be placed into two broad categories: training dynamics models that aim to predict the actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). In this paper, we introduce a novel approach that learns values, rather than rewards, directly from observations. We show that by using values, we can significantly speed up RL by removing the need to bootstrap action-values, as compared to sparse-reward specifications.",0
"An approach to learning from expert demonstrations that lack action information, such as videos, is imitation by observation. There are two categories of recent approaches to this problem: training dynamics models to predict actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). Our paper introduces a new approach that learns values instead of rewards directly from observations. We demonstrate that using values can accelerate RL significantly by eliminating the need to bootstrap action-values compared to sparse-reward specifications.",1
"Reinforcement learning has steadily improved and outperform human in lots of traditional games since the resurgence of deep neural network. However, these success is not easy to be copied to autonomous driving because the state spaces in real world are extreme complex and action spaces are continuous and fine control is required. Moreover, the autonomous driving vehicles must also keep functional safety under the complex environments. To deal with these challenges, we first adopt the deep deterministic policy gradient (DDPG) algorithm, which has the capacity to handle complex state and action spaces in continuous domain. We then choose The Open Racing Car Simulator (TORCS) as our environment to avoid physical damage. Meanwhile, we select a set of appropriate sensor information from TORCS and design our own rewarder. In order to fit DDPG algorithm to TORCS, we design our network architecture for both actor and critic inside DDPG paradigm. To demonstrate the effectiveness of our model, We evaluate on different modes in TORCS and show both quantitative and qualitative results.",0
"Since the resurgence of deep neural networks, reinforcement learning has made significant progress and surpassed human performance in many traditional games. However, transferring these achievements to autonomous driving is not easy due to the extreme complexity of real-world state spaces, continuous action spaces, and the need for precise control to maintain functional safety in complex environments. To tackle these challenges, we have employed the deep deterministic policy gradient (DDPG) algorithm, which can handle complex state and action spaces in the continuous domain. We have chosen The Open Racing Car Simulator (TORCS) as our environment to avoid physical damage, and we have carefully selected a set of appropriate sensor information from TORCS and developed our own rewarder. To adapt the DDPG algorithm to TORCS, we have designed network architectures for both the actor and critic within the DDPG paradigm. To demonstrate the effectiveness of our approach, we have evaluated it on different TORCS modes and presented both quantitative and qualitative results.",1
"The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the Bellman operator. Surprisingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when combined with Deep Q-learning, leads to Q-functions with superior policies in practice, even outperforming its double Q-learning counterpart. To better understand how and why this occurs, we revisit theoretical properties of the softmax Bellman operator, and prove that $(i)$ it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and $(ii)$ the distance of its Q function from the optimal one can be bounded. These alone do not explain its superior performance, so we also show that the softmax operator can reduce the overestimation error, which may give some insight into why a sub-optimal operator leads to better performance in the presence of value function approximation. A comparison among different Bellman operators is then presented, showing the trade-offs when selecting them.",0
"The use of softmax in reinforcement learning (RL) is often seen as problematic because it can result in suboptimal value functions and interfere with the contraction properties of the Bellman operator. Despite these concerns, when combined with Deep Q-learning, the softmax Bellman operator has been shown to produce superior policies in practice, even outperforming its double Q-learning counterpart. To better understand why this occurs, the theoretical properties of the softmax Bellman operator have been revisited, and it has been proven that it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and the distance of its Q function from the optimal one can be bounded. However, these properties alone do not explain its superior performance. It has also been found that the softmax operator can reduce overestimation error, which may provide some insight into why a suboptimal operator leads to better performance when using value function approximation. A comparison of different Bellman operators is presented, showing the trade-offs involved in selecting them.",1
"In this paper, a review of model-free reinforcement learning for learning of dynamical systems in uncertain environments has discussed. For this purpose, the Markov Decision Process (MDP) will be reviewed. Furthermore, some learning algorithms such as Temporal Difference (TD) learning, Q-Learning, and Approximate Q-learning as model-free algorithms which constitute the main part of this article have been investigated, and benefits and drawbacks of each algorithm will be discussed. The discussed concepts in each section are explaining with details and examples.",0
"This paper provides an overview of model-free reinforcement learning in uncertain environments for learning dynamical systems. The Markov Decision Process (MDP) is reviewed for this purpose. Additionally, the main focus is on some learning algorithms, including Temporal Difference (TD) learning, Q-Learning, and Approximate Q-learning, as model-free algorithms. The advantages and disadvantages of each algorithm are also discussed. Furthermore, each section is explained in detail with examples.",1
"This paper provides a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.",0
"By defining functionals on probability measures, this paper presents a comprehensive perspective on various machine learning problems. Our framework reveals that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be interpreted within it. Additionally, we introduce a universal optimization algorithm, Probability Functional Descent (PFD), and demonstrate how it can recover methods previously developed in the mentioned contexts.",1
"Many continuous control tasks have easily formulated objectives, yet using them directly as a reward in reinforcement learning (RL) leads to suboptimal policies. Therefore, many classical control tasks guide RL training using complex rewards, which require tedious hand-tuning. We automate the reward search with AutoRL, an evolutionary layer over standard RL that treats reward tuning as hyperparameter optimization and trains a population of RL agents to find a reward that maximizes the task objective. AutoRL, evaluated on four Mujoco continuous control tasks over two RL algorithms, shows improvements over baselines, with the the biggest uplift for more complex tasks. The video can be found at: \url{https://youtu.be/svdaOFfQyC8}.",0
"Although continuous control tasks may have objectives that are easy to define, using them as a reward in reinforcement learning (RL) can result in suboptimal policies. As a result, many classical control tasks require complex rewards that need to be painstakingly hand-tuned to guide RL training. To simplify this process, we have developed AutoRL, an evolutionary layer that treats reward tuning as hyperparameter optimization and trains a population of RL agents to find a reward that maximizes the task objective. Our evaluation of AutoRL on four Mujoco continuous control tasks using two RL algorithms showed improvements over baselines, particularly for more complex tasks. A video of our work can be found at: \url{https://youtu.be/svdaOFfQyC8}.",1
"We assume that we are given a time series of data from a dynamical system and our task is to learn the flow map of the dynamical system. We present a collection of results on how to enforce constraints coming from the dynamical system in order to accelerate the training of deep neural networks to represent the flow map of the system as well as increase their predictive ability. In particular, we provide ways to enforce constraints during training for all three major modes of learning, namely supervised, unsupervised and reinforcement learning. In general, the dynamic constraints need to include terms which are analogous to memory terms in model reduction formalisms. Such memory terms act as a restoring force which corrects the errors committed by the learned flow map during prediction.   For supervised learning, the constraints are added to the objective function. For the case of unsupervised learning, in particular generative adversarial networks, the constraints are introduced by augmenting the input of the discriminator. Finally, for the case of reinforcement learning and in particular actor-critic methods, the constraints are added to the reward function. In addition, for the reinforcement learning case, we present a novel approach based on homotopy of the action-value function in order to stabilize and accelerate training. We use numerical results for the Lorenz system to illustrate the various constructions.",0
"Our objective is to learn the flow map of a given dynamical system using a time series of data. To enhance the predictive ability of deep neural networks and expedite their training to represent the system's flow map, we suggest enforcing constraints based on the dynamical system. To achieve this, we provide methods to enforce constraints during the three major modes of learning: supervised, unsupervised, and reinforcement learning. These dynamic constraints need to include memory terms that act as a restoring force to correct errors committed during prediction. We incorporate constraints in the objective function for supervised learning and augment the discriminator's input for unsupervised learning, particularly generative adversarial networks. For reinforcement learning, we add constraints to the reward function and introduce a new approach based on homotopy of the action-value function to stabilize and expedite training. We present numerical results for the Lorenz system to illustrate the various techniques.",1
"Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.",0
"Model-agnostic meta-learning (MAML) is a technique that enables a model to be trained on multiple learning tasks, preparing it for the few-shot learning of new tasks. Although the MAML algorithm performs well on classification, regression, and fine-tuning of policy gradients in reinforcement learning, it requires costly hyperparameter tuning for training stability. To address this issue, we introduce a new extension to MAML known as Alpha MAML, which includes an online hyperparameter adaptation scheme that eliminates the need for meta-learning and learning rates tuning. Our experiments with the Omniglot database demonstrate a significant reduction in the requirement for MAML training hyperparameters tuning, and improved training stability with less sensitivity to hyperparameter selection.",1
"As global greenhouse gas emissions continue to rise, the use of stratospheric aerosol injection (SAI), a form of solar geoengineering, is increasingly considered in order to artificially mitigate climate change effects. However, initial research in simulation suggests that naive SAI can have catastrophic regional consequences, which may induce serious geostrategic conflicts. Current geo-engineering research treats SAI control in low-dimensional approximation only. We suggest treating SAI as a high-dimensional control problem, with policies trained according to a context-sensitive reward function within the Deep Reinforcement Learning (DRL) paradigm. In order to facilitate training in simulation, we suggest to emulate HadCM3, a widely used General Circulation Model, using deep learning techniques. We believe this is the first application of DRL to the climate sciences.",0
"The use of stratospheric aerosol injection (SAI) as a solar geoengineering solution is being considered to mitigate the effects of climate change in light of increasing global greenhouse gas emissions. However, initial simulations indicate that this approach can have serious regional consequences, leading to potential geostrategic conflicts. Current geo-engineering research focuses on low-dimensional SAI control, but we propose a high-dimensional control problem that utilizes deep reinforcement learning (DRL) to train policies based on a context-sensitive reward function. To facilitate simulation training, we suggest using deep learning techniques to emulate HadCM3, a widely used General Circulation Model, representing the first application of DRL to the climate sciences.",1
"Off-policy reinforcement learning with eligibility traces is challenging because of the discrepancy between target policy and behavior policy. One common approach is to measure the difference between two policies in a probabilistic way, such as importance sampling and tree-backup. However, existing off-policy learning methods based on probabilistic policy measurement are inefficient when utilizing traces under a greedy target policy, which is ineffective for control problems. The traces are cut immediately when a non-greedy action is taken, which may lose the advantage of eligibility traces and slow down the learning process. Alternatively, some non-probabilistic measurement methods such as General Q($\lambda$) and Naive Q($\lambda$) never cut traces, but face convergence problems in practice. To address the above issues, this paper introduces a new method named TBQ($\sigma$), which effectively unifies the tree-backup algorithm and Naive Q($\lambda$). By introducing a new parameter $\sigma$ to illustrate the \emph{degree} of utilizing traces, TBQ($\sigma$) creates an effective integration of TB($\lambda$) and Naive Q($\lambda$) and continuous role shift between them. The contraction property of TB($\sigma$) is theoretically analyzed for both policy evaluation and control settings. We also derive the online version of TBQ($\sigma$) and give the convergence proof. We empirically show that, for $\epsilon\in(0,1]$ in $\epsilon$-greedy policies, there exists some degree of utilizing traces for $\lambda\in[0,1]$, which can improve the efficiency in trace utilization for off-policy reinforcement learning, to both accelerate the learning process and improve the performance.",0
"The use of eligibility traces in off-policy reinforcement learning poses a challenge due to the mismatch between target policy and behavior policy. Various methods, including importance sampling and tree-backup, have been employed to measure policy differences in a probabilistic manner. However, these methods prove to be inefficient and ineffective for control problems when utilizing traces under a greedy target policy. On the other hand, non-probabilistic measurement methods, such as General Q($\lambda$) and Naive Q($\lambda$), never cut traces, but face convergence problems in practice. This paper proposes a new method called TBQ($\sigma$) that effectively combines the tree-backup algorithm and Naive Q($\lambda$) by introducing a new parameter $\sigma$ to illustrate the degree of utilizing traces. TBQ($\sigma$) unifies TB($\lambda$) and Naive Q($\lambda$) and allows for a continuous role shift between them, while maintaining the contraction property for both policy evaluation and control settings. The online version of TBQ($\sigma$) is derived and its convergence proof is established. Empirical results show that utilizing traces to a certain degree can improve the efficiency in off-policy reinforcement learning and accelerate the learning process while improving performance, particularly for $\epsilon$-greedy policies with $\lambda\in[0,1]$.",1
"Shaping in humans and animals has been shown to be a powerful tool for learning complex tasks as compared to learning in a randomized fashion. This makes the problem less complex and enables one to solve the easier sub task at hand first. Generating a curriculum for such guided learning involves subjecting the agent to easier goals first, and then gradually increasing their difficulty. This paper takes a similar direction and proposes a dual curriculum scheme for solving robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro curriculum scheme which divides the task into multiple sub-tasks followed by a micro curriculum scheme which enables the agent to learn between such discovered sub-tasks. We show how combining macro and micro curriculum strategies help in overcoming major exploratory constraints considered in robot manipulation tasks without having to engineer any complex rewards. We also illustrate the meaning of the individual curricula and how they can be used independently based on the task. The performance of such a dual curriculum scheme is analyzed on the Fetch environments.",0
"Learning through shaping has been proven to be a more effective method for tackling complex tasks, as opposed to randomized learning. This approach simplifies the problem, allowing for easier completion of sub-tasks. To create a guided learning curriculum, the agent is first subjected to simpler goals that gradually increase in difficulty. This paper introduces the MaMiC scheme, which utilizes a dual curriculum approach to solve robotic manipulation tasks with sparse rewards. The macro curriculum divides the task into sub-tasks, while the micro curriculum enables learning between these sub-tasks. By combining these strategies, MaMiC overcomes exploratory constraints in robot manipulation tasks without requiring complex rewards. The curricula can also be used independently based on the task. The performance of the dual curriculum is evaluated on Fetch environments.",1
"Recent work in reinforcement learning demonstrated that learning solely through self-play is not only possible, but could also result in novel strategies that humans never would have thought of. However, optimization methods cast as a game between two players require careful tuning to prevent suboptimal results. Hence, we look at random play as an alternative method. In this paper, we train a DQN agent to play Sungka, a two-player turn-based board game wherein the players compete to obtain more stones than the other. We show that even with purely random play, our training algorithm converges very fast and is stable. Moreover, we test our trained agent against several baselines and show its ability to consistently win against these.",0
"Recent research on reinforcement learning has shown that self-play can lead to the development of innovative strategies that humans may not have considered. However, when optimization methods are framed as a two-player game, careful calibration is necessary to avoid suboptimal outcomes. Consequently, we explore random play as an alternative approach. Our study involves training a DQN agent to play Sungka, a two-player board game where the goal is to acquire more stones than the opponent. We demonstrate that our training algorithm converges quickly and remains stable even with purely random play. Furthermore, we assess our agent's performance against various baselines and prove that it consistently outperforms them.",1
"Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used---e.g., dilated convolutions---to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with state-of-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/drsleep/nas-segm-pytorch",0
"Exploring the avenue of automated design of neural network architectures tailored for specific tasks is highly promising, yet challenging. While previous studies have focused on image classification and language modelling problems, our research centers on dense per-pixel tasks, specifically semantic image segmentation through fully convolutional networks. Compared to previous areas, designing a fully convolutional network requires various changes, including the type of operations used, such as dilated convolutions, and solving more challenging optimization problems. Our main focus is to search for high-performance compact segmentation architectures that can run in real-time with limited resources. To achieve this, we intentionally over-parameterize the architecture during training with auxiliary cells that provide intermediate supervisory signals and can be disregarded during evaluation. The design of the auxiliary cell is generated by a controller, which is a neural network with a fixed structure trained using reinforcement learning. Our research demonstrates an efficient approach to searching for these architectures within limited time and computational budgets, using a progressive strategy to terminate non-promising architectures and Polyak averaging with knowledge distillation to hasten convergence. Our approach discovers a set of architectures that perform on-par with the state-of-the-art among compact models in semantic segmentation, pose estimation, and depth prediction tasks in just 8 GPU-days. We will make the code available in the following link: https://github.com/drsleep/nas-segm-pytorch.",1
"We consider a model-based approach to perform batch off-policy evaluation in reinforcement learning. Our method takes a mixture-of-experts approach to combine parametric and non-parametric models of the environment such that the final value estimate has the least expected error. We do so by first estimating the local accuracy of each model and then using a planner to select which model to use at every time step as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based approach outperforms the individual models alone as well as state-of-the-art importance sampling-based estimators.",0
"In reinforcement learning, we adopt a model-centered strategy to execute batch off-policy evaluation. Our technique merges parametric and non-parametric models of the surroundings using a mixture-of-experts approach to obtain the final value estimate with minimal anticipated error. Initially, we assess the accuracy of each model locally and then apply a planner to determine the model to be used at each time step to minimize the return error estimate throughout trajectories. Our mixture-based method surpasses the individual models and importance sampling-based estimators in various domains.",1
"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.",0
"The integration of deep neural network models and reinforcement learning algorithms can enable the learning of policies for robotic behaviors that can directly interpret raw sensory inputs, such as camera images. This approach efficiently combines estimation and control into a single model. However, applying reinforcement learning in real-world scenarios necessitates manual programming of a reward function to specify the task's goal. This practice requires designing a perception pipeline that end-to-end reinforcement learning aims to bypass or introducing additional sensors in the environment to determine task completion. In this study, we propose a method that eliminates the need for manual reward engineering by allowing a robot to learn from a small number of successful examples and actively requesting input from the user to label states that indicate task completion. Our approach only requires labels for a small fraction of the states observed during training, making it a practical and efficient way of learning skills without manually programmed rewards. We evaluated our approach on real-world robotic manipulation tasks using images captured by the robot's camera. Our experiments demonstrate that the proposed method can successfully arrange objects, place books, and drape cloth directly from images without any predetermined reward functions, and with only a few hours of interaction with the real world.",1
"We consider an agent who is involved in a Markov decision process and receives a vector of outcomes every round. Her objective is to maximize a global concave reward function on the average vectorial outcome. The problem models applications such as multi-objective optimization, maximum entropy exploration, and constrained optimization in Markovian environments. In our general setting where a stationary policy could have multiple recurrent classes, the agent faces a subtle yet consequential trade-off in alternating among different actions for balancing the vectorial outcomes. In particular, stationary policies are in general sub-optimal. We propose a no-regret algorithm based on online convex optimization (OCO) tools (Agrawal and Devanur 2014) and UCRL2 (Jaksch et al. 2010). Importantly, we introduce a novel gradient threshold procedure, which carefully controls the switches among actions to handle the subtle trade-off. By delaying the gradient updates, our procedure produces a non-stationary policy that diversifies the outcomes for optimizing the objective. The procedure is compatible with a variety of OCO tools.",0
"The article discusses an agent who operates within a Markov decision process and receives a series of outcome vectors during each round. The agent's primary aim is to maximize a global reward function that is concave, focusing on the average vectorial outcome. This type of problem is useful for modelling a range of applications, including multi-objective optimization, Markovian constrained optimization, and maximum entropy exploration. However, in situations where stationary policies have multiple recurrent classes, the agent must balance the vectorial outcomes by alternating between different actions. The study proposes a no-regret algorithm that utilizes online convex optimization and UCRL2 tools. Additionally, the authors introduce a new gradient threshold procedure that controls the switching of actions to achieve the optimal outcome. By delaying the gradient updates, the procedure produces a non-stationary policy that diversifies the outcomes for optimizing the objective. The approach is compatible with various OCO tools.",1
"We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective to introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.",0
"In our research, we analyze the geometric and topological features of the value function space in Markov decision processes with finite states and actions. Our primary achievement is identifying its shape as a general polytope, as previously described by Aigner et al. (2010). We showcase various aspects of the structural interplay between policies and value functions, such as the line theorem, which reveals that value functions of policies limited to all but one state depict a line segment. Ultimately, we utilize this innovative approach to introduce visual aids that improve comprehension of the mechanics of reinforcement learning algorithms.",1
"Model-free reinforcement learning based methods such as Proximal Policy Optimization, or Q-learning typically require thousands of interactions with the environment to approximate the optimum controller which may not always be feasible in robotics due to safety and time consumption. Model-based methods such as PILCO or BlackDrops, while data-efficient, provide solutions with limited robustness and complexity. To address this tradeoff, we introduce active uncertainty reduction-based virtual environments, which are formed through limited trials conducted in the original environment. We provide an efficient method for uncertainty management, which is used as a metric for self-improvement by identification of the points with maximum expected improvement through adaptive sampling. Capturing the uncertainty also allows for better mimicking of the reward responses of the original system. Our approach enables the use of complex policy structures and reward functions through a unique combination of model-based and model-free methods, while still retaining the data efficiency. We demonstrate the validity of our method on several classic reinforcement learning problems in OpenAI gym. We prove that our approach offers a better modeling capacity for complex system dynamics as compared to established methods.",0
"Reinforcement learning methods, such as Proximal Policy Optimization or Q-learning, that rely on model-free approaches usually require a significant number of interactions with the environment to approximate the optimal controller. However, this may not always be feasible in robotics due to safety and time constraints. On the other hand, model-based methods like PILCO or BlackDrops are data-efficient but may provide limited robustness and complexity. To balance this tradeoff, we propose using active uncertainty reduction-based virtual environments, which are created through a few trials in the original environment. Our approach efficiently manages uncertainty by identifying the points with the highest expected improvement through adaptive sampling, which also allows for better mimicry of reward responses. By combining model-based and model-free methods, our approach enables the use of complex policy structures and reward functions while retaining data efficiency. We validate our method on several reinforcement learning problems in OpenAI gym and demonstrate its superior modeling capacity for complex system dynamics compared to established methods.",1
"A long-standing challenge in Reinforcement Learning is enabling agents to learn a model of their environment which can be transferred to solve other problems in a world with the same underlying rules. One reason this is difficult is the challenge of learning accurate models of an environment. If such a model is inaccurate, the agent's plans and actions will likely be sub-optimal, and likely lead to the wrong outcomes. Recent progress in model-based reinforcement learning has improved the ability for agents to learn and use predictive models. In this paper, we extend a recent deep learning architecture which learns a predictive model of the environment that aims to predict only the value of a few key measurements, which are be indicative of an agent's performance. Predicting only a few measurements rather than the entire future state of an environment makes it more feasible to learn a valuable predictive model. We extend this predictive model with a small, evolving neural network that suggests the best goals to pursue in the current state. We demonstrate that this allows the predictive model to transfer to new scenarios where goals are different, and that the adaptive goals can even adjust agent behavior on-line, changing its strategy to fit the current context.",0
"Developing a model of an environment that can be applied to solve other problems with similar rules has been a persistent challenge in Reinforcement Learning. The difficulty lies in the ability to learn an accurate model, as an erroneous model can lead to sub-optimal plans and actions by the agent, resulting in unfavorable outcomes. Recent advancements in model-based reinforcement learning have improved the agent's ability to learn and use predictive models. This paper extends a deep learning architecture that aims to predict only a few key measurements indicative of an agent's performance, making it more feasible to learn a valuable predictive model. The extension involves the integration of a small, evolving neural network that suggests the best goals to pursue in the current state. This approach enables the predictive model to transfer to new scenarios with different goals and adapt agent behavior on-line to fit the current context.",1
"Recently, the state-of-the-art models for image captioning have overtaken human performance based on the most popular metrics, such as BLEU, METEOR, ROUGE, and CIDEr. Does this mean we have solved the task of image captioning? The above metrics only measure the similarity of the generated caption to the human annotations, which reflects its accuracy. However, an image contains many concepts and multiple levels of detail, and thus there is a variety of captions that express different concepts and details that might be interesting for different humans. Therefore only evaluating accuracy is not sufficient for measuring the performance of captioning models --- the diversity of the generated captions should also be considered. In this paper, we proposed a new metric for measuring the diversity of image captions, which is derived from latent semantic analysis and kernelized to use CIDEr similarity. We conduct extensive experiments to re-evaluate recent captioning models in the context of both diversity and accuracy. We find that there is still a large gap between the model and human performance in terms of both accuracy and diversity and the models that have optimized accuracy (CIDEr) have low diversity. We also show that balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy of the generated captions.",0
"The latest image captioning models have surpassed human performance on common metrics like BLEU, METEOR, ROUGE, and CIDEr. However, this does not mean that the task of image captioning has been fully solved. These metrics only evaluate the accuracy of the generated captions, but images have multiple concepts and levels of detail that can be expressed in various ways. Therefore, measuring only accuracy is insufficient, and diversity of the generated captions should also be taken into account. To address this, we introduce a new metric that uses latent semantic analysis and CIDEr similarity to measure diversity. Our experiments show that there is still a significant gap between model and human performance in terms of both accuracy and diversity. Models that prioritize accuracy tend to have lower diversity, but balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy.",1
"In an episodic Markov Decision Process (MDP) problem, an online algorithm chooses from a set of actions in a sequence of $H$ trials, where $H$ is the episode length, in order to maximize the total payoff of the chosen actions. Q-learning, as the most popular model-free reinforcement learning (RL) algorithm, directly parameterizes and updates value functions without explicitly modeling the environment. Recently, [Jin et al. 2018] studies the sample complexity of Q-learning with finite states and actions. Their algorithm achieves nearly optimal regret, which shows that Q-learning can be made sample efficient. However, MDPs with large discrete states and actions [Silver et al. 2016] or continuous spaces [Mnih et al. 2013] cannot learn efficiently in this way. Hence, it is critical to develop new algorithms to solve this dilemma with provable guarantee on the sample complexity. With this motivation, we propose a novel algorithm that works for MDPs with a more general setting, which has infinitely many states and actions and assumes that the payoff function and transition kernel are Lipschitz continuous. We also provide corresponding theory justification for our algorithm. It achieves the regret $\tilde{\mathcal{O}}(K^{\frac{d+1}{d+2}}\sqrt{H^3}),$ where $K$ denotes the number of episodes and $d$ denotes the dimension of the joint space. To the best of our knowledge, this is the first analysis in the model-free setting whose established regret matches the lower bound up to a logarithmic factor.",0
"An online algorithm is utilized in an episodic Markov Decision Process (MDP) problem to select actions in a sequence of $H$ trials, with the objective of maximizing the total payoff of the chosen actions. Q-learning, which is the most widely used model-free reinforcement learning (RL) algorithm, updates value functions without explicitly modeling the environment. Recent research by Jin et al. (2018) examines the sample complexity of Q-learning with finite states and actions, which shows that Q-learning can be made sample efficient. However, MDPs with large discrete states and actions or continuous spaces cannot be learned efficiently using this method. Therefore, it is important to develop new algorithms with provable guarantees on sample complexity. Our proposed algorithm works for MDPs with infinitely many states and actions, assuming that the payoff function and transition kernel are Lipschitz continuous. We provide theoretical justification for our algorithm, which achieves the regret $\tilde{\mathcal{O}}(K^{\frac{d+1}{d+2}}\sqrt{H^3})$, where $K$ denotes the number of episodes and $d$ denotes the dimension of the joint space. This is the first analysis in the model-free setting whose established regret matches the lower bound up to a logarithmic factor.",1
"In most machine learning training paradigms a fixed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sample efficient reinforcement learning approach for adapting the loss dynamically during training. We empirically show how this formulation improves performance by simultaneously optimizing the evaluation metric and smoothing the loss landscape. We verify our method in metric learning and classification scenarios, showing considerable improvements over the state-of-the-art on a diverse set of tasks. Importantly, our method is applicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned policies are transferable across tasks and data, demonstrating the versatility of the method.",0
"The common practice in machine learning training is to use a fixed loss function as a substitute for the evaluation metric. However, we aim to challenge this assumption by meta-learning an adaptable loss function that directly optimizes the evaluation metric. Our proposed approach involves using reinforcement learning to dynamically adjust the loss during training, leading to better performance and smoother loss landscape. We have tested our method in metric learning and classification scenarios and have observed significant improvements over the current state-of-the-art in various tasks. It is worth noting that our method is not limited to specific loss functions or evaluation metrics and can be applied to a wide range of scenarios. Additionally, the policies learned can be transferred across different tasks and data, indicating the versatility of our approach.",1
"Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.",0
"Many reinforcement learning methods without a model use state representations for generalization, but they do not consider the structure of the action space or assume that structure is already known. Our study shows how a policy can be broken down into two parts: one that operates in a low-dimensional space of action representations and another that converts these representations into actual actions. These representations enhance generalization over large, finite action sets by allowing the agent to deduce the outcomes of actions that are similar to those already taken. We present an algorithm that can both learn and use action representations, along with convergence criteria. The effectiveness of this approach is demonstrated in real-world problems of significant scale.",1
"The options framework in reinforcement learning models the notion of a skill or a temporally extended sequence of actions. The discovery of a reusable set of skills has typically entailed building options, that navigate to bottleneck states. This work adopts a complementary approach, where we attempt to discover options that navigate to landmark states. These states are prototypical representatives of well-connected regions and can hence access the associated region with relative ease. In this work, we propose Successor Options, which leverages Successor Representations to build a model of the state space. The intra-option policies are learnt using a novel pseudo-reward and the model scales to high-dimensional spaces easily. Additionally, we also propose an Incremental Successor Options model that iterates between constructing Successor Representations and building options, which is useful when robust Successor Representations cannot be built solely from primitive actions. We demonstrate the efficacy of our approach on a collection of grid-worlds, and on the high-dimensional robotic control environment of Fetch.",0
"Reinforcement learning's options framework represents skills or sequences of actions over time. Traditionally, discovering reusable skills meant creating options that navigate to bottleneck states. However, this approach is supplemented by the discovery of options that navigate to landmark states, which represent well-connected regions and can access associated regions with ease. This work introduces Successor Options, which uses Successor Representations to model the state space. Intra-option policies are learned via a novel pseudo-reward, and the model is scalable to high-dimensional spaces. Additionally, the Incremental Successor Options model alternates between constructing Successor Representations and building options when robust representations cannot be built from primitive actions alone. The approach's effectiveness is demonstrated in a grid-world collection and the high-dimensional robotic control environment of Fetch.",1
"Policy gradient methods are powerful reinforcement learning algorithms and have been demonstrated to solve many complex tasks. However, these methods are also data-inefficient, afflicted with high variance gradient estimates, and frequently get stuck in local optima. This work addresses these weaknesses by combining recent improvements in the reuse of off-policy data and exploration in parameter space with deterministic behavioral policies. The resulting objective is amenable to standard neural network optimization strategies like stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. Incorporation of previous rollouts via importance sampling greatly improves data-efficiency, whilst stochastic optimization schemes facilitate the escape from local optima. We evaluate the proposed approach on a series of continuous control benchmark tasks. The results show that the proposed algorithm is able to successfully and reliably learn solutions using fewer system interactions than standard policy gradient methods.",0
"Although policy gradient methods are effective reinforcement learning algorithms, they suffer from inefficiencies in data utilization, high variance in gradient estimates, and the tendency to become trapped in local optima. In response to these limitations, we present a novel approach that combines recent advancements in off-policy data reuse and parameter space exploration with deterministic behavioral policies. This approach enables us to optimize the objective using conventional neural network optimization strategies, such as stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. The incorporation of previous rollouts through importance sampling enhances data efficiency, while stochastic optimization techniques facilitate the avoidance of local optima. Our proposed method is evaluated through a series of continuous control benchmark tasks, which demonstrate its ability to learn solutions with fewer system interactions than traditional policy gradient methods, while maintaining reliability and success.",1
"In this paper, we propose TauRieL and target Traveling Salesman Problem (TSP) since it has broad applicability in theoretical and applied sciences. TauRieL utilizes an actor-critic inspired architecture that adopts ordinary feedforward nets to obtain a policy update vector $v$. Then, we use $v$ to improve the state transition matrix from which we generate the policy. Also, the state transition matrix allows the solver to initialize from precomputed solutions such as nearest neighbors. In an online learning setting, TauRieL unifies the training and the search where it can generate near-optimal results in seconds. The input to the neural nets in the actor-critic architecture are raw 2-D inputs, and the design idea behind this decision is to keep neural nets relatively smaller than the architectures with wide embeddings with the tradeoff of omitting any distributed representations of the embeddings. Consequently, TauRieL generates TSP solutions two orders of magnitude faster per TSP instance as compared to state-of-the-art offline techniques with a performance impact of 6.1\% in the worst case.",0
"TauRieL is proposed in this paper to target the Traveling Salesman Problem (TSP) due to its wide applicability in theoretical and applied sciences. The proposed architecture of TauRieL is inspired by actor-critic and uses ordinary feedforward nets to obtain a policy update vector, which is then utilized to improve the state transition matrix. This matrix enables the solver to initialize from precomputed solutions, such as nearest neighbors. In an online learning setting, TauRieL unifies the training and search processes and can generate near-optimal results in seconds. The neural nets in the actor-critic architecture take raw 2-D inputs, which keeps them relatively smaller than architectures with wide embeddings, at the expense of omitting any distributed representations of the embeddings. Consequently, TauRieL generates TSP solutions two orders of magnitude faster per TSP instance than state-of-the-art offline techniques, with a worst-case performance impact of 6.1%.",1
"We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.",0
"In this article, we investigate solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime. While VDN and QMIX are commonly used examples that factorize the joint action-value function into individual ones for decentralized execution, they have structural constraints such as additivity and monotonicity, limiting their ability to address all factorizable MARL tasks. To overcome this limitation, we propose a new method called QTRAN, which transforms the original joint action-value function into an easily factorizable one without any structural constraints. QTRAN guarantees more general factorization than previous methods, covering a wider range of MARL tasks. Our experiments demonstrate the superior performance of QTRAN in multi-domain Gaussian-squeeze and modified predator-prey tasks, especially in games where non-cooperative behavior is heavily penalized.",1
"Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. Focusing on problems arising in continuous control, we propose a functional regularization approach to augmenting model-free RL. In particular, we regularize the behavior of the deep policy to be similar to a policy prior, i.e., we regularize in function space. We show that functional regularization yields a bias-variance trade-off, and propose an adaptive tuning strategy to optimize this trade-off. When the policy prior has control-theoretic stability guarantees, we further show that this regularization approximately preserves those stability guarantees throughout learning. We validate our approach empirically on a range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone.",0
"A major obstacle in model-free reinforcement learning (RL) is managing high variance. Current techniques display inconsistency, leading to varying performance with different seed initializations. Our focus is on the issues that arise in continuous control, and we suggest an approach to augmenting model-free RL with functional regularization. Specifically, we regulate the deep policy's behavior to be similar to a policy prior in function space. We observe that functional regularization provides a bias-variance trade-off and present an adaptive tuning method to optimize it. When the policy prior has control-theoretic stability guarantees, we demonstrate that this regularization approximately maintains those stability guarantees during learning. We validate our approach empirically on various settings and show significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone.",1
"Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.",0
"Simulation serves as a valuable tool in situations where the annotation of training data for machine learning models is costly or difficult to obtain. Our proposed method employs reinforcement learning to automatically adjust the parameters of any non-differentiable simulator, thereby regulating the distribution of synthesized data to enhance the accuracy of a model trained on said data. Unlike previous methods that manually manipulate simulation parameters or only alter select parameters, our approach fully controls the simulator with the ultimate goal of maximizing accuracy, rather than simply mimicking real data distribution or generating copious amounts of data at random. Our findings indicate that our approach efficiently converges to optimal simulation parameters in controlled experiments and effectively identifies good parameter sets for an image rendering simulator in computer vision applications.",1
"In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",0
"A new exploration method for deep RL is proposed, which incorporates the estimated distribution of value function used in distributional RL to model both parametric and intrinsic uncertainties. The method is comprised of two components: a decaying schedule to suppress intrinsic uncertainty, and an exploration bonus based on the upper quantiles of the learned distribution. In the Atari 2600 games, the proposed method outperforms QR-DQN in 12 out of 14 hard games, resulting in an average gain of 483% across 49 games in cumulative rewards over QR-DQN, with a significant win in Venture. In a challenging 3D driving simulator (CARLA), the proposed algorithm achieves near-optimal safety rewards twice as fast as QR-DQN, as demonstrated by the results of a comparison between the two algorithms.",1
"Many real-world decision problems are characterized by multiple conflicting objectives which must be balanced based on their relative importance. In the dynamic weights setting the relative importance changes over time and specialized algorithms that deal with such change, such as a tabular Reinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are required. However, this earlier work is not feasible for RL settings that necessitate the use of function approximators. We generalize across weight changes and high-dimensional inputs by proposing a multi-objective Q-network whose outputs are conditioned on the relative importance of objectives and we introduce Diverse Experience Replay (DER) to counter the inherent non-stationarity of the Dynamic Weights setting. We perform an extensive experimental evaluation and compare our methods to adapted algorithms from Deep Multi-Task/Multi-Objective Reinforcement Learning and show that our proposed network in combination with DER dominates these adapted algorithms across weight change scenarios and problem domains.",0
"Decision problems in the real-world often involve multiple objectives that conflict with one another. Balancing these objectives based on their relative importance is crucial. When the relative importance changes over time, specialized algorithms are required to manage this change, such as the tabular Reinforcement Learning (RL) algorithm proposed by Natarajan and Tadepalli in 2005. However, this approach is not suitable for RL settings that require the use of function approximators. To address this issue, we propose a multi-objective Q-network that can generalize across weight changes and high-dimensional inputs. This network's outputs are conditioned on the relative importance of objectives, and we introduce Diverse Experience Replay (DER) to handle the non-stationarity of the Dynamic Weights setting. Our extensive experimental evaluation shows that our proposed approach outperforms adapted algorithms from Deep Multi-Task/Multi-Objective Reinforcement Learning across various weight change scenarios and problem domains.",1
"Physical construction---the ability to compose objects, subject to physical dynamics, to serve some function---is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.",0
"Human intelligence relies on the ability to create objects based on physical dynamics in order to fulfill a purpose. In this study, we present a set of intricate physical construction tasks that are inspired by how children play with blocks. These tasks include matching configurations, connecting objects through stacking blocks, and creating shelter-like structures over target objects. We evaluate the performance of deep reinforcement learning agents on these challenges and introduce new approaches that demonstrate superior performance. Our findings indicate that agents utilizing structured representations and policies that revolve around objects outperform those that do not. Additionally, our research reveals that model-based agents that use Monte-Carlo Tree Search outperform strictly model-free agents in the most challenging construction problems. Our results suggest that agents that combine structured representations and reasoning with potent learning are a promising route towards developing agents that have a profound understanding of physics, scene interpretation, and planning.",1
"The Exploration-Exploitation tradeoff arises in Reinforcement Learning when one cannot tell if a policy is optimal. Then, there is a constant need to explore new actions instead of exploiting past experience. In practice, it is common to resolve the tradeoff by using a fixed exploration mechanism, such as $\epsilon$-greedy exploration or by adding Gaussian noise, while still trying to learn an optimal policy. In this work, we take a different approach and study exploration-conscious criteria, that result in optimal policies with respect to the exploration mechanism. Solving these criteria, as we establish, amounts to solving a surrogate Markov Decision Process. We continue and analyze properties of exploration-conscious optimal policies and characterize two general approaches to solve such criteria. Building on the approaches, we apply simple changes in existing tabular and deep Reinforcement Learning algorithms and empirically demonstrate superior performance relatively to their non-exploration-conscious counterparts, both for discrete and continuous action spaces.",0
"When it's unclear whether a policy is optimal, the Exploration-Exploitation tradeoff becomes relevant in Reinforcement Learning. This means that instead of relying solely on past experience, it's necessary to frequently explore new actions. In practice, a fixed exploration mechanism like $\epsilon$-greedy exploration or added Gaussian noise is often used to address the tradeoff while still striving for an optimal policy. However, our approach is different. We focus on exploration-conscious criteria that lead to optimal policies considering the exploration mechanism. By solving these criteria, we establish a surrogate Markov Decision Process. We analyze the characteristics of exploration-conscious optimal policies and identify two general methods to solve the criteria. Using these methods, we modify existing tabular and deep Reinforcement Learning algorithms and demonstrate superior performance compared to non-exploration-conscious counterparts for both discrete and continuous action spaces.",1
"Hierarchical Reinforcement Learning (HRL) exploits temporally extended actions, or options, to make decisions from a higher-dimensional perspective to alleviate the sparse reward problem, one of the most challenging problems in reinforcement learning. The majority of existing HRL algorithms require either significant manual design with respect to the specific environment or enormous exploration to automatically learn options from data. To achieve fast exploration without using manual design, we devise a multi-goal HRL algorithm, consisting of a high-level policy Manager and a low-level policy Worker. The Manager provides the Worker multiple subgoals at each time step. Each subgoal corresponds to an option to control the environment. Although the agent may show some confusion at the beginning of training since it is guided by three diverse subgoals, the agent's behavior policy will quickly learn how to respond to multiple subgoals from the high-level controller on different occasions. By exploiting multiple subgoals, the exploration efficiency is significantly improved. We conduct experiments in Atari's Montezuma's Revenge environment, a well-known sparse reward environment, and in doing so achieve the same performance as state-of-the-art HRL methods with substantially reduced training time cost.",0
"To solve the challenging issue of sparse reward in reinforcement learning, Hierarchical Reinforcement Learning (HRL) uses options to make decisions from a higher-level perspective. However, existing HRL algorithms require either manual design or extensive exploration to learn options from data. To overcome this, we have developed a multi-goal HRL algorithm with a high-level policy Manager and a low-level policy Worker. The Manager provides the Worker with multiple subgoals, each corresponding to an option to control the environment. Although the agent may initially struggle with the diverse subgoals, it quickly learns to respond to them and improve exploration efficiency. We conducted experiments in the sparse reward environment of Atari's Montezuma's Revenge and achieved state-of-the-art performance with significantly reduced training time.",1
"Increasingly available city data and advanced learning techniques have empowered people to improve the efficiency of our city functions. Among them, improving the urban transportation efficiency is one of the most prominent topics. Recent studies have proposed to use reinforcement learning (RL) for traffic signal control. Different from traditional transportation approaches which rely heavily on prior knowledge, RL can learn directly from the feedback. On the other side, without a careful model design, existing RL methods typically take a long time to converge and the learned models may not be able to adapt to new scenarios. For example, a model that is trained well for morning traffic may not work for the afternoon traffic because the traffic flow could be reversed, resulting in a very different state representation. In this paper, we propose a novel design called FRAP, which is based on the intuitive principle of phase competition in traffic signal control: when two traffic signals conflict, priority should be given to one with larger traffic movement (i.e., higher demand). Through the phase competition modeling, our model achieves invariance to symmetrical cases such as flipping and rotation in traffic flow. By conducting comprehensive experiments, we demonstrate that our model finds better solutions than existing RL methods in the complicated all-phase selection problem, converges much faster during training, and achieves superior generalizability for different road structures and traffic conditions.",0
"City data and advanced learning techniques have given people the ability to enhance the efficiency of urban functions, with a focus on improving transportation. Reinforcement learning (RL) has been recommended as a means of traffic signal control, as it can learn from feedback rather than relying solely on prior knowledge. However, existing RL methods can be slow to converge and may not adapt well to new situations. The paper proposes a new approach called FRAP, which utilizes phase competition in traffic signal control to prioritize larger traffic movements. This design achieves invariance to symmetrical cases and outperforms existing RL methods by finding better solutions, training faster, and achieving superior generalizability for different road structures and traffic conditions.",1
"With the increasing availability of traffic data and advance of deep reinforcement learning techniques, there is an emerging trend of employing reinforcement learning (RL) for traffic signal control. A key question for applying RL to traffic signal control is how to define the reward and state. The ultimate objective in traffic signal control is to minimize the travel time, which is difficult to reach directly. Hence, existing studies often define reward as an ad-hoc weighted linear combination of several traffic measures. However, there is no guarantee that the travel time will be optimized with the reward. In addition, recent RL approaches use more complicated state (e.g., image) in order to describe the full traffic situation. However, none of the existing studies has discussed whether such a complex state representation is necessary. This extra complexity may lead to significantly slower learning process but may not necessarily bring significant performance gain.   In this paper, we propose to re-examine the RL approaches through the lens of classic transportation theory. We ask the following questions: (1) How should we design the reward so that one can guarantee to minimize the travel time? (2) How to design a state representation which is concise yet sufficient to obtain the optimal solution? Our proposed method LIT is theoretically supported by the classic traffic signal control methods in transportation field. LIT has a very simple state and reward design, thus can serve as a building block for future RL approaches to traffic signal control. Extensive experiments on both synthetic and real datasets show that our method significantly outperforms the state-of-the-art traffic signal control methods.",0
"Reinforcement learning (RL) is increasingly being used for traffic signal control due to the availability of traffic data and advances in deep reinforcement learning techniques. However, defining the reward and state for RL in traffic signal control is a key challenge. The goal is to minimize travel time, but existing studies use ad-hoc weighted linear combinations of traffic measures as rewards, which may not optimize travel time. Moreover, recent RL approaches use complex states, such as images, without exploring whether such complexity is necessary. In this paper, we propose a method called LIT that re-examines RL approaches through classic transportation theory. We design a concise yet sufficient state representation and a reward that guarantees the minimization of travel time. LIT outperforms existing methods and can serve as a foundation for future RL approaches in traffic signal control.",1
"Assemblies of modular subsystems are being pressed into service to perform sensing, reasoning, and decision making in high-stakes, time-critical tasks in such areas as transportation, healthcare, and industrial automation. We address the opportunity to maximize the utility of an overall computing system by employing reinforcement learning to guide the configuration of the set of interacting modules that comprise the system. The challenge of doing system-wide optimization is a combinatorial problem. Local attempts to boost the performance of a specific module by modifying its configuration often leads to losses in overall utility of the system's performance as the distribution of inputs to downstream modules changes drastically. We present metareasoning techniques which consider a rich representation of the input, monitor the state of the entire pipeline, and adjust the configuration of modules on-the-fly so as to maximize the utility of a system's operation. We show significant improvement in both real-world and synthetic pipelines across a variety of reinforcement learning techniques.",0
"Modular subsystems are being utilized for sensing, reasoning, and decision-making in crucial and time-sensitive tasks, such as transportation, healthcare, and industrial automation. Our focus is on optimizing the overall computing system by employing reinforcement learning to guide the configuration of the interacting modules that make up the system. The difficulty lies in the combinatorial problem of system-wide optimization. Attempts to improve the performance of individual modules by modifying their configuration can result in decreased overall utility as the input distribution to downstream modules changes significantly. To overcome this challenge, we present metareasoning techniques that consider a comprehensive representation of input, monitor the entire pipeline's state, and adjust the module configuration dynamically to maximize system operation's utility. Our approach results in significant improvements in both real-world and synthetic pipelines across multiple reinforcement learning techniques.",1
"This paper studies accelerations in Q-learning algorithms. We propose an accelerated target update scheme by incorporating the historical iterates of Q functions. The idea is conceptually inspired by the momentum-based accelerated methods in the optimization theory. Conditions under which the proposed accelerated algorithms converge are established. The algorithms are validated using commonly adopted testing problems in reinforcement learning, including the FrozenLake grid world game, two discrete-time LQR problems from the Deepmind Control Suite, and the Atari 2600 games. Simulation results show that the proposed accelerated algorithms can improve the convergence performance compared with the vanilla Q-learning algorithm.",0
"The focus of this paper is on exploring the acceleration of Q-learning algorithms. Our proposal involves incorporating the past iterations of Q functions to create an accelerated target update scheme, inspired by the momentum-based accelerated methods in optimization theory. We have established the conditions necessary for the convergence of the accelerated algorithms. To test our algorithms, we have used popular problems in reinforcement learning, such as the FrozenLake grid world game, two discrete-time LQR problems from the Deepmind Control Suite, and the Atari 2600 games. Our simulation results demonstrate that the proposed accelerated algorithms are more effective than the vanilla Q-learning algorithm in terms of convergence performance.",1
"Parameterised actions in reinforcement learning are composed of discrete actions with continuous action-parameters. This provides a framework for solving complex domains that require combining high-level actions with flexible control. The recent P-DQN algorithm extends deep Q-networks to learn over such action spaces. However, it treats all action-parameters as a single joint input to the Q-network, invalidating its theoretical foundations. We analyse the issues with this approach and propose a novel method, multi-pass deep Q-networks, or MP-DQN, to address them. We empirically demonstrate that MP-DQN significantly outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.",0
"Reinforcement learning utilises parameterised actions that consist of discrete actions with continuous action-parameters, enabling the resolution of intricate domains that necessitate the fusion of high-level actions with adaptable control. The P-DQN algorithm, which extends deep Q-networks to learn over such action spaces, has been introduced recently. However, it considers all action-parameters as a single joint input to the Q-network, hence undermining its theoretical foundations. We scrutinise the downsides of this approach and offer a new method, called multi-pass deep Q-networks or MP-DQN, to address them. We demonstrate through empirical evidence that MP-DQN outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.",1
"Mapping states to actions in deep reinforcement learning is mainly based on visual information. The commonly used approach for dealing with visual information is to extract pixels from images and use them as state representation for reinforcement learning agent. But, any vision only agent is handicapped by not being able to sense audible cues. Using hearing, animals are able to sense targets that are outside of their visual range. In this work, we propose the use of audio as complementary information to visual only in state representation. We assess the impact of such multi-modal setup in reach-the-goal tasks in ViZDoom environment. Results show that the agent improves its behavior when visual information is accompanied with audio features.",0
"In deep reinforcement learning, states are linked to actions primarily through visual input. The conventional method of processing visual data involves extracting pixels from images and using them as a state representation for the reinforcement learning agent. However, this approach leaves vision-only agents at a disadvantage, as they are unable to perceive auditory cues. Animals, on the other hand, use their sense of hearing to detect targets that are beyond their line of sight. In this study, we suggest augmenting the state representation of the agent with audio information to complement the visual data. We evaluate the impact of this multi-modal approach on the agent's performance in reaching its goals in the ViZDoom environment. Our findings reveal that the agent's behavior is enhanced when it is provided with audio features alongside visual input.",1
"Reinforcement learning (RL) is capable of managing wireless, energy-harvesting IoT nodes by solving the problem of autonomous management in non-stationary, resource-constrained settings. We show that the state-of-the-art policy-gradient approaches to RL are appropriate for the IoT domain and that they outperform previous approaches. Due to the ability to model continuous observation and action spaces, as well as improved function approximation capability, the new approaches are able to solve harder problems, permitting reward functions that are better aligned with the actual application goals. We show such a reward function and use policy-gradient approaches to learn capable policies, leading to behavior more appropriate for IoT nodes with less manual design effort, increasing the level of autonomy in IoT.",0
"By solving the challenge of autonomous management in non-stationary, resource-constrained settings, reinforcement learning (RL) proves to be effective in handling wireless, energy-harvesting IoT nodes. Our research demonstrates that state-of-the-art policy-gradient approaches in RL are suitable for the IoT domain and yield better results than previous methods. With the ability to model continuous observation and action spaces, as well as enhanced function approximation capability, these new approaches can tackle tougher problems and achieve reward functions that better align with the application goals. We present an example of such a reward function and utilize policy-gradient approaches to acquire proficient policies, resulting in more suitable behavior for IoT nodes with less manual design effort, ultimately increasing the level of autonomy in IoT.",1
"In order perform a large variety of tasks and to achieve human-level performance in complex real-world environments, Artificial Intelligence (AI) Agents must be able to learn from their past experiences and gain both knowledge and an accurate representation of their environment from raw sensory inputs. Traditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy. Deep reinforcement learning algorithms have provided a solution to this issue. In this study, the performance of different conventional and novel deep reinforcement learning algorithms was analysed. The proposed method utilises two types of algorithms, one trained with a variant of Q-learning (DQN) and another trained with SARSA learning (DSN) to assess the feasibility of using direct feedback alignment, a novel biologically plausible method for back-propagating the error. These novel agents, alongside two similar agents trained with the conventional backpropagation algorithm, were tested by using the OpenAI Gym toolkit on several classic control theory problems and Atari 2600 video games. The results of this investigation open the way into new, biologically-inspired deep reinforcement learning algorithms, and their implementation on neuromorphic hardware.",0
"For Artificial Intelligence (AI) Agents to achieve human-like performance in complex real-world environments, they must learn from past experiences and gain accurate knowledge of their environment from raw sensory inputs. However, traditional AI agents have struggled to use sensory inputs to map an efficient control policy. Fortunately, deep reinforcement learning algorithms have provided a solution to this problem. This study analyzed the performance of various conventional and novel deep reinforcement learning algorithms that utilize Q-learning and SARSA learning to assess the feasibility of using direct feedback alignment. The agents were tested on classic control theory problems and Atari 2600 games using the OpenAI Gym toolkit. These results pave the way for new, biologically-inspired deep reinforcement learning algorithms and their implementation on neuromorphic hardware.",1
"Partial domain adaptation aims to transfer knowledge from a label-rich source domain to a label-scarce target domain which relaxes the fully shared label space assumption across different domains. In this more general and practical scenario, a major challenge is how to select source instances in the shared classes across different domains for positive transfer. To address this issue, we propose a Domain Adversarial Reinforcement Learning (DARL) framework to automatically select source instances in the shared classes for circumventing negative transfer as well as to simultaneously learn transferable features between domains by reducing the domain shift. Specifically, in this framework, we employ deep Q-learning to learn policies for an agent to make selection decisions by approximating the action-value function. Moreover, domain adversarial learning is introduced to learn domain-invariant features for the selected source instances by the agent and the target instances, and also to determine rewards for the agent based on how relevant the selected source instances are to the target domain. Experiments on several benchmark datasets demonstrate that the superior performance of our DARL method over existing state of the arts for partial domain adaptation.",0
"The objective of partial domain adaptation is to transfer knowledge from a source domain with abundant labels to a target domain with scarce labels, which relaxes the assumption of fully shared label space across different domains. In this situation, a critical issue is how to choose source instances in the shared classes across different domains to achieve positive transfer. To overcome this challenge, we propose a Domain Adversarial Reinforcement Learning (DARL) framework that selects source instances in the shared classes to avoid negative transfer and simultaneously learns transferable features between domains by reducing the domain shift. The DARL framework utilizes deep Q-learning to train an agent to make selection decisions by approximating the action-value function. Additionally, domain adversarial learning is used to train the agent to select domain-invariant features for the source instances and target instances and to determine rewards based on the relevance of the selected source instances to the target domain. Our experiments on multiple benchmark datasets demonstrate that the DARL method outperforms existing state-of-the-art methods for partial domain adaptation.",1
"In reinforcement learning algorithms, it is a common practice to account for only a single view of the environment to make the desired decisions; however, utilizing multiple views of the environment can help to promote the learning of complicated policies. Since the views may frequently suffer from partial observability, their provided observation can have different levels of importance. In this paper, we present a novel attention-based deep reinforcement learning method in a multi-view environment in which each view can provide various representative information about the environment. Specifically, our method learns a policy to dynamically attend to views of the environment based on their importance in the decision-making process. We evaluate the performance of our method on TORCS racing car simulator and three other complex 3D environments with obstacles.",0
"It is typical for reinforcement learning algorithms to consider just one perspective of the environment when making decisions. However, incorporating multiple perspectives can facilitate the acquisition of complex policies. Since these perspectives may be subject to partial observability, their observations may vary in significance. This study introduces a new deep reinforcement learning approach that employs attention mechanisms in a multi-view setting, where each perspective provides distinctive environmental information. The method adapts by learning to allocate attention to each perspective dynamically, according to its relevance to the decision-making process. We assess our approach's effectiveness in the TORCS racing car simulator and three other intricate 3D obstacle environments.",1
"The game of Tetris is an important benchmark for research in artificial intelligence and machine learning. This paper provides a historical account of the algorithmic developments in Tetris and discusses open challenges. Handcrafted controllers, genetic algorithms, and reinforcement learning have all contributed to good solutions. However, existing solutions fall far short of what can be achieved by expert players playing without time pressure. Further study of the game has the potential to contribute to important areas of research, including feature discovery, autonomous learning of action hierarchies, and sample-efficient reinforcement learning.",0
"This paper offers a historical overview of the algorithmic advancements made in Tetris and explores unresolved obstacles, emphasizing its significance as a benchmark for artificial intelligence and machine learning research. Although handcrafted controllers, genetic algorithms, and reinforcement learning have produced favorable outcomes, they pale in comparison to the proficiency of skilled players operating without time restraints. Further exploration of Tetris could spur important research in feature discovery, independent acquisition of action hierarchies, and efficient reinforcement learning techniques.",1
"Pretraining reinforcement learning methods with demonstrations has been an important concept in the study of reinforcement learning since a large amount of computing power is spent on online simulations with existing reinforcement learning algorithms. Pretraining reinforcement learning remains a significant challenge in exploiting expert demonstrations whilst keeping exploration potentials, especially for value based methods. In this paper, we propose a pretraining method for soft Q-learning. Our work is inspired by pretraining methods for actor-critic algorithms since soft Q-learning is a value based algorithm that is equivalent to policy gradient. The proposed method is based on $\gamma$-discounted biased policy evaluation with entropy regularization, which is also the updating target of soft Q-learning. Our method is evaluated on various tasks from Atari 2600. Experiments show that our method effectively learns from imperfect demonstrations, and outperforms other state-of-the-art methods that learn from expert demonstrations.",0
"Since a considerable amount of computing power is utilized in online simulations using existing reinforcement learning algorithms, pretraining reinforcement learning techniques with demonstrations has become a crucial concept in the field of reinforcement learning. However, pretraining reinforcement learning while maintaining exploration potential, particularly for value-based methods, remains a significant challenge. In this study, we propose a pretraining approach for soft Q-learning, which is a value-based algorithm comparable to policy gradient and has been inspired by actor-critic algorithm pretraining methods. Our approach is based on $\gamma$-discounted biased policy evaluation with entropy regularization, which is also the updating target of soft Q-learning. We have evaluated our method on numerous tasks from Atari 2600, and the experiments demonstrate that our approach effectively learns from suboptimal demonstrations and outperforms other state-of-the-art techniques that learn from expert demonstrations.",1
"Learning an effective representation for high-dimensional data is a challenging problem in reinforcement learning (RL). Deep reinforcement learning (DRL) such as Deep Q networks (DQN) achieves remarkable success in computer games by learning deeply encoded representation from convolution networks. In this paper, we propose a simple yet very effective method for representation learning with DRL algorithms. Our key insight is that features learned by DRL algorithms are highly correlated, which interferes with learning. By adding a regularized loss that penalizes correlation in latent features (with only slight computation), we decorrelate features represented by deep neural networks incrementally. On 49 Atari games, with the same regularization factor, our decorrelation algorithms perform $70\%$ in terms of human-normalized scores, which is $40\%$ better than DQN. In particular, ours performs better than DQN on 39 games with 4 close ties and lost only slightly on $6$ games. Empirical results also show that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly boosts performance. Further experiments on the losing games show that our decorelation algorithms can win over DQN and QR-DQN with a fined tuned regularization factor.",0
"Reinforcement learning (RL) faces a difficult challenge in generating an effective representation for data with high dimensions. Deep reinforcement learning (DRL), such as Deep Q networks (DQN), has achieved remarkable success in computer games by utilizing convolution networks to learn a deeply encoded representation. This paper proposes a straightforward yet highly effective method for representation learning with DRL algorithms. By recognizing that DRL algorithms learn highly correlated features, which hinders learning, we introduce a regularized loss that penalizes correlation in latent features. This addition incrementally decorrelates deep neural network features with minimal computation. Our decorrelation algorithms achieved a 70% human-normalized score on 49 Atari games, which is 40% better than DQN with the same regularization factor. Specifically, our method outperformed DQN on 39 games, with only slight losses on six games. Empirical results demonstrate that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly enhances performance. Further experiments on losing games demonstrate that our decorrelation algorithms can outperform DQN and QR-DQN with a fine-tuned regularization factor.",1
"While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.",0
"Despite the growing interest in Bayesian neural networks (BNNs), the challenge of conducting posterior inference still persists, mainly due to their high-dimensional and over-parameterized nature. To address this challenge, various flexible and scalable variational inference procedures based on particle optimization have been introduced. These procedures involve the direct optimization of a group of particles to approximate the target posterior. However, their application to BNNs often results in sub-optimal performance as they have a tendency to fail on over-parameterized models. This paper suggests a solution to this problem by conducting particle optimization directly in the space of regression functions. Extensive experiments have demonstrated that our method has successfully overcome this issue and outperformed strong baselines in various tasks, including prediction, defense against adversarial examples, and reinforcement learning.",1
"Policy gradient algorithms are among the best candidates for the much anticipated application of reinforcement learning to real-world control tasks, such as the ones arising in robotics. However, the trial-and-error nature of these methods introduces safety issues whenever the learning phase itself must be performed on a physical system. In this paper, we address a specific safety formulation, where danger is encoded in the reward signal and the learning agent is constrained to never worsen its performance. By studying actor-only policy gradient from a stochastic optimization perspective, we establish improvement guarantees for a wide class of parametric policies, generalizing existing results on Gaussian policies. This, together with novel upper bounds on the variance of policy gradient estimators, allows to identify those meta-parameter schedules that guarantee monotonic improvement with high probability. The two key meta-parameters are the step size of the parameter updates and the batch size of the gradient estimators. By a joint, adaptive selection of these meta-parameters, we obtain a safe policy gradient algorithm.",0
"Reinforcement learning has the potential to be used in real-world control tasks, particularly in robotics, and policy gradient algorithms are considered to be some of the best options. However, the trial-and-error approach of these methods can create safety concerns when the learning phase takes place on a physical system. This paper focuses on a specific safety formulation where danger is reflected in the reward signal and the learning agent must not worsen its performance. By examining actor-only policy gradient from a stochastic optimization perspective, we establish improvement guarantees for various types of parametric policies, building on existing results on Gaussian policies. Additionally, we introduce new upper bounds on the variance of policy gradient estimators, enabling us to determine meta-parameter schedules that ensure monotonic improvement with a high probability. Two key meta-parameters are identified: the step size of the parameter updates and the batch size of the gradient estimators. Using an adaptive selection of these meta-parameters, we can create a safe policy gradient algorithm.",1
"In this paper, we propose a novel meta-learning method in a reinforcement learning setting, based on evolution strategies (ES), exploration in parameter space and deterministic policy gradients. ES methods are easy to parallelize, which is desirable for modern training architectures; however, such methods typically require a huge number of samples for effective training. We use deterministic policy gradients during adaptation and other techniques to compensate for the sample-efficiency problem while maintaining the inherent scalability of ES methods. We demonstrate that our method achieves good results compared to gradient-based meta-learning in high-dimensional control tasks in the MuJoCo simulator. In addition, because of gradient-free methods in the meta-training phase, which do not need information about gradients and policies in adaptation training, we predict and confirm our algorithm performs better in tasks that need multi-step adaptation.",0
"This paper presents a new approach to meta-learning in a reinforcement learning context, utilizing evolution strategies (ES), exploration in parameter space, and deterministic policy gradients. Although ES methods are easy to parallelize, they typically require a large number of samples for effective training. To address this, we incorporate deterministic policy gradients during adaptation and other strategies to improve sample efficiency while maintaining scalability. Our method outperforms gradient-based meta-learning in high-dimensional control tasks within the MuJoCo simulator. Additionally, due to the use of gradient-free methods in the meta-training phase, our algorithm is better suited for tasks that require multi-step adaptation.",1
"Evaluation of deep reinforcement learning (RL) is inherently challenging. In particular, learned policies are largely opaque, and hypotheses about the behavior of deep RL agents are difficult to test in black-box environments. Considerable effort has gone into addressing opacity, but almost no effort has been devoted to producing high quality environments for experimental evaluation of agent behavior. We present TOYBOX, a new high-performance, open-source* subset of Atari environments re-designed for the experimental evaluation of deep RL. We show that TOYBOX enables a wide range of experiments and analyses that are impossible in other environments.   *https://kdl-umass.github.io/Toybox/",0
"The assessment of deep reinforcement learning (RL) is a complex task due to the opaqueness of learned policies and the difficulty of testing hypotheses about deep RL agents' behavior in black-box environments. Although attempts have been made to address opacity, there has been almost no emphasis on developing top-quality environments for experimental evaluation of agent behavior. TOYBOX is a new, high-performance, open-source subset of Atari environments, designed specifically for the experimental evaluation of deep RL. With TOYBOX, a wide range of experiments and analyses are possible that are not feasible in other environments.",1
"Although deep reinforcement learning has advanced significantly over the past several years, sample efficiency remains a major challenge. Careful choice of input representations can help improve efficiency depending on the structure present in the problem. In this work, we present an attention-based method to project inputs into an efficient representation space that is invariant under changes to input ordering. We show that our proposed representation results in a search space that is a factor of m! smaller for inputs of m objects. Our experiments demonstrate improvements in sample efficiency for policy gradient methods on a variety of tasks. We show that our representation allows us to solve problems that are otherwise intractable when using naive approaches.",0
"Despite significant advancements in deep reinforcement learning, sample efficiency remains a major hurdle. Depending on the problem structure, careful selection of input representations can enhance efficiency. This study introduces an attention-based approach to project inputs into a space that is invariant to alterations in input ordering, resulting in an efficient representation. The proposed representation results in a search space that is a factor of m! smaller for inputs consisting of m objects. Our experiments indicate that policy gradient methods on various tasks benefit from improved sample efficiency with our representation. Furthermore, our approach allows us to tackle problems that are intractable using naive techniques.",1
"A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Specifically, we consider two scenarios in which the agent attempts to perform an action $a$, and (i) with probability $\alpha$, an alternative adversarial action $\bar a$ is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.",0
"The concept of robustness in a policy refers to its ability to obtain the maximum reward in the presence of a bad or adversarial model. In this study, we introduce two new criteria for robustness to address action uncertainty. The first scenario involves an agent attempting to perform an action $a$, but with a probability of $\alpha$, an alternative adversarial action $\bar a$ is taken. The second scenario occurs in a continuous action space, where an adversary adds a perturbation to the selected action. These criteria are relevant to robotics domains where abrupt forces may occur. We propose algorithms for the tabular case and extend them to deep reinforcement learning (DRL), conducting experiments across various MuJoCo domains. Our findings demonstrate that our approach not only produces robust policies but also enhances performance in the absence of perturbations. Therefore, we conclude that action-robustness can serve as implicit regularization in RL problems.",1
"Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyperparameters. One reason is that most approaches use a noisy version of their operating policy to explore - thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners - typically proven algorithms like TD3 - optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient - notably solving the Mujoco Humanoid benchmark where all of its composite learners (TD3) fail entirely in isolation.",0
"A variety of complex control tasks have been successfully tackled using deep reinforcement learning algorithms. However, these methods often encounter difficulties in achieving effective exploration and are highly sensitive to the choice of hyperparameters. This is partly because most approaches employ a noisy version of their operating policy for exploration, which limits the range of exploration. To address this issue, we present Collaborative Evolutionary Reinforcement Learning (CERL), which is a scalable framework that comprises a portfolio of policies that explore and exploit diverse regions of the solution space simultaneously. A group of learners, such as proven algorithms like TD3, optimize over different time-horizons, leading to a diverse portfolio. All learners use a shared replay buffer to achieve greater sample efficiency, and computational resources are flexibly allocated to favor the best learners as a form of online algorithm selection. Neuroevolution integrates this entire process to produce a single emergent learner that surpasses the capabilities of any individual learner. Experiments in various continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while also being more sample-efficient overall. Notably, it solves the Mujoco Humanoid benchmark, where all of its composite learners (TD3) fail in isolation.",1
"Recent success in deep reinforcement learning for continuous control has been dominated by model-free approaches which, unlike model-based approaches, do not suffer from representational limitations in making assumptions about the world dynamics and model errors inevitable in complex domains. However, they require a lot of experiences compared to model-based approaches that are typically more sample-efficient. We propose to combine the benefits of the two approaches by presenting an integrated approach called Curious Meta-Controller. Our approach alternates adaptively between model-based and model-free control using a curiosity feedback based on the learning progress of a neural model of the dynamics in a learned latent space. We demonstrate that our approach can significantly improve the sample efficiency and achieve near-optimal performance on learning robotic reaching and grasping tasks from raw-pixel input in both dense and sparse reward settings.",0
"Current progress in deep reinforcement learning for continuous control has mainly been led by model-free techniques. Unlike model-based methods, they do not encounter limitations in representing the world dynamics and inevitable model errors in complex environments. However, they do require more experiences compared to model-based techniques, which are typically more efficient in terms of sample usage. To combine the advantages of both methods, we introduce an integrated approach called Curious Meta-Controller. Our approach alternates between model-based and model-free control using a curiosity feedback dependent on the neural model's learning progress of the dynamics in a learned latent space. We demonstrate that our approach can significantly enhance sample efficiency and attain near-optimal performance when learning robotic reaching and grasping tasks from raw-pixel input in dense and sparse reward settings.",1
"We study online reinforcement learning for finite-horizon deterministic control systems with {\it arbitrary} state and action spaces. Suppose that the transition dynamics and reward function is unknown, but the state and action space is endowed with a metric that characterizes the proximity between different states and actions. We provide a surprisingly simple upper-confidence reinforcement learning algorithm that uses a function approximation oracle to estimate optimistic Q functions from experiences. We show that the regret of the algorithm after $K$ episodes is $O(HL(KH)^{\frac{d-1}{d}}) $ where $L$ is a smoothness parameter, and $d$ is the doubling dimension of the state-action space with respect to the given metric. We also establish a near-matching regret lower bound. The proposed method can be adapted to work for more structured transition systems, including the finite-state case and the case where value functions are linear combinations of features, where the method also achieve the optimal regret.",0
"Our focus is on finite-horizon deterministic control systems with unrestricted state and action spaces, where the transition dynamics and reward function are unknown. However, we assume that a metric is in place to gauge the proximity of different states and actions. To estimate optimistic Q functions from experiences, we propose a straightforward upper-confidence reinforcement learning algorithm that utilizes a function approximation oracle. Our analysis reveals that after $K$ episodes, the regret of the algorithm is $O(HL(KH)^{\frac{d-1}{d}})$, where $L$ denotes the smoothness parameter and $d$ represents the doubling dimension of the state-action space with respect to the given metric. We also provide a nearly-matching regret lower bound. With the proposed approach, we can adapt our method for more structured transition systems, such as the finite-state case and the linear combination of features case, where it achieves the optimal regret.",1
"Face hallucination is a domain-specific super-resolution problem that aims to generate a high-resolution (HR) face image from a low-resolution~(LR) input. In contrast to the existing patch-wise super-resolution models that divide a face image into regular patches and independently apply LR to HR mapping to each patch, we implement deep reinforcement learning and develop a novel attention-aware face hallucination (Attention-FH) framework, which recurrently learns to attend a sequence of patches and performs facial part enhancement by fully exploiting the global interdependency of the image. Specifically, our proposed framework incorporates two components: a recurrent policy network for dynamically specifying a new attended region at each time step based on the status of the super-resolved image and the past attended region sequence, and a local enhancement network for selected patch hallucination and global state updating. The Attention-FH model jointly learns the recurrent policy network and local enhancement network through maximizing a long-term reward that reflects the hallucination result with respect to the whole HR image. Extensive experiments demonstrate that our Attention-FH significantly outperforms the state-of-the-art methods on in-the-wild face images with large pose and illumination variations.",0
"The objective of face hallucination is to create a high-resolution image of a face from a low-resolution input. While existing models divide the face image into patches and apply a low-to-high resolution mapping independently to each patch, we have developed a new approach called Attention-FH. This uses deep reinforcement learning to learn how to attend to a sequence of patches and enhance facial features by utilizing the global interdependency of the image. Attention-FH comprises two components: a recurrent policy network to dynamically specify the attended region at each time step, and a local enhancement network for patch hallucination and global state updating. The model learns both networks jointly by maximizing a long-term reward that reflects the hallucination result with respect to the entire high-resolution image. Our experiments show that Attention-FH outperforms state-of-the-art methods on face images captured in the wild, including those with significant pose and illumination variations.",1
"In this letter, we address the problem of controlling energy storage systems (ESSs) for arbitrage in real-time electricity markets under price uncertainty. We first formulate this problem as a Markov decision process, and then develop a deep reinforcement learning based algorithm to learn a stochastic control policy that maps a set of available information processed by a recurrent neural network to ESSs' charging/discharging actions. Finally, we verify the effectiveness of our algorithm using real-time electricity prices from PJM.",0
The focus of this correspondence is on the management of energy storage systems (ESSs) for arbitrage in electricity markets with fluctuating prices. Our approach involves formulating the issue as a Markov decision process and utilizing a deep reinforcement learning algorithm to develop a stochastic control policy. This policy maps processed information from a recurrent neural network to the charging and discharging actions of ESSs. We conclude by demonstrating the efficacy of our algorithm through the application of real-time electricity prices from PJM.,1
"Motor control is a set of time-varying muscle excitations which generate desired motions for a biomechanical system. Muscle excitations cannot be directly measured from live subjects. An alternative approach is to estimate muscle activations using inverse motion-driven simulation. In this article, we propose a deep reinforcement learning method to estimate the muscle excitations in simulated biomechanical systems. Here, we introduce a custom-made reward function which incentivizes faster point-to-point tracking of target motion. Moreover, we deploy two new techniques, namely, episode-based hard update and dual buffer experience replay, to avoid feedback training loops. The proposed method is tested in four simulated 2D and 3D environments with 6 to 24 axial muscles. The results show that the models were able to learn muscle excitations for given motions after nearly 100,000 simulated steps. Moreover, the root mean square error in point-to-point reaching of the target across experiments was less than 1% of the length of the domain of motion. Our reinforcement learning method is far from the conventional dynamic approaches as the muscle control is derived functionally by a set of distributed neurons. This can open paths for neural activity interpretation of this phenomenon.",0
"The generation of desired movements for a biomechanical system is accomplished through a set of muscle excitations that vary over time. However, it is not possible to measure these excitations directly from live subjects. Instead, an alternative approach is to estimate muscle activations through inverse motion-driven simulation. In this article, we propose a deep reinforcement learning technique to estimate muscle excitations in simulated biomechanical systems. Our method includes a customized reward function that encourages faster target motion tracking and introduces two new techniques, episode-based hard update and dual buffer experience replay, to prevent feedback training loops. We tested our approach in four simulated 2D and 3D environments with 6 to 24 axial muscles. The results demonstrate that the models could learn muscle excitations for given motions after almost 100,000 simulated steps. Furthermore, the root mean square error in point-to-point target reaching was less than 1% of the motion domain length across experiments. Our proposed reinforcement learning method differs from conventional dynamic approaches as the muscle control is functionally derived by a set of distributed neurons, which may pave the way for interpreting neural activity in this phenomenon.",1
"Meta-learning is a tool that allows us to build sample-efficient learning systems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just faster learners than their sample-inefficient deep learning (DL) and reinforcement learning (RL) brethren, but that they actually pursue fundamentally different learning trajectories. We study their learning dynamics on three sets of structured tasks for which the corresponding learning dynamics of DL and RL systems have been previously described: linear regression (Saxe et al., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and contextual bandits (Schaul et al., 2019). In each case, while sample-inefficient DL and RL Learners uncover the task structure in a staggered manner, meta-trained LSTM Meta-Learners uncover almost all task structure concurrently, congruent with the patterns expected from Bayes-optimal inference algorithms. This has implications for research areas wherever the learning behaviour itself is of interest, such as safety, curriculum design, and human-in-the-loop machine learning.",0
"Meta-learning is a useful tool that enables the development of learning systems that are efficient with samples. Our research demonstrates that LSTM Meta-Learners, once trained, not only learn faster than their inefficient counterparts in deep learning (DL) and reinforcement learning (RL), but also follow distinct learning trajectories. We conducted an analysis of their learning dynamics on three sets of structured tasks, which have previously been studied in the context of DL and RL: linear regression, nonlinear regression, and contextual bandits. In each scenario, while the sample-inefficient DL and RL Learners gradually uncover the task structure, the meta-trained LSTM Meta-Learners efficiently uncover almost all task structure concurrently, which aligns with the expected patterns of Bayes-optimal inference algorithms. This finding has significant implications for research areas that focus on learning behavior, including safety, curriculum design, and human-in-the-loop machine learning.",1
"Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.",0
"One of the biggest challenges in neural network learning is the inability to perform consistently in non-stationary data distributions, which limits the ability to scale to more human-like settings. To address this challenge, we propose a new approach that considers the trade-off between transfer and interference in continual learning, and optimizes it through gradient alignment. Our proposed algorithm, Meta-Experience Replay (MER), combines experience replay with optimization-based meta-learning to learn parameters that reduce interference and increase transfer based on future gradients. Through experiments in lifelong supervised learning and non-stationary reinforcement learning, we demonstrate that MER outperforms recent baselines for continual learning. Moreover, the performance gap between our approach and baseline algorithms increases with greater non-stationarity and lower fraction of stored experiences.",1
"Air traffic control is a real-time safety-critical decision making process in highly dynamic and stochastic environments. In today's aviation practice, a human air traffic controller monitors and directs many aircraft flying through its designated airspace sector. With the fast growing air traffic complexity in traditional (commercial airliners) and low-altitude (drones and eVTOL aircraft) airspace, an autonomous air traffic control system is needed to accommodate high density air traffic and ensure safe separation between aircraft. We propose a deep multi-agent reinforcement learning framework that is able to identify and resolve conflicts between aircraft in a high-density, stochastic, and dynamic en-route sector with multiple intersections and merging points. The proposed framework utilizes an actor-critic model, A2C that incorporates the loss function from Proximal Policy Optimization (PPO) to help stabilize the learning process. In addition we use a centralized learning, decentralized execution scheme where one neural network is learned and shared by all agents in the environment. We show that our framework is both scalable and efficient for large number of incoming aircraft to achieve extremely high traffic throughput with safety guarantee. We evaluate our model via extensive simulations in the BlueSky environment. Results show that our framework is able to resolve 99.97% and 100% of all conflicts both at intersections and merging points, respectively, in extreme high-density air traffic scenarios.",0
"Air traffic control is a high-stakes decision-making process that takes place in environments that are constantly changing and unpredictable. Typically, a human air traffic controller is responsible for directing numerous aircraft within a designated airspace sector. However, with the rise of air traffic in both traditional (commercial airliners) and low-altitude (drones and eVTOL aircraft) airspace, there is a need for an autonomous air traffic control system to manage the high volume of traffic and ensure aircraft stay safe.  To address this need, we propose a deep multi-agent reinforcement learning framework that can identify and resolve conflicts between aircraft in complex, dynamic, and stochastic en-route sectors with multiple intersections and merging points. Our framework uses the actor-critic model, A2C, with the loss function from Proximal Policy Optimization (PPO) to stabilize the learning process. We also adopt a centralized learning, decentralized execution approach where one neural network is learned and shared by all agents in the environment.  Our simulations in the BlueSky environment demonstrate that our framework is scalable and efficient for handling large numbers of incoming aircraft, achieving high traffic throughput while maintaining safety. Specifically, our framework resolves 99.97% and 100% of conflicts at intersections and merging points, respectively, in extreme high-density air traffic scenarios.",1
"Reinforcement learning (RL) is about sequential decision making and is traditionally opposed to supervised learning (SL) and unsupervised learning (USL). In RL, given the current state, the agent makes a decision that may influence the next state as opposed to SL (and USL) where, the next state remains the same, regardless of the decisions taken, either in batch or online learning. Although this difference is fundamental between SL and RL, there are connections that have been overlooked. In particular, we prove in this paper that gradient policy method can be cast as a supervised learning problem where true label are replaced with discounted rewards. We provide a new proof of policy gradient methods (PGM) that emphasizes the tight link with the cross entropy and supervised learning. We provide a simple experiment where we interchange label and pseudo rewards. We conclude that other relationships with SL could be made if we modify the reward functions wisely.",0
"Reinforcement learning (RL) involves making sequential decisions, which contrasts with supervised learning (SL) and unsupervised learning (USL). In RL, the agent's current state influences the decision made, whereas in SL and USL, the next state is unaffected by the decisions taken in batch or online learning. Despite this fundamental difference between SL and RL, there are overlooked connections between the two. This paper demonstrates that the gradient policy method can be seen as a supervised learning problem, where the true labels are replaced with discounted rewards. The policy gradient methods (PGM) are proven in a new way that highlights their close relationship with cross entropy and supervised learning. An experiment is presented where label and pseudo rewards are interchanged, and it is concluded that further connections between SL and RL are possible by intelligently modifying the reward functions.",1
"Model-free Reinforcement Learning (RL) algorithms such as Q-learning [Watkins, Dayan 92] have been widely used in practice and can achieve human level performance in applications such as video games [Mnih et al. 15]. Recently, equipped with the idea of optimism in the face of uncertainty, Q-learning algorithms [Jin, Allen-Zhu, Bubeck, Jordan 18] can be proven to be sample efficient for discrete tabular Markov Decision Processes (MDPs) which have finite number of states and actions. In this work, we present an efficient model-free Q-learning based algorithm in MDPs with a natural metric on the state-action space--hence extending efficient model-free Q-learning algorithms to continuous state-action space. Compared to previous model-based RL algorithms for metric spaces [Kakade, Kearns, Langford 03], our algorithm does not require access to a black-box planning oracle.",0
"Q-learning, a model-free reinforcement learning (RL) algorithm, has been widely utilized in various applications, including video games, and is capable of achieving human-level performance. Recent advancements in Q-learning algorithms, incorporating the concept of optimism in uncertain situations, have made them more sample efficient for discrete tabular Markov Decision Processes (MDPs) with a finite number of states and actions. This study introduces a novel model-free Q-learning-based algorithm that is efficient in MDPs with a natural metric on the state-action space, enabling the extension of efficient model-free Q-learning algorithms to continuous state-action space. Unlike previous model-based RL algorithms for metric spaces, our algorithm does not necessitate access to a black-box planning oracle.",1
"Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (""why do we need them?"") and the naturalness (""when do they hold?"") of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",0
"Reinforcement learning (RL) heavily relies on batch mode value-function approximation methods. The effectiveness of these methods is often dependent on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the importance and likelihood of these assumptions have not been extensively explored in literature. This paper aims to address these questions by revisiting the assumptions and providing theoretical results to deepen our understanding of value-function approximation.",1
"Vision-based deep reinforcement learning (RL) typically obtains performance benefit by using high capacity and relatively large convolutional neural networks (CNN). However, a large network leads to higher inference costs (power, latency, silicon area, MAC count). Many inference optimizations have been developed for CNNs. Some optimization techniques offer theoretical efficiency, such as sparsity, but designing actual hardware to support them is difficult. On the other hand, distillation is a simple general-purpose optimization technique which is broadly applicable for transferring knowledge from a trained, high capacity teacher network to an untrained, low capacity student network. DQN distillation extended the original distillation idea to transfer information stored in a high performance, high capacity teacher Q-function trained via the Deep Q-Learning (DQN) algorithm. Our work adapts the DQN distillation work to the actor-critic Proximal Policy Optimization algorithm. PPO is simple to implement and has much higher performance than the seminal DQN algorithm. We show that a distilled PPO student can attain far higher performance compared to a DQN teacher. We also show that a low capacity distilled student is generally able to outperform a low capacity agent that directly trains in the environment. Finally, we show that distillation, followed by ""fine-tuning"" in the environment, enables the distilled PPO student to achieve parity with teacher performance. In general, the lessons learned in this work should transfer to other modern actor-critic RL algorithms.",0
"Usually, vision-based deep reinforcement learning (RL) employs large convolutional neural networks (CNN) to achieve high performance. Nevertheless, this approach entails higher inference costs, such as power, latency, silicon area, and MAC count. Although several optimization techniques for CNNs have been developed, some of them, such as sparsity, are challenging to design into hardware. Conversely, distillation is a straightforward optimization technique that enables the transfer of knowledge from a trained, high capacity teacher network to an untrained, low capacity student network. DQN distillation is an extension of the original distillation concept that transfers information from a high-performance, high-capacity teacher Q-function trained via the Deep Q-Learning (DQN) algorithm. Our study adapts the DQN distillation approach to the actor-critic Proximal Policy Optimization (PPO) algorithm, which is easy to implement and outperforms the seminal DQN algorithm. Our findings show that a distilled PPO student can achieve significantly higher performance than a DQN teacher. Furthermore, we demonstrate that a low-capacity distilled student can generally outperform a low-capacity agent that directly trains in the environment. Finally, we reveal that distillation followed by ""fine-tuning"" in the environment enables the distilled PPO student to achieve parity with the teacher's performance. In general, the lessons learned in this study could apply to other modern actor-critic RL algorithms.",1
"As an efficient and scalable graph neural network, GraphSAGE has enabled an inductive capability for inferring unseen nodes or graphs by aggregating subsampled local neighborhoods and by learning in a mini-batch gradient descent fashion. The neighborhood sampling used in GraphSAGE is effective in order to improve computing and memory efficiency when inferring a batch of target nodes with diverse degrees in parallel. Despite this advantage, the default uniform sampling suffers from high variance in training and inference, leading to sub-optimum accuracy. We propose a new data-driven sampling approach to reason about the real-valued importance of a neighborhood by a non-linear regressor, and to use the value as a criterion for subsampling neighborhoods. The regressor is learned using a value-based reinforcement learning. The implied importance for each combination of vertex and neighborhood is inductively extracted from the negative classification loss output of GraphSAGE. As a result, in an inductive node classification benchmark using three datasets, our method enhanced the baseline using the uniform sampling, outperforming recent variants of a graph neural network in accuracy.",0
"GraphSAGE is a graph neural network that is both efficient and scalable. It allows for inductive inference of unseen nodes or graphs by aggregating subsampled local neighborhoods and learning in mini-batch gradient descent. The neighborhood sampling used in GraphSAGE is effective for improving computing and memory efficiency when inferring a batch of target nodes with diverse degrees in parallel. Despite this advantage, the default uniform sampling suffers from high variance in training and inference, resulting in sub-optimal accuracy. Our proposed solution is a new data-driven sampling approach that uses a non-linear regressor to reason about the real-valued importance of a neighborhood and then subsamples based on this value. The regressor is trained using value-based reinforcement learning. The importance for each combination of vertex and neighborhood is inductively extracted from the negative classification loss output of GraphSAGE. As a result, our method outperformed recent variants of a graph neural network in accuracy in an inductive node classification benchmark using three datasets, improving upon the baseline that used uniform sampling.",1
"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",0
"Reinforcement learning (RL) has demonstrated its value in various artificial environments and is starting to exhibit some accomplishments in real-life situations. However, many of the advancements in RL research are difficult to apply in practical systems because they rely on assumptions that are rarely met. To address this issue, we identify nine unique challenges that need to be tackled to facilitate the implementation of RL in real-world problems. For each challenge, we provide a definition, review some relevant approaches from the literature, and establish metrics for evaluating its effectiveness. An approach that overcomes all nine challenges would be suitable for a broad range of real-world issues. In addition, we introduce an example domain that has been adapted to present these challenges as a platform for practical RL research.",1
"We present RL-GAN-Net, where a reinforcement learning (RL) agent provides fast and robust control of a generative adversarial network (GAN). Our framework is applied to point cloud shape completion that converts noisy, partial point cloud data into a high-fidelity completed shape by controlling the GAN. While a GAN is unstable and hard to train, we circumvent the problem by (1) training the GAN on the latent space representation whose dimension is reduced compared to the raw point cloud input and (2) using an RL agent to find the correct input to the GAN to generate the latent space representation of the shape that best fits the current input of incomplete point cloud. The suggested pipeline robustly completes point cloud with large missing regions. To the best of our knowledge, this is the first attempt to train an RL agent to control the GAN, which effectively learns the highly nonlinear mapping from the input noise of the GAN to the latent space of point cloud. The RL agent replaces the need for complex optimization and consequently makes our technique real time. Additionally, we demonstrate that our pipelines can be used to enhance the classification accuracy of point cloud with missing data.",0
"Our article introduces RL-GAN-Net, a framework that employs a reinforcement learning (RL) agent to effectively control a generative adversarial network (GAN) for quick and stable point cloud shape completion. By converting noisy, partial point cloud data into a high-quality, finished shape, the GAN is able to overcome its inherent instability and training difficulties. We achieve this by training the GAN on a reduced-dimension latent space representation of the raw point cloud input and using the RL agent to identify the optimal input for the GAN to generate the correct latent space representation for the current incomplete point cloud input. The proposed pipeline efficiently addresses point clouds with significant missing regions. Our approach is novel in that it is the first to employ an RL agent to control the GAN, thereby eliminating the need for complicated optimization and enabling real-time operation. Furthermore, we demonstrate that our framework can improve the classification accuracy of point clouds with missing data.",1
"Due to burdensome data requirements, learning from demonstration often falls short of its promise to allow users to quickly and naturally program robots. Demonstrations are inherently ambiguous and incomplete, making correct generalization to unseen situations difficult without a large number of demonstrations in varying conditions. By contrast, humans are often able to learn complex tasks from a single demonstration (typically observations without action labels) by leveraging context learned over a lifetime. Inspired by this capability, our goal is to enable robots to perform one-shot learning of multi-step tasks from observation by leveraging auxiliary video data as context. Our primary contribution is a novel system that achieves this goal by: (1) using a single user-segmented demonstration to define the primitive actions that comprise a task, (2) localizing additional examples of these actions in unsegmented auxiliary videos via a metalearning-based approach, (3) using these additional examples to learn a reward function for each action, and (4) performing reinforcement learning on top of the inferred reward functions to learn action policies that can be combined to accomplish the task. We empirically demonstrate that a robot can learn multi-step tasks more effectively when provided auxiliary video, and that performance greatly improves when localizing individual actions, compared to learning from unsegmented videos.",0
"Learning from demonstration often fails to live up to its potential of allowing users to easily and intuitively program robots due to the demanding data requirements. Demonstrations are inherently ambiguous and incomplete, making it challenging to correctly generalize to unforeseen circumstances without a significant number of demonstrations in varying conditions. On the other hand, humans can often learn complicated tasks from a single demonstration by utilizing context acquired over their lifetime. Our aim is to enable robots to learn multi-step tasks from observation using auxiliary video data as context. Our innovative system achieves this by: (1) using a single user-segmented demonstration to define primitive actions, (2) localizing additional examples of these actions in unsegmented auxiliary videos, (3) learning a reward function for each action using these additional examples, and (4) using reinforcement learning to learn action policies that can be combined to accomplish the task. We demonstrate through experiments that robots learn multi-step tasks more effectively when provided with auxiliary video, and that performance significantly improves when localizing individual actions compared to learning from unsegmented videos.",1
"By the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature that does not need too much cooperation and can significantly improve the user's experience at the same time. Face recognition is one of the main functions of electronic equipment propaganda. Hence it's virtually worth researching in computer vision. Previous work in this field has focused on two directions: converting loss function to improve recognition accuracy in traditional deep convolution neural networks (Resnet); combining the latest loss function with the lightweight system (MobileNet) to reduce network size at the minimal expense of accuracy. But none of these has changed the network structure. With the development of AutoML, neural architecture search (NAS) has shown excellent performance in the benchmark of image classification. In this paper, we integrate NAS technology into face recognition to customize a more suitable network. We quote the framework of neural architecture search which trains child and controller network alternately. At the same time, we mutate NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize policy gradient algorithm to search the architecture automatically with the most classical cross-entropy loss. The network architectures we searched out have got state-of-the-art accuracy in the large-scale face dataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with relatively small network size. To the best of our knowledge, this proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.",0
"Biometric technology has brought convenience to user authentication, surpassing traditional password and mode unlocking due to the widespread popularity of electronic devices. Among various biological characteristics, the face is a universal and irreplaceable feature that requires minimal cooperation and enhances the user's experience. Face recognition is a fundamental function of electronic devices, making it a worthwhile area of research in computer vision. Previous research has focused on improving recognition accuracy using traditional deep convolution neural networks or reducing network size with the latest loss function and lightweight systems. However, none of these approaches have altered the network structure. Neural architecture search (NAS) has shown excellent performance in image classification benchmarks, and this paper integrates NAS technology into face recognition to create a more suitable network. The proposed method trains child and controller networks alternately and mutates NAS by incorporating evaluation latency into reinforcement learning rewards. The policy gradient algorithm searches the architecture automatically with the classical cross-entropy loss. The searched network architectures achieve state-of-the-art accuracy in large-scale face datasets, with a top-1 accuracy of 98.77% in MS-Celeb-1M and 99.89% in LFW, despite having a relatively small network size. This proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.",1
"Supervised learning is widely used in training autonomous driving vehicle. However, it is trained with large amount of supervised labeled data. Reinforcement learning can be trained without abundant labeled data, but we cannot train it in reality because it would involve many unpredictable accidents. Nevertheless, training an agent with good performance in virtual environment is relatively much easier. Because of the huge difference between virtual and real, how to fill the gap between virtual and real is challenging. In this paper, we proposed a novel framework of reinforcement learning with image semantic segmentation network to make the whole model adaptable to reality. The agent is trained in TORCS, a car racing simulator.",0
"The training of autonomous driving vehicles commonly involves supervised learning, which necessitates a vast amount of labeled data. On the other hand, reinforcement learning does not require abundant labeled data, but its usage in reality is not feasible due to the potential for unpredictable accidents. However, it is more straightforward to train an agent to perform well in a virtual environment. Despite the significant contrast between virtual and real, bridging the gap between the two is a complex task. In this study, we present a new framework for reinforcement learning that incorporates an image semantic segmentation network to enhance the model's adaptability to reality. The agent is trained in TORCS, a car racing simulator.",1
"Rather than proposing a new method, this paper investigates an issue present in existing learning algorithms. We study the learning dynamics of reinforcement learning (RL), specifically a characteristic coupling between learning and data generation that arises because RL agents control their future data distribution. In the presence of function approximation, this coupling can lead to a problematic type of 'ray interference', characterized by learning dynamics that sequentially traverse a number of performance plateaus, effectively constraining the agent to learn one thing at a time even when learning in parallel is better. We establish the conditions under which ray interference occurs, show its relation to saddle points and obtain the exact learning dynamics in a restricted setting. We characterize a number of its properties and discuss possible remedies.",0
"Instead of introducing a new approach, this article examines a challenge that exists in current learning algorithms. Specifically, we explore the learning dynamics of reinforcement learning (RL) and the interplay between learning and data generation, which is unique to RL since agents can control their future data distribution. This coupling can result in a problematic phenomenon called 'ray interference' when function approximation is present. Ray interference causes learning dynamics to move sequentially through several performance plateaus, which restricts the agent to learn one thing at a time, even when parallel learning would be more effective. We identify the conditions that lead to ray interference, explain its relationship with saddle points, and provide the exact learning dynamics in a limited context. We describe several of its attributes and suggest potential solutions.",1
"We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning (Ashok et al., 2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training. Code is publicly available here: https://github.com/Friedrich1006/ESNAC .",0
"Our proposal involves a technique for progressively acquiring knowledge of an embedding space across the network architecture domain. This facilitates the careful selection of architectures for assessment during the compressed architecture search. Utilizing Bayesian Optimization (BO) with a kernel function over the proposed embedding space, we search for a compressed network architecture given a teacher network. Our search algorithm significantly outperforms various baseline methods, including random search and reinforcement learning (Ashok et al., 2018). The compressed architectures discovered by our method even surpass the state-of-the-art manually-designed compact architecture, ShuffleNet (Zhang et al., 2018). Additionally, we demonstrate that the learned embedding space can be transferred to new settings for architecture search without any training, such as a larger teacher network or a teacher network in a different architecture family. Code is available publicly at https://github.com/Friedrich1006/ESNAC.",1
"Learning to take actions based on observations is a core requirement for artificial agents to be able to be successful and robust at their task. Reinforcement Learning (RL) is a well-known technique for learning such policies. However, current RL algorithms often have to deal with reward shaping, have difficulties generalizing to other environments and are most often sample inefficient. In this paper, we explore active inference and the free energy principle, a normative theory from neuroscience that explains how self-organizing biological systems operate by maintaining a model of the world and casting action selection as an inference problem. We apply this concept to a typical problem known to the RL community, the mountain car problem, and show how active inference encompasses both RL and learning from demonstrations.",0
"To achieve success and resilience in their tasks, artificial agents must learn to act based on their observations. Reinforcement Learning (RL) is a commonly used approach for acquiring such policies. However, existing RL algorithms face issues such as reward shaping, limited generalization to new environments, and low sample efficiency. This study explores the free energy principle and active inference, a neuroscience-based theory that explains how self-organizing biological systems function by maintaining a model of the world and treating action selection as an inference matter. We apply this concept to the mountain car problem, a well-known challenge in the RL community, and demonstrate how active inference integrates both RL and learning from demonstrations.",1
"Opioids are the preferred medications for the treatment of pain in the intensive care unit. While undertreatment leads to unrelieved pain and poor clinical outcomes, excessive use of opioids puts patients at risk of experiencing multiple adverse effects. In this work, we present a sequential decision making framework for opioid dosing based on deep reinforcement learning. It provides real-time clinically interpretable dosing recommendations, personalized according to each patient's evolving pain and physiological condition. We focus on morphine, one of the most commonly prescribed opioids. To train and evaluate the model, we used retrospective data from the publicly available MIMIC-3 database. Our results demonstrate that reinforcement learning may be used to aid decision making in the intensive care setting by providing personalized pain management interventions.",0
"The intensive care unit prefers opioids as the go-to medication for pain relief. However, the use of opioids must be monitored as underutilization may lead to untreated pain and poor patient outcomes, while overuse can cause multiple adverse effects. To address this, we developed a deep reinforcement learning-based framework for opioid dosing, which offers real-time and clinically relevant dosing recommendations that are personalized to the evolving pain and physiological condition of each patient. Our focus is on morphine, a commonly prescribed opioid, and we utilized retrospective data from the publicly available MIMIC-3 database to train and evaluate the model. Our findings show that personalized pain management interventions can be achieved using reinforcement learning and can aid decision-making in the intensive care setting.",1
"Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study. To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.",0
"Various applications have widely utilized machine learning, some of which involve training with data that requires privacy protection. Although a few data breaches have been examined, such as credit card information in natural language data and identities from facial datasets, most studies have concentrated on supervised learning models. With the deployment of deep reinforcement learning (DRL) in actual systems, including indoor robot navigation, there is a need for a thorough investigation of whether trained DRL policies can leak sensitive information. To address this issue, we propose two primary methods: genetic algorithm-based environment dynamics search and shadow policy-based candidate inference. We perform extensive experiments to demonstrate privacy vulnerabilities in DRL under different circumstances. We employ the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The algorithm successfully infers most of the floor plans, with an average recovery rate of 95.83% using policy gradient-trained agents. Additionally, we accurately recover the robot configuration in continuous control environments and an autonomous driving simulator. This research is the first to investigate privacy leakage in DRL settings, demonstrating that DRL-based agents can potentially leak sensitive information from trained policies.",1
"This paper presents a novel approach to synthesize automatically age-progressed facial images in video sequences using Deep Reinforcement Learning. The proposed method models facial structures and the longitudinal face-aging process of given subjects coherently across video frames. The approach is optimized using a long-term reward, Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network. Unlike previous age-progression methods that are only able to synthesize an aged likeness of a face from a single input image, the proposed approach is capable of age-progressing facial likenesses in videos with consistently synthesized facial features across frames. In addition, the deep reinforcement learning method guarantees preservation of the visual identity of input faces after age-progression. Results on videos of our new collected aging face AGFW-v2 database demonstrate the advantages of the proposed solution in terms of both quality of age-progressed faces, temporal smoothness, and cross-age face verification.",0
"Using Deep Reinforcement Learning, this paper introduces a new method for generating age-progressed facial images in video sequences automatically. By modeling facial structures and the aging process of subjects in a coherent manner across video frames, the proposed approach optimizes a Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network using long-term rewards. Unlike previous methods that could only synthesize aged facial images from a single input image, this approach can consistently synthesize facial features across frames in videos. Furthermore, it ensures the preservation of visual identity after age-progression. Results from our new aging face AGFW-v2 database show the proposed solution's benefits in terms of quality, temporal smoothness, and cross-age face verification.",1
"In this paper, we point out a fundamental property of the objective in reinforcement learning, with which we can reformulate the policy gradient objective into a perceptron-like loss function, removing the need to distinguish between on and off policy training. Namely, we posit that it is sufficient to only update a policy $\pi$ for cases that satisfy the condition $A(\frac{\pi}{\mu}-1)\leq0$, where $A$ is the advantage, and $\mu$ is another policy. Furthermore, we show via theoretic derivation that a perceptron-like loss function matches the clipped surrogate objective for PPO. With our new formulation, the policies $\pi$ and $\mu$ can be arbitrarily apart in theory, effectively enabling off-policy training. To examine our derivations, we can combine the on-policy PPO clipped surrogate (which we show to be equivalent with one instance of the new reformation) with the off-policy IMPALA method. We first verify the combined method on the OpenAI Gym pendulum toy problem. Next, we use our method to train a quadrotor position controller in a simulator. Our trained policy is efficient and lightweight enough to perform in a low cost micro-controller at a minimum update rate of 500 Hz. For the quadrotor, we show two experiments to verify our method and demonstrate performance: 1) hovering at a fixed position, and 2) tracking along a specific trajectory. In preliminary trials, we are also able to apply the method to a real-world quadrotor.",0
"The objective of this research is to present a crucial characteristic of reinforcement learning. By doing so, we can modify the policy gradient objective and convert it into a perceptron-like loss function, eliminating the need for distinguishing between on and off policy training. Our hypothesis suggests that it is necessary to update the policy $\pi$ only for cases that fulfill the condition $A(\frac{\pi}{\mu}-1)\leq0$, where $A$ represents the advantage, and $\mu$ is another policy. Additionally, our theoretical derivation demonstrates that the perceptron-like loss function corresponds to the clipped surrogate objective for PPO. With this new formulation, policies $\pi$ and $\mu$ can be distant from each other, which facilitates off-policy training. We evaluate our theory by combining the on-policy PPO clipped surrogate with the off-policy IMPALA method and implementing the method on the OpenAI Gym pendulum toy problem and a quadrotor position controller in a simulator. Our trained policy is efficient and lightweight enough to perform on a low-cost micro-controller at a minimum update rate of 500 Hz. Furthermore, we demonstrate two experiments to verify our method for the quadrotor: hovering at a fixed position and tracking along a specific trajectory. In preliminary trials, we were also able to test the method on a real-world quadrotor.",1
"Model-free learning for multi-agent stochastic games is an active area of research. Existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings. Here, we develop a new data efficient Deep-Q-learning methodology for model-free learning of Nash equilibria for general-sum stochastic games. The algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. The expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs. We study symmetry properties of the algorithm stemming from label-invariant stochastic games and as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.",0
"The study of model-free learning for multi-agent stochastic games is an active area of research. However, current reinforcement learning algorithms are often limited to zero-sum games and can only be used in simplified settings or small state-action spaces. In this study, we introduce a new Deep-Q-learning methodology that efficiently learns Nash equilibria for general-sum stochastic games without the need to experience all state-action pairs. Our algorithm is based on a local linear-quadratic expansion of the game, which allows for analytically solvable optimal actions. We use deep neural networks to parameterize the expansion and provide the algorithm with enough flexibility to learn the environment. Additionally, we investigate symmetry properties of the algorithm, which are derived from label-invariant stochastic games. As a proof of concept, we apply our algorithm to learn optimal trading strategies in competitive electronic markets.",1
"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",0
"In this paper, the issue of scalability in architecture search is addressed through a differentiable approach. Rather than traditional methods that involve the use of evolution or reinforcement learning on a non-differentiable and discrete search space, our technique involves a continuous relaxation of the architecture representation. This allows for efficient architecture search using gradient descent. Our algorithm has been extensively tested on various datasets, including CIFAR-10, ImageNet, Penn Treebank, and WikiText-2, and has proven to be highly effective in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling. Furthermore, our method is significantly faster than current non-differentiable techniques. We have made our implementation publicly available to encourage further research on efficient architecture search algorithms.",1
"Decomposition methods have been proposed to approximate solutions to large sequential decision making problems. In contexts where an agent interacts with multiple entities, utility decomposition can be used to separate the global objective into local tasks considering each individual entity independently. An arbitrator is then responsible for combining the individual utilities and selecting an action in real time to solve the global problem. Although these techniques can perform well empirically, they rely on strong assumptions of independence between the local tasks and sacrifice the optimality of the global solution. This paper proposes an approach that improves upon such approximate solutions by learning a correction term represented by a neural network. We demonstrate this approach on a fisheries management problem where multiple boats must coordinate to maximize their catch over time as well as on a pedestrian avoidance problem for autonomous driving. In each problem, decomposition methods can scale to multiple boats or pedestrians by using strategies involving one entity. We verify empirically that the proposed correction method significantly improves the decomposition method and outperforms a policy trained on the full scale problem without utility decomposition.",0
"To tackle large sequential decision making problems involving multiple entities, decomposition methods have been proposed to approximate solutions. Utility decomposition is utilized in such scenarios, which helps divide the global objective into local tasks, considering each entity individually. An arbitrator then combines the individual utilities and chooses an action in real time to solve the global problem. While these methods perform well, they rely on the assumption of independence between local tasks, which might compromise the optimality of the global solution. This paper introduces a novel approach, where a neural network learns a correction term to improve the accuracy of the approximation. The approach is tested on two different problems: fisheries management, where multiple boats must coordinate to maximize their catch over time, and pedestrian avoidance, a problem related to autonomous driving. The proposed correction method significantly improves the decomposition method and outperforms a policy trained on the full-scale problem without utility decomposition.",1
"Abstracting complex 3D shapes with parsimonious part-based representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.",0
"The goal of computer vision has long been to abstract complex 3D shapes into parsimonious part-based representations. This paper proposes a solution to this problem that utilizes superquadrics as atomic elements rather than the traditional 3D cuboid representation. Our research shows that superquadrics are more expressive and easier to learn than cuboid representations, and we have developed an analytical solution to the Chamfer loss that eliminates the need for computationally expensive reinforcement learning or iterative prediction. Our model can learn to parse 3D objects into consistent superquadric representations without supervision, and it has proven its flexibility in capturing fine details and complex poses that cannot be modeled with cuboids. We have demonstrated this through our experiments on various ShapeNet categories and the SURREAL human body dataset.",1
"Power system emergency control is generally regarded as the last safety net for grid security and resiliency. Existing emergency control schemes are usually designed off-line based on either the conceived ""worst"" case scenario or a few typical operation scenarios. These schemes are facing significant adaptiveness and robustness issues as increasing uncertainties and variations occur in modern electrical grids. To address these challenges, for the first time, this paper developed novel adaptive emergency control schemes using deep reinforcement learning (DRL), by leveraging the high-dimensional feature extraction and non-linear generalization capabilities of DRL for complex power systems. Furthermore, an open-source platform named RLGC has been designed for the first time to assist the development and benchmarking of DRL algorithms for power system control. Details of the platform and DRL-based emergency control schemes for generator dynamic braking and under-voltage load shedding are presented. Extensive case studies performed in both two-area four-machine system and IEEE 39-Bus system have demonstrated the excellent performance and robustness of the proposed schemes.",0
"The last resort for ensuring the security and resilience of the power grid is commonly known as power system emergency control. Currently, emergency control strategies are typically created offline and are based on either a few typical operation scenarios or the perceived worst-case scenario. However, these schemes face significant adaptiveness and robustness challenges due to the increased uncertainties and variations present in modern electrical grids. To tackle these issues, this paper introduces innovative adaptive emergency control schemes that utilize deep reinforcement learning (DRL). DRL's high-dimensional feature extraction and non-linear generalization capabilities are leveraged to address complex power systems. Additionally, an open-source platform called RLGC has been designed specifically for developing and benchmarking DRL algorithms for power system control. The paper provides detailed information on the platform and DRL-based emergency control schemes, including generator dynamic braking and under-voltage load shedding. The proposed schemes' superior performance and robustness were demonstrated through extensive case studies in both two-area four-machine systems and IEEE 39-Bus systems.",1
"How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. delivering a package)? We study this question by integrating a generic perceptual skill set (e.g. a distance estimator, an edge detector, etc.) within a reinforcement learning framework--see Figure 1. This skill set (hereafter mid-level perception) provides the policy with a more processed state of the world compared to raw images.   We find that using a mid-level perception confers significant advantages over training end-to-end from scratch (i.e. not leveraging priors) in navigation-oriented tasks. Agents are able to generalize to situations where the from-scratch approach fails and training becomes significantly more sample efficient. However, we show that realizing these gains requires careful selection of the mid-level perceptual skills. Therefore, we refine our findings into an efficient max-coverage feature set that can be adopted in lieu of raw images. We perform our study in completely separate buildings for training and testing and compare against visually blind baseline policies and state-of-the-art feature learning methods.",0
"Our study aims to determine the extent to which visual priors, such as the world being three-dimensional, aid in learning downstream motor tasks like package delivery. We achieve this by incorporating a set of generic perceptual skills, such as an edge detector and distance estimator, into a reinforcement learning framework. This set of skills, referred to as mid-level perception, provides the policy with a more refined view of the world than raw images. Our research shows that using mid-level perception offers significant benefits over training from scratch, particularly in navigation-related tasks, enabling agents to generalize to scenarios where the from-scratch approach fails and reducing the amount of training required. However, we also discovered that the selection of mid-level perceptual skills is crucial to realizing these advantages. As a result, we have developed an efficient max-coverage feature set that can be used instead of raw images. Our study was conducted in separate buildings for training and testing and compared against visually impaired baseline policies and state-of-the-art feature learning techniques.",1
"An important facet of reinforcement learning (RL) has to do with how the agent goes about exploring the environment. Traditional exploration strategies typically focus on efficiency and ignore safety. However, for practical applications, ensuring safety of the agent during exploration is crucial since performing an unsafe action or reaching an unsafe state could result in irreversible damage to the agent. The main challenge of safe exploration is that characterizing the unsafe states and actions is difficult for large continuous state or action spaces and unknown environments. In this paper, we propose a novel approach to incorporate estimations of safety to guide exploration and policy search in deep reinforcement learning. By using a cost function to capture trajectory-based safety, our key idea is to formulate the state-action value function of this safety cost as a candidate Lyapunov function and extend control-theoretic results to approximate its derivative using online Gaussian Process (GP) estimation. We show how to use these statistical models to guide the agent in unknown environments to obtain high-performance control policies with provable stability certificates.",0
"Exploration in reinforcement learning (RL) is a crucial aspect that requires attention. The traditional approach focuses on efficiency and ignores safety, which is essential for practical applications. Unsafe actions or reaching hazardous states can cause irreversible harm to the agent. However, determining unsafe states and actions is challenging in large continuous state or action spaces and unknown environments. In this study, we propose a new method to incorporate safety estimates to guide exploration and policy search in deep RL. Our approach involves using a cost function to capture trajectory-based safety, where we formulate the state-action value function of this safety cost as a candidate Lyapunov function. To approximate its derivative, we extend control-theoretic outcomes using online Gaussian Process (GP) estimation. We demonstrate how these statistical models can guide the agent in unknown environments to obtain high-performance control policies with provable stability certificates.",1
"In decision making problems for continuous state and action spaces, linear dynamical models are widely employed. Specifically, policies for stochastic linear systems subject to quadratic cost functions capture a large number of applications in reinforcement learning. Selected randomized policies have been studied in the literature recently that address the trade-off between identification and control. However, little is known about policies based on bootstrapping observed states and actions. In this work, we show that bootstrap-based policies achieve a square root scaling of regret with respect to time. We also obtain results on the accuracy of learning the model's dynamics. Corroborative numerical analysis that illustrates the technical results is also provided.",0
"Linear dynamical models are commonly utilized in solving decision-making problems with continuous state and action spaces. Stochastic linear systems with quadratic cost functions are especially applicable in reinforcement learning. Randomized policies have been explored in literature to balance identification and control trade-offs. However, policies based on bootstrapping observed states and actions have not been thoroughly investigated. Our research demonstrates that bootstrap-based policies result in a regret scaling of square root with respect to time. Additionally, we present findings on the accuracy of learning the model's dynamics, along with numerical analysis to support our results.",1
"Deep neural networks have become commonplace in the domain of reinforcement learning, but are often expensive in terms of the number of parameters needed. While compressing deep neural networks has of late assumed great importance to overcome this drawback, little work has been done to address this problem in the context of reinforcement learning agents. This work aims at making first steps towards model compression in an RL agent. In particular, we compress networks to drastically reduce the number of parameters in them (to sizes less than 3% of their original size), further facilitated by applying a global max pool after the final convolution layer, and propose using Actor-Mimic in the context of compression. Finally, we show that this global max-pool allows for weakly supervised object localization, improving the ability to identify the agent's points of focus.",0
"Reinforcement learning is a domain where deep neural networks are commonly used, but they require a large number of parameters which can be expensive. Although compressing these networks has recently become important, there is little research on this topic for reinforcement learning agents. This study aims to take the first steps towards compressing models in RL agents by significantly reducing the number of parameters (to less than 3% of their original size) through network compression and global max pooling. Additionally, Actor-Mimic is proposed for use in compression. Finally, the global max-pooling technique improves weakly supervised object localization, which enhances the agent's ability to focus.",1
"Boltzmann exploration is widely used in reinforcement learning to provide a trade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et al., 2017) it has been shown that pure Boltzmann exploration does not perform well from a regret perspective, even in the simplest setting of stochastic multi-armed bandit (MAB) problems. In this paper, we show that a simple modification to Boltzmann exploration, motivated by a variation of the standard doubling trick, achieves $O(K\log^{1+\alpha} T)$ regret for a stochastic MAB problem with $K$ arms, where $\alpha>0$ is a parameter of the algorithm. This improves on the result in (Cesa-Bianchi et al., 2017), where an algorithm inspired by the Gumbel-softmax trick achieves $O(K\log^2 T)$ regret. We also show that our algorithm achieves $O(\beta(G) \log^{1+\alpha} T)$ regret in stochastic MAB problems with graph-structured feedback, without knowledge of the graph structure, where $\beta(G)$ is the independence number of the feedback graph. Additionally, we present extensive experimental results on real datasets and applications for multi-armed bandits with both traditional bandit feedback and graph-structured feedback. In all cases, our algorithm performs as well or better than the state-of-the-art.",0
"Reinforcement learning often utilizes Boltzmann exploration to balance exploration and exploitation. However, recent research (Cesa-Bianchi et al., 2017) revealed that pure Boltzmann exploration falls short in terms of regret, even in basic stochastic multi-armed bandit (MAB) problems. This study proposes a modified Boltzmann exploration algorithm, inspired by a variation of the doubling trick, that achieves $O(K\log^{1+\alpha} T)$ regret for a MAB task with $K$ arms and $\alpha>0$. This outperforms the Gumbel-softmax trick in Cesa-Bianchi et al. (2017), which only achieves $O(K\log^2 T)$ regret. The study also demonstrates that the modified algorithm obtains $O(\beta(G) \log^{1+\alpha} T)$ regret in MAB problems with feedback graphs, without prior knowledge of the graph structure. The independence number of the feedback graph, $\beta(G)$, is used in this context. Furthermore, the study presents experimental findings on actual datasets and MAB applications with both traditional and graph-structured feedback, indicating that the modified algorithm's performance is on par with or exceeds that of the current state-of-the-art.",1
"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",0
"The optimization process of reinforcement learning agents is limited to the features stated in the reward function, disregarding anything not included. Therefore, it is crucial to specify not only what actions to take, but also what actions should be avoided. It is easy to overlook these preferences, as they are already satisfied within the environment. This led to the realization that when a robot operates in a human-inhabited environment, the state of the environment is already optimized for human preferences. Thus, we can use the implicit preference information from the state to fill in any missing information. To test this idea, we created an algorithm based on Maximum Causal Entropy IRL and evaluated it in a set of proof-of-concept environments. Our findings indicate that initial state information can be used to identify both unwanted side effects and preferred environmental organization. Our code is available at https://github.com/HumanCompatibleAI/rlsp.",1
"Within Reinforcement Learning, there is a growing collection of research which aims to express all of an agent's knowledge of the world through predictions about sensation, behaviour, and time. This work can be seen not only as a collection of architectural proposals, but also as the beginnings of a theory of machine knowledge in reinforcement learning. Recent work has expanded what can be expressed using predictions, and developed applications which use predictions to inform decision-making on a variety of synthetic and real-world problems. While promising, we here suggest that the notion of predictions as knowledge in reinforcement learning is as yet underdeveloped: some work explicitly refers to predictions as knowledge, what the requirements are for considering a prediction to be knowledge have yet to be well explored. This specification of the necessary and sufficient conditions of knowledge is important; even if claims about the nature of knowledge are left implicit in technical proposals, the underlying assumptions of such claims have consequences for the systems we design. These consequences manifest in both the way we choose to structure predictive knowledge architectures, and how we evaluate them. In this paper, we take a first step to formalizing predictive knowledge by discussing the relationship of predictive knowledge learning methods to existing theories of knowledge in epistemology. Specifically, we explore the relationships between Generalized Value Functions and epistemic notions of Justification and Truth.",0
"The field of Reinforcement Learning is producing an increasing amount of research aimed at expressing an agent's knowledge of the world through predictions about sensation, behavior, and time. This work not only proposes new architectures but also establishes a theory of machine knowledge in reinforcement learning. Recent studies have expanded the range of applications by using predictions to inform decision-making on both real-world and synthetic problems. However, the idea of predictions being knowledge is still in its early stages, and the requirements for considering a prediction to be knowledge have not been well explored. Therefore, it is essential to specify the necessary and sufficient conditions of knowledge because it affects the design and evaluation of predictive knowledge architectures. In this paper, we take the first step in formalizing predictive knowledge by examining the relationship between predictive knowledge learning methods and existing theories of knowledge in epistemology. We specifically explore the connections between Generalized Value Functions and epistemic concepts of Justification and Truth.",1
"We present a Reinforcement Learning (RL) methodology to bypass Google reCAPTCHA v3. We formulate the problem as a grid world where the agent learns how to move the mouse and click on the reCAPTCHA button to receive a high score. We study the performance of the agent when we vary the cell size of the grid world and show that the performance drops when the agent takes big steps toward the goal. Finally, we used a divide and conquer strategy to defeat the reCAPTCHA system for any grid resolution. Our proposed method achieves a success rate of 97.4% on a 100x100 grid and 96.7% on a 1000x1000 screen resolution.",0
"Our approach employs Reinforcement Learning (RL) to circumvent Google reCAPTCHA v3. We treat the problem as a grid world, where the agent is trained to navigate the mouse and click the reCAPTCHA button for a high score. By experimenting with different grid cell sizes, we observe that the agent's performance decreases with larger steps towards the objective. To overcome this, we employ a divide and conquer technique that enables us to defeat the reCAPTCHA system at any grid resolution. Our method attains a success rate of 97.4% for a 100x100 grid and 96.7% for a 1000x1000 screen resolution.",1
"Optimism about the poorly understood states and actions is the main driving force of exploration for many provably-efficient reinforcement learning algorithms. We propose optimism in the face of sensible value functions (OFVF)- a novel data-driven Bayesian algorithm to constructing Plausibility sets for MDPs to explore robustly minimizing the worst case exploration cost. The method computes policies with tighter optimistic estimates for exploration by introducing two new ideas. First, it is based on Bayesian posterior distributions rather than distribution-free bounds. Second, OFVF does not construct plausibility sets as simple confidence intervals. Confidence intervals as plausibility sets are a sufficient but not a necessary condition. OFVF uses the structure of the value function to optimize the location and shape of the plausibility set to guarantee upper bounds directly without necessarily enforcing the requirement for the set to be a confidence interval. OFVF proceeds in an episodic manner, where the duration of the episode is fixed and known. Our algorithm is inherently Bayesian and can leverage prior information. Our theoretical analysis shows the robustness of OFVF, and the empirical results demonstrate its practical promise.",0
"Many reinforcement learning algorithms rely on optimism regarding the unknown states and actions to drive their exploration efforts. To this end, we propose a new Bayesian algorithm called Optimism in the Face of Sensible Value Functions (OFVF) that constructs Plausibility sets for MDPs to enable robust exploration while minimizing worst-case exploration costs. OFVF introduces two key ideas to achieve tighter optimistic estimates for exploration. Firstly, it uses Bayesian posterior distributions instead of distribution-free bounds. Secondly, it does not create plausibility sets as simple confidence intervals, as this is not a necessary condition. Instead, OFVF optimizes the location and shape of the plausibility set based on the structure of the value function, thus guaranteeing upper bounds directly without necessarily enforcing the requirement for the set to be a confidence interval. OFVF operates in an episodic manner with a fixed, known episode duration, and can leverage prior information due to its Bayesian nature. Our theoretical analysis demonstrates the robustness of OFVF, and our empirical results show its practical promise.",1
"Efficient exploration is one of the key challenges for reinforcement learning (RL) algorithms. Most traditional sample efficiency bounds require strategic exploration. Recently many deep RL algorithms with simple heuristic exploration strategies that have few formal guarantees, achieve surprising success in many domains. These results pose an important question about understanding these exploration strategies such as $e$-greedy, as well as understanding what characterize the difficulty of exploration in MDPs. In this work we propose problem specific sample complexity bounds of $Q$ learning with random walk exploration that rely on several structural properties. We also link our theoretical results to some empirical benchmark domains, to illustrate if our bound gives polynomial sample complexity in these domains and how that is related with the empirical performance.",0
"Reinforcement learning (RL) algorithms face the challenge of efficient exploration, which has been traditionally addressed through strategic exploration and sample efficiency bounds. However, recent successes of deep RL algorithms with simple heuristic exploration strategies, such as $e$-greedy, raise questions about the nature of these strategies and the difficulty of exploration in Markov Decision Processes (MDPs). In this study, we introduce problem-specific sample complexity bounds for $Q$ learning with random walk exploration, based on various structural properties. We also analyze the empirical performance of these bounds in benchmark domains to determine their polynomial sample complexity and relationship with real-world situations.",1
"This paper addresses the problem of learning the optimal control policy for a nonlinear stochastic dynamical system with continuous state space, continuous action space and unknown dynamics. This class of problems are typically addressed in stochastic adaptive control and reinforcement learning literature using model-based and model-free approaches respectively. Both methods rely on solving a dynamic programming problem, either directly or indirectly, for finding the optimal closed loop control policy. The inherent `curse of dimensionality' associated with dynamic programming method makes these approaches also computationally difficult.   This paper proposes a novel decoupled data-based control (D2C) algorithm that addresses this problem using a decoupled, `open loop - closed loop', approach. First, an open-loop deterministic trajectory optimization problem is solved using a black-box simulation model of the dynamical system. Then, a closed loop control is developed around this open loop trajectory by linearization of the dynamics about this nominal trajectory. By virtue of linearization, a linear quadratic regulator based algorithm can be used for this closed loop control. We show that the performance of D2C algorithm is approximately optimal. Moreover, simulation performance suggests significant reduction in training time compared to other state of the art algorithms.",0
"The focus of this paper is on finding the optimal control policy for a nonlinear stochastic dynamical system with continuous state and action spaces and unknown dynamics. Traditionally, stochastic adaptive control and reinforcement learning literature has tackled this problem using model-based and model-free approaches, both of which involve solving a dynamic programming problem to determine the optimal closed loop control policy. However, these methods are computationally challenging due to the `curse of dimensionality' associated with dynamic programming. To overcome this, the paper proposes a decoupled data-based control (D2C) algorithm that takes an open loop-closed loop approach. The algorithm first solves an open-loop deterministic trajectory optimization problem using a black-box simulation model, followed by developing a closed loop control around this trajectory by linearizing the dynamics. This allows for the use of a linear quadratic regulator based algorithm for closed loop control, resulting in approximately optimal performance. Simulation results show that the D2C algorithm significantly reduces training time compared to existing state of the art algorithms.",1
"We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage-a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.",0
"The focus of our research is on generating responses in visual dialogues over multiple rounds, where the response is based on a conversation history that is grounded in visual cues. The current methods for this task follow a supervised learning approach, using a codec model to encode the visual triplet of image, Q&A history, and current question, before decoding the current answer based on ground-truth supervision. However, this approach neglects the impact of imperfect history on the conversation, leading to bias towards history rather than contextual reasoning. To address this, we present a new training paradigm called History Advantage Sequence Training (HAST), inspired by actor-critic policy gradient in reinforcement learning. HAST intentionally introduces incorrect answers into the history, creating an adverse critic to evaluate how historic errors affect the codec's future behavior. The proposed approach also includes a novel attention network called History-Aware Co-Attention Network (HACAN) to improve the codec's sensitivity to the conversation history. Experimental results on three benchmark datasets demonstrate that HAST outperforms current supervised methods consistently.",1
"Reinforcement Learning (RL) algorithms allow artificial agents to improve their action selections so as to increase rewarding experiences in their environments. Deep Reinforcement Learning algorithms require solving a nonconvex and nonlinear unconstrained optimization problem. Methods for solving the optimization problems in deep RL are restricted to the class of first-order algorithms, such as stochastic gradient descent (SGD). The major drawback of the SGD methods is that they have the undesirable effect of not escaping saddle points and their performance can be seriously obstructed by ill-conditioning. Furthermore, SGD methods require exhaustive trial and error to fine-tune many learning parameters. Using second derivative information can result in improved convergence properties, but computing the Hessian matrix for large-scale problems is not practical. Quasi-Newton methods require only first-order gradient information, like SGD, but they can construct a low rank approximation of the Hessian matrix and result in superlinear convergence. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular quasi-Newton methods that construct positive definite Hessian approximations. In this paper, we introduce an efficient optimization method, based on the limited memory BFGS quasi-Newton method using line search strategy -- as an alternative to SGD methods. Our method bridges the disparity between first order methods and second order methods by continuing to use gradient information to calculate a low-rank Hessian approximations. We provide formal convergence analysis as well as empirical results on a subset of the classic ATARI 2600 games. Our results show a robust convergence with preferred generalization characteristics, as well as fast training time and no need for the experience replaying mechanism.",0
"Artificial agents can enhance their action selections and increase rewarding experiences in their environments through Reinforcement Learning (RL) algorithms. However, Deep Reinforcement Learning algorithms face a nonconvex and nonlinear unconstrained optimization problem that requires first-order algorithms, such as stochastic gradient descent (SGD), to be used for optimization. Unfortunately, SGD methods have limitations such as being unable to escape saddle points and being obstructed by ill-conditioning. Additionally, they require exhaustive trial and error to fine-tune many learning parameters. Although second derivative information can improve convergence properties, computing the Hessian matrix for large-scale problems is not practical. Quasi-Newton methods, however, can construct low rank approximations of the Hessian matrix using only first-order gradient information, resulting in superlinear convergence. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one popular quasi-Newton method that constructs positive definite Hessian approximations. In this paper, we propose an alternative to SGD methods, an efficient optimization method based on the limited memory BFGS quasi-Newton method, using a line search strategy. Our method uses gradient information to calculate a low-rank Hessian approximation, bridging the gap between first-order and second-order methods. We provide formal convergence analysis and empirical results on a subset of classic ATARI 2600 games. Our results show that our method has robust convergence with preferred generalization characteristics, fast training time, and no need for the experience replaying mechanism.",1
"We present Simion Zoo, a Reinforcement Learning (RL) workbench that provides a complete set of tools to design, run, and analyze the results,both statistically and visually, of RL control applications. The main features that set apart Simion Zoo from similar software packages are its easy-to-use GUI, its support for distributed execution including deployment over graphics processing units (GPUs) , and the possibility to explore concurrently the RL metaparameter space, which is key to successful RL experimentation.",0
"Simion Zoo is a Reinforcement Learning (RL) workbench that offers a comprehensive range of instruments for creating, executing, and evaluating the outcomes of RL control applications, both statistically and visually. What distinguishes Simion Zoo from other software packages is its user-friendly graphical user interface (GUI), its ability to operate on distributed systems such as graphics processing units (GPUs), and the capacity to concurrently examine the RL metaparameter space, which is critical for successful RL experimentation.",1
"In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks such as machine translation, headline generation, text summarization, speech to text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder-decoder models produce competitive results, many researchers have proposed additional improvements over these sequence-to-sequence models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with sequence-to-sequence models that enable remembering long-term memories. We present some of the most recent frameworks that combine concepts from RL and deep neural networks and explain how these two areas could benefit from each other in solving complex seq2seq tasks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization.",0
"Sequence-to-sequence (seq2seq) models have become increasingly popular and are now widely used in tasks such as machine translation, headline generation, text summarization, speech to text conversion, and image caption generation. These models typically consist of a deep neural network with an encoder and a decoder, though researchers have proposed various improvements such as attention-based models, pointer-generation models, and self-attention models. Despite their usefulness, seq2seq models still face two common problems: exposure bias and inconsistency between train/test measurement. Recently, researchers have begun to address these problems by using reinforcement learning (RL) methods. In this survey, we explore how RL can be used to improve seq2seq models and provide a formulation that combines the strengths of both approaches. We examine recent frameworks that combine RL and deep neural networks and demonstrate how they can solve complex seq2seq tasks. Our work aims to provide insights into the challenges associated with existing approaches and how RL models can be used to overcome them. We also provide source code for implementing RL models in abstractive text summarization.",1
"When watching omnidirectional images (ODIs), subjects can access different viewports by moving their heads. Therefore, it is necessary to predict subjects' head fixations on ODIs. Inspired by generative adversarial imitation learning (GAIL), this paper proposes a novel approach to predict saliency of head fixations on ODIs, named SalGAIL. First, we establish a dataset for attention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset is large-scale, which contains the head fixations of 30 subjects viewing 600 ODIs. Next, we mine our AOI dataset and determine three findings: (1) The consistency of head fixations are consistent among subjects, and it grows alongside the increased subject number; (2) The head fixations exist with a front center bias (FCB); and (3) The magnitude of head movement is similar across subjects. According to these findings, our SalGAIL approach applies deep reinforcement learning (DRL) to predict the head fixations of one subject, in which GAIL learns the reward of DRL, rather than the traditional human-designed reward. Then, multi-stream DRL is developed to yield the head fixations of different subjects, and the saliency map of an ODI is generated via convoluting predicted head fixations. Finally, experiments validate the effectiveness of our approach in predicting saliency maps of ODIs, significantly better than 10 state-of-the-art approaches.",0
"To access different viewports while watching omnidirectional images (ODIs), viewers must move their heads, making it crucial to predict their head fixations. In this paper, we introduce a new method called SalGAIL, inspired by generative adversarial imitation learning (GAIL), to predict the saliency of head fixations on ODIs. Our approach involves creating a large-scale attention on ODIs (AOI) dataset, consisting of head fixations from 30 subjects viewing 600 ODIs. After analyzing the dataset, we discovered three key findings: (1) head fixations are consistent among subjects, (2) they exhibit a front center bias (FCB), and (3) the magnitude of head movement is similar across subjects. To predict the head fixations of one subject, we utilize deep reinforcement learning (DRL), in which GAIL learns the reward of DRL. Additionally, we develop multi-stream DRL to yield the head fixations of different subjects, and a saliency map of an ODI is generated via convoluting predicted head fixations. Our experiments show that SalGAIL is significantly more effective in predicting saliency maps of ODIs compared to 10 state-of-the-art approaches.",1
"In reinforcement learning (RL), temporal abstraction still remains as an important and unsolved problem. The options framework provided clues to temporal abstraction in the RL, and the option-critic architecture elegantly solved the two problems of finding options and learning RL agents in an end-to-end manner. However, it is necessary to examine whether the options learned through this method play a mutually exclusive role. In this paper, we propose a Hellinger distance regularizer, a method for disentangling options. In addition, we will shed light on various indicators from the statistical point of view to compare with the options learned through the existing option-critic architecture.",0
"Temporal abstraction remains a significant yet unresolved issue in reinforcement learning (RL). The options framework offers insights into temporal abstraction in RL, and the option-critic architecture successfully addresses the challenge of identifying options and training RL agents end-to-end. However, it is essential to investigate whether the options learned with this approach serve a mutually exclusive purpose. Thus, this paper introduces a Hellinger distance regularizer, which is a technique for disentangling options. Moreover, we will use several statistical indicators to compare the options learned with the existing option-critic architecture.",1
"Existing methods for image captioning are usually trained by cross entropy loss, which leads to exposure bias and the inconsistency between the optimizing function and evaluation metrics. Recently it has been shown that these two issues can be addressed by incorporating techniques from reinforcement learning, where one of the popular techniques is the advantage actor-critic algorithm that calculates per-token advantage by estimating state value with a parametrized estimator at the cost of introducing estimation bias. In this paper, we estimate state value without using a parametrized value estimator. With the properties of image captioning, namely, the deterministic state transition function and the sparse reward, state value is equivalent to its preceding state-action value, and we reformulate advantage function by simply replacing the former with the latter. Moreover, the reformulated advantage is extended to n-step, which can generally increase the absolute value of the mean of reformulated advantage while lowering variance. Then two kinds of rollout are adopted to estimate state-action value, which we call self-critical n-step training. Empirically we find that our method can obtain better performance compared to the state-of-the-art methods that use the sequence level advantage and parametrized estimator respectively on the widely used MSCOCO benchmark.",0
"Commonly used techniques for image captioning involve training through cross entropy loss, leading to exposure bias and inconsistencies between optimizing function and evaluation metrics. Recently, reinforcement learning has been introduced as a solution to these issues, with the advantage actor-critic algorithm being a popular choice. However, this method introduces estimation bias. In this study, we propose a method that estimates state value without using a parametrized value estimator. With the deterministic state transition function and sparse reward of image captioning, state value is equivalent to its preceding state-action value, allowing us to reformulate the advantage function. Additionally, we extend the reformulated advantage to n-step, which increases the mean of reformulated advantage while lowering variance. Our self-critical n-step training method outperforms state-of-the-art methods using sequence level advantage and parametrized estimator on the MSCOCO benchmark.",1
"Contextual policy search (CPS) is a class of multi-task reinforcement learning algorithms that is particularly useful for robotic applications. A recent state-of-the-art method is Contextual Covariance Matrix Adaptation Evolution Strategies (C-CMA-ES). It is based on the standard black-box optimization algorithm CMA-ES. There are two useful extensions of CMA-ES that we will transfer to C-CMA-ES and evaluate empirically: ACM-ES, which uses a comparison-based surrogate model, and aCMA-ES, which uses an active update of the covariance matrix. We will show that improvements with these methods can be impressive in terms of sample-efficiency, although this is not relevant any more for the robotic domain.",0
"Contextual policy search (CPS) is a type of multi-task reinforcement learning algorithms that finds practical application in robotics. A recently developed state-of-the-art approach is the Contextual Covariance Matrix Adaptation Evolution Strategies (C-CMA-ES), which builds on the CMA-ES optimization algorithm. We will investigate two beneficial modifications of CMA-ES, namely ACM-ES, which relies on a comparison-based surrogate model, and aCMA-ES, which employs an active update of the covariance matrix. Our empirical evaluation reveals that these methods can significantly enhance sample-efficiency, though this is no longer a relevant concern in the field of robotics.",1
"End-to-end deep reinforcement learning has enabled agents to learn with little preprocessing by humans. However, it is still difficult to learn stably and efficiently because the learning method usually uses a nonlinear function approximation. Neural Episodic Control (NEC), which has been proposed in order to improve sample efficiency, is able to learn stably by estimating action values using a non-parametric method. In this paper, we propose an architecture that incorporates random projection into NEC to train with more stability. In addition, we verify the effectiveness of our architecture by Atari's five games. The main idea is to reduce the number of parameters that have to learn by replacing neural networks with random projection in order to reduce dimensions while keeping the learning end-to-end.",0
"Agents can now learn with minimal human preprocessing thanks to end-to-end deep reinforcement learning. However, learning in a stable and efficient manner remains challenging due to the use of nonlinear function approximation. To improve sample efficiency, Neural Episodic Control (NEC) was introduced, utilizing non-parametric methods to estimate action values and ensure stable learning. In this study, we propose an architecture that integrates random projection into NEC for increased stability during training. Our approach aims to reduce the number of parameters to learn by replacing neural networks with random projection, which reduces dimensions while maintaining end-to-end learning. We validate our architecture on five Atari games to demonstrate its effectiveness.",1
"Reproducibility in reinforcement learning is challenging: uncontrolled stochasticity from many sources, such as the learning algorithm, the learned policy, and the environment itself have led researchers to report the performance of learned agents using aggregate metrics of performance over multiple random seeds for a single environment. Unfortunately, there are still pernicious sources of variability in reinforcement learning agents that make reporting common summary statistics an unsound metric for performance. Our experiments demonstrate the variability of common agents used in the popular OpenAI Baselines repository. We make the case for reporting post-training agent performance as a distribution, rather than a point estimate.",0
"It is difficult to achieve reproducibility in reinforcement learning due to various unpredictable sources of stochasticity, including the learning algorithm, the learned policy, and the environment. To address this issue, researchers have resorted to using aggregate performance metrics across multiple random seeds for a single environment. However, there are still underlying factors that cause variability in reinforcement learning agents, rendering common summary statistics unreliable for evaluating performance. Our research has shown the variability of commonly used agents in the OpenAI Baselines repository, emphasizing the need to report post-training agent performance as a distribution rather than a single value.",1
"Designing RNA molecules has garnered recent interest in medicine, synthetic biology, biotechnology and bioinformatics since many functional RNA molecules were shown to be involved in regulatory processes for transcription, epigenetics and translation. Since an RNA's function depends on its structural properties, the RNA Design problem is to find an RNA sequence which satisfies given structural constraints. Here, we propose a new algorithm for the RNA Design problem, dubbed LEARNA. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Methodologically, for what we believe to be the first time, we jointly optimize over a rich space of architectures for the policy network, the hyperparameters of the training procedure and the formulation of the decision process. Comprehensive empirical results on two widely-used RNA Design benchmarks, as well as a third one that we introduce, show that our approach achieves new state-of-the-art performance on the former while also being orders of magnitudes faster in reaching the previous state-of-the-art performance. In an ablation study, we analyze the importance of our method's different components.",0
"Recently, there has been growing interest in the field of medicine, synthetic biology, biotechnology, and bioinformatics regarding the design of RNA molecules. This is due to the discovery that many functional RNA molecules play a crucial role in regulatory processes such as transcription, epigenetics, and translation. As an RNA's function is dependent on its structural properties, the RNA Design problem aims to find an RNA sequence that fulfills specific structural constraints. Our newly proposed algorithm, LEARNA, tackles this problem by utilizing deep reinforcement learning to train a policy network that can design an entire RNA sequence in a sequential manner based on a specified target structure. Through meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Our method is unique in that it optimizes over a range of architectures for the policy network, hyperparameters of the training procedure, and decision-making formulation. We conducted comprehensive empirical studies on two widely-used RNA Design benchmarks, as well as a third one that we introduced, and found that our approach achieves new state-of-the-art performance on the former while being orders of magnitude faster than the previous state-of-the-art performance. Furthermore, in an ablation study, we analyzed the significance of our method's various components.",1
"Recent research on Software-Defined Networking (SDN) strongly promotes the adoption of distributed controller architectures. To achieve high network performance, designing a scheduling function (SF) to properly dispatch requests from each switch to suitable controllers becomes critical. However, existing literature tends to design the SF targeted at specific network settings. In this paper, a reinforcement-learning-based (RL) approach is proposed with the aim to automatically learn a general, effective, and efficient SF. In particular, a new dispatching system is introduced in which the SF is represented as a neural network that determines the priority of each controller. Based on the priorities, a controller is selected using our proposed probability selection scheme to balance the trade-off between exploration and exploitation during learning. In order to train a general SF, we first formulate the scheduling function design problem as an RL problem. Then a new training approach is developed based on a state-of-the-art deep RL algorithm. Our simulation results show that our RL approach can rapidly design (or learn) SFs with optimal performance. Apart from that, the trained SF can generalize well and outperforms commonly used scheduling heuristics under various network settings.",0
"Current research on Software-Defined Networking (SDN) strongly advocates for the implementation of distributed controller architectures. The scheduling function (SF) plays a crucial role in achieving optimal network performance by effectively dispatching requests from switches to appropriate controllers. However, the existing literature tends to design the SF for specific network configurations. This study proposes a reinforcement-learning-based (RL) approach to automatically acquire a general, effective, and efficient SF. A new dispatching system is introduced in which a neural network represents the SF and determines the priority of each controller. Our proposed probability selection scheme balances exploration and exploitation during learning to select a controller based on its priority. To train a general SF, we formulate the scheduling function design problem as an RL problem and develop a new training approach using a state-of-the-art deep RL algorithm. The simulation results indicate that our RL approach can swiftly design (or learn) SFs with optimal performance. Furthermore, the trained SF can generalize well and surpasses commonly used scheduling heuristics across various network settings.",1
"Larger networks generally have greater representational power at the cost of increased computational complexity. Sparsifying such networks has been an active area of research but has been generally limited to static regularization or dynamic approaches using reinforcement learning. We explore a mixture of experts (MoE) approach to deep dynamic routing, which activates certain experts in the network on a per-example basis. Our novel DeepMoE architecture increases the representational power of standard convolutional networks by adaptively sparsifying and recalibrating channel-wise features in each convolutional layer. We employ a multi-headed sparse gating network to determine the selection and scaling of channels for each input, leveraging exponential combinations of experts within a single convolutional network. Our proposed architecture is evaluated on four benchmark datasets and tasks, and we show that Deep-MoEs are able to achieve higher accuracy with lower computation than standard convolutional networks.",0
"Increased computational complexity is a tradeoff for greater representational power in larger networks. Though sparsifying networks has been researched, it has been limited to static regularization or dynamic reinforcement learning. Our approach involves using a mixture of experts (MoE) to dynamically route through the network, activating specific experts per-example. Our novel DeepMoE architecture adapts by sparsifying and recalibrating channel-wise features for each convolutional layer, increasing representational power. We use a multi-headed sparse gating network to select and scale channels for each input, utilizing exponential combinations of experts within a single convolutional network. Our architecture is evaluated on four benchmark datasets, and we demonstrate that Deep-MoEs achieve higher accuracy with lower computation than standard convolutional networks.",1
"A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves 'knowledge' from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other 'knowledge exchange' methods.",0
"Nowadays, there are numerous deep nets available for different tasks, making it harder to determine which one to use for a new task or to fine-tune a new model. To tackle this issue, we introduce knowledge flow in this paper. It involves transferring knowledge from multiple deep nets, known as teachers, to a new model called the student. The teachers and student can have different structures and be trained on distinct tasks and output spaces. After training using knowledge flow, the student becomes independent of the teachers. Our approach outperforms other knowledge exchange methods and fine-tuning on different supervised and reinforcement learning tasks.",1
"Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization. Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories. This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.",0
"High variance often causes instability in several Reinforcement Learning applications, particularly in domains with high dimensions. Machine learning commonly uses regularization to decrease variance, but it introduces some bias. While existing regularization techniques usually focus on perceptual regularization, reinforcement learning presents an opportunity to utilize temporal regularization by exploiting smoothness in value estimates over trajectories due to the Bellman equation's nature. This paper examines a category of methods for temporal regularization and formally characterizes the bias this technique induces using Markov chain concepts. We demonstrate the characteristics of temporal regularization using a series of simple discrete and continuous MDPs and show that even in high-dimensional Atari games, the technique provides improvement.",1
"Machine learning pipeline potentially consists of several stages of operations like data preprocessing, feature engineering and machine learning model training. Each operation has a set of hyper-parameters, which can become irrelevant for the pipeline when the operation is not selected. This gives rise to a hierarchical conditional hyper-parameter space. To optimize this mixed continuous and discrete conditional hierarchical hyper-parameter space, we propose an efficient pipeline search and configuration algorithm which combines the power of Reinforcement Learning and Bayesian Optimization. Empirical results show that our method performs favorably compared to state of the art methods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.",0
"The machine learning pipeline is composed of several stages, including data preprocessing, feature engineering, and machine learning model training. Each stage has its own set of hyper-parameters, which may not be relevant if the stage is not selected. This creates a hierarchical conditional hyper-parameter space. To efficiently optimize this mixed continuous and discrete conditional hierarchical hyper-parameter space, we suggest a pipeline search and configuration algorithm that combines Reinforcement Learning and Bayesian Optimization. Our method has been proven to perform well in comparison to other state-of-the-art methods such as Auto-sklearn, TPOT, Tree Parzen Window, and Random Search.",1
"Safe reinforcement learning has many variants and it is still an open research problem. Here, we focus on how to use action guidance by means of a non-expert demonstrator to avoid catastrophic events in a domain with sparse, delayed, and deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. This domain is very challenging for reinforcement learning (RL) --- past work has shown that model-free RL algorithms fail to achieve significant learning. In this paper, we shed light into the reasons behind this failure by exemplifying and analyzing the high rate of catastrophic events (i.e., suicides) that happen under random exploration in this domain. While model-free random exploration is typically futile, we propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with small number of rollouts, can be integrated to asynchronous distributed deep reinforcement learning methods. Compared to vanilla deep RL algorithms, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.",0
"The research problem of safe reinforcement learning has numerous variations and has yet to be fully resolved. This study focuses on utilizing non-expert guidance to prevent disastrous occurrences in a domain with minimal, delayed, and misleading rewards, specifically the Pommerman multi-agent benchmark. This domain poses a significant challenge for reinforcement learning, as earlier efforts in model-free RL have been unsuccessful in attaining noteworthy results. The study sheds light on the causes of these failures by examining the high frequency of catastrophic events that arise during random exploration. Rather than relying on futile model-free random exploration, the study proposes a novel approach that incorporates a non-expert simulated demonstrator, such as Monte Carlo tree search with a small number of rollouts, into asynchronous distributed deep reinforcement learning methods. Compared to plain deep RL algorithms, this approach leads to faster learning and improved convergence to better policies in a two-player mini version of the Pommerman game.",1
"Most approaches to visual scene analysis have emphasised parallel processing of the image elements. However, one area in which the sequential nature of vision is apparent, is that of segmenting multiple, potentially similar and partially occluded objects in a scene. In this work, we revisit the recurrent formulation of this challenging problem in the context of reinforcement learning. Motivated by the limitations of the global max-matching assignment of the ground-truth segments to the recurrent states, we develop an actor-critic approach in which the actor recurrently predicts one instance mask at a time and utilises the gradient from a concurrently trained critic network. We formulate the state, action, and the reward such as to let the critic model long-term effects of the current prediction and incorporate this information into the gradient signal. Furthermore, to enable effective exploration in the inherently high-dimensional action space of instance masks, we learn a compact representation using a conditional variational auto-encoder. We show that our actor-critic model consistently provides accuracy benefits over the recurrent baseline on standard instance segmentation benchmarks.",0
"The majority of visual scene analysis methods focus on processing image elements in parallel. However, when it comes to segmenting multiple objects that are partially hidden or similar, the sequential aspect of vision becomes apparent. This study re-examines the challenging task of segmenting such objects using reinforcement learning and a recurrent approach. The authors noticed limitations with assigning ground-truth segments to recurrent states and therefore developed an actor-critic method. The actor predicts one instance mask at a time and receives feedback from a concurrently trained critic network. To explore the high-dimensional action space of instance masks, the authors learned a compact representation using a conditional variational auto-encoder. The state, action, and reward were formulated to incorporate the critic's long-term effects on the prediction. The results show that the actor-critic model consistently produces higher accuracy than the recurrent baseline on standard instance segmentation benchmarks.",1
"We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural ""oracle"". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.",0
"Our research introduces Programmatically Interpretable Reinforcement Learning (PIRL), a reinforcement learning approach that produces understandable and verifiable agent policies. Unlike Deep Reinforcement Learning (DRL), which employs neural networks to represent policies, PIRL employs a high-level programming language specific to the domain. Programmatic policies are easier to interpret than neural networks and can be verified using symbolic methods. We propose a new technique called Neurally Directed Program Search (NDPS) to address the challenging issue of discovering a programmatic policy with maximum reward. NDPS uses DRL to learn a neural policy network, followed by a local search for programmatic policies that minimize the distance from the neural ""oracle."" We assess NDPS's performance by using it to teach a simulated car to drive in the TORCS car-racing environment. Our results demonstrate that NDPS can identify human-readable policies that meet significant performance criteria, and that PIRL policies have smoother trajectories and are more transferable to new environments than DRL policies.",1
"To optimize clinical outcomes, fertility clinics must strategically select which embryos to transfer. Common selection heuristics are formulas expressed in terms of the durations required to reach various developmental milestones, quantities historically annotated manually by experienced embryologists based on time-lapse EmbryoScope videos. We propose a new method for automatic embryo staging that exploits several sources of structure in this time-lapse data. First, noting that in each image the embryo occupies a small subregion, we jointly train a region proposal network with the downstream classifier to isolate the embryo. Notably, because we lack ground-truth bounding boxes, our we weakly supervise the region proposal network optimizing its parameters via reinforcement learning to improve the downstream classifier's loss. Moreover, noting that embryos reaching the blastocyst stage progress monotonically through earlier stages, we develop a dynamic-programming-based decoder that post-processes our predictions to select the most likely monotonic sequence of developmental stages. Our methods outperform vanilla residual networks and rival the best numbers in contemporary papers, as measured by both per-frame accuracy and transition prediction error, despite operating on smaller data than many.",0
"In order to improve the success of fertility treatments, it is crucial for clinics to carefully choose which embryos to transfer. Traditionally, experts have manually annotated embryos by observing their developmental milestones through time-lapse EmbryoScope videos. However, we propose a new approach that utilizes the structural patterns present in this data to automatically stage the embryos. Our method involves training a region proposal network and downstream classifier to isolate the embryo in each image. As we lack ground-truth bounding boxes, we weakly supervise the network using reinforcement learning to enhance the classifier's accuracy. Additionally, we have developed a dynamic-programming-based decoder that selects the most likely monotonic sequence of developmental stages, as embryos progress uniformly through these stages. Our approach outperforms vanilla residual networks and achieves comparable results to state-of-the-art methods, despite working with less data.",1
"Dense video captioning is an extremely challenging task since accurate and coherent description of events in a video requires holistic understanding of video contents as well as contextual reasoning of individual events. Most existing approaches handle this problem by first detecting event proposals from a video and then captioning on a subset of the proposals. As a result, the generated sentences are prone to be redundant or inconsistent since they fail to consider temporal dependency between events. To tackle this challenge, we propose a novel dense video captioning framework, which models temporal dependency across events in a video explicitly and leverages visual and linguistic context from prior events for coherent storytelling. This objective is achieved by 1) integrating an event sequence generation network to select a sequence of event proposals adaptively, and 2) feeding the sequence of event proposals to our sequential video captioning network, which is trained by reinforcement learning with two-level rewards at both event and episode levels for better context modeling. The proposed technique achieves outstanding performances on ActivityNet Captions dataset in most metrics.",0
"Writing accurate and coherent descriptions of events in a video is a difficult task that requires a comprehensive understanding of the video's contents and contextual reasoning of individual events. Most current methods for handling this problem involve detecting event proposals and captioning a subset of them. Unfortunately, this approach often leads to redundant or inconsistent sentences because it fails to consider the temporal dependency between events. To address this challenge, we have developed a novel dense video captioning framework that explicitly models temporal dependency across events and utilizes contextual information from prior events to create a coherent narrative. Our approach achieves this objective by integrating an event sequence generation network to select a sequence of event proposals adaptively and by feeding this sequence to our sequential video captioning network. Our network is trained using reinforcement learning with two-level rewards at both event and episode levels to improve context modeling. We have tested our technique on the ActivityNet Captions dataset and it has achieved outstanding performance in most metrics.",1
"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using a deep neural network as its function approximator and by learning directly from raw images. A drawback of using raw images is that deep RL must learn the state feature representation from the raw images in addition to learning a policy. As a result, deep RL can require a prohibitively large amount of training time and data to reach reasonable performance, making it difficult to use deep RL in real-world applications, especially when data is expensive. In this work, we speed up training by addressing half of what deep RL is trying to solve --- learning features. Our approach is to learn some of the important features by pre-training deep RL network's hidden layers via supervised learning using a small set of human demonstrations. We empirically evaluate our approach using deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600 games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training with human demonstrations in a supervised learning manner is better at discovering features relative to pre-training naively in DQN, and 2) initializing a deep RL network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of human demonstrations.",0
"By using a deep neural network as a function approximator and learning directly from raw images, deep reinforcement learning (deep RL) has excelled in accomplishing intricate sequential tasks. However, a disadvantage of using raw images is that deep RL must not only learn a policy but also learn the state feature representation from the raw images, which can necessitate a considerable amount of training time and data to achieve satisfactory performance. This can make it challenging to use deep RL in real-world applications, particularly when data is costly. Thus, in this study, our aim is to expedite training by concentrating on half of what deep RL seeks to resolve, i.e., learning features. To achieve this, we pre-train the hidden layers of deep RL networks through supervised learning, using a small set of human demonstrations to learn essential features. We evaluate this approach using the deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on Pong, Freeway, and Beamrider Atari 2600 games. Our results demonstrate that our approach is superior to pre-training naively in DQN, and initializing a deep RL network with a pre-trained model leads to a significant reduction in training time, even when pre-training from a few human demonstrations.",1
"We introduce Bayesian least-squares policy iteration (BLSPI), an off-policy, model-free, policy iteration algorithm that uses the Bayesian least-squares temporal-difference (BLSTD) learning algorithm to evaluate policies. An online variant of BLSPI has been also proposed, called randomised BLSPI (RBLSPI), that improves its policy based on an incomplete policy evaluation step. In online setting, the exploration-exploitation dilemma should be addressed as we try to discover the optimal policy by using samples collected by ourselves. RBLSPI exploits the advantage of BLSTD to quantify our uncertainty about the value function. Inspired by Thompson sampling, RBLSPI first samples a value function from a posterior distribution over value functions, and then selects actions based on the sampled value function. The effectiveness and the exploration abilities of RBLSPI are demonstrated experimentally in several environments.",0
"Bayesian least-squares policy iteration (BLSPI) is a policy iteration algorithm that evaluates policies using the Bayesian least-squares temporal-difference (BLSTD) learning algorithm. It is off-policy and model-free. Additionally, a variant called randomised BLSPI (RBLSPI) has been proposed for online use, which improves policy based on incomplete policy evaluation. In the online setting, the exploration-exploitation dilemma must be addressed to discover the optimal policy using self-collected samples. RBLSPI addresses this by using BLSTD to quantify uncertainty about the value function. It first samples a value function from a posterior distribution and then selects actions based on the sampled value function, inspired by Thompson sampling. Experimental results demonstrate the effectiveness and exploration abilities of RBLSPI in various environments.",1
"Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.",0
"The technique of model quantization is commonly used to speed up and compress deep neural network (DNN) inference. However, with the emergence of DNN hardware accelerators that support mixed precision (1-8 bits), finding the optimal bitwidth for each layer poses a significant challenge. This requires domain experts to explore a vast design space, taking into account various factors such as accuracy, latency, energy, and model size. Conventional quantization algorithms overlook the distinct hardware architectures and quantize all layers uniformly. To address this, we present the Hardware-Aware Automated Quantization (HAQ) framework that employs reinforcement learning to determine the quantization policy automatically while taking the hardware accelerator's feedback into account. Rather than relying on proxy signals, we use a hardware simulator to generate direct feedback signals such as latency and energy to the RL agent. Our framework is fully automated, and we can tailor the quantization policy for different neural network and hardware architectures. Compared to conventional methods, our framework significantly reduces latency and energy consumption with negligible accuracy loss. We also identify that the optimal quantization policies for different hardware architectures and resource constraints are dramatically different, providing insights for both neural network and hardware architecture design.",1
"Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).",0
"The aim of Vision-language navigation (VLN) is to direct a physical agent to execute natural language commands in real 3D environments. This study explores three major obstacles in VLN: cross-modal grounding, ill-defined feedback, and generalization difficulties. The authors propose an innovative Reinforced Cross-Modal Matching (RCM) method that uses reinforcement learning (RL) to facilitate both local and global cross-modal grounding. The RCM model significantly outperforms previous approaches by 10% on SPL and achieves a new state-of-the-art performance. Furthermore, the authors present a Self-Supervised Imitation Learning (SIL) technique to enhance policy generalizability by emulating past successful decisions. SIL approximates an improved, more effective policy that reduces the performance gap between familiar and unfamiliar environments from 30.7% to 11.7%.",1
"Attention models have had a significant positive impact on deep learning across a range of tasks. However previous attempts at integrating attention with reinforcement learning have failed to produce significant improvements. We propose the first combination of self attention and reinforcement learning that is capable of producing significant improvements, including new state of the art results in the Arcade Learning Environment. Unlike the selective attention models used in previous attempts, which constrain the attention via preconceived notions of importance, our implementation utilises the Markovian properties inherent in the state input. Our method produces a faithful visualisation of the policy, focusing on the behaviour of the agent. Our experiments demonstrate that the trained policies use multiple simultaneous foci of attention, and are able to modulate attention over time to deal with situations of partial observability.",0
"Although attention models have positively impacted deep learning in various tasks, previous efforts to integrate attention with reinforcement learning have been unsuccessful in generating significant enhancements. Our proposal introduces a novel combination of self-attention and reinforcement learning, which has resulted in substantial improvements, including new state-of-the-art outcomes in the Arcade Learning Environment. Unlike selective attention models that limit attention based on preconceived ideas of importance, our approach employs the intrinsic Markovian characteristics of the state input. The visualization of our method is a reliable representation of the policy, highlighting the agent's behavior. Our experiments demonstrate that the trained policies utilize multiple attentional foci simultaneously and can regulate attention over time to cope with partial observability situations.",1
"Policy gradient algorithms typically combine discounted future rewards with an estimated value function, to compute the direction and magnitude of parameter updates. However, for most Reinforcement Learning tasks, humans can provide additional insight to constrain the policy learning. We introduce a general method to incorporate multiple different feedback channels into a single policy gradient loss. In our formulation, the Multi-Preference Actor Critic (M-PAC), these different types of feedback are implemented as constraints on the policy. We use a Lagrangian relaxation to satisfy these constraints using gradient descent while learning a policy that maximizes rewards. Experiments in Atari and Pendulum verify that constraints are being respected and can accelerate the learning process.",0
"Usually, policy gradient algorithms use a combination of discounted future rewards and estimated value function to determine the direction and magnitude of parameter updates. However, when it comes to most Reinforcement Learning tasks, human input can provide further guidance to restrict policy learning. Our study proposes a universal technique to integrate various feedback channels into a single policy gradient loss. With our Multi-Preference Actor Critic (M-PAC) formulation, we employ these feedback types as constraints on the policy. We use a Lagrangian relaxation method to comply with these constraints while maximizing rewards through gradient descent. Our experiments on Atari and Pendulum demonstrate that our approach respects the constraints and accelerates the learning process.",1
"Wasserstein distances are increasingly used in a wide variety of applications in machine learning. Sliced Wasserstein distances form an important subclass which may be estimated efficiently through one-dimensional sorting operations. In this paper, we propose a new variant of sliced Wasserstein distance, study the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances and draw connections with stratified sampling, and evaluate our approaches experimentally in a range of large-scale experiments in generative modelling and reinforcement learning.",0
"The use of Wasserstein distances has been growing in machine learning applications. Among them, sliced Wasserstein distances are significant as they can be efficiently estimated through one-dimensional sorting operations. This paper introduces a new variation of sliced Wasserstein distance, explores the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances, and establishes links with stratified sampling. The proposed approaches are empirically evaluated in various large-scale experiments in generative modelling and reinforcement learning.",1
"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",0
"Reinforcement learning faces a significant obstacle of developing effective policies for tasks that have few rewards. To tackle this issue, we suggest that a successful exploration strategy should aim to locate decision states in the absence of useful reward signals. These states are positioned at crucial points in the state space, allowing the agent to move to new, uncharted territories. To discover decision states, we propose using previous experience to train a goal-conditioned policy with an information bottleneck. By analyzing where the model employs the goal state, we can identify decision states. This approach is effective even in settings where observations are incomplete, since the model learns sensory cues that correspond with potential subgoals. In new environments, the model can then identify fresh subgoals for further exploration, directing the agent through a succession of possible decision states and into new areas of the state space.",1
"Deep Reinforcement Learning (DRL) algorithms are known to be data inefficient. One reason is that a DRL agent learns both the feature and the policy tabula rasa. Integrating prior knowledge into DRL algorithms is one way to improve learning efficiency since it helps to build helpful representations. In this work, we consider incorporating human knowledge to accelerate the asynchronous advantage actor-critic (A3C) algorithm by pre-training a small amount of non-expert human demonstrations. We leverage the supervised autoencoder framework and propose a novel pre-training strategy that jointly trains a weighted supervised classification loss, an unsupervised reconstruction loss, and an expected return loss. The resulting pre-trained model learns more useful features compared to independently training in supervised or unsupervised fashion. Our pre-training method drastically improved the learning performance of the A3C agent in Atari games of Pong and MsPacman, exceeding the performance of the state-of-the-art algorithms at a much smaller number of game interactions. Our method is light-weight and easy to implement in a single machine. For reproducibility, our code is available at github.com/gabrieledcjr/DeepRL/tree/A3C-ALA2019",0
"DRL algorithms are known to lack efficiency due to their data inefficiency, which is caused by the agent learning both the feature and policy tabula rasa. To enhance the learning efficiency, prior knowledge can be integrated into DRL algorithms as it assists in building useful representations. This research aims to accelerate the asynchronous advantage actor-critic (A3C) algorithm by incorporating human knowledge through pre-training a small amount of non-expert human demonstrations. The researchers propose a pre-training strategy that jointly trains a weighted supervised classification loss, an unsupervised reconstruction loss, and an expected return loss using the supervised autoencoder framework. The resulting model learns more useful features compared to independent supervised or unsupervised training. The pre-training method significantly improves the learning performance of the A3C agent in Atari games of Pong and MsPacman, surpassing the state-of-the-art algorithms with fewer game interactions. The method is simple to implement on a single machine and is available for reproducibility on github.com/gabrieledcjr/DeepRL/tree/A3C-ALA2019.",1
"We propose a new automated digital painting framework, based on a painting agent trained through reinforcement learning. To synthesize an image, the agent selects a sequence of continuous-valued actions representing primitive painting strokes, which are accumulated on a digital canvas. Action selection is guided by a given reference image, which the agent attempts to replicate subject to the limitations of the action space and the agent's learned policy. The painting agent policy is determined using a variant of proximal policy optimization reinforcement learning. During training, our agent is presented with patches sampled from an ensemble of reference images. To accelerate training convergence, we adopt a curriculum learning strategy, whereby reference patches are sampled according to how challenging they are using the current policy. We experiment with differing loss functions, including pixel-wise and perceptual loss, which have consequent differing effects on the learned policy. We demonstrate that our painting agent can learn an effective policy with a high dimensional continuous action space comprising pen pressure, width, tilt, and color, for a variety of painting styles. Through a coarse-to-fine refinement process our agent can paint arbitrarily complex images in the desired style.",0
"Our proposed framework involves an automated digital painting approach that utilizes a painting agent that has been trained through reinforcement learning. The agent performs its task by selecting a series of continuous-valued actions that represent primitive painting strokes, and these actions are then accumulated on a digital canvas to create an image. To ensure that the image replicates the given reference image, the agent's action selection is guided by it, while accounting for the limitations of the action space and the learned policy. We employ a variant of proximal policy optimization reinforcement learning to determine the painting agent's policy. During the training phase, we present the agent with patches taken from a group of reference images. To quicken the training process, we adopt a curriculum learning strategy that samples reference patches based on their level of difficulty relative to the current policy. We experiment with different loss functions, including pixel-wise and perceptual loss, which affect the learned policy differently. Our painting agent can create images in various painting styles, using a high-dimensional continuous action space that comprises pen pressure, width, tilt, and color. Through a coarse-to-fine refinement process, the agent can generate complex images in the desired style.",1
"An important goal of research in Deep Reinforcement Learning in mobile robotics is to train agents capable of solving complex tasks, which require a high level of scene understanding and reasoning from an egocentric perspective. When trained from simulations, optimal environments should satisfy a currently unobtainable combination of high-fidelity photographic observations, massive amounts of different environment configurations and fast simulation speeds. In this paper we argue that research on training agents capable of complex reasoning can be simplified by decoupling from the requirement of high fidelity photographic observations. We present a suite of tasks requiring complex reasoning and exploration in continuous, partially observable 3D environments. The objective is to provide challenging scenarios and a robust baseline agent architecture that can be trained on mid-range consumer hardware in under 24h. Our scenarios combine two key advantages: (i) they are based on a simple but highly efficient 3D environment (ViZDoom) which allows high speed simulation (12000fps); (ii) the scenarios provide the user with a range of difficulty settings, in order to identify the limitations of current state of the art algorithms and network architectures. We aim to increase accessibility to the field of Deep-RL by providing baselines for challenging scenarios where new ideas can be iterated on quickly. We argue that the community should be able to address challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.",0
"The main objective of Deep Reinforcement Learning research in mobile robotics is to train agents that can solve complex tasks requiring a high level of scene understanding and reasoning from an egocentric perspective. However, optimal environments for training these agents from simulations require a combination of high-fidelity photographic observations, various environment configurations, and fast simulation speeds that are currently unobtainable. In this article, we propose a simplified approach to training agents capable of complex reasoning by eliminating the need for high fidelity photographic observations. We present a range of challenging tasks that require complex reasoning and exploration in continuous, partially observable 3D environments, using a simple but efficient 3D environment (ViZDoom) that allows for high-speed simulations (12000fps). Our objective is to provide a robust baseline agent architecture and challenging scenarios that can be trained on mid-range consumer hardware in under 24 hours, making Deep-RL research more accessible to the community. We believe that the community can overcome challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.",1
"We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study \citep{bellemare2017distributional}. In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cram{\'e}r distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using the similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexities of above three algorithms are linear w.r.t.\ the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.",0
"Our approach involves introducing a distributional version of gradient temporal-difference (TD) learning. Recent research has demonstrated the superiority of distributional reinforcement learning over the regular approach (Bellemare et al., 2017). In the context of policy evaluation, we have developed two fresh algorithms, namely distributional GTD2 and distributional TDC. These algorithms use the Cram{\'e}r distance on the distributional variation of the Bellman error objective function, combining the benefits of nonlinear gradient TD algorithms and distributional RL techniques. For control settings, we have proposed the distributional Greedy-GQ, utilizing a similar derivation. We have also shown that distributional GTD2 and TDC converge almost surely to a local optimal solution for general smooth function approximators, including neural networks that have been widely used to tackle real-world RL challenges. These algorithms have a linear computational complexity with respect to the number of parameters in the function approximator, making them efficient for implementation in neural networks.",1
"In this paper, we study a multi-step interactive recommendation problem, where the item recommended at current step may affect the quality of future recommendations. To address the problem, we develop a novel and effective approach, named CFRL, which seamlessly integrates the ideas of both collaborative filtering (CF) and reinforcement learning (RL). More specifically, we first model the recommender-user interactive recommendation problem as an agent-environment RL task, which is mathematically described by a Markov decision process (MDP). Further, to achieve collaborative recommendations for the entire user community, we propose a novel CF-based MDP by encoding the states of all users into a shared latent vector space. Finally, we propose an effective Q-network learning method to learn the agent's optimal policy based on the CF-based MDP. The capability of CFRL is demonstrated by comparing its performance against a variety of existing methods on real-world datasets.",0
"The focus of this paper is on a multi-step interactive recommendation problem that involves the potential impact of current recommendations on future ones. To tackle this issue, we introduce a new and efficient approach called CFRL, which amalgamates concepts from both collaborative filtering (CF) and reinforcement learning (RL). Firstly, we model the problem of recommender-user interaction as an agent-environment RL task, which is represented mathematically by a Markov decision process (MDP). Secondly, we propose a CF-based MDP that involves a shared latent vector space for all users to achieve collaborative recommendations. Finally, we suggest a Q-network learning method to acquire the optimal policy of the agent based on the CF-based MDP. We demonstrate the effectiveness of CFRL by comparing its performance with other existing methods on actual datasets.",1
"Recognition of defects in concrete infrastructure, especially in bridges, is a costly and time consuming crucial first step in the assessment of the structural integrity. Large variation in appearance of the concrete material, changing illumination and weather conditions, a variety of possible surface markings as well as the possibility for different types of defects to overlap, make it a challenging real-world task. In this work we introduce the novel COncrete DEfect BRidge IMage dataset (CODEBRIM) for multi-target classification of five commonly appearing concrete defects. We investigate and compare two reinforcement learning based meta-learning approaches, MetaQNN and efficient neural architecture search, to find suitable convolutional neural network architectures for this challenging multi-class multi-target task. We show that learned architectures have fewer overall parameters in addition to yielding better multi-target accuracy in comparison to popular neural architectures from the literature evaluated in the context of our application.",0
"Detecting defects in concrete infrastructure, particularly in bridges, is a crucial initial step in assessing their structural integrity, but it can be expensive and time-consuming due to various challenges. These challenges include the concrete's diverse appearance, varying lighting and weather conditions, different surface markings, and the possibility of overlapping defects. To address this issue, we created the COncrete DEfect BRidge IMage dataset (CODEBRIM) to classify five common concrete defects. We explored two reinforcement learning-based meta-learning techniques, MetaQNN and efficient neural architecture search, to identify suitable convolutional neural network models for this complex multi-target classification task. Our results demonstrate that the learned models have fewer parameters and achieve better multi-target accuracy than existing neural architectures evaluated in our specific application.",1
"We propose a general-purpose approach to discovering active learning (AL) strategies from data. These strategies are transferable from one domain to another and can be used in conjunction with many machine learning models. To this end, we formalize the annotation process as a Markov decision process, design universal state and action spaces and introduce a new reward function that precisely model the AL objective of minimizing the annotation cost. We seek to find an optimal (non-myopic) AL strategy using reinforcement learning. We evaluate the learned strategies on multiple unrelated domains and show that they consistently outperform state-of-the-art baselines.",0
"Our proposal presents a versatile method for identifying active learning (AL) tactics through data that can be applied across domains and combined with various machine learning models. To achieve this, we establish the annotation process as a Markov decision process, create broad state and action spaces, and introduce a novel reward function that accurately reflects the AL goal of reducing annotation costs. Our aim is to utilize reinforcement learning to identify the optimal (non-myopic) AL strategy. We test the learned strategies in several distinct domains and demonstrate that they consistently outperform existing state-of-the-art methods.",1
"As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.",0
"The increased use of deep reinforcement learning via visual perception has created a growing need to better comprehend and investigate the learned agents. By understanding the decision-making process and its connection to visual inputs, it becomes possible to identify problems within the learned behavior. However, this topic has been relatively neglected in the research community. In this study, we present a method for producing visual inputs that are pertinent to a trained agent. These inputs or states could depict situations that necessitate specific actions. Additionally, critical states that offer high or low rewards are valuable in gauging the system's situational awareness since they may correspond to risky states. To achieve this, we trained a generative model over the environment's state space and utilized its latent space to optimize a target function for the state of interest. Our experiments demonstrate that this method can provide insights for various environments and reinforcement learning methods. We tested this approach on both the standard Atari benchmark games and an autonomous driving simulator, and we were able to identify behavioral weaknesses in an efficient manner. Therefore, we believe that this general approach could be a vital tool for AI safety applications.",1
"In this paper we present our scientific discovery that good representation can be learned via continuous attention during the interaction between Unsupervised Learning(UL) and Reinforcement Learning(RL) modules driven by intrinsic motivation. Specifically, we designed intrinsic rewards generated from UL modules for driving the RL agent to focus on objects for a period of time and to learn good representations of objects for later object recognition task. We evaluate our proposed algorithm in both with and without extrinsic reward settings. Experiments with end-to-end training in simulated environments with applications to few-shot object recognition demonstrated the effectiveness of the proposed algorithm.",0
"Our scientific breakthrough is revealed in this paper, which shows that continuous attention during the interaction between Unsupervised Learning (UL) and Reinforcement Learning (RL) modules driven by intrinsic motivation can lead to the acquisition of good representation. The intrinsic rewards generated from the UL modules are designed to prompt the RL agent to concentrate on objects for a specified duration and attain a strong understanding of them for future object recognition tasks. Our algorithm is tested with and without extrinsic rewards, and the effectiveness of the proposed algorithm is demonstrated through end-to-end training in simulated environments with applications to few-shot object recognition.",1
"Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary.",0
"Significant advancements have been made in deep reinforcement learning for solving computer games and robotic control tasks. However, policies developed through this method tend to overfit to the training environment and may fail to prevent rare, catastrophic events such as automotive accidents. To address this issue, researchers have traditionally trained reinforcement learning algorithms on a set of randomized environments, although this approach only offers protection against common situations. Recently, robust adversarial reinforcement learning (RARL) has emerged as a more efficient method for applying both random and systematic perturbations through a trained adversary. However, RARL only optimizes the expected control objective without explicitly modeling or optimizing risk, meaning that agents do not consider the probability of catastrophic events except through their impact on the expected objective. To address this limitation, this paper introduces risk-averse robust adversarial reinforcement learning (RARARL) by using a risk-averse protagonist and a risk-seeking adversary. The approach is tested on a self-driving vehicle controller using an ensemble of policy networks to model risk as the variance of value functions. Results show that a risk-averse agent is better equipped to handle a risk-seeking adversary and experiences fewer crashes than agents trained without an adversary.",1
"Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.",0
"Unsupervised representation learning through mutual information maximization has been highly effective in achieving state-of-the-art performance in object recognition, speech recognition, and reinforcement learning. However, the use of mutual information as a learning objective is limited due to the exponential sample size required to achieve a tight lower bound of mutual information. This presents a challenge for high-mutual-information prediction tasks like video understanding or reinforcement learning, where overfitting can occur, and only a few relevant factors of variation are captured, resulting in incomplete representations that are suboptimal for downstream tasks. Empirical evidence shows that mutual information-based representation learning approaches fail to produce complete representations in designed and real-world tasks. To address this limitation, the Wasserstein dependency measure uses the Wasserstein distance instead of KL divergence in mutual information estimation, resulting in more complete representations. A practical approximation to this solution, which uses Lipschitz constraint techniques from the GAN literature, demonstrated significant improvement in tasks where incomplete representations are a major challenge.",1
"Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world",0
"Reinforcement learning (RL) algorithms have achieved impressive success in learning complex behaviors from minimal input, primarily in simulations. Despite some progress in using RL algorithms on real systems, their performance on more intricate systems is limited due to RL algorithm's relative data inefficiency. Domain randomization is a promising research direction that has shown impressive results in controlling real robots using RL algorithms. This method involves training a policy on a distribution of environmental conditions in simulations, which, if diverse enough, can generalize to the real world. However, the form and parameters of the simulated environment distribution have a significant impact on the trained policies' real-world performance. Previous research relied on hand-tuned distributions, but this extended abstract demonstrates that optimizing the parameter of the distribution can maximize the trained policies' real-world performance.",1
"Reinforcement learning algorithms rely on exploration to discover new behaviors, which is typically achieved by following a stochastic policy. In continuous control tasks, policies with a Gaussian distribution have been widely adopted. Gaussian exploration however does not result in smooth trajectories that generally correspond to safe and rewarding behaviors in practical tasks. In addition, Gaussian policies do not result in an effective exploration of an environment and become increasingly inefficient as the action rate increases. This contributes to a low sample efficiency often observed in learning continuous control tasks. We introduce a family of stationary autoregressive (AR) stochastic processes to facilitate exploration in continuous control domains. We show that proposed processes possess two desirable features: subsequent process observations are temporally coherent with continuously adjustable degree of coherence, and the process stationary distribution is standard normal. We derive an autoregressive policy (ARP) that implements such processes maintaining the standard agent-environment interface. We show how ARPs can be easily used with the existing off-the-shelf learning algorithms. Empirically we demonstrate that using ARPs results in improved exploration and sample efficiency in both simulated and real world domains, and, furthermore, provides smooth exploration trajectories that enable safe operation of robotic hardware.",0
"Exploration is a crucial component of reinforcement learning algorithms that involves discovering new behaviors by following a stochastic policy. Gaussian distribution policies are commonly used in continuous control tasks but do not necessarily produce safe or rewarding behaviors. Furthermore, Gaussian exploration becomes less efficient as the action rate increases, leading to low sample efficiency in learning continuous control tasks. To address these issues, we propose a family of stationary autoregressive stochastic processes that facilitate exploration in continuous control domains. These processes exhibit two desirable properties: temporally coherent observations and a standard normal stationary distribution. We introduce an autoregressive policy that implements these processes and maintains the standard agent-environment interface. We demonstrate the effectiveness of this approach through empirical experiments in simulated and real-world domains, where we observe improved exploration, sample efficiency, and safe operation of robotic hardware.",1
"Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.",0
"The typical method for learning optimal behavior from an expert's demonstrations assumes that the learner understands the key features of the reward function. However, in this study, we examine the scenario where this is not the case - where the learner and expert have different perspectives. We introduce the concept of teaching risk, which measures the risk of suboptimal policies that may appear optimal to the learner. By establishing bounds on teaching risk, we demonstrate that standard algorithms based on inverse reinforcement learning can produce near-optimal policies. Additionally, we propose a teaching scheme in which the expert can reduce teaching risk by adjusting the learner's perspective, ultimately leading to a near-optimal policy.",1
"Learning is an inherently continuous phenomenon. When humans learn a new task there is no explicit distinction between training and inference. As we learn a task, we keep learning about it while performing the task. What we learn and how we learn it varies during different stages of learning. Learning how to learn and adapt is a key property that enables us to generalize effortlessly to new settings. This is in contrast with conventional settings in machine learning where a trained model is frozen during inference. In this paper we study the problem of learning to learn at both training and test time in the context of visual navigation. A fundamental challenge in navigation is generalization to unseen scenes. In this paper we propose a self-adaptive visual navigation method (SAVN) which learns to adapt to new environments without any explicit supervision. Our solution is a meta-reinforcement learning approach where an agent learns a self-supervised interaction loss that encourages effective navigation. Our experiments, performed in the AI2-THOR framework, show major improvements in both success rate and SPL for visual navigation in novel scenes. Our code and data are available at: https://github.com/allenai/savn .",0
"The process of learning is ongoing and uninterrupted. When humans acquire a new skill, there is no clear distinction between the training phase and the inference phase. We continue to gather knowledge about the task as we perform it, and the way we learn and what we learn changes during different stages of the learning process. The ability to learn how to learn and adapt is crucial in order to effortlessly apply our knowledge to new situations. This is different from conventional machine learning approaches where the model is fixed once it has been trained. In this research, we explore the problem of learning to learn during both training and testing phases in the context of visual navigation. An important challenge in navigation is being able to generalize to new environments. To address this, we propose a self-adaptive visual navigation method (SAVN) that can adapt to new surroundings without explicit guidance. Our method is based on meta-reinforcement learning, where an agent learns through a self-supervised interaction loss that encourages effective navigation. Our experiments, conducted using the AI2-THOR framework, demonstrate significant improvements in both success rate and SPL for visual navigation in unfamiliar scenes. The code and data are available at https://github.com/allenai/savn.",1
"Estimating over-amplification of human epidermal growth factor receptor 2 (HER2) on invasive breast cancer (BC) is regarded as a significant predictive and prognostic marker. We propose a novel deep reinforcement learning (DRL) based model that treats immunohistochemical (IHC) scoring of HER2 as a sequential learning task. For a given image tile sampled from multi-resolution giga-pixel whole slide image (WSI), the model learns to sequentially identify some of the diagnostically relevant regions of interest (ROIs) by following a parameterized policy. The selected ROIs are processed by recurrent and residual convolution networks to learn the discriminative features for different HER2 scores and predict the next location, without requiring to process all the sub-image patches of a given tile for predicting the HER2 score, mimicking the histopathologist who would not usually analyze every part of the slide at the highest magnification. The proposed model incorporates a task-specific regularization term and inhibition of return mechanism to prevent the model from revisiting the previously attended locations. We evaluated our model on two IHC datasets: a publicly available dataset from the HER2 scoring challenge contest and another dataset consisting of WSIs of gastroenteropancreatic neuroendocrine tumor sections stained with Glo1 marker. We demonstrate that the proposed model outperforms other methods based on state-of-the-art deep convolutional networks. To the best of our knowledge, this is the first study using DRL for IHC scoring and could potentially lead to wider use of DRL in the domain of computational pathology reducing the computational burden of the analysis of large multigigapixel histology images.",0
"A significant predictive and prognostic marker for invasive breast cancer (BC) is the estimation of over-amplification of human epidermal growth factor receptor 2 (HER2). Our proposed novel model utilizes deep reinforcement learning (DRL) to treat the immunohistochemical (IHC) scoring of HER2 as a sequential learning task. By following a parameterized policy, the model learns to identify diagnostically relevant regions of interest (ROIs) in a given image tile sampled from a multi-resolution giga-pixel whole slide image (WSI). Recurrent and residual convolution networks process the selected ROIs to learn discriminative features for different HER2 scores and predict the next location, mimicking the process of a histopathologist who would not analyze every part of the slide at the highest magnification. The proposed model includes a task-specific regularization term and inhibition of return mechanism to prevent revisiting previously attended locations. Our evaluation on two IHC datasets demonstrates that the proposed model outperforms other methods based on state-of-the-art deep convolutional networks. This is the first study using DRL for IHC scoring and could potentially lead to wider use of DRL in the domain of computational pathology, reducing the computational burden of analyzing large multigigapixel histology images.",1
"In autonomous embedded systems, it is often vital to reduce the amount of actions taken in the real world and energy required to learn a policy. Training reinforcement learning agents from high dimensional image representations can be very expensive and time consuming. Autoencoders are deep neural network used to compress high dimensional data such as pixelated images into small latent representations. This compression model is vital to efficiently learn policies, especially when learning on embedded systems. We have implemented this model on the NVIDIA Jetson TX2 embedded GPU, and evaluated the power consumption, throughput, and energy consumption of the autoencoders for various CPU/GPU core combinations, frequencies, and model parameters. Additionally, we have shown the reconstructions generated by the autoencoder to analyze the quality of the generated compressed representation and also the performance of the reinforcement learning agent. Finally, we have presented an assessment of the viability of training these models on embedded systems and their usefulness in developing autonomous policies. Using autoencoders, we were able to achieve 4-5 $\times$ improved performance compared to a baseline RL agent with a convolutional feature extractor, while using less than 2W of power.",0
"Reducing the number of actions taken in the real world and the energy required for policy learning is crucial in autonomous embedded systems. However, training reinforcement learning agents from high dimensional image representations can be both expensive and time-consuming. To address this issue, autoencoders, which are deep neural networks used to compress high dimensional data into small latent representations, are employed. This compression model is vital for efficient policy learning, particularly in embedded systems. Our study involved implementing this model on the NVIDIA Jetson TX2 embedded GPU and analyzing the power consumption, throughput, and energy consumption of the autoencoders for various CPU/GPU core combinations, frequencies, and model parameters. Furthermore, we evaluated the quality of the generated compressed representation and the performance of the reinforcement learning agent by analyzing the reconstructions generated by the autoencoder. Ultimately, we assessed the feasibility of training these models on embedded systems and their usefulness in developing autonomous policies. Our results show that using autoencoders, we were able to achieve 4-5 times better performance than a baseline RL agent with a convolutional feature extractor while using less than 2W of power.",1
"In this paper we study a new reinforcement learning setting where the environment is non-rewarding, contains several possibly related objects of various controllability, and where an apt agent Bob acts independently, with non-observable intentions. We argue that this setting defines a realistic scenario and we present a generic discrete-state discrete-action model of such environments. To learn in this environment, we propose an unsupervised reinforcement learning agent called CLIC for Curriculum Learning and Imitation for Control. CLIC learns to control individual objects in its environment, and imitates Bob's interactions with these objects. It selects objects to focus on when training and imitating by maximizing its learning progress. We show that CLIC is an effective baseline in our new setting. It can effectively observe Bob to gain control of objects faster, even if Bob is not explicitly teaching. It can also follow Bob when he acts as a mentor and provides ordered demonstrations. Finally, when Bob controls objects that the agent cannot, or in presence of a hierarchy between objects in the environment, we show that CLIC ignores non-reproducible and already mastered interactions with objects, resulting in a greater benefit from imitation.",0
"The objective of this paper is to examine a novel reinforcement learning scenario in which the environment lacks rewards, comprises multiple possibly correlated objects with varying degrees of controllability, and involves an independent agent Bob with non-observable intentions. We contend that this situation mirrors real-life settings and therefore present a comprehensive model of such environments with discrete-state and discrete-action components. To enable learning in such a context, we introduce an unsupervised reinforcement learning agent, CLIC (Curriculum Learning and Imitation for Control), which is capable of managing individual objects in the surroundings and emulating Bob's interactions with them. CLIC selects specific objects for training and imitation to optimize its learning progression. Our findings indicate that CLIC is a reliable benchmark in this new environment; it can efficiently monitor Bob's actions to gain control of objects more quickly, even in the absence of explicit instruction. It can also track Bob's guidance when he serves as a mentor and provides ordered demonstrations. Additionally, when Bob controls objects that the agent cannot, or when a hierarchy exists among objects in the environment, CLIC refrains from duplicating already mastered interactions with objects, resulting in a greater benefit from imitation.",1
"With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the ""gist"" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.",0
"The human visual system can quickly comprehend a complex scene with just a brief glance, thanks to its ability to recognize the ""gist"" of the environment based on prior knowledge. This paper explores whether incorporating memory into computer vision systems can enhance object detection accuracy and reduce computation time. By integrating lightweight feature extractors that only identify the scene's gist with traditional ones, we demonstrate that minimal computation is necessary for precise detections when temporal memory is present. Furthermore, our model's memory holds sufficient data for implementing reinforcement learning algorithms to learn an adaptive inference policy. Our approach achieves top-notch performance on the Imagenet VID 2015 dataset using mobile devices such as a Pixel 3 phone, operating at speeds of over 70 FPS.",1
"Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",0
"Reinforcement learning faces a major challenge in efficient exploration. One reason for this is that the variability of the rewards depends on the current state and action, making it heteroscedastic. Traditional exploration methods such as upper confidence bound algorithms and Thompson sampling fail to account for heteroscedasticity, even in the bandit setting. To address this issue, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning, building on recent findings in the bandit setting. Our approach utilizes advances in distributional reinforcement learning to create a novel, tractable approximation of IDS for deep Q-learning. This exploration strategy accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate significant improvement over other approaches.",1
"Using reinforcement learning to learn control policies is a challenge when the task is complex with potentially long horizons. Ensuring adequate but safe exploration is also crucial for controlling physical systems. In this paper, we use temporal logic to facilitate specification and learning of complex tasks. We combine temporal logic with control Lyapunov functions to improve exploration. We incorporate control barrier functions to safeguard the exploration and deployment process. We develop a flexible and learnable system that allows users to specify task objectives and constraints in different forms and at various levels. The framework is also able to take advantage of known system dynamics and handle unknown environmental dynamics by integrating model-free learning with model-based planning.",0
"Learning control policies through reinforcement learning can be difficult when dealing with complex tasks that have long horizons. Additionally, ensuring safe exploration is crucial when controlling physical systems. This study utilizes temporal logic to enhance the specification and learning of complex tasks. Control Lyapunov functions are combined with temporal logic to improve exploration, and control barrier functions are implemented to ensure safe exploration and deployment. The system is flexible and learnable, allowing users to specify task objectives and constraints in different forms and at various levels. It can also integrate model-free learning with model-based planning to handle unknown environmental dynamics while taking advantage of known system dynamics.",1
"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model 2) they are an ""end-to-end"" approach, directly optimizing the performance metric of interest 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties. This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities.",0
"Reinforcement learning and continuous control problems often use direct policy gradient methods, which have gained popularity due to their ease of implementation without knowledge of the underlying model, their direct optimization of performance metrics, and their ability to handle complex policies. However, a major drawback is that these methods require solving non-convex optimization problems, which can be computationally and statistically inefficient. In contrast, optimal control theory's system identification and model based planning have a stronger theoretical foundation and are more efficient. Nevertheless, recent work has shown that policy gradient methods can globally converge to optimal solutions and be efficient in terms of sample and computational complexities.",1
"Reinforcement learning algorithms can be used to optimally solve dynamic decision-making and control problems. With continuous-valued state and input variables, reinforcement learning algorithms must rely on function approximators to represent the value function and policy mappings. Commonly used numerical approximators, such as neural networks or basis function expansions, have two main drawbacks: they are black-box models offering no insight in the mappings learned, and they require significant trial and error tuning of their meta-parameters. In this paper, we propose a new approach to constructing smooth value functions by means of symbolic regression. We introduce three off-line methods for finding value functions based on a state transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. The methods are illustrated on four nonlinear control problems: velocity control under friction, one-link and two-link pendulum swing-up, and magnetic manipulation. The results show that the value functions not only yield well-performing policies, but also are compact, human-readable and mathematically tractable. This makes them potentially suitable for further analysis of the closed-loop system. A comparison with alternative approaches using neural networks shows that our method constructs well-performing value functions with substantially fewer parameters.",0
"Dynamic decision-making and control problems can be optimally solved using reinforcement learning algorithms. However, when dealing with continuous-valued state and input variables, these algorithms require function approximators to represent the value function and policy mappings. Unfortunately, commonly used numerical approximators like neural networks or basis function expansions have two main disadvantages: they offer no insight into the learned mappings and need extensive trial and error tuning of their meta-parameters. To address these issues, this paper proposes a new approach that uses symbolic regression to construct smooth value functions. The authors introduce three off-line methods to find value functions based on a state transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. These methods are demonstrated on four nonlinear control problems, and the results show that the value functions are not only well-performing but also compact, human-readable, and mathematically tractable. This makes them ideal for further analysis of the closed-loop system. Additionally, the authors compared their method with alternative approaches using neural networks and found that their method constructs well-performing value functions with substantially fewer parameters.",1
"In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.",0
"In more intricate transfer learning situations, it is possible that new tasks may not have a strong connection to previous tasks. This means that methods that only transfer information from the final parameters of a source model may not be effective. To address this, a higher level of abstraction is necessary. The Leap framework, which transfers knowledge across learning processes, is proposed to address this issue. Each task is linked to a manifold, and a meta-learning objective is constructed to minimize the expected length of the path travelled in the training process. This method only utilizes information obtained during training and is inexpensive to compute. It is demonstrated that Leap outperforms other methods in both meta-learning and transfer learning, particularly in computer vision tasks. Additionally, it is shown that Leap can successfully transfer knowledge across learning processes in complex reinforcement learning environments such as Atari, which involves millions of gradient steps.",1
"The recommender system is an important form of intelligent application, which assists users to alleviate from information redundancy. Among the metrics used to evaluate a recommender system, the metric of conversion has become more and more important. The majority of existing recommender systems perform poorly on the metric of conversion due to its extremely sparse feedback signal. To tackle this challenge, we propose a deep hierarchical reinforcement learning based recommendation framework, which consists of two components, i.e., high-level agent and low-level agent. The high-level agent catches long-term sparse conversion signals, and automatically sets abstract goals for low-level agent, while the low-level agent follows the abstract goals and interacts with real-time environment. To solve the inherent problem in hierarchical reinforcement learning, we propose a novel deep hierarchical reinforcement learning algorithm via multi-goals abstraction (HRL-MG). Our proposed algorithm contains three characteristics: 1) the high-level agent generates multiple goals to guide the low-level agent in different stages, which reduces the difficulty of approaching high-level goals; 2) different goals share the same state encoder parameters, which increases the update frequency of the high-level agent and thus accelerates the convergence of our proposed algorithm; 3) an appreciate benefit assignment function is designed to allocate rewards in each goal so as to coordinate different goals in a consistent direction. We evaluate our proposed algorithm based on a real-world e-commerce dataset and validate its effectiveness.",0
"The recommender system is a useful intelligent application that helps users avoid receiving repetitive information. One of the crucial metrics used to assess recommender systems is conversion, which has become increasingly vital. Unfortunately, most existing recommender systems are not efficient in achieving the conversion metric due to the scarce feedback signal. To tackle this issue, we introduce a deep hierarchical reinforcement learning-based recommendation framework with two agents; a high-level and a low-level agent. The high-level agent captures long-term sparse conversion signals and establishes abstract goals for the low-level agent, which then interacts with the real-time environment. To overcome inherent hierarchical reinforcement learning problems, we propose a new algorithm via multi-goals abstraction (HRL-MG), which has three distinct features. Firstly, the high-level agent generates multiple goals to guide the low-level agent during different stages, reducing the high-level goals' difficulty. Secondly, different objectives use the same state encoder parameters, accelerating the convergence of our proposed algorithm. Finally, we designed an appropriate benefit assignment function to coordinate different goals in a consistent direction. We tested our proposed algorithm using a real-world e-commerce dataset and validated its effectiveness.",1
"We propose Deep Q-Networks (DQN) with model-based exploration, an algorithm combining both model-free and model-based approaches that explores better and learns environments with sparse rewards more efficiently. DQN is a general-purpose, model-free algorithm and has been proven to perform well in a variety of tasks including Atari 2600 games since it's first proposed by Minh et el. However, like many other reinforcement learning (RL) algorithms, DQN suffers from poor sample efficiency when rewards are sparse in an environment. As a result, most of the transitions stored in the replay memory have no informative reward signal, and provide limited value to the convergence and training of the Q-Network. However, one insight is that these transitions can be used to learn the dynamics of the environment as a supervised learning problem. The transitions also provide information of the distribution of visited states. Our algorithm utilizes these two observations to perform a one-step planning during exploration to pick an action that leads to states least likely to be seen, thus improving the performance of exploration. We demonstrate our agent's performance in two classic environments with sparse rewards in OpenAI gym: Mountain Car and Lunar Lander.",0
"An algorithm called Deep Q-Networks (DQN) with model-based exploration is proposed to efficiently learn environments with sparse rewards by combining model-free and model-based approaches. DQN is a widely used model-free algorithm that has performed well in various tasks, including Atari 2600 games. However, like other reinforcement learning algorithms, DQN has poor sample efficiency when rewards are scarce, resulting in transitions stored in the replay memory with no informative reward signal. Nevertheless, these transitions can be used for supervised learning to learn the environment's dynamics and the distribution of visited states. Our algorithm leverages these observations to perform one-step planning during exploration, selecting actions that lead to the least likely states to improve exploration performance. We showcase our agent's performance on two classic sparse reward environments in OpenAI gym: Mountain Car and Lunar Lander.",1
"In this paper, we propose a distributed off-policy actor critic method to solve multi-agent reinforcement learning problems. Specifically, we assume that all agents keep local estimates of the global optimal policy parameter and update their local value function estimates independently. Then, we introduce an additional consensus step to let all the agents asymptotically achieve agreement on the global optimal policy function. The convergence analysis of the proposed algorithm is provided and the effectiveness of the proposed algorithm is validated using a distributed resource allocation example. Compared to relevant distributed actor critic methods, here the agents do not share information about their local tasks, but instead they coordinate to estimate the global policy function.",0
"This paper presents a method for solving multi-agent reinforcement learning problems by proposing a distributed off-policy actor critic approach. The method involves each agent maintaining a local estimate of the global optimal policy parameter and independently updating their local value function estimates. Additionally, a consensus step is introduced to allow all agents to achieve agreement on the global optimal policy function. The algorithm's convergence is analyzed, and a distributed resource allocation example is used to validate its effectiveness. Unlike other distributed actor critic methods, the proposed method does not involve sharing information about local tasks but instead focuses on coordinating the estimation of the global policy function.",1
"Multi-task learning, as it is understood nowadays, consists of using one single model to carry out several similar tasks. From classifying hand-written characters of different alphabets to figuring out how to play several Atari games using reinforcement learning, multi-task models have been able to widen their performance range across different tasks, although these tasks are usually of a similar nature. In this work, we attempt to widen this range even further, by including heterogeneous tasks in a single learning procedure. To do so, we firstly formally define a multi-network model, identifying the necessary components and characteristics to allow different adaptations of said model depending on the tasks it is required to fulfill. Secondly, employing the formal definition as a starting point, we develop an illustrative model example consisting of three different tasks (classification, regression and data sampling). The performance of this model implementation is then analyzed, showing its capabilities. Motivated by the results of the analysis, we enumerate a set of open challenges and future research lines over which the full potential of the proposed model definition can be exploited.",0
"Nowadays, multi-task learning involves using a single model to perform multiple similar tasks, such as recognizing hand-written characters or playing Atari games using reinforcement learning. However, these tasks usually have a comparable nature. In this study, we aim to expand the scope of multi-task learning by incorporating diverse tasks into a single learning process. Firstly, we define a multi-network model that outlines the necessary components and characteristics to enable various adaptations based on the required tasks. Secondly, we create an example model that includes three distinct tasks: classification, regression, and data sampling. We evaluate the performance of this model and analyze its capabilities. Based on our findings, we identify several open challenges and future research opportunities for fully exploiting the potential of our proposed model definition.",1
"Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",0
"The use of three techniques in reinforcement learning, known collectively as the `deadly triad', can lead to divergence in Q-learning algorithms. These techniques are bootstrapping, off-policy learning, and function approximation, and their impact on divergence is not yet fully understood. This paper presents a simple analysis based on a linear approximation to the Q-value updates, which sheds light on the conditions under which divergence occurs under the deadly triad. The analysis focuses on whether the leading order approximation to the deep-Q update is a contraction in the sup norm. Based on this analysis, the authors develop an algorithm that enables stable deep Q-learning for continuous control without the use of conventional tricks such as target networks, adaptive gradient optimizers, or multiple Q functions. The algorithm is shown to perform above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",1
"Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) on-line learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.   Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous car-following with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.",0
"RL algorithms have had limited success in real-world scenarios due to the lack of safety guarantees during the learning process. The absence of such guarantees could result in system failure or breakage before an optimal controller can be created. To address this issue, we propose a controller architecture that combines a model-free RL-based controller with model-based controllers that utilize control barrier functions (CBFs) and on-line learning of unknown system dynamics. This approach ensures safety during learning. Our framework uses RL algorithms to learn high-performance controllers and CBF-based controllers to guarantee safety and guide the learning process by limiting the set of explorable policies. We use Gaussian Processes (GPs) to model system dynamics and uncertainties. Our novel RL-CBF algorithm guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We tested our algorithm on an inverted pendulum and autonomous car-following with wireless vehicle-to-vehicle communication. Our algorithm achieved much greater sample efficiency in learning than other state-of-the-art algorithms and maintained safety throughout the learning process.",1
"When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.",0
"When attempting to learn policies for real-world situations, there are two significant issues to consider: (i) how to effectively use previously collected off-policy, non-optimal behavior data, and (ii) how to balance competing objectives and constraints. As a result, we have investigated the problem of batch policy learning under multiple constraints and have provided a systematic solution. Our approach includes a versatile meta-algorithm that can accommodate any batch reinforcement learning and online learning procedure as subroutines. We have also developed a specific algorithm and have provided performance guarantees for both the primary objective and all constraints. To ensure constraint satisfaction, we have introduced a straightforward method for off-policy policy evaluation (OPE) and have derived PAC-style bounds. Our algorithm has demonstrated impressive results in various domains, including a challenging simulated car driving problem with multiple constraints such as lane keeping and smooth driving. Furthermore, our OPE method has outperformed other popular OPE techniques on a standalone basis, particularly in a high-dimensional environment.",1
"Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.",0
"Learning a single task using deep reinforcement learning algorithms necessitates a vast amount of experience. Meta-reinforcement learning (meta-RL) algorithms could theoretically enable agents to acquire new skills using only a small amount of experience, but their practicality is limited by significant challenges. Current methods often rely on on-policy experience, which reduces their sample efficiency. Furthermore, they lack the ability to reason about task uncertainty when adapting to new tasks, which limits their effectiveness in solving problems with sparse rewards. To overcome these challenges, we present an off-policy meta-RL algorithm that separates task inference from control. Our method uses online probabilistic filtering of latent task variables to deduce how to tackle a new task with limited experience. This probabilistic interpretation makes structured and efficient exploration feasible through posterior sampling. We demonstrate how to combine these task variables with off-policy RL algorithms to achieve greater efficiency in both meta-training and adaptation. Our method outperforms prior algorithms in sample efficiency by 20-100X and asymptotic performance on numerous meta-RL benchmarks.",1
"Compared to reinforcement learning, imitation learning (IL) is a powerful paradigm for training agents to learn control policies efficiently from expert demonstrations. However, in most cases, obtaining demonstration data is costly and laborious, which poses a significant challenge in some scenarios. A promising alternative is to train agent learning skills via imitation learning without expert demonstrations, which, to some extent, would extremely expand imitation learning areas. To achieve such expectation, in this paper, we propose Hindsight Generative Adversarial Imitation Learning (HGAIL) algorithm, with the aim of achieving imitation learning satisfying no need of demonstrations. Combining hindsight idea with the generative adversarial imitation learning (GAIL) framework, we realize implementing imitation learning successfully in cases of expert demonstration data are not available. Experiments show that the proposed method can train policies showing comparable performance to current imitation learning methods. Further more, HGAIL essentially endows curriculum learning mechanism which is critical for learning policies.",0
"Imitation learning (IL) is a more efficient approach for training agents to learn control policies from expert demonstrations compared to reinforcement learning. However, obtaining demonstration data can be expensive and difficult, which is a significant challenge in some scenarios. To address this issue, we propose the Hindsight Generative Adversarial Imitation Learning (HGAIL) algorithm, which aims to expand imitation learning areas by enabling agent learning without expert demonstrations. The HGAIL algorithm combines the hindsight idea with the generative adversarial imitation learning (GAIL) framework to successfully implement imitation learning in cases where expert demonstration data is not available. Experiments show that the proposed method can train policies with comparable performance to current imitation learning methods and also includes a curriculum learning mechanism that is critical for learning policies.",1
"This report first provides a brief overview of a number of supervised learning algorithms for regression tasks. Among those are neural networks, regression trees, and the recently introduced Nexting. Nexting has been presented in the context of reinforcement learning where it was used to predict a large number of signals at different timescales. In the second half of this report, we apply the algorithms to historical weather data in order to evaluate their suitability to forecast a local weather trend. Our experiments did not identify one clearly preferable method, but rather show that choosing an appropriate algorithm depends on the available side information. For slowly varying signals and a proficient number of training samples, Nexting achieved good results in the studied cases.",0
"To begin with, this report briefly introduces various supervised learning algorithms that can be applied to regression tasks. These algorithms include neural networks, regression trees, and a recently developed method called Nexting. Nexting was initially introduced in reinforcement learning to predict numerous signals at different timescales. In the latter part of this report, we implemented these algorithms on historical weather data to assess their effectiveness in predicting local weather trends. Our experiments did not reveal a distinct preference for any one method; instead, we discovered that the appropriate algorithm choice is reliant on the available supplementary information. In cases where the signals vary slowly and there are ample training samples, Nexting yielded satisfactory outcomes.",1
"In model-based reinforcement learning, the agent interleaves between model learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the executed planner would exploit model flaws, which can yield catastrophic failures. This paper focuses on building a model that reasons about the long-term future and demonstrates how to use this for efficient planning and exploration. To this end, we build a latent-variable autoregressive model by leveraging recent ideas in variational inference. We argue that forcing latent variables to carry future information through an auxiliary task substantially improves long-term predictions. Moreover, by planning in the latent space, the planner's solution is ensured to be within regions where the model is valid. An exploration strategy can be devised by searching for unlikely trajectories under the model. Our method achieves higher reward faster compared to baselines on a variety of tasks and environments in both the imitation learning and model-based reinforcement learning settings.",0
"The process of model-based reinforcement learning involves the agent alternating between learning and planning using a model. The success of this approach is dependent on the quality of the model's ability to predict long-term outcomes. If the model is not accurate, the planner's execution could result in disastrous consequences. The focus of this study is on developing a model that can reason about the future in the long term, which can be used for efficient planning and exploration. The researchers created a latent-variable autoregressive model using variational inference, which they claim improves long-term prediction by forcing latent variables to carry future information through an auxiliary task. Planning in the latent space ensures that the planner's solution falls within areas where the model is reliable, and an exploration strategy can be devised by identifying unlikely trajectories under the model. The method has been tested on various tasks and environments, and it has been found to produce higher rewards faster than the baselines in both the imitation learning and model-based reinforcement learning settings.",1
"We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",0
"THOR, the House of Interactions, is a visual AI research framework accessible at http://ai2thor.allenai.org. The framework features true-to-life 3D indoor environments where AI agents can navigate and execute tasks by interacting with objects. AI2-THOR supports various research areas, such as deep reinforcement learning, planning, unsupervised representation learning, and learning models of cognition, among others. The main objective of AI2-THOR is to support the development of visually intelligent models and advance research in this field.",1
"Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but agents often fail to generalize beyond the environment they were trained in. As a result, deep RL algorithms that promote generalization are receiving increasing attention. However, works in this area use a wide variety of tasks and experimental setups for evaluation. The literature lacks a controlled assessment of the merits of different generalization schemes. Our aim is to catalyze community-wide progress on generalization in deep RL. To this end, we present a benchmark and experimental protocol, and conduct a systematic empirical study. Our framework contains a diverse set of environments, our methodology covers both in-distribution and out-of-distribution generalization, and our evaluation includes deep RL algorithms that specifically tackle generalization. Our key finding is that `vanilla' deep RL algorithms generalize better than specialized schemes that were proposed specifically to tackle generalization.",0
"Although deep reinforcement learning (RL) has accomplished remarkable results on numerous tasks, agents often struggle to apply their learning to environments beyond their training. As a result, more attention is being given to deep RL algorithms that promote generalization. However, the evaluation of various generalization schemes lacks a controlled assessment in current literature, as works in this area use a wide range of tasks and experimental setups. Our goal is to stimulate community-wide progress on generalization in deep RL by presenting a benchmark and experimental protocol and conducting a systematic empirical study. Our framework consists of a diverse set of environments, and our methodology covers both in-distribution and out-of-distribution generalization, while our evaluation includes deep RL algorithms specifically designed to tackle generalization. Our primary finding is that ""vanilla"" deep RL algorithms perform better in generalization than specialized schemes developed explicitly for this purpose.",1
"The area of building energy management has received a significant amount of interest in recent years. This area is concerned with combining advancements in sensor technologies, communications and advanced control algorithms to optimize energy utilization. Reinforcement learning is one of the most prominent machine learning algorithms used for control problems and has had many successful applications in the area of building energy management. This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems. The main direction for future research and challenges in reinforcement learning are also outlined.",0
"In recent times, considerable attention has been directed towards the field of building energy management, which aims to maximize energy usage by combining advancements in sensor technology, communications, and advanced control algorithms. One of the most widely-used machine learning algorithms for control problems is reinforcement learning, which has proven successful in various applications of building energy management. This study provides a thorough analysis of the existing literature on the use of reinforcement learning for creating self-sufficient building energy management systems, along with an overview of future research directions and the challenges involved.",1
"Multiagent reinforcement learning algorithms (MARL) have been demonstrated on complex tasks that require the coordination of a team of multiple agents to complete. Existing works have focused on sharing information between agents via centralized critics to stabilize learning or through communication to increase performance, but do not generally look at how information can be shared between agents to address the curse of dimensionality in MARL. We posit that a multiagent problem can be decomposed into a multi-task problem where each agent explores a subset of the state space instead of exploring the entire state space. This paper introduces a multiagent actor-critic algorithm and method for combining knowledge from homogeneous agents through distillation and value-matching that outperforms policy distillation alone and allows further learning in both discrete and continuous action spaces.",0
"MARL algorithms have been proven effective in tackling complex tasks that require the coordination of multiple agents. While previous research has focused on utilizing centralized critics or communication to enhance learning, little attention has been given to addressing the issue of dimensionality. In this paper, we propose a solution whereby a multiagent problem can be broken down into a multi-task problem, with each agent exploring a specific subset of the state space. Our approach involves a multiagent actor-critic algorithm and a method for combining knowledge from similar agents through distillation and value-matching. This approach yields better results than policy distillation alone and enables learning in both discrete and continuous action spaces.",1
"Deep Reinforcement Learning has enabled the control of increasingly complex and high-dimensional problems. However, the need of vast amounts of data before reasonable performance is attained prevents its widespread application. We employ binary corrective feedback as a general and intuitive manner to incorporate human intuition and domain knowledge in model-free machine learning. The uncertainty in the policy and the corrective feedback is combined directly in the action space as probabilistic conditional exploration. As a result, the greatest part of the otherwise ignorant learning process can be avoided. We demonstrate the proposed method, Predictive Probabilistic Merging of Policies (PPMP), in combination with DDPG. In experiments on continuous control problems of the OpenAI Gym, we achieve drastic improvements in sample efficiency, final performance, and robustness to erroneous feedback, both for human and synthetic feedback. Additionally, we show solutions beyond the demonstrated knowledge.",0
"The utilization of Deep Reinforcement Learning has facilitated the management of intricate and multidimensional predicaments. However, its extensive data requirement for satisfactory results hinders its widespread usage. We have devised a general and comprehensible binary corrective feedback approach to incorporate human insight and expertise in model-free machine learning. This method entails a probabilistic conditional exploration, allowing the combination of policy uncertainty and corrective feedback directly in the action space. Consequently, a significant portion of the otherwise uninformed learning process can be avoided. We have demonstrated this technique, named Predictive Probabilistic Merging of Policies (PPMP), with DDPG. In our experiments on continuous control issues of the OpenAI Gym, we have witnessed a remarkable improvement in sample efficiency, final performance, and resistance to erroneous feedback for both human and synthetic feedback. Furthermore, we have provided solutions beyond the demonstrated knowledge.",1
"Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.",0
"The ability to decipher a person's objective through their actions is a significant issue in AI applications like automated assistants and recommender systems. The most commonly used model for this task is the rational actor model, wherein individuals are assumed to have stable reward functions, discount the future exponentially, and create optimal plans. By utilizing the rational actor assumption, techniques like inverse reinforcement learning (IRL) can be employed to deduce a person's goals based on their actions. However, there is a competing model known as the dual-system model. Here, decision-making is a result of the interplay between two systems- a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We have expanded the dual-system framework to Markov decision problems and demonstrated how to compute optimal plans for dual-system agents. Our research shows that dual-system agents exhibit behaviors that contradict the rational actor assumption. Naive applications of rational-actor IRL to the behavior of dual-system agents can lead to incorrect inferences about their goals and interventions that negatively impact their overall utility. Therefore, we have developed a simple IRL algorithm to accurately deduce the goals of dual-system decision-makers. This enables us to make interventions that aid, rather than obstruct, the dual-system agent's ability to achieve their true goals.",1
"We propose a new low-cost machine-learning-based methodology which assists designers in reducing the gap between the problem and the solution in the design process. Our work applies reinforcement learning (RL) to find the optimal task-oriented design solution through the construction of the design action for each task. For this task-oriented design, the 3D design process in product design is assigned to an action space in Deep RL, and the desired 3D model is obtained by training each design action according to the task. By showing that this method achieves satisfactory design even when applied to a task pursuing multiple goals, we suggest the direction of how machine learning can contribute to the design process. Also, we have validated with product designers that this methodology can assist the creative part in the process of design.",0
"A new approach is suggested in this article that utilizes machine learning to help bridge the gap between design problems and solutions in a cost-effective manner. The proposed methodology employs reinforcement learning (RL) to identify the optimal task-based design solution by constructing design actions for each task. The 3D design process in product design is mapped to an action space in Deep RL for task-oriented design, and each design action is trained based on the task to obtain the desired 3D model. The study demonstrates that this method is effective even when applied to tasks with multiple goals, indicating the potential for machine learning to contribute to the design process. In addition, product designers have confirmed that this methodology can assist with the creative aspect of the design process.",1
"How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies. We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.",0
"Determining the emergence of communication in a multi-agent system can be indicated by an increase in task success or reward, as observed in recent studies. However, this provides a limited understanding of the agents' communication abilities. As more complex environments are introduced, it is necessary to have more precise and comprehensive tools for evaluating the emergence of communication to facilitate fault detection, performance assessment, and trust-building. This paper examines various metrics for measuring communication, highlighting their limitations and potential for misleading results. Through training deep reinforcement learning agents in matrix games with communication channels, we demonstrate a scenario where agents seemingly communicate without any impact on the environment or other agents. We explain this phenomenon using ablation studies and visualization of learned policies. We also review commonly used metrics and provide recommendations for their appropriate usage.",1
"Deep Reinforcement Learning (DRL) has been applied to address a variety of cooperative multi-agent problems with either discrete action spaces or continuous action spaces. However, to the best of our knowledge, no previous work has ever succeeded in applying DRL to multi-agent problems with discrete-continuous hybrid (or parameterized) action spaces which is very common in practice. Our work fills this gap by proposing two novel algorithms: Deep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent Hierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized training but decentralized execution paradigm: different levels of communication between different agents are used to facilitate the training process, while each agent executes its policy independently based on local observations during execution. Our empirical results on several challenging tasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN and Deep MAHHQN are effective and significantly outperform existing independent deep parameterized Q-learning method.",0
"DRL has been utilized to tackle cooperative multi-agent problems that have either discrete or continuous action spaces. However, it has never been successful in handling multi-agent problems with hybrid action spaces, which are commonly found in practical scenarios. This gap is addressed in our work by introducing two innovative algorithms: Deep MAPQN and Deep MAHHQN. We adopt the centralized training but decentralized execution approach, using various communication levels between agents during training while allowing each agent to execute its policy independently based on local observations. Our experimental results on challenging tasks, such as simulated RoboCup Soccer and Ghost Story, demonstrate the effectiveness of both Deep MAPQN and Deep MAHHQN, which significantly outperform the existing independent deep parameterized Q-learning method.",1
"We present a deep reinforcement learning method of progressive view inpainting for 3D point scene completion under volume guidance, achieving high-quality scene reconstruction from only a single depth image with severe occlusion. Our approach is end-to-end, consisting of three modules: 3D scene volume reconstruction, 2D depth map inpainting, and multi-view selection for completion. Given a single depth image, our method first goes through the 3D volume branch to obtain a volumetric scene reconstruction as a guide to the next view inpainting step, which attempts to make up the missing information; the third step involves projecting the volume under the same view of the input, concatenating them to complete the current view depth, and integrating all depth into the point cloud. Since the occluded areas are unavailable, we resort to a deep Q-Network to glance around and pick the next best view for large hole completion progressively until a scene is adequately reconstructed while guaranteeing validity. All steps are learned jointly to achieve robust and consistent results. We perform qualitative and quantitative evaluations with extensive experiments on the SUNCG data, obtaining better results than the state of the art.",0
"Our study introduces a deep reinforcement learning approach to progressively complete 3D point scenes through view inpainting, while adhering to volume guidance, resulting in superior scene reconstruction from a single depth image with extensive occlusion. We have devised an end-to-end strategy comprising three modules: 3D scene volume reconstruction, 2D depth map inpainting, and multi-view selection for completion. Our method initially employs the 3D volume module to obtain a volumetric scene reconstruction as guidance for the following view inpainting stage, which endeavors to compensate for the missing information. The third step involves projecting the volume under the same view of the input, concatenating it to complete the current view depth, and integrating all depth into the point cloud. As the occluded areas are not available, we have implemented a deep Q-network to scan the area and select the next best view for progressive large hole completion, while ensuring validity until a scene has satisfactorily reconstructed. All stages are learned together to achieve consistent and robust outcomes. We have conducted qualitative and quantitative assessments with extensive experiments on SUNCG data, obtaining better results than the current state of the art.",1
"Reinforcement learning (RL) is a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, and deep neural networks further enhance its learning power. However, centralized RL is infeasible for large-scale ATSC due to the extremely high dimension of the joint action space. Multi-agent RL (MARL) overcomes the scalability issue by distributing the global control to each local RL agent, but it introduces new challenges: now the environment becomes partially observable from the viewpoint of each local agent due to limited communication among agents. Most existing studies in MARL focus on designing efficient communication and coordination among traditional Q-learning agents. This paper presents, for the first time, a fully scalable and decentralized MARL algorithm for the state-of-the-art deep RL agent: advantage actor critic (A2C), within the context of ATSC. In particular, two methods are proposed to stabilize the learning procedure, by improving the observability and reducing the learning difficulty of each local agent. The proposed multi-agent A2C is compared against independent A2C and independent Q-learning algorithms, in both a large synthetic traffic grid and a large real-world traffic network of Monaco city, under simulated peak-hour traffic dynamics. Results demonstrate its optimality, robustness, and sample efficiency over other state-of-the-art decentralized MARL algorithms.",0
"Adaptive traffic signal control (ATSC) in complex urban traffic networks can be achieved using reinforcement learning (RL) with the aid of deep neural networks. However, the high dimension of the joint action space makes centralized RL impractical for large-scale ATSC. Multi-agent RL (MARL) is a promising solution that distributes global control to local RL agents, but it presents new challenges such as limited communication among agents and partial observability of the environment. Existing studies in MARL focus on communication and coordination among traditional Q-learning agents. In this paper, we introduce a fully scalable and decentralized MARL algorithm for the deep RL agent A2C, specifically for ATSC. We propose two methods to enhance the learning procedure by improving observability and reducing the learning difficulty of each local agent. The performance of the proposed multi-agent A2C is compared against independent A2C and independent Q-learning algorithms in both a large synthetic traffic grid and a large real-world traffic network of Monaco city under simulated peak-hour traffic dynamics. Results demonstrate the optimality, robustness, and sample efficiency of the proposed algorithm over other state-of-the-art decentralized MARL algorithms.",1
"Machine learning, especially deep neural networks, has been rapidly developed in fields including computer vision, speech recognition and reinforcement learning. Although Mini-batch SGD is one of the most popular stochastic optimization methods in training deep networks, it shows a slow convergence rate due to the large noise in gradient approximation. In this paper, we attempt to remedy this problem by building more efficient batch selection method based on typicality sampling, which reduces the error of gradient estimation in conventional Minibatch SGD. We analyze the convergence rate of the resulting typical batch SGD algorithm and compare convergence properties between Minibatch SGD and the algorithm. Experimental results demonstrate that our batch selection scheme works well and more complex Minibatch SGD variants can benefit from the proposed batch selection strategy.",0
"The development of machine learning, particularly deep neural networks, has advanced rapidly in various fields such as computer vision, speech recognition, and reinforcement learning. Despite being a popular stochastic optimization technique for training deep networks, Mini-batch SGD exhibits a slow rate of convergence due to the considerable noise in gradient approximation. This paper addresses this issue by introducing a more efficient batch selection method based on typicality sampling, which decreases the error of gradient estimation in conventional Minibatch SGD. The convergence rate of the resulting algorithm, typical batch SGD, is analyzed and compared to that of Minibatch SGD. Experimental results demonstrate the effectiveness of our batch selection approach, and more complicated Minibatch SGD variations can benefit from it.",1
"Existing imitation learning approaches often require that the complete demonstration data, including sequences of actions and states, are available. In this paper, we consider a more realistic and difficult scenario where a reinforcement learning agent only has access to the state sequences of an expert, while the expert actions are unobserved. We propose a novel tensor-based model to infer the unobserved actions of the expert state sequences. The policy of the agent is then optimized via a hybrid objective combining reinforcement learning and imitation learning. We evaluated our hybrid approach on an illustrative domain and Atari games. The empirical results show that (1) the agents are able to leverage state expert sequences to learn faster than pure reinforcement learning baselines, (2) our tensor-based action inference model is advantageous compared to standard deep neural networks in inferring expert actions, and (3) the hybrid policy optimization objective is robust against noise in expert state sequences.",0
"Many current methods for imitation learning require complete demonstration data, including both action and state sequences. This paper, however, addresses a more challenging and realistic scenario in which a reinforcement learning agent only has access to the state sequences of an expert, with no information about their actions. To overcome this difficulty, a novel tensor-based model is proposed to infer the unobserved actions of the expert state sequences. The agent's policy is then optimized through a hybrid objective that combines reinforcement learning and imitation learning. The effectiveness of this approach is demonstrated through experiments in an illustrative domain and Atari games, which show that the agents can learn faster than pure reinforcement learning baselines by leveraging expert state sequences. Furthermore, the proposed tensor-based action inference model outperforms standard deep neural networks in inferring expert actions, and the hybrid policy optimization objective is robust against noise in expert state sequences.",1
"Many robotic applications require the agent to perform long-horizon tasks in partially observable environments. In such applications, decision making at any step can depend on observations received far in the past. Hence, being able to properly memorize and utilize the long-term history is crucial. In this work, we propose a novel memory-based policy, named Scene Memory Transformer (SMT). The proposed policy embeds and adds each observation to a memory and uses the attention mechanism to exploit spatio-temporal dependencies. This model is generic and can be efficiently trained with reinforcement learning over long episodes. On a range of visual navigation tasks, SMT demonstrates superior performance to existing reactive and memory-based policies by a margin.",0
"Partially observable environments in robotic applications often require agents to perform long-term tasks where decision making depends on past observations. Properly memorizing and utilizing long-term history is vital. The Scene Memory Transformer (SMT), a novel memory-based policy, is proposed in this study. SMT embeds and adds each observation to a memory and utilizes the attention mechanism to exploit spatio-temporal dependencies. This model is generic and efficiently trained with reinforcement learning over long episodes. SMT outperforms existing reactive and memory-based policies by a margin in various visual navigation tasks.",1
"To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.",0
"Research has been focused on developing reliable and secure learning methods to enhance efficiency and minimize malfunctions in autonomous vehicles. The current body of literature on robust reinforcement learning approaches the learning problem as a two-player game between the autonomous system and disturbances. This study compares the performance of two algorithms, namely Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, on an autonomous driving scenario. We expand the game formulation to a semi-competitive environment and demonstrate that the resulting adversary takes into account significant disturbances, resulting in better overall performance. The resulting robust policy displays improved driving efficiency while concurrently reducing collision rates compared to conventional reinforcement learning methods' baseline control policies.",1
"Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.",0
"When the rewards are delayed and sparse, Reinforcement Learning (RL) algorithms may not be efficient in terms of sample usage. To address this issue, we propose a solution that enables agents to learn temporally extended actions at different levels of abstraction, in an automated and efficient manner. Our approach incorporates universal value functions and hindsight learning, which facilitates parallel learning of policies belonging to different time scales. Our study demonstrates that our approach accelerates learning significantly in various continuous and discrete tasks.",1
"We introduce a multi-agent meta-modeling game to generate data, knowledge, and models that make predictions on constitutive responses of elasto-plastic materials. We introduce a new concept from graph theory where a modeler agent is tasked with evaluating all the modeling options recast as a directed multigraph and find the optimal path that links the source of the directed graph (e.g. strain history) to the target (e.g. stress) measured by an objective function. Meanwhile, the data agent, which is tasked with generating data from real or virtual experiments (e.g. molecular dynamics, discrete element simulations), interacts with the modeling agent sequentially and uses reinforcement learning to design new experiments to optimize the prediction capacity. Consequently, this treatment enables us to emulate an idealized scientific collaboration as selections of the optimal choices in a decision tree search done automatically via deep reinforcement learning.",0
"To generate predictions on the constitutive responses of elasto-plastic materials, we propose a multi-agent meta-modeling game. Our approach includes a novel concept from graph theory where a modeler agent evaluates modeling options recast as a directed multigraph. The modeler agent identifies the optimal path that connects the source of the directed graph (e.g. strain history) to the target (e.g. stress) using an objective function. In addition, a data agent generates data from real or virtual experiments (e.g. molecular dynamics, discrete element simulations) and uses reinforcement learning to design new experiments to optimize prediction capacity. By automating the decision tree search process via deep reinforcement learning, our approach emulates an ideal scientific collaboration.",1
"The game of Chinese Checkers is a challenging traditional board game of perfect information that differs from other traditional games in two main aspects: first, unlike Chess, all checkers remain indefinitely in the game and hence the branching factor of the search tree does not decrease as the game progresses; second, unlike Go, there are also no upper bounds on the depth of the search tree since repetitions and backward movements are allowed. Therefore, even in a restricted game instance, the state-space of the game can still be unbounded, making it challenging for a computer program to excel. In this work, we present an approach that effectively combines the use of heuristics, Monte Carlo tree search, and deep reinforcement learning for building a Chinese Checkers agent without the use of any human game-play data. Experiment results show that our agent is competent under different scenarios and reaches the level of experienced human players.",0
"Chinese Checkers is a traditional board game that is challenging and requires perfect information. It differs from other traditional games in two main ways. Firstly, unlike Chess, all checkers remain in the game indefinitely, and the branching factor of the search tree does not decrease as the game progresses. Secondly, unlike Go, there are no upper bounds on the depth of the search tree as repetitions and backward movements are allowed. As a result, the state-space of the game can be unbounded, which makes it difficult for computer programs to excel, even in restricted game instances. In this study, we present an approach that combines the use of heuristics, Monte Carlo tree search, and deep reinforcement learning to build a Chinese Checkers agent without human game-play data. The experiment results show that our agent is competent in different scenarios and can reach the level of experienced human players.",1
"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",0
"In what way can we create reinforcement learning agents that are safe and do not cause unnecessary disruptions in their environment? Our research indicates that current methods of penalizing side effects may actually lead to negative incentives, such as a desire to prevent irreversible changes in the environment, even those caused by other agents. To determine the origin of these undesirable incentives, we divide side effects penalties into two parts: a baseline state and a measurement of deviation from this state. We argue that some of these incentives come from the chosen baseline, while others come from the chosen deviation measurement. To address this issue, we introduce a new version of the stepwise inaction baseline and a new deviation measurement based on the relative accessibility of states. By combining these design choices, we are able to avoid the negative incentives mentioned earlier; however, simpler baselines and the unreachability measurement do not accomplish this. Through a series of gridworld experiments intended to demonstrate possible negative incentives, we demonstrate this empirically by comparing different combinations of baseline and deviation measurement choices.",1
"Nonlinear optimal control problems are often solved with numerical methods that require knowledge of system's dynamics which may be difficult to infer, and that carry a large computational cost associated with iterative calculations. We present a novel neurobiologically inspired hierarchical learning framework, Reinforcement Learning Optimal Control, which operates on two levels of abstraction and utilises a reduced number of controllers to solve nonlinear systems with unknown dynamics in continuous state and action spaces. Our approach is inspired by research at two levels of abstraction: first, at the level of limb coordination human behaviour is explained by linear optimal feedback control theory. Second, in cognitive tasks involving learning symbolic level action selection, humans learn such problems using model-free and model-based reinforcement learning algorithms. We propose that combining these two levels of abstraction leads to a fast global solution of nonlinear control problems using reduced number of controllers. Our framework learns the local task dynamics from naive experience and forms locally optimal infinite horizon Linear Quadratic Regulators which produce continuous low-level control. A top-level reinforcement learner uses the controllers as actions and learns how to best combine them in state space while maximising a long-term reward. A single optimal control objective function drives high-level symbolic learning by providing training signals on desirability of each selected controller. We show that a small number of locally optimal linear controllers are able to solve global nonlinear control problems with unknown dynamics when combined with a reinforcement learner in this hierarchical framework. Our algorithm competes in terms of computational cost and solution quality with sophisticated control algorithms and we illustrate this with solutions to benchmark problems.",0
"To solve nonlinear optimal control problems, numerical methods are often used, which require knowledge of the system's dynamics and entail a high computational cost. However, we propose a new approach called Reinforcement Learning Optimal Control, which is inspired by both limb coordination and cognitive learning research. Our approach operates on two levels of abstraction and utilizes a reduced number of controllers to solve nonlinear systems with unknown dynamics in continuous state and action spaces. We learn the local task dynamics from naive experience and form locally optimal infinite horizon Linear Quadratic Regulators, which produce continuous low-level control. A top-level reinforcement learner uses the controllers as actions and learns how to best combine them in state space while maximizing a long-term reward. By providing training signals on the desirability of each selected controller, a single optimal control objective function drives high-level symbolic learning. We show that our algorithm competes in terms of computational cost and solution quality with sophisticated control algorithms. This is demonstrated through solutions to benchmark problems, which prove that a small number of locally optimal linear controllers can solve global nonlinear control problems with unknown dynamics when combined with a reinforcement learner in this hierarchical framework.",1
"Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.",0
"Tasks in the real world are typically well-organized. Researchers have taken an interest in hierarchical reinforcement learning (HRL) as a way to leverage the hierarchical structure of a given task in reinforcement learning (RL). However, it is not an easy task to identify the hierarchical policy structure that improves the performance of RL. This article proposes an HRL method that learns a hidden variable of a hierarchical policy using mutual information maximization. Our method is a means of learning a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. The gating policy selects option policies based on an option-value function, and these option policies are optimized using the deterministic policy gradient method. This framework is derived by drawing a comparison between a monolithic policy in standard RL and a hierarchical policy in HRL, using a deterministic option policy. Experimental results demonstrate that our HRL approach can learn a range of options and that it can improve the performance of RL in continuous control tasks.",1
"Active vision is inherently attention-driven: The agent actively selects views to attend in order to fast achieve the vision task while improving its internal representation of the scene being observed. Inspired by the recent success of attention-based models in 2D vision tasks based on single RGB images, we propose to address the multi-view depth-based active object recognition using attention mechanism, through developing an end-to-end recurrent 3D attentional network. The architecture takes advantage of a recurrent neural network (RNN) to store and update an internal representation. Our model, trained with 3D shape datasets, is able to iteratively attend to the best views targeting an object of interest for recognizing it. To realize 3D view selection, we derive a 3D spatial transformer network which is differentiable for training with backpropagation, achieving much faster convergence than the reinforcement learning employed by most existing attention-based models. Experiments show that our method, with only depth input, achieves state-of-the-art next-best-view performance in time efficiency and recognition accuracy.",0
"Active vision involves the agent deliberately choosing views to focus on in order to quickly complete the vision task and enhance its understanding of the observed scene. Given the success of attention-based models in 2D vision tasks that rely on single RGB images, we propose a solution for multi-view depth-based active object recognition using attention mechanisms. Our approach involves creating a recurrent 3D attentional network that benefits from a recurrent neural network (RNN) to maintain and update an internal representation. Using 3D shape datasets, our model can repeatedly attend to the best views for recognizing an object of interest. We have developed a differentiable 3D spatial transformer network to enable 3D view selection, which facilitates faster convergence during training than reinforcement learning methods used in existing attention-based models. Our experiments demonstrate that our method, which only requires depth input, achieves superior next-best-view performance in terms of time efficiency and recognition accuracy.",1
"Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",0
"It is challenging to learn understandable and transferable subpolicies and break down complex tasks into simpler ones. Traditional hierarchical reinforcement learning methods enforce top-down decomposition, while meta-learning techniques need a task distribution to learn such breakdowns. This paper introduces a framework that uses various suboptimal world models to divide complex task solutions into more straightforward modular subpolicies. The framework does this automatically, starting from a single source task and concurrently learning the necessary modular subpolicies and a controller to coordinate them. We conduct experiments on high dimensional continuous action control tasks to demonstrate the usefulness of this approach in both complex single task learning and lifelong learning. Finally, we perform ablation studies to comprehend the importance and resilience of different elements in the framework and the limitations of this approach.",1
"This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.",0
"The aim of this study is to explore if acquiring knowledge of contingencies and controllable features of an environment can improve exploration in reinforcement learning. The study is conducted using the Arcade Learning Element (ALE) as the testing platform. An attentive dynamics model (ADM) is developed to identify controllable elements in the observations, such as the character's location in Atari games. The ADM is trained in a self-supervised manner to predict the agent's actions, and the contingency information obtained is used as part of the state representation to facilitate exploration. The combination of actor-critic algorithm with count-based exploration using the ADM representation achieves remarkable results in challenging Atari games with sparse rewards. For instance, the study reports a new high score of >11,000 points in Montezuma's Revenge without expert demonstrations, high-level information or supervisory data. The findings demonstrate that contingency-awareness is a powerful tool in addressing exploration issues in reinforcement learning and suggest further research opportunities.",1
"Efficiently adapting to new environments and changes in dynamics is critical for agents to successfully operate in the real world. Reinforcement learning (RL) based approaches typically rely on external reward feedback for adaptation. However, in many scenarios this reward signal might not be readily available for the target task, or the difference between the environments can be implicit and only observable from the dynamics. To this end, we introduce a method that allows for self-adaptation of learned policies: No-Reward Meta Learning (NoRML). NoRML extends Model Agnostic Meta Learning (MAML) for RL and uses observable dynamics of the environment instead of an explicit reward function in MAML's finetune step. Our method has a more expressive update step than MAML, while maintaining MAML's gradient based foundation. Additionally, in order to allow more targeted exploration, we implement an extension to MAML that effectively disconnects the meta-policy parameters from the fine-tuned policies' parameters. We first study our method on a number of synthetic control problems and then validate our method on common benchmark environments, showing that NoRML outperforms MAML when the dynamics change between tasks.",0
"Being able to adapt to new environments and changes is crucial for agents to operate effectively in the real world. Reinforcement learning (RL) approaches typically require external reward feedback for adaptation, but this reward signal may not always be available or the difference between environments may be implicit. To address this, we present a method called No-Reward Meta Learning (NoRML) which allows for self-adaptation of learned policies. NoRML builds upon Model Agnostic Meta Learning (MAML) for RL, but uses observable dynamics of the environment instead of an explicit reward function during the finetune step. Our method has a more expressive update step and maintains MAML's gradient based foundation. Additionally, we implement an extension to MAML that disconnects the meta-policy parameters from the fine-tuned policies' parameters to allow for more targeted exploration. We validate our method on synthetic control problems and common benchmark environments, demonstrating that NoRML outperforms MAML when the dynamics change between tasks.",1
"Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls). These are still capable of achieving results comparable---and occasionally superior---to state-of-the-art techniques which use two orders of magnitude more neurons.",0
"The concept of deep reinforcement learning involves using neural networks to process images and make decisions simultaneously. However, separating these processes could lead to a better understanding of the complexity of each task and potentially result in smaller policy representations that are easier for humans to comprehend and may generalize better. Our proposal is a new approach to learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. This involves an encoder that generates state representations based on two innovative algorithms. The encoder can grow its dictionary size over time to address new observations as they appear in an open-ended online-learning context. It also encodes observations by disregarding reconstruction error minimization and aiming for the highest information inclusion. The encoder autonomously selects observations online to train on to maximize code sparsity. Using a variation of the Exponential Natural Evolution Strategies algorithm, the system adapts its probability distribution dimensionality along the run. We tested our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons, which achieved comparable or occasionally superior results compared to state-of-the-art techniques that use two orders of magnitude more neurons.",1
"Deep Deterministic Policy Gradient (DDPG) has been proved to be a successful reinforcement learning (RL) algorithm for continuous control tasks. However, DDPG still suffers from data insufficiency and training inefficiency, especially in computationally complex environments. In this paper, we propose Asynchronous Episodic DDPG (AE-DDPG), as an expansion of DDPG, which can achieve more effective learning with less training time required. First, we design a modified scheme for data collection in an asynchronous fashion. Generally, for asynchronous RL algorithms, sample efficiency or/and training stability diminish as the degree of parallelism increases. We consider this problem from the perspectives of both data generation and data utilization. In detail, we re-design experience replay by introducing the idea of episodic control so that the agent can latch on good trajectories rapidly. In addition, we also inject a new type of noise in action space to enrich the exploration behaviors. Experiments demonstrate that our AE-DDPG achieves higher rewards and requires less time consuming than most popular RL algorithms in Learning to Run task which has a computationally complex environment. Not limited to the control tasks in computationally complex environments, AE-DDPG also achieves higher rewards and 2- to 4-fold improvement in sample efficiency on average compared to other variants of DDPG in MuJoCo environments. Furthermore, we verify the effectiveness of each proposed technique component through abundant ablation study.",0
"Although Deep Deterministic Policy Gradient (DDPG) has proven to be an effective reinforcement learning (RL) algorithm for continuous control tasks, it still faces challenges with data insufficiency and training inefficiency, particularly in computationally complex environments. This paper introduces Asynchronous Episodic DDPG (AE-DDPG), an expansion of DDPG that enhances learning efficiency while requiring less training time. The authors propose a modified scheme for data collection in an asynchronous manner, addressing the issue of sample efficiency and training stability as the degree of parallelism increases. The experience replay is redesigned using episodic control to enable the agent to quickly learn from good trajectories, and a new type of noise is introduced in the action space to enrich exploration behaviors. Experiments demonstrate that AE-DDPG outperforms other popular RL algorithms in Learning to Run task and achieves higher rewards and 2- to 4-fold improvement in sample efficiency on average compared to other variants of DDPG in MuJoCo environments. The effectiveness of each proposed technique component is verified through extensive ablation study.",1
"In this empirical paper, we investigate how learning agents can be arranged in more efficient communication topologies for improved learning. This is an important problem because a common technique to improve speed and robustness of learning in deep reinforcement learning and many other machine learning algorithms is to run multiple learning agents in parallel. The standard communication architecture typically involves all agents intermittently communicating with each other (fully connected topology) or with a centralized server (star topology). Unfortunately, optimizing the topology of communication over the space of all possible graphs is a hard problem, so we borrow results from the networked optimization and collective intelligence literatures which suggest that certain families of network topologies can lead to strong improvements over fully-connected networks. We start by introducing alternative network topologies to DRL benchmark tasks under the Evolution Strategies paradigm which we call Network Evolution Strategies. We explore the relative performance of the four main graph families and observe that one such family (Erdos-Renyi random graphs) empirically outperforms all other families, including the de facto fully-connected communication topologies. Additionally, the use of alternative network topologies has a multiplicative performance effect: we observe that when 1000 learning agents are arranged in a carefully designed communication topology, they can compete with 3000 agents arranged in the de facto fully-connected topology. Overall, our work suggests that distributed machine learning algorithms would learn more efficiently if the communication topology between learning agents was optimized.",0
"The aim of our empirical study is to investigate the potential benefits of arranging learning agents in more efficient communication topologies for improved learning. This is particularly relevant as multiple learning agents are frequently used in parallel to enhance the speed and robustness of deep reinforcement learning and other machine learning algorithms. Typically, communication occurs intermittently between all agents (fully connected topology) or with a centralized server (star topology). However, optimizing the topology of communication is a complex task. To address this, we draw on existing research from the networked optimization and collective intelligence fields, which suggest that certain network topologies can lead to significant improvements over fully connected networks. We introduce alternative network topologies to DRL benchmark tasks using Network Evolution Strategies. We observe that one particular family (Erdos-Renyi random graphs) outperforms all others, including the de facto fully connected communication topologies. Furthermore, using alternative network topologies has a multiplicative effect on performance. Our findings highlight the potential benefits of optimizing the communication topology between learning agents in distributed machine learning algorithms.",1
"Building a good predictive model requires an array of activities such as data imputation, feature transformations, estimator selection, hyper-parameter search and ensemble construction. Given the large, complex and heterogenous space of options, off-the-shelf optimization methods are infeasible for realistic response times. In practice, much of the predictive modeling process is conducted by experienced data scientists, who selectively make use of available tools. Over time, they develop an understanding of the behavior of operators, and perform serial decision making under uncertainty, colloquially referred to as educated guesswork. With an unprecedented demand for application of supervised machine learning, there is a call for solutions that automatically search for a good combination of parameters across these tasks to minimize the modeling error. We introduce a novel system called APRL (Autonomous Predictive modeler via Reinforcement Learning), that uses past experience through reinforcement learning to optimize such sequential decision making from within a set of diverse actions under a time constraint on a previously unseen predictive learning problem. APRL actions are taken to optimize the performance of a final ensemble. This is in contrast to other systems, which maximize individual model accuracy first and create ensembles as a disconnected post-processing step. As a result, APRL is able to reduce up to 71\% of classification error on average over a wide variety of problems.",0
"To build an effective predictive model, various tasks such as data imputation, feature transformations, estimator selection, hyper-parameter search, and ensemble construction are required. However, due to the vast and complex array of options, using off-the-shelf optimization methods is impractical for achieving realistic response times. Experienced data scientists usually carry out most of the predictive modeling process, selectively utilizing the available tools and relying on their knowledge of operator behavior to make educated guesses. With the increasing demand for supervised machine learning, there is a need for solutions that automatically search for an optimal combination of parameters to minimize modeling errors. To address this issue, we introduce APRL (Autonomous Predictive modeler via Reinforcement Learning), a novel system that employs reinforcement learning to optimize sequential decision making under time constraints for various tasks while considering a previously unseen predictive learning problem. APRL takes actions to optimize the performance of a final ensemble, rather than focusing on maximizing individual model accuracy and then creating ensembles as a separate post-processing step. As a result, APRL can reduce classification errors by up to 71% on average across a wide range of problems.",1
"Pedestrian detection is one of the most explored topics in computer vision and robotics. The use of deep learning methods allowed the development of new and highly competitive algorithms. Deep Reinforcement Learning has proved to be within the state-of-the-art in terms of both detection in perspective cameras and robotics applications. However, for detection in omnidirectional cameras, the literature is still scarce, mostly because of their high levels of distortion. This paper presents a novel and efficient technique for robust pedestrian detection in omnidirectional images. The proposed method uses deep Reinforcement Learning that takes advantage of the distortion in the image. By considering the 3D bounding boxes and their distorted projections into the image, our method is able to provide the pedestrian's position in the world, in contrast to the image positions provided by most state-of-the-art methods for perspective cameras. Our method avoids the need of pre-processing steps to remove the distortion, which is computationally expensive. Beyond the novel solution, our method compares favorably with the state-of-the-art methodologies that do not consider the underlying distortion for the detection task.",0
"Computer vision and robotics have extensively explored pedestrian detection, with the development of competitive algorithms facilitated by the use of deep learning methods. Deep Reinforcement Learning is a cutting-edge approach in both perspective camera detection and robotics. However, literature on the detection of pedestrians using omnidirectional cameras remains limited due to high levels of distortion. This study introduces a new and efficient technique that leverages deep Reinforcement Learning to detect pedestrians in omnidirectional images. Our method considers the distortion in the image by analyzing the 3D bounding boxes and their projections, enabling us to provide the pedestrian's position in the world. This approach eliminates the need for computationally expensive pre-processing steps to remove the distortion. Compared to state-of-the-art methodologies that do not account for distortion in the detection task, our method performs favorably.",1
"Point of care ultrasound (POCUS) consists in the use of ultrasound imaging in critical or emergency situations to support clinical decisions by healthcare professionals and first responders. In this setting it is essential to be able to provide means to obtain diagnostic data to potentially inexperienced users who did not receive an extensive medical training. Interpretation and acquisition of ultrasound images is not trivial. First, the user needs to find a suitable sound window which can be used to get a clear image, and then he needs to correctly interpret it to perform a diagnosis. Although many recent approaches focus on developing smart ultrasound devices that add interpretation capabilities to existing systems, our goal in this paper is to present a reinforcement learning (RL) strategy which is capable to guide novice users to the correct sonic window and enable them to obtain clinically relevant pictures of the anatomy of interest. We apply our approach to cardiac images acquired from the parasternal long axis (PLAx) view of the left ventricle of the heart.",0
"The utilization of ultrasound imaging in critical or emergency situations by healthcare professionals and first responders is known as Point of Care Ultrasound (POCUS). In such scenarios, it is crucial to provide diagnostic data to untrained users. However, interpreting and acquiring ultrasound images is not a simple task. The user must first locate a suitable sound window to capture a clear image, and then accurately interpret it to make a diagnosis. Although there are efforts to develop smart ultrasound devices that incorporate interpretation capabilities, this paper focuses on presenting a reinforcement learning (RL) strategy. This strategy can guide novice users to the correct sonic window and enable them to obtain clinically relevant pictures of the heart's anatomy. Specifically, we apply our approach to cardiac images acquired from the parasternal long axis (PLAx) view of the left ventricle.",1
"Humans are capable of attributing latent mental contents such as beliefs or intentions to others. The social skill is critical in daily life for reasoning about the potential consequences of others' behaviors so as to plan ahead. It is known that humans use such reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policies, to which each agent finds the best response and then improve their own policies. We develop decentralized-training-decentralized-execution algorithms, namely PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenarios when there exists one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.",0
"The ability of humans to attribute latent mental states, such as beliefs and intentions, to others is crucial for daily life as it allows us to reason about potential consequences and plan ahead. Humans use this reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning, starting from level-1 recursion. Our hypothesis is that it is beneficial for each agent to consider how opponents would react to their future behaviors. We adopt variational Bayes methods to approximate the opponents' conditional policies, allowing each agent to find the best response and improve their own policies. Our decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, are proven to converge in self-play scenarios with one Nash equilibrium. We test our methods on matrix and differential games, where common gradient-based methods fail to converge, and find that it is critical to consider opponents' beliefs about what the agent believes. Our work contributes a new idea for modeling opponents in multi-agent reinforcement learning.",1
"We present a framework, which we call Molecule Deep $Q$-Networks (MolDQN), for molecule optimization by combining domain knowledge of chemistry and state-of-the-art reinforcement learning techniques (double $Q$-learning and randomized value functions). We directly define modifications on molecules, thereby ensuring 100\% chemical validity. Further, we operate without pre-training on any dataset to avoid possible bias from the choice of that set. Inspired by problems faced during medicinal chemistry lead optimization, we extend our model with multi-objective reinforcement learning, which maximizes drug-likeness while maintaining similarity to the original molecule. We further show the path through chemical space to achieve optimization for a molecule to understand how the model works.",0
"Our framework, MolDQN, combines chemical expertise with advanced reinforcement learning techniques (double $Q$-learning and randomized value functions) to optimize molecules while guaranteeing their chemical validity. We achieve this by directly modifying the molecules themselves, without pre-training on any particular dataset to avoid bias. We also address the challenges of medicinal chemistry lead optimization by incorporating multi-objective reinforcement learning to maximize drug-likeness while preserving the original molecule's similarity. Additionally, we demonstrate how our model operates by tracing the chemical space pathway to achieve optimal molecule optimization.",1
"Reinforcement learning (RL) tasks are challenging to implement, execute and test due to algorithmic instability, hyper-parameter sensitivity, and heterogeneous distributed communication patterns. We argue for the separation of logical component composition, backend graph definition, and distributed execution. To this end, we introduce RLgraph, a library for designing and executing reinforcement learning tasks in both static graph and define-by-run paradigms. The resulting implementations are robust, incrementally testable, and yield high performance across different deep learning frameworks and distributed backends.",0
"The implementation, execution, and testing of reinforcement learning (RL) tasks pose a challenge due to algorithmic instability, hyper-parameter sensitivity, and heterogeneous distributed communication patterns. Our suggestion is to separate the logical component composition, backend graph definition, and distributed execution. We present RLgraph, a library that enables the design and execution of reinforcement learning tasks in both static graph and define-by-run paradigms. The resulting implementations are robust, can be tested incrementally, and demonstrate high performance across various deep learning frameworks and distributed backends.",1
"Despite the recent progress in deep reinforcement learning field (RL), and, arguably because of it, a large body of work remains to be done in reproducing and carefully comparing different RL algorithms. We present catalyst.RL, an open source framework for RL research with a focus on reproducibility and flexibility. Main features of our library include large-scale asynchronous distributed training, easy-to-use configuration files with the complete list of hyperparameters for the particular experiments, efficient implementations of various RL algorithms and auxiliary tricks, such as frame stacking, n-step returns, value distributions, etc. To vindicate the usefulness of our framework, we evaluate it on a range of benchmarks in a continuous control, as well as on the task of developing a controller to enable a physiologically-based human model with a prosthetic leg to walk and run. The latter task was introduced at NeurIPS 2018 AI for Prosthetics Challenge, where our team took the 3rd place, capitalizing on the ability of catalyst.RL to train high-quality and sample-efficient RL agents.",0
"Despite recent advancements in the deep reinforcement learning field, there is still a significant amount of work to be done in replicating and comparing various RL algorithms. We introduce catalyst.RL, an open source framework designed for RL research with an emphasis on reproducibility and flexibility. Our library offers large-scale asynchronous distributed training, easy-to-use configuration files with a comprehensive list of experiment-specific hyperparameters, and efficient implementations of various RL algorithms and auxiliary techniques, such as frame stacking, n-step returns, and value distributions. To demonstrate the effectiveness of our framework, we tested it on different benchmarks for continuous control and also used it to develop a controller for a human model with a prosthetic leg to walk and run. Our team secured the 3rd place in the NeurIPS 2018 AI for Prosthetics Challenge, showcasing the ability of catalyst.RL to train high-quality and sample-efficient RL agents.",1
"Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process. The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences. In this work we revisit the fundamentals of discounting in RL and bridge this disconnect by implementing an RL agent that acts via hyperbolic discounting. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL. Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over a strong value-based RL agent, Rainbow.",0
"The traditional method of reinforcement learning (RL) includes a discount factor in the Markov Decision Process, which calculates the value of future rewards through an exponential system and guarantees theoretical convergence of the Bellman equation. However, research in psychology, economics, and neuroscience indicates that humans and animals have hyperbolic time-preferences. To reconcile this discrepancy, we present an RL agent that uses hyperbolic discounting, which we accomplish by revisiting the fundamentals of discounting in RL. Our approach approximates hyperbolic discount functions using familiar temporal-difference learning techniques in RL. Interestingly, we also discover that simultaneously learning value functions over multiple time-horizons is a helpful auxiliary task that often outperforms a strong value-based RL agent, Rainbow, independent of hyperbolic discounting.",1
"Artificial Neural Networks (ANNs) are currently being used as function approximators in many state-of-the-art Reinforcement Learning (RL) algorithms. Spiking Neural Networks (SNNs) have been shown to drastically reduce the energy consumption of ANNs by encoding information in sparse temporal binary spike streams, hence emulating the communication mechanism of biological neurons. Due to their low energy consumption, SNNs are considered to be important candidates as co-processors to be implemented in mobile devices. In this work, the use of SNNs as stochastic policies is explored under an energy-efficient first-to-spike action rule, whereby the action taken by the RL agent is determined by the occurrence of the first spike among the output neurons. A policy gradient-based algorithm is derived considering a Generalized Linear Model (GLM) for spiking neurons. Experimental results demonstrate the capability of online trained SNNs as stochastic policies to gracefully trade energy consumption, as measured by the number of spikes, and control performance. Significant gains are shown as compared to the standard approach of converting an offline trained ANN into an SNN.",0
"Currently, Artificial Neural Networks (ANNs) are being utilized as function approximators in advanced Reinforcement Learning (RL) algorithms. Spiking Neural Networks (SNNs) have proven to substantially decrease ANNs' energy consumption by encoding information into sparse temporal binary spike streams, resembling the biological neurons' communication mechanism. As a result of their low energy consumption, SNNs are deemed significant candidates as co-processors for mobile devices. This study examines the utilization of SNNs as stochastic policies under an energy-efficient first-to-spike action rule, where the RL agent's action is determined by the initial output neuron spike. A policy gradient-based algorithm is developed, taking into account a Generalized Linear Model (GLM) for spiking neurons. Experimental findings indicate that online trained SNNs as stochastic policies can effectively balance energy consumption, measured by the number of spikes, and control performance. In comparison to the standard practice of converting an offline trained ANN into an SNN, significant gains are demonstrated.",1
"Traditional reinforcement learning agents learn from experience, past or present, gained through interaction with their environment. Our approach synthesizes experience, without requiring an agent to interact with their environment, by asking the policy directly ""Are there situations X, Y, and Z, such that in these situations you would select actions A, B, and C?"" In this paper we present Introspection Learning, an algorithm that allows for the asking of these types of questions of neural network policies. Introspection Learning is reinforcement learning algorithm agnostic and the states returned may be used as an indicator of the health of the policy or to shape the policy in a myriad of ways. We demonstrate the usefulness of this algorithm both in the context of speeding up training and improving robustness with respect to safety constraints.",0
"Traditional reinforcement learning agents acquire knowledge from their present or past experiences through interactions with their surroundings. Our approach involves creating experiences without the need for agents to interact with their environment. Instead, we directly ask the policy whether it would select certain actions in specific situations. This method is called Introspection Learning, and it allows for asking these types of questions to neural network policies. The algorithm is independent of the reinforcement learning method used, and the states obtained can be utilized for various purposes, such as assessing the policy's health or shaping it. We demonstrate the algorithm's usefulness in terms of speeding up training and improving the policy's robustness when dealing with safety constraints.",1
"Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.",0
"While reinforcement learning techniques can produce impressive results in simulations, there are two significant challenges when applying them to the real world. Firstly, generating samples is costly, and secondly, proficient but specialized policies may fail when faced with unexpected disturbances or novel situations during testing. As it is unfeasible to train separate policies for all possible scenarios, this study suggests learning how to adapt quickly and effectively to new tasks online. To facilitate sample-efficient learning, the authors propose an approach that utilizes meta-learning to train a dynamics model prior, which, when combined with recent data, can be swiftly adapted to the local context. The research demonstrates online adaptation for continuous control tasks on both simulated and real-world agents. Simulated agents exhibit online adaptation to various terrains, dynamic environments, and damaged body parts. The authors also showcase the significance of incorporating online adaptation into autonomous agents that operate in the real world by applying their method to a dynamic legged millirobot. The robot can adapt quickly to various challenges such as missing limbs, novel terrains and slopes, errors in pose estimation, and payload pulling.",1
"In this paper, the distributed edge caching problem in fog radio access networks (F-RANs) is investigated. By considering the unknown spatio-temporal content popularity and user preference, a user request model based on hidden Markov process is proposed to characterize the fluctuant spatio-temporal traffic demands in F-RANs. Then, the Q-learning method based on the reinforcement learning (RL) framework is put forth to seek the optimal caching policy in a distributed manner, which enables fog access points (F-APs) to learn and track the potential dynamic process without extra communications cost. Furthermore, we propose a more efficient Q-learning method with value function approximation (Q-VFA-learning) to reduce complexity and accelerate convergence. Simulation results show that the performance of our proposed method is superior to those of the traditional methods.",0
"This paper examines the issue of distributed edge caching in fog radio access networks (F-RANs). The authors propose a model based on a hidden Markov process to address the unknown spatio-temporal content popularity and user preference, which characterizes the ever-changing spatio-temporal traffic demands in F-RANs. They then introduce the Q-learning method, based on the reinforcement learning (RL) framework, to determine the optimal caching policy in a distributed manner. This allows fog access points (F-APs) to learn and track the potential dynamic process without incurring any additional communication costs. Additionally, the authors suggest a more efficient Q-learning method with value function approximation (Q-VFA-learning) to reduce complexity and speed up convergence. The simulation results demonstrate that the proposed approach outperforms traditional methods.",1
"We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.",0
"Our focus is on reinforcement learning within input-driven environments, where the dynamics of the system are influenced by an exogenous and stochastic input process. These input processes are prevalent in various scenarios such as robotics control with disturbances, object tracking, and queuing systems. Given that the state dynamics and rewards are affected by the input process, relying solely on the state for expected future returns is insufficient. Standard state-dependent baselines used in policy gradient methods result in high variance during training. To address this issue, we establish an input-dependent baseline that is free of bias and has been proven to perform better than state-dependent baselines. We also propose a meta-learning approach to facilitate the learning of a baseline that is dependent on a long sequence of inputs. Our experimental results demonstrate that input-dependent baselines enhance training stability and lead to better eventual policies across a range of environments such as computer networks, MuJoCo robotic locomotion, and queuing systems.",1
"Q-learning methods represent a commonly used class of algorithms in reinforcement learning: they are generally efficient and simple, and can be combined readily with function approximators for deep reinforcement learning (RL). However, the behavior of Q-learning methods with function approximation is poorly understood, both theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with modern deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.",0
"Q-learning methods are widely used in reinforcement learning due to their simplicity and efficiency, and they can be easily combined with function approximators for deep RL. However, there is a lack of understanding about their behavior with function approximation, both theoretically and empirically. To address this issue, we conducted an experimental investigation using a ""unit testing"" framework that employs oracles to identify sources of error. Our focus was on function approximation, sampling error, and nonstationarity, and we checked if the trends found in oracle settings were consistent with modern deep RL methods. Our results show that large neural network architectures can enhance learning stability, compensate for overfitting, and introduce a novel sampling method that accounts for function approximation error, which leads to significant improvement in high-dimensional continuous control domains.",1
"Autonomous driving is a challenging domain that entails multiple aspects: a vehicle should be able to drive to its destination as fast as possible while avoiding collision, obeying traffic rules and ensuring the comfort of passengers. In this paper, we present a deep learning variant of thresholded lexicographic Q-learning for the task of urban driving. Our multi-objective DQN agent learns to drive on multi-lane roads and intersections, yielding and changing lanes according to traffic rules. We also propose an extension for factored Markov Decision Processes to the DQN architecture that provides auxiliary features for the Q function. This is shown to significantly improve data efficiency. We then show that the learned policy is able to zero-shot transfer to a ring road without sacrificing performance.",0
"The field of autonomous driving poses several challenges, including the need for a vehicle to reach its destination quickly without causing accidents, following traffic laws, and ensuring passengers' comfort. This study introduces a deep learning version of thresholded lexicographic Q-learning for urban driving. Our agent, using multi-objective DQN, learns to navigate multi-lane roads and intersections while adhering to traffic rules, changing lanes, and yielding. We also suggest an addition to the DQN architecture with factored Markov Decision Processes, which improves data efficiency. Our findings demonstrate that the policy learned by the agent can successfully transfer to a ring road without any performance loss.",1
"High-level driving behavior decision-making is an open-challenging problem for connected vehicle technology, especially in heterogeneous traffic scenarios. In this paper, a deep reinforcement learning based high-level driving behavior decision-making approach is proposed for connected vehicle in heterogeneous traffic situations. The model is composed of three main parts: a data preprocessor that maps hybrid data into a data format called hyper-grid matrix, a two-stream deep neural network that extracts the hidden features, and a deep reinforcement learning network that learns the optimal policy. Moreover, a simulation environment, which includes different heterogeneous traffic scenarios, is built to train and test the proposed method. The results demonstrate that the model has the capability to learn the optimal high-level driving policy such as driving fast through heterogeneous traffic without unnecessary lane changes. Furthermore, two separate models are used to compare with the proposed model, and the performances are analyzed in detail.",0
"Connected vehicle technology faces a complex issue in making high-level driving behavior decisions, particularly in scenarios involving diverse traffic. This paper proposes a deep reinforcement learning-based approach for making such decisions in heterogeneous traffic situations. The approach has three main components: a data preprocessor that converts hybrid data to hyper-grid matrix format, a two-stream deep neural network that uncovers hidden features, and a deep reinforcement learning network that learns the best policy. A simulation environment with various traffic scenarios is also developed to train and test the approach. Results show that the model can learn the optimal driving policy, enabling a fast drive through heterogeneous traffic without unnecessary lane changes. Additionally, two other models are compared with the proposed model, and their performances are extensively analyzed.",1
"Actor-critic methods can achieve incredible performance on difficult reinforcement learning problems, but they are also prone to instability. This is partly due to the interaction between the actor and critic during learning, e.g., an inaccurate step taken by one of them might adversely affect the other and destabilize the learning. To avoid such issues, we propose to regularize the learning objective of the actor by penalizing the temporal difference (TD) error of the critic. This improves stability by avoiding large steps in the actor update whenever the critic is highly inaccurate. The resulting method, which we call the TD-regularized actor-critic method, is a simple plug-and-play approach to improve stability and overall performance of the actor-critic methods. Evaluations on standard benchmarks confirm this.",0
"Although actor-critic methods can excel at challenging reinforcement learning tasks, they are susceptible to instability, partially due to the interplay between the actor and critic during learning. If one of them takes an imprecise step, it could harm the other and disrupt the learning process. To circumvent these issues, we suggest regulating the actor's learning objective by penalizing the critic's temporal difference (TD) error. This technique enhances stability by preventing significant changes in the actor update when the critic is notably inaccurate. Our resulting method, dubbed the TD-regularized actor-critic method, is a straightforward, adaptable approach to enhancing the stability and overall performance of actor-critic methods. Standard benchmarks have validated this.",1
"Real-time traffic volume inference is key to an intelligent city. It is a challenging task because accurate traffic volumes on the roads can only be measured at certain locations where sensors are installed. Moreover, the traffic evolves over time due to the influences of weather, events, holidays, etc. Existing solutions to the traffic volume inference problem often rely on dense GPS trajectories, which inevitably fail to account for the vehicles which carry no GPS devices or have them turned off. Consequently, the results are biased to taxicabs because they are almost always online for GPS tracking. In this paper, we propose a novel framework for the citywide traffic volume inference using both dense GPS trajectories and incomplete trajectories captured by camera surveillance systems. Our approach employs a high-fidelity traffic simulator and deep reinforcement learning to recover full vehicle movements from the incomplete trajectories. In order to jointly model the recovered trajectories and dense GPS trajectories, we construct spatiotemporal graphs and use multi-view graph embedding to encode the multi-hop correlations between road segments into real-valued vectors. Finally, we infer the citywide traffic volumes by propagating the traffic values of monitored road segments to the unmonitored ones through masked pairwise similarities. Extensive experiments with two big regions in a provincial capital city in China verify the effectiveness of our approach.",0
"The ability to accurately infer real-time traffic volume is crucial for a smart city, but it is a complex task due to the limited availability of sensors and the dynamic nature of traffic influenced by various factors. Traditional methods rely on GPS data, which is biased towards taxis and does not account for vehicles without GPS devices. In this study, we propose a new approach that combines dense GPS trajectories with incomplete trajectories captured by camera surveillance systems. We use a traffic simulator and deep reinforcement learning to recover missing vehicle movements and construct spatiotemporal graphs to model road segments' correlations. Our approach uses multi-view graph embedding to encode these correlations into real-valued vectors and infer citywide traffic volumes through pairwise similarities. Our experiments in two regions of a Chinese provincial capital city demonstrate the effectiveness of our approach.",1
"Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.",0
"Fundamental to research in chemistry, biology, and social science is the generation of new graph structures that optimize given objectives while adhering to underlying rules. In molecular graph generation, for example, the aim is to discover new molecules with desired properties while following physical laws such as chemical valency. However, designing models to achieve this remains challenging due to the complexity of the rules involved. In this paper, we propose the Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model that uses reinforcement learning for goal-directed graph generation. GCPN is trained to optimize domain-specific rewards and adversarial loss through policy gradient and operates in an environment that includes domain-specific rules. Our experimental results demonstrate that GCPN can improve chemical property optimization by 61% over state-of-the-art baselines while resembling known molecules and can achieve a 184% improvement on the constrained property optimization task.",1
"Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.",0
"For Reinforcement Learning (RL) agents, the difficulty lies in knowing the appropriate action to take when there are multiple options available in each state. The presence of redundant or irrelevant actions exacerbates this challenge. To overcome this, identifying which actions should not be taken is sometimes easier. This study introduces the Action-Elimination Deep Q-Network (AE-DQN) architecture that incorporates an Action Elimination Network (AEN) with a Deep RL algorithm to eliminate sub-optimal actions. The AEN is trained to predict invalid actions using an external elimination signal provided by the environment. Simulations demonstrate that AE-DQN significantly improves both speed and robustness over vanilla DQN in text-based games with over a thousand discrete actions.",1
"Deep hashing methods have received much attention recently, which achieve promising results by taking advantage of the strong representation power of deep networks. However, most existing deep hashing methods learn a whole set of hashing functions independently, while ignore the correlations between different hashing functions that can promote the retrieval accuracy greatly. Inspired by the sequential decision ability of deep reinforcement learning, we propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH). Our proposed DRLIH approach models the hashing learning problem as a sequential decision process, which learns each hashing function by correcting the errors imposed by previous ones and promotes retrieval accuracy. To the best of our knowledge, this is the first work to address hashing problem from deep reinforcement learning perspective. The main contributions of our proposed DRLIH approach can be summarized as follows: (1) We propose a deep reinforcement learning hashing network. In the proposed network, we utilize recurrent neural network (RNN) as agents to model the hashing functions, which take actions of projecting images into binary codes sequentially, so that the current hashing function learning can take previous hashing functions' error into account. (2) We propose a sequential learning strategy based on proposed DRLIH. We define the state as a tuple of internal features of RNN's hidden layers and image features, which can reflect history decisions made by the agents. We also propose an action group method to enhance the correlation of hash functions in the same group. Experiments on three widely-used datasets demonstrate the effectiveness of our proposed DRLIH approach.",0
"Recent attention has been given to deep hashing methods, which utilize the strong representation power of deep networks to achieve promising results. However, existing methods tend to learn a set of hashing functions independently, disregarding the correlations between functions that can greatly improve retrieval accuracy. To address this issue, we propose a novel approach called Deep Reinforcement Learning for Image Hashing (DRLIH). Our DRLIH approach models the hashing learning problem as a sequential decision process, utilizing deep reinforcement learning to correct errors and promote retrieval accuracy. This is the first work to approach hashing from a deep reinforcement learning perspective. Our proposed approach involves a deep reinforcement learning hashing network, which uses recurrent neural network (RNN) agents to model the hashing functions in a sequential manner. This allows previous hashing functions' errors to be taken into account during current learning. We also propose a sequential learning strategy based on DRLIH, defining the state as a tuple of internal features of RNN's hidden layers and image features to reflect agents' history decisions. Additionally, we introduce an action group method to enhance the correlation of hash functions in the same group. Our experiments demonstrate the effectiveness of our proposed DRLIH approach on three widely-used datasets.",1
"The success of popular algorithms for deep reinforcement learning, such as policy-gradients and Q-learning, relies heavily on the availability of an informative reward signal at each timestep of the sequential decision-making process. When rewards are only sparsely available during an episode, or a rewarding feedback is provided only after episode termination, these algorithms perform sub-optimally due to the difficultly in credit assignment. Alternatively, trajectory-based policy optimization methods, such as cross-entropy method and evolution strategies, do not require per-timestep rewards, but have been found to suffer from high sample complexity by completing forgoing the temporal nature of the problem. Improving the efficiency of RL algorithms in real-world problems with sparse or episodic rewards is therefore a pressing need. In this work, we introduce a self-imitation learning algorithm that exploits and explores well in the sparse and episodic reward settings. We view each policy as a state-action visitation distribution and formulate policy optimization as a divergence minimization problem. We show that with Jensen-Shannon divergence, this divergence minimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results indicate that our algorithm works comparable to existing algorithms in environments with dense rewards, and significantly better in environments with sparse and episodic rewards. We then discuss limitations of self-imitation learning, and propose to solve them by using Stein variational policy gradient descent with the Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate its effectiveness on a challenging variant of continuous-control MuJoCo locomotion tasks.",0
"Deep reinforcement learning algorithms, such as policy-gradients and Q-learning, rely heavily on an informative reward signal at each timestep for successful performance. However, when rewards are sparse or only provided after the episode ends, these algorithms struggle due to difficulty in credit assignment. Trajectory-based policy optimization methods, such as cross-entropy and evolution strategies, do not require per-timestep rewards but are limited by high sample complexity. Therefore, there is a need to improve the efficiency of RL algorithms in real-world problems with sparse or episodic rewards. This study introduces a self-imitation learning algorithm that performs well in such settings. The algorithm views each policy as a visitation distribution and optimizes it through divergence minimization, resulting in a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results show that the algorithm works comparably to existing methods in dense reward environments and significantly better in sparse and episodic reward environments. Limitations of self-imitation learning are discussed, and Stein variational policy gradient descent with the Jensen-Shannon kernel is proposed to overcome them by learning multiple diverse policies. The effectiveness of this method is demonstrated on a challenging variant of continuous-control MuJoCo locomotion tasks.",1
"We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. The results show that our proposed method not only learns near-optimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms.",0
"The objective of our study is to acquire knowledge of the reward and policy from the expert examples amidst unknown dynamics. Our suggested technique employs generative adversarial networks and incorporates the empowerment-regularized maximum-entropy inverse reinforcement learning to grasp rewards and policies that are almost optimal. Our empowerment-based regularization ensures that the policy does not overfit to the expert demonstrations, resulting in more generalized behaviors that result in near-optimal rewards. Our approach learns empowerment, reward, and policy simultaneously through variational information maximization within the adversarial learning formulation. We have tested our method on various complex control tasks with high dimensions. Furthermore, we have evaluated our learned rewards on transfer learning problems that pose a challenge due to the differences in the dynamics or structure of the training and testing environments. Our results demonstrate that our proposed approach not only acquires near-optimal rewards and policies that match the expert behavior but also outperforms existing inverse reinforcement learning algorithms.",1
"Unsupervised learning of compact and relevant state representations has been proved very useful at solving complex reinforcement learning tasks. In this paper, we propose a recurrent capsule network that learns such representations by trying to predict the future observations in an agent's trajectory.",0
"The acquisition of concise and pertinent state representations through unsupervised learning has demonstrated its efficacy in tackling intricate reinforcement learning assignments. In this article, we present a recurrent capsule network that obtains these representations by endeavoring to anticipate the future observations in an agent's path.",1
"For the initial shoulder preoperative diagnosis, it is essential to obtain a three-dimensional (3D) bone mask from medical images, e.g., magnetic resonance (MR). However, obtaining high-resolution and dense medical scans is both costly and time-consuming. In addition, the imaging parameters for each 3D scan may vary from time to time and thus increase the variance between images. Therefore, it is practical to consider the bone extraction on low-resolution data which may influence imaging contrast and make the segmentation work difficult. In this paper, we present a joint segmentation for the humerus and scapula bones on a small dataset with low-contrast and high-shape-variability 3D MR images. The proposed network has a deep end-to-end architecture to obtain the initial 3D bone masks. Because the existing scarce and inaccurate human-labeled ground truth, we design a self-reinforced learning strategy to increase performance. By comparing with the non-reinforced segmentation and a classical multi-atlas method with joint label fusion, the proposed approach obtains better results.",0
"Obtaining a 3D bone mask from medical images, such as MR scans, is crucial for the initial shoulder preoperative diagnosis. However, this process is time-consuming and expensive due to the need for high-resolution and dense scans. Moreover, the imaging parameters may vary from scan to scan, leading to increased variance between images. Therefore, it may be necessary to extract bones from low-resolution data, which can affect imaging contrast and make segmentation challenging. In this study, we propose a joint segmentation method for humerus and scapula bones using a small dataset of low-contrast and high-shape-variability 3D MR images. Our deep end-to-end network is designed to obtain initial 3D bone masks, and we use a self-reinforced learning strategy to improve performance due to scarce and inaccurate ground truth data. Our proposed approach outperforms non-reinforced segmentation and classical multi-atlas methods with joint label fusion.",1
"Our research is focused on understanding and applying biological memory transfers to new AI systems that can fundamentally improve their performance, throughout their fielded lifetime experience. We leverage current understanding of biological memory transfer to arrive at AI algorithms for memory consolidation and replay. In this paper, we propose the use of generative memory that can be recalled in batch samples to train a multi-task agent in a pseudo-rehearsal manner. We show results motivating the need for task-agnostic separation of latent space for the generative memory to address issues of catastrophic forgetting in lifelong learning.",0
"The main objective of our research is to gain a comprehensive understanding of biological memory transfers and utilize them to enhance the performance of AI systems throughout their operational lifespan. We utilize our existing knowledge of biological memory transfer to develop AI algorithms focused on memory consolidation and replay. Our proposed solution involves the use of generative memory, which can be efficiently retrieved in batches to train a multi-task agent in a pseudo-rehearsal manner. Our findings demonstrate the importance of separating the latent space for generative memory to mitigate issues related to catastrophic forgetting during lifelong learning.",1
"We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomposed as the combination of some statistical estimator and a method for imputing a return distribution consistent with that set of statistics. With this new understanding, we are able to provide improved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based upon estimation of the expectiles of the return distribution. We compare EDRL with existing methods on a variety of MDPs to illustrate concrete aspects of our analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.",0
"A comprehensive framework is presented in this paper to design and analyze distributional reinforcement learning (DRL) algorithms. The framework involves recursively estimating statistical parameters of the return distribution. The authors suggest that DRL algorithms can be broken down into two parts: a statistical estimator and a method for imputing a return distribution that is consistent with the set of statistics. This new understanding has led to improved analyses of current DRL algorithms and the creation of a new algorithm (EDRL) based on the estimation of expectiles of the return distribution. To demonstrate the effectiveness of the framework, EDRL is compared to existing methods on various MDPs. Additionally, the authors introduce a deep RL variant of the EDRL algorithm, named ER-DQN, which is evaluated on the Atari-57 suite of games.",1
"Since their introduction a year ago, distributional approaches to reinforcement learning (distributional RL) have produced strong results relative to the standard approach which models expected values (expected RL). However, aside from convergence guarantees, there have been few theoretical results investigating the reasons behind the improvements distributional RL provides. In this paper we begin the investigation into this fundamental question by analyzing the differences in the tabular, linear approximation, and non-linear approximation settings. We prove that in many realizations of the tabular and linear approximation settings, distributional RL behaves exactly the same as expected RL. In cases where the two methods behave differently, distributional RL can in fact hurt performance when it does not induce identical behaviour. We then continue with an empirical analysis comparing distributional and expected RL methods in control settings with non-linear approximators to tease apart where the improvements from distributional RL methods are coming from.",0
"Distributional approaches to reinforcement learning (distributional RL) have shown promising results in comparison to the standard approach of modeling expected values (expected RL) since their introduction a year ago. However, despite convergence guarantees, there have been limited theoretical findings that examine the reasons behind the success of distributional RL. This paper addresses this fundamental question by analyzing differences in tabular, linear approximation, and non-linear approximation settings. Our study reveals that distributional RL and expected RL behave identically in many tabular and linear approximation scenarios. However, in cases where distributional RL does not induce identical behavior, it can negatively impact performance. We also conducted an empirical analysis of distributional and expected RL in control settings with non-linear approximators to identify where the improvements from distributional RL arise.",1
"Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.",0
"Though reinforcement learning shows promise for solving control problems, its practical use is hindered by the difficulty of engineering reward functions. Establishing objectives and tasks for autonomous machines is challenging, with reward functions and goal states traditionally used to communicate objectives. However, humans can communicate objectives by merely describing or demonstrating them. This raises the question of how to create learning algorithms that allow us to instruct machines on what we want them to achieve. This study delves into the issue of grounding language commands as reward functions using inverse reinforcement learning. It argues that language-conditioned rewards are more transferable than language-conditioned policies to new environments. The authors propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. The model learns rewards that can transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands. In contrast, directly learning a language-conditioned policy results in poor performance.",1
"Deep Reinforcement Learning has shown great success in a variety of control tasks. However, it is unclear how close we are to the vision of putting Deep RL into practice to solve real world problems. In particular, common practice in the field is to train policies on largely deterministic simulators and to evaluate algorithms through training performance alone, without a train/test distinction to ensure models generalise and are not overfitted. Moreover, it is not standard practice to check for generalisation under domain shift, although robustness to such system change between training and testing would be necessary for real-world Deep RL control, for example, in robotics. In this paper we study these issues by first characterising the sources of uncertainty that provide generalisation challenges in Deep RL. We then provide a new benchmark and thorough empirical evaluation of generalisation challenges for state of the art Deep RL methods. In particular, we show that, if generalisation is the goal, then common practice of evaluating algorithms based on their training performance leads to the wrong conclusions about algorithm choice. Finally, we evaluate several techniques for improving generalisation and draw conclusions about the most robust techniques to date.",0
"The success of Deep Reinforcement Learning in various control tasks is evident, but its practical application to solve real-world problems remains uncertain. The current practice in the field involves training policies on deterministic simulators and evaluating algorithms solely based on training performance, without a train/test distinction to ensure models' generalization and prevent overfitting. Additionally, there is no standard practice to test for generalization under domain shift, which is crucial in real-world Deep RL control, such as robotics. This research aims to address these issues by identifying the sources of uncertainty that pose generalization challenges in Deep RL and providing a new benchmark for empirical evaluation of state-of-the-art Deep RL methods. The study shows that evaluating algorithms based on training performance leads to incorrect conclusions about algorithm choice if generalization is the goal. Finally, the research evaluates various techniques for improving generalization and identifies the most robust techniques.",1
"Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.",0
"Reinforcement learning can use Robust MDPs (RMDPs) to generate policies that have guaranteed worst-case outcomes. The reliability and strength of an RMDP solution depend on the ambiguity set, which represents the possible transition probabilities and is typically a multi-dimensional confidence region. However, current methods construct ambiguity sets through concentration inequalities, which can result in excessively cautious solutions. This study proposes a new approach that can achieve the same level of robustness without relying on confidence regions as ambiguity sets. Our algorithms optimize ambiguity sets by leveraging prior knowledge through Bayesian inference. The proposed method is theoretically sound and shows practical potential through empirical results.",1
"A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency.",0
"To pursue different goals in various episodes, a reinforcement learning agent requires a goal-conditional policy. Such policies have the potential to generalize desirable behavior to unknown goals and allow higher-level planning based on subgoals. In sparse-reward settings, the ability to exploit information on the extent to which an arbitrary goal has been achieved while another goal was intended is crucial for efficient learning. However, incorporating such hindsight capacity in reinforcement learning agents is recent. This paper demonstrates how hindsight can be introduced to policy gradient methods, which can be generalized to a wide range of successful algorithms. The experiments conducted on various sparse-reward environments illustrate that hindsight can significantly increase sample efficiency.",1
"Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.",0
"Significant advancements in knowledge understanding and representation learning can be achieved by developing agents that can interact with the web. However, existing deep reinforcement learning (RL) models struggle with web navigation tasks due to the vast discrete action space and varying number of actions between states. To overcome these challenges, we present DOM-Q-NET, a new RL-based web navigation architecture. Our approach leverages separate networks to parameterize Q functions for different action categories, namely, clicking a DOM element and typing a string input. Additionally, we utilize a graph neural network to represent the tree-structured HTML of a standard web page. Our model achieves impressive results on the MiniWoB environment, matching or exceeding existing work without the need for expert demonstrations. Moreover, we demonstrate that our model's sample efficiency is improved twofold when training in the multi-task setting, enabling it to transfer learned behaviours across tasks.",1
"Thompson sampling (TS) is a class of algorithms for sequential decision-making, which requires maintaining a posterior distribution over a model. However, calculating exact posterior distributions is intractable for all but the simplest models. Consequently, efficient computation of an approximate posterior distribution is a crucial problem for scalable TS with complex models, such as neural networks. In this paper, we use distribution optimization techniques to approximate the posterior distribution, solved via Wasserstein gradient flows. Based on the framework, a principled particle-optimization algorithm is developed for TS to approximate the posterior efficiently. Our approach is scalable and does not make explicit distribution assumptions on posterior approximations. Extensive experiments on both synthetic data and real large-scale data demonstrate the superior performance of the proposed methods.",0
"Sequential decision-making using Thompson sampling (TS) involves maintaining a posterior distribution over a model, but exact calculations of this distribution are only feasible for simple models. To address this issue for TS with complex models, such as neural networks, approximating the posterior distribution efficiently is crucial. In this study, we utilize distribution optimization techniques and solve through Wasserstein gradient flows to approximate the posterior distribution. Our particle-optimization algorithm for TS is developed based on this framework, which is scalable and does not require explicit assumptions on posterior approximations. Extensive experiments, including synthetic data and large-scale real data, demonstrate the superior performance of our proposed methods.",1
"Deep reinforcement learning (DRL) has gained a lot of attention in recent years, and has been proven to be able to play Atari games and Go at or above human levels. However, those games are assumed to have a small fixed number of actions and could be trained with a simple CNN network. In this paper, we study a special class of Asian popular card games called Dou Di Zhu, in which two adversarial groups of agents must consider numerous card combinations at each time step, leading to huge number of actions. We propose a novel method to handle combinatorial actions, which we call combinational Q-learning (CQL). We employ a two-stage network to reduce action space and also leverage order-invariant max-pooling operations to extract relationships between primitive actions. Results show that our method prevails over state-of-the art methods like naive Q-learning and A3C. We develop an easy-to-use card game environments and train all agents adversarially from sractch, with only knowledge of game rules and verify that our agents are comparative to humans. Our code to reproduce all reported results will be available online.",0
"Recently, Deep reinforcement learning (DRL) has received considerable attention for its ability to perform at or above human levels in games such as Atari and Go. However, these games have a limited number of actions that can be trained using a simple CNN network. In this study, we investigated a particular type of popular Asian card game known as Dou Di Zhu, which requires two opposing groups of agents to consider numerous card combinations at each time step, leading to a vast number of actions. To address this issue, we proposed a new approach called combinational Q-learning (CQL) to handle combinational actions. We utilized a two-stage network to reduce the action space and order-invariant max-pooling operations to extract the relationships between primitive actions. Our results demonstrate that our method outperforms other state-of-the-art techniques, such as naive Q-learning and A3C. We developed a user-friendly card game environment and trained all agents adversarially from scratch with only knowledge of game rules, and our agents were shown to be comparable to humans. Our code to reproduce all reported results will be available online.",1
"We present a method for fast training of vision based control policies on real robots. The key idea behind our method is to perform multi-task Reinforcement Learning with auxiliary tasks that differ not only in the reward to be optimized but also in the state-space in which they operate. In particular, we allow auxiliary task policies to utilize task features that are available only at training-time. This allows for fast learning of auxiliary policies, which subsequently generate good data for training the main, vision-based control policies. This method can be seen as an extension of the Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of our method by using both a simulated and real-world Ball-in-a-Cup game controlled by a robot arm. In simulation, our approach leads to significant learning speed-ups when compared to standard SAC-X. On the real robot we show that the task can be learned from-scratch, i.e., with no transfer from simulation and no imitation learning. Videos of our learned policies running on the real robot can be found at https://sites.google.com/view/rss-2019-sawyer-bic/.",0
"Our approach introduces a technique for quickly training vision-based control policies on physical robots. We achieve this by employing multi-task Reinforcement Learning with auxiliary tasks that vary in both the reward and state-space domains. Additionally, our method allows for the use of task features that are only available during the training phase, which leads to rapid learning of auxiliary policies. These policies then generate high-quality data for training the primary vision-based control policies. Our method is an expansion of the Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the effectiveness of our approach by applying it to a Ball-in-a-Cup game controlled by a robot arm in both a simulated and real-world environment. Our results show significant learning speed enhancements in comparison to standard SAC-X in the simulation. On the physical robot, we prove that the task can be learned from scratch without any transfer from simulation or imitation learning. Videos of our trained policies running on the real robot are available at https://sites.google.com/view/rss-2019-sawyer-bic/.",1
"The goal of task transfer in reinforcement learning is migrating the action policy of an agent to the target task from the source task. Given their successes on robotic action planning, current methods mostly rely on two requirements: exactly-relevant expert demonstrations or the explicitly-coded cost function on target task, both of which, however, are inconvenient to obtain in practice. In this paper, we relax these two strong conditions by developing a novel task transfer framework where the expert preference is applied as a guidance. In particular, we alternate the following two steps: Firstly, letting experts apply pre-defined preference rules to select related expert demonstrates for the target task. Secondly, based on the selection result, we learn the target cost function and trajectory distribution simultaneously via enhanced Adversarial MaxEnt IRL and generate more trajectories by the learned target distribution for the next preference selection. The theoretical analysis on the distribution learning and convergence of the proposed algorithm are provided. Extensive simulations on several benchmarks have been conducted for further verifying the effectiveness of the proposed method.",0
"The aim of task transfer in reinforcement learning involves moving the action policy of an agent from the source task to the target task. Current methods, which have demonstrated success in robotic action planning, typically require either expert demonstrations or an explicitly-coded cost function for the target task. Unfortunately, these requirements are challenging to acquire in practical settings. To address this issue, we present a new task transfer framework that employs expert preference as guidance. Our approach involves two steps: 1) experts use predetermined preference rules to select relevant demonstrations for the target task, and 2) we learn the target cost function and trajectory distribution simultaneously using an enhanced Adversarial MaxEnt IRL method based on the selected demonstrations. We then use the learned target distribution to generate additional trajectories for the next preference selection. We provide theoretical analysis of the algorithm's convergence and distribution learning, and conduct extensive simulations on several benchmarks to demonstrate the effectiveness of our method.",1
"In this paper, we propose a new learning technique named message-dropout to improve the performance for multi-agent deep reinforcement learning under two application scenarios: 1) classical multi-agent reinforcement learning with direct message communication among agents and 2) centralized training with decentralized execution. In the first application scenario of multi-agent systems in which direct message communication among agents is allowed, the message-dropout technique drops out the received messages from other agents in a block-wise manner with a certain probability in the training phase and compensates for this effect by multiplying the weights of the dropped-out block units with a correction probability. The applied message-dropout technique effectively handles the increased input dimension in multi-agent reinforcement learning with communication and makes learning robust against communication errors in the execution phase. In the second application scenario of centralized training with decentralized execution, we particularly consider the application of the proposed message-dropout to Multi-Agent Deep Deterministic Policy Gradient (MADDPG), which uses a centralized critic to train a decentralized actor for each agent. We evaluate the proposed message-dropout technique for several games, and numerical results show that the proposed message-dropout technique with proper dropout rate improves the reinforcement learning performance significantly in terms of the training speed and the steady-state performance in the execution phase.",0
"This paper introduces a novel learning technique called ""message-dropout"" that enhances the performance of multi-agent deep reinforcement learning in two different scenarios. Firstly, in the classical multi-agent reinforcement learning scenario where agents communicate directly with each other, the message-dropout technique randomly drops out received messages in a block-wise manner during training and compensates for this by adjusting weights of the dropped-out block units with a correction probability. This technique handles the increased input dimension in multi-agent reinforcement learning with communication and makes learning more robust against communication errors during execution. Secondly, in the scenario of centralized training with decentralized execution, the proposed message-dropout technique is applied to Multi-Agent Deep Deterministic Policy Gradient (MADDPG), which uses a centralized critic to train a decentralized actor for each agent. The proposed technique is evaluated for several games, and the results show that the performance significantly improves in terms of training speed and steady-state performance in the execution phase with proper dropout rate.",1
"Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks. We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors. We observe that in these cases such zero-mean parameterization leads to a much better training objective than conventional parameterizations where the mean is being learned.",0
"Ordinary stochastic neural networks utilize the expected values of their weights to make predictions and use induced noise to capture uncertainty, prevent overfitting, and slightly enhance performance through test-time averaging. This paper introduces variance layers, a distinct kind of stochastic layer where each weight follows a zero-mean distribution and is only parameterized by its variance. It is demonstrated that these layers can learn effectively, function as an efficient exploration tool in reinforcement learning, and serve as a decent defense against adversarial attacks. Additionally, conventional Bayesian neural networks naturally converge to these zero-mean posteriors, which leads to a much better training objective than traditional parameterizations where the mean is being learned.",1
"In many real-world learning scenarios, features are only acquirable at a cost constrained under a budget. In this paper, we propose a novel approach for cost-sensitive feature acquisition at the prediction-time. The suggested method acquires features incrementally based on a context-aware feature-value function. We formulate the problem in the reinforcement learning paradigm, and introduce a reward function based on the utility of each feature. Specifically, MC dropout sampling is used to measure expected variations of the model uncertainty which is used as a feature-value function. Furthermore, we suggest sharing representations between the class predictor and value function estimator networks. The suggested approach is completely online and is readily applicable to stream learning setups. The solution is evaluated on three different datasets including the well-known MNIST dataset as a benchmark as well as two cost-sensitive datasets: Yahoo Learning to Rank and a dataset in the medical domain for diabetes classification. According to the results, the proposed method is able to efficiently acquire features and make accurate predictions.",0
"In numerous instances of real-life learning, obtaining features incurs a cost that must be constrained within a budget. This research introduces a new technique for acquiring cost-sensitive features during prediction time. This approach involves acquiring features incrementally using a context-aware feature-value function. The problem is framed within the reinforcement learning paradigm, with a reward function based on the usefulness of each feature. The method employs MC dropout sampling to calculate the expected variations of model uncertainty, which serves as the feature-value function. Additionally, the study proposes sharing representations between the class predictor and value function estimator networks. This online approach is easily applied to stream learning setups. The technique is evaluated on three datasets, including the MNIST dataset for benchmarking, as well as two cost-sensitive datasets: Yahoo Learning to Rank and a medical dataset for diabetes classification. The results demonstrate that the proposed method is effective in acquiring features and producing accurate predictions.",1
"Finite-horizon lookahead policies are abundantly used in Reinforcement Learning and demonstrate impressive empirical success. Usually, the lookahead policies are implemented with specific planning methods such as Monte Carlo Tree Search (e.g. in AlphaZero). Referring to the planning problem as tree search, a reasonable practice in these implementations is to back up the value only at the leaves while the information obtained at the root is not leveraged other than for updating the policy. Here, we question the potency of this approach. Namely, the latter procedure is non-contractive in general, and its convergence is not guaranteed. Our proposed enhancement is straightforward and simple: use the return from the optimal tree path to back up the values at the descendants of the root. This leads to a $\gamma^h$-contracting procedure, where $\gamma$ is the discount factor and $h$ is the tree depth. To establish our results, we first introduce a notion called \emph{multiple-step greedy consistency}. We then provide convergence rates for two algorithmic instantiations of the above enhancement in the presence of noise injected to both the tree search stage and value estimation stage.",0
"Reinforcement Learning commonly employs finite-horizon lookahead policies, which have proven to be highly effective in practice. These policies are typically implemented using planning methods such as Monte Carlo Tree Search, as seen in AlphaZero. However, the conventional approach of backing up the value solely at the leaves and disregarding the information obtained at the root except for policy updates is not always effective. This method lacks the guarantee of convergence and is non-contractive. To overcome this, we suggest a simple alternative: using the return from the optimal tree path to back up the values at the descendants of the root, resulting in a $\gamma^h$-contracting approach. We introduce the concept of multiple-step greedy consistency and establish convergence rates for two algorithmic implementations of this approach in the presence of noise during both the tree search stage and value estimation stage.",1
"Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided. However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization. This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required. It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals. We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents. Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum. We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm. Each task provides only binary rewards indicating whether or not the goal is achieved. Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration. Extensive experiments demonstrate that this method leads to faster converge and improved task performance.",0
"Although deep learning has achieved impressive results in solving challenging reinforcement learning (RL) problems that have a dense reward function, it still faces difficulties in sparse reward environments where the reward function must be carefully shaped to guide policy optimization. This limitation reduces the applicability of RL in the real world, where both reinforcement learning and domain-specific knowledge are required. Therefore, it is crucial to develop algorithms that can learn from a binary signal that indicates a successful task completion or other unshaped, sparse reward signals. Our proposed method, competitive experience replay, efficiently supplements sparse rewards by placing learning in the context of an exploration competition between a pair of agents. This approach complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum. We evaluated our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm, both of which provide only binary rewards indicating goal achievement. Our method asymmetrically augments these sparse rewards for a pair of agents learning the same task, creating a competitive game designed to drive exploration. Extensive experiments demonstrate that this method leads to faster convergence and improved task performance.",1
"When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture on-line and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice.",0
"When dealing with large state spaces in reinforcement learning (RL) algorithms, it is typical to use an approximation architecture for the value function (VF). However, the choice of architecture can greatly impact an agent's performance, and selecting an appropriate one can be a difficult task. As a result, researchers are interested in the possibility of RL algorithms adaptively generating approximation architectures. One promising method involves using feedback on an agent's state visit frequency to guide the level of approximation in different areas of the state space. In this article, we will explore the benefits of this approach, introduce a new algorithm that adapts a state aggregation approximation architecture online and is intended for use with SARSA, provide theoretical results on its complexity, convergence properties, and ability to reduce VF error in policy evaluation, and finally, experimentally test its effectiveness on a variety of problems. Our findings suggest that this algorithm (and similar methods) can significantly enhance RL performance in common practical scenarios, while remaining versatile and computationally efficient.",1
"Many complex domains, such as robotics control and real-time strategy (RTS) games, require an agent to learn a continuous control. In the former, an agent learns a policy over $\mathbb{R}^d$ and in the latter, over a discrete set of actions each of which is parametrized by a continuous parameter. Such problems are naturally solved using policy based reinforcement learning (RL) methods, but unfortunately these often suffer from high variance leading to instability and slow convergence. Unnecessary variance is introduced whenever policies over bounded action spaces are modeled using distributions with unbounded support by applying a transformation $T$ to the sampled action before execution in the environment. Recently, the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, but to date no variance reduced methods exist when the action is a direction, something often seen in RTS games. To this end we introduce the angular policy gradient (APG), a stochastic policy gradient method for directional control. With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task show that the APG estimator offers a substantial improvement over the standard policy gradient.",0
"Learning a continuous control is necessary in complex domains like robotics control and real-time strategy (RTS) games. The former involves an agent learning a policy over $\mathbb{R}^d$, while the latter involves learning a policy over a discrete set of actions with continuous parameters. Policy-based reinforcement learning (RL) methods are typically used to solve such problems, but they often suffer from high variance, leading to instability and slow convergence. This is because policies over bounded action spaces are modeled using distributions with unbounded support, which introduces unnecessary variance when a transformation $T$ is applied to the sampled action before execution in the environment. While the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, no variance reduced methods exist for directional control, which is often seen in RTS games. To address this, we propose the angular policy gradient (APG), a stochastic policy gradient method for directional control. Our analysis using the marginal policy gradients family of estimators shows that APG offers a stronger guarantee for variance reduction than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task demonstrate that APG estimator significantly outperforms the standard policy gradient.",1
"We present the first model-free Reinforcement Learning (RL) algorithm to synthesise policies for an unknown Markov Decision Process (MDP), such that a linear time property is satisfied. The given temporal property is converted into a Limit Deterministic Buchi Automaton (LDBA) and a robust reward function is defined over the state-action pairs of the MDP according to the resulting LDBA. With this reward function, the policy synthesis procedure is ""constrained"" by the given specification. These constraints guide the MDP exploration so as to minimize the solution time by only considering the portion of the MDP that is relevant to satisfaction of the LTL property. This improves performance and scalability of the proposed method by avoiding an exhaustive update over the whole state space while the efficiency of standard methods such as dynamic programming is hindered by excessive memory requirements, caused by the need to store a full-model in memory. Additionally, we show that the RL procedure sets up a local value iteration method to efficiently calculate the maximum probability of satisfying the given property, at any given state of the MDP. We prove that our algorithm is guaranteed to find a policy whose traces probabilistically satisfy the LTL property if such a policy exists, and additionally we show that our method produces reasonable control policies even when the LTL property cannot be satisfied. The performance of the algorithm is evaluated via a set of numerical examples. We observe an improvement of one order of magnitude in the number of iterations required for the synthesis compared to existing approaches.",0
"Our study presents a novel Reinforcement Learning (RL) algorithm that can synthesize policies for an unknown Markov Decision Process (MDP) without the need for a model. The algorithm ensures that a linear time property is satisfied by converting the given temporal property into a Limit Deterministic Buchi Automaton (LDBA) and defining a robust reward function over the state-action pairs of the MDP based on the resulting LDBA. This reward function ""constrains"" the policy synthesis process by guiding MDP exploration to focus only on the relevant portion of the MDP that satisfies the LTL property. This approach enhances performance and scalability by avoiding exhaustive updates of the entire state space, unlike standard methods such as dynamic programming that require excessive memory requirements. Moreover, we demonstrate that the RL procedure enables a local value iteration method to efficiently calculate the maximum probability of satisfying the given property at any state of the MDP. Our algorithm is guaranteed to find a policy that satisfies the LTL property, if such a policy exists, and produces reasonable control policies even when the LTL property cannot be satisfied. We evaluate the algorithm's performance using numerical examples and observe a significant improvement in the number of iterations required for synthesis compared to existing approaches.",1
"Hierarchical reinforcement learning deals with the problem of breaking down large tasks into meaningful sub-tasks. Autonomous discovery of these sub-tasks has remained a challenging problem. We propose a novel method of learning sub-tasks by combining paradigms of routing in computer networks and graph based skill discovery within the options framework to define meaningful sub-goals. We apply the recent advancements of learning embeddings using Riemannian optimisation in the hyperbolic space to embed the state set into the hyperbolic space and create a model of the environment. In doing so we enforce a global topology on the states and are able to exploit this topology to learn meaningful sub-tasks. We demonstrate empirically, both in discrete and continuous domains, how these embeddings can improve the learning of meaningful sub-tasks.",0
"The main concern of hierarchical reinforcement learning is how to divide complex tasks into smaller, more manageable sub-tasks. It has been a difficult challenge to autonomously identify these sub-tasks. To address this issue, we propose a new approach that combines the concepts of routing in computer networks and graph-based skill discovery within the options framework to establish significant sub-goals. We utilize the latest developments in Riemannian optimization to embed the state set into the hyperbolic space and create an environment model. This allows us to implement a global topology on the states and utilize it to learn meaningful sub-tasks. We provide empirical evidence, in both discrete and continuous domains, of how these embeddings improve the learning of significant sub-tasks.",1
"In real-world scenarios, the observation data for reinforcement learning with continuous control is commonly noisy and part of it may be dynamically missing over time, which violates the assumption of many current methods developed for this. We addressed the issue within the framework of partially observable Markov Decision Process (POMDP) using a model-based method, in which the transition model is estimated from the incomplete and noisy observations using a newly proposed surrogate loss function with local approximation, while the policy and value function is learned with the help of belief imputation. For the latter purpose, a generative model is constructed and is seamlessly incorporated into the belief updating procedure of POMDP, which enables robust execution even under a significant incompleteness and noise. The effectiveness of the proposed method is verified on a collection of benchmark tasks, showing that our approach outperforms several compared methods under various challenging scenarios.",0
"Many current methods developed for reinforcement learning with continuous control assume that observation data is complete and noise-free, but in real-world scenarios, this is often not the case. Dynamic missing data can be a common occurrence, which can be problematic. To address this issue, we used a partially observable Markov decision process (POMDP) framework and a model-based method. We estimated the transition model from incomplete and noisy observations using a newly proposed surrogate loss function with local approximation. We learned the policy and value function using belief imputation and constructed a generative model to incorporate belief updating. This approach allows for robust execution even with significant incompleteness and noise. Our method was tested on a collection of benchmark tasks and was found to outperform several other methods under various challenging scenarios.",1
"We propose the use of Bayesian networks, which provide both a mean value and an uncertainty estimate as output, to enhance the safety of learned control policies under circumstances in which a test-time input differs significantly from the training set. Our algorithm combines reinforcement learning and end-to-end imitation learning to simultaneously learn a control policy as well as a threshold over the predictive uncertainty of the learned model, with no hand-tuning required. Corrective action, such as a return of control to the model predictive controller or human expert, is taken when the uncertainty threshold is exceeded. We validate our method on fully-observable and vision-based partially-observable systems using cart-pole and autonomous driving simulations using deep convolutional Bayesian neural networks. We demonstrate that our method is robust to uncertainty resulting from varying system dynamics as well as from partial state observability.",0
"To enhance the safety of learned control policies when a test-time input differs significantly from the training set, we suggest utilizing Bayesian networks. These networks provide both a mean value and an uncertainty estimate as output. Our algorithm uses a combination of reinforcement learning and end-to-end imitation learning to learn a control policy and a threshold over the predictive uncertainty of the learned model without any hand-tuning requirements. When the uncertainty threshold is exceeded, corrective action is taken, such as returning control to the model predictive controller or human expert. We validate our approach on fully-observable and vision-based partially-observable systems, including cart-pole and autonomous driving simulations, using deep convolutional Bayesian neural networks. Our method is robust to uncertainty caused by varying system dynamics and partial state observability.",1
"As the most successful variant and improvement for Trust Region Policy Optimization (TRPO), proximal policy optimization (PPO) has been widely applied across various domains with several advantages: efficient data utilization, easy implementation, and good parallelism. In this paper, a first-order gradient reinforcement learning algorithm called Policy Optimization with Penalized Point Probability Distance (POP3D), which is a lower bound to the square of total variance divergence is proposed as another powerful variant. Firstly, we talk about the shortcomings of several commonly used algorithms, by which our method is partly motivated. Secondly, we address to overcome these shortcomings by applying POP3D. Thirdly, we dive into its mechanism from the perspective of solution manifold. Finally, we make quantitative comparisons among several state-of-the-art algorithms based on common benchmarks. Simulation results show that POP3D is highly competitive compared with PPO. Besides, our code is released in https://github.com/paperwithcode/pop3d.",0
"Proximal policy optimization (PPO) has become the most successful and improved version of Trust Region Policy Optimization (TRPO) due to its efficient data utilization, ease of implementation, and good parallelism across various domains. This paper presents another powerful variant called Policy Optimization with Penalized Point Probability Distance (POP3D), a first-order gradient reinforcement learning algorithm that offers a lower bound to the square of total variance divergence. We first discuss the limitations of commonly used algorithms that partly motivated our approach. Then, we demonstrate how POP3D overcomes these limitations. We also provide insight into POP3D's mechanism from the perspective of the solution manifold. Finally, we showcase quantitative comparisons of several state-of-the-art algorithms based on common benchmarks. Our simulation results show that POP3D is highly competitive compared to PPO. Additionally, the code for POP3D is available on https://github.com/paperwithcode/pop3d.",1
"Employing one or more additional classifiers to break the self-learning loop in tracing-by-detection has gained considerable attention. Most of such trackers merely utilize the redundancy to address the accumulating label error in the tracking loop, and suffer from high computational complexity as well as tracking challenges that may interrupt all classifiers (e.g. temporal occlusions). We propose the active co-tracking framework, in which the main classifier of the tracker labels samples of the video sequence, and only consults auxiliary classifier when it is uncertain. Based on the source of the uncertainty and the differences of two classifiers (e.g. accuracy, speed, update frequency, etc.), different policies should be taken to exchange the information between two classifiers. Here, we introduce a reinforcement learning approach to find the appropriate policy by considering the state of the tracker in a specific sequence. The proposed method yields promising results in comparison to the best tracking-by-detection approaches.",0
"The use of additional classifiers to prevent the self-learning loop in tracing-by-detection has become popular. However, most of these trackers have high computational complexity and tracking difficulties, such as temporal occlusions. To address this, we present the active co-tracking framework, wherein the main classifier labels samples of the video sequence and only seeks the assistance of the auxiliary classifier when there is uncertainty. Depending on the cause of the uncertainty and the discrepancies between the classifiers (e.g. accuracy, speed, update frequency), different policies should be implemented to share information between them. To determine the appropriate policy for a specific sequence, we propose a reinforcement learning approach that considers the tracker's state. Our method shows promising results compared to the top tracking-by-detection approaches.",1
"We propose a new policy iteration theory as an important extension of soft policy iteration and Soft Actor-Critic (SAC), one of the most efficient model free algorithms for deep reinforcement learning. Supported by the new theory, arbitrary entropy measures that generalize Shannon entropy, such as Tsallis entropy and Renyi entropy, can be utilized to properly randomize action selection while fulfilling the goal of maximizing expected long-term rewards. Our theory gives birth to two new algorithms, i.e., Tsallis entropy Actor-Critic (TAC) and Renyi entropy Actor-Critic (RAC). Theoretical analysis shows that these algorithms can be more effective than SAC. Moreover, they pave the way for us to develop a new Ensemble Actor-Critic (EAC) algorithm in this paper that features the use of a bootstrap mechanism for deep environment exploration as well as a new value-function based mechanism for high-level action selection. Empirically we show that TAC, RAC and EAC can achieve state-of-the-art performance on a range of benchmark control tasks, outperforming SAC and several cutting-edge learning algorithms in terms of both sample efficiency and effectiveness.",0
"We introduce a novel policy iteration theory that expands upon soft policy iteration and Soft Actor-Critic (SAC), which is a highly efficient model-free algorithm for deep reinforcement learning. With the support of this new theory, we can utilize arbitrary entropy measures that go beyond Shannon entropy, such as Tsallis entropy and Renyi entropy, to randomize action selection in a way that maximizes expected long-term rewards. Our theory has led to the development of two new algorithms, namely Tsallis entropy Actor-Critic (TAC) and Renyi entropy Actor-Critic (RAC). Theoretical analysis indicates that these algorithms can be more effective than SAC. Furthermore, they have paved the way for the creation of a new Ensemble Actor-Critic (EAC) algorithm that incorporates a bootstrap mechanism for deep environment exploration and a value-function based mechanism for high-level action selection. Through empirical testing, we demonstrate that TAC, RAC, and EAC can achieve state-of-the-art performance in various benchmark control tasks, surpassing SAC and several other advanced learning algorithms in terms of both sample efficiency and effectiveness.",1
"Deep reinforcement learning has recently gained a focus on problems where policy or value functions are independent of goals. Evidence exists that the sampling of goals has a strong effect on the learning performance, but there is a lack of general mechanisms that focus on optimizing the goal sampling process. In this work, we present a simple and general goal masking method that also allows us to estimate a goal's difficulty level and thus realize a curriculum learning approach for deep RL. Our results indicate that focusing on goals with a medium difficulty level is appropriate for deep deterministic policy gradient (DDPG) methods, while an ""aim for the stars and reach the moon-strategy"", where hard goals are sampled much more often than simple goals, leads to the best learning performance in cases where DDPG is combined with for hindsight experience replay (HER). We demonstrate that the approach significantly outperforms standard goal sampling for different robotic object manipulation problems.",0
"Recently, there has been a growing interest in applying deep reinforcement learning to problems where policy or value functions are not reliant on specific goals. However, research suggests that the selection of goals can greatly impact the learning performance, yet there are no established methods for optimizing this selection process. Our research introduces a simple and versatile goal masking technique that enables the estimation of a goal's difficulty level, allowing for a curriculum learning approach to deep RL. Our findings reveal that focusing on moderately challenging goals is optimal for deep deterministic policy gradient (DDPG) methods, while a ""shoot for the stars and land on the moon"" strategy, where difficult goals are more frequently sampled than simple goals, yields the best results when combined with hindsight experience replay (HER). Through various robotic object manipulation tasks, we demonstrate that our approach surpasses standard goal sampling.",1
"Reinforcement learning is a promising approach to learning robot controllers. It has recently been shown that algorithms based on finite-difference estimates of the policy gradient are competitive with algorithms based on the policy gradient theorem. We propose a theoretical framework for understanding this phenomenon. Our key insight is that many dynamical systems (especially those of interest in robot control tasks) are \emph{nearly deterministic}---i.e., they can be modeled as a deterministic system with a small stochastic perturbation. We show that for such systems, finite-difference estimates of the policy gradient can have substantially lower variance than estimates based on the policy gradient theorem. We interpret these results in the context of counterfactual estimation. Finally, we empirically evaluate our insights in an experiment on the inverted pendulum.",0
"Learning robot controllers through reinforcement learning shows great potential. Recent studies have revealed that algorithms using finite-difference estimates of the policy gradient can compete with those using the policy gradient theorem. To explain this phenomenon, we propose a theoretical framework. Our research suggests that many dynamical systems, particularly those used in robot control tasks, are mostly deterministic but with a small stochastic perturbation. In such systems, finite-difference estimates of the policy gradient can provide more precise results than estimates based on the policy gradient theorem. We explain these findings in the context of counterfactual estimation and test them in an experiment on the inverted pendulum.",1
"In this paper, we propose a novel conditional-generative-adversarial-nets-based image captioning framework as an extension of traditional reinforcement-learning (RL)-based encoder-decoder architecture. To deal with the inconsistent evaluation problem among different objective language metrics, we are motivated to design some ""discriminator"" networks to automatically and progressively determine whether generated caption is human described or machine generated. Two kinds of discriminator architectures (CNN and RNN-based structures) are introduced since each has its own advantages. The proposed algorithm is generic so that it can enhance any existing RL-based image captioning framework and we show that the conventional RL training method is just a special case of our approach. Empirically, we show consistent improvements over all language evaluation metrics for different state-of-the-art image captioning models. In addition, the well-trained discriminators can also be viewed as objective image captioning evaluators",0
"In this article, we suggest a new framework for image captioning that is based on conditional-generative-adversarial-nets. This is an extension of the traditional RL-based encoder-decoder architecture. Our aim is to address the issue of inconsistent evaluation results across different objective language metrics. To achieve this, we have developed ""discriminator"" networks that can automatically determine whether a caption has been generated by a human or a machine. We have introduced two types of discriminator architectures - CNN and RNN-based - each with its own benefits. Our algorithm is versatile and can be used to improve any existing RL-based image captioning framework. We have demonstrated that the conventional RL training method is just a specific example of our approach. Through empirical testing, we have shown that our approach consistently outperforms other state-of-the-art image captioning models across multiple language evaluation metrics. Furthermore, the well-trained discriminators can also act as objective evaluators for image captioning.",1
"Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.",0
"Reinforcement learning (RL) is faced with a difficult problem known as ""sparse reward"". To address this issue, Hindsight Experience Replay (HER) is used to convert a failed experience into a successful one by relabeling the goals. However, HER's effectiveness is limited due to its lack of a compact and universal goal representation. To solve this problem, we introduce Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient RL technique that extends HER by using natural language as the goal representation. Our analysis shows that ACTRCE is capable of solving difficult 3D navigation tasks efficiently, whereas HER with non-language goal representation failed to learn. Additionally, we demonstrate that the agent can generalize to unseen instructions and even instructions with unseen lexicons when language goal representations are used. We emphasize the importance of hindsight advice in solving challenging tasks and show that even a small amount of advice can lead to good performance.",1
"We study active object tracking, where a tracker takes visual observations (i.e., frame sequences) as input and produces the corresponding camera control signals as output (e.g., move forward, turn left, etc.). Conventional methods tackle tracking and camera control tasks separately, and the resulting system is difficult to tune jointly. These methods also require significant human efforts for image labeling and expensive trial-and-error system tuning in the real world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning. A ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates good generalization behaviors in the case of unseen object moving paths, unseen object appearances, unseen backgrounds, and distracting objects. The system is robust and can restore tracking after occasional lost of the target being tracked. We also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios. We demonstrate successful examples of such transfer, via experiments over the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.",0
"Our focus is on active object tracking, where a tracker receives visual observations (i.e., frame sequences) and produces camera control signals (e.g., move forward, turn left, etc.) as output. Traditional methods address tracking and camera control separately, which makes it challenging to tune the system effectively. These approaches also require significant human effort for image labeling and expensive trial-and-error system tuning in real-world scenarios. In this paper, we propose an end-to-end solution using deep reinforcement learning. We use a ConvNet-LSTM function approximator for direct frame-to-action prediction and introduce an environment augmentation technique and customized reward function for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates robust generalization behaviors, even in the face of unseen object moving paths, appearances, backgrounds, and distracting objects. The system can recover tracking after occasional losses of the target, and we find that the tracking ability acquired in the simulators can potentially transfer to real-world scenarios. We demonstrate this transfer through experiments on the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.",1
"To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.",0
"For intelligent agents to become more accessible and useful, they must learn complex behaviors from non-expert human users while efficiently using their sparse feedback. Recent studies have shown that human feedback can be a critique of the agent's current behavior rather than an alternate reward signal. The COnvergent Actor-Critic by Humans (COACH) algorithm has been developed to make direct policy updates based on human feedback. Our work builds on COACH by using a deep neural network to represent the agent's policy. We have made several modifications to the original COACH algorithm to enable successful learning of behaviors from high-dimensional observations with reduced sample complexity. Our Deep COACH algorithm has been tested in the 3D world of Minecraft, where an agent learns to perform tasks using only real-time human feedback in 10-15 minutes.",1
"In reinforcement learning, a decision needs to be made at some point as to whether it is worthwhile to carry on with the learning process or to terminate it. In many such situations, stochastic elements are often present which govern the occurrence of rewards, with the sequential occurrences of positive rewards randomly interleaved with negative rewards. For most practical learners, the learning is considered useful if the number of positive rewards always exceeds the negative ones. A situation that often calls for learning termination is when the number of negative rewards exceeds the number of positive rewards. However, while this seems reasonable, the error of premature termination, whereby termination is enacted along with the conclusion of learning failure despite the positive rewards eventually far outnumber the negative ones, can be significant. In this paper, using combinatorial analysis we study the error probability in wrongly terminating a reinforcement learning activity which undermines the effectiveness of an optimal policy, and we show that the resultant error can be quite high. Whilst we demonstrate mathematically that such errors can never be eliminated, we propose some practical mechanisms that can effectively reduce such errors. Simulation experiments have been carried out, the results of which are in close agreement with our theoretical findings.",0
"In reinforcement learning, deciding when to continue or end the learning process is crucial. Often, stochastic elements are present that determine the occurrence of rewards, with positive and negative rewards randomly alternating. For practical learners, the learning is considered valuable if positive rewards exceed negative ones. If negative rewards outnumber positive ones, it may be necessary to terminate the learning process. However, premature termination can occur when learning is erroneously deemed unsuccessful despite eventually having more positive than negative rewards. This paper uses combinatorial analysis to investigate the probability of such errors and shows that they can be significant. While it is impossible to eliminate such errors, practical mechanisms can reduce them. Simulation experiments confirm our theoretical findings.",1
"In reinforcement learning episodes, the rewards and punishments are often non-deterministic, and there are invariably stochastic elements governing the underlying situation. Such stochastic elements are often numerous and cannot be known in advance, and they have a tendency to obscure the underlying rewards and punishments patterns. Indeed, if stochastic elements were absent, the same outcome would occur every time and the learning problems involved could be greatly simplified. In addition, in most practical situations, the cost of an observation to receive either a reward or punishment can be significant, and one would wish to arrive at the correct learning conclusion by incurring minimum cost. In this paper, we present a stochastic approach to reinforcement learning which explicitly models the variability present in the learning environment and the cost of observation. Criteria and rules for learning success are quantitatively analyzed, and probabilities of exceeding the observation cost bounds are also obtained.",0
"The rewards and punishments in reinforcement learning episodes are often unpredictable due to stochastic elements that govern the situation. These elements are numerous and cannot be anticipated, making it challenging to identify patterns in the rewards and punishments. If the stochastic elements were absent, the learning problems would be simpler, and the same outcome would occur every time. However, in real-world scenarios, the cost of observation to receive a reward or punishment can be substantial, and it is crucial to minimize it when arriving at the right learning conclusion. This paper proposes a stochastic approach to reinforcement learning that explicitly models the variability and cost of observation. The study analyzes the criteria and rules for learning success quantitatively and calculates the probabilities of exceeding the observation cost limits.",1
"We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",0
"Our focus is on continuous action reinforcement learning problems where the agent's interaction with the environment must be limited to safe policies. To address these issues, we introduce constrained Markov decision processes (CMDPs) and safe policy optimization algorithms that utilize a Lyapunov approach. Our algorithms work with standard policy gradient (PG) methods, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy. Our algorithms guarantee near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the feasible solutions set induced by state-dependent linearized Lyapunov constraints. Our algorithms are more data-efficient than existing constrained PG algorithms because they can utilize both on-policy and off-policy data. Furthermore, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms on several simulated (MuJoCo) tasks and a real-world indoor robot navigation problem, comparing them to state-of-the-art baselines. Our results demonstrate the effectiveness of our algorithms in balancing performance and constraint satisfaction. Videos of our experiments can be found at the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",1
"Machine learning can provide efficient solutions to the complex problems encountered in autonomous driving, but ensuring their safety remains a challenge. A number of authors have attempted to address this issue, but there are few publicly-available tools to adequately explore the trade-offs between functionality, scalability, and safety.   We thus present WiseMove, a software framework to investigate safe deep reinforcement learning in the context of motion planning for autonomous driving. WiseMove adopts a modular learning architecture that suits our current research questions and can be adapted to new technologies and new questions. We present the details of WiseMove, demonstrate its use on a common traffic scenario, and describe how we use it in our ongoing safe learning research.",0
"Although machine learning can offer efficient solutions for the complex issues that arise in autonomous driving, ensuring safety still poses a significant challenge. Many authors have tried to tackle this problem, but there is a scarcity of publicly available tools that can appropriately balance functionality, scalability, and safety. Thus, our team has developed WiseMove, a software framework that enables the examination of safe deep reinforcement learning in motion planning for autonomous driving. WiseMove features a modular learning architecture that is well-suited for our current research inquiries and can be adapted to new technologies and queries. In this article, we present the particulars of WiseMove, demonstrate its application to a typical traffic scenario, and explain how we employ it in our ongoing safe learning research.",1
"Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.",0
"There are two popular approaches to policy search: deep neuroevolution and deep reinforcement learning (deep RL). Deep neuroevolution is widely applicable and stable, but it is not very sample efficient. On the other hand, deep RL is more sample efficient, but the most efficient variants are unstable and sensitive to hyper-parameter settings. Traditionally, these approaches have been compared as competing tools, but a new approach is emerging that combines them to get the best of both worlds. Two existing combinations use an ad hoc evolutionary algorithm or a goal exploration process with the Deep Deterministic Policy Gradient (DDPG) algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, which improves over DDPG. We evaluate the resulting method, CEM-RL, on a set of benchmarks used in deep RL and show that it offers a satisfactory trade-off between performance and sample efficiency while providing several advantages over its competitors.",1
"Model-free reinforcement learning has recently been shown to successfully learn navigation policies from raw sensor data. In this work, we address the problem of learning driving policies for an autonomous agent in a high-fidelity simulator. Building upon recent research that applies deep reinforcement learning to navigation problems, we present a modular deep reinforcement learning approach to predict the steering angle of the car from raw images. The first module extracts a low-dimensional latent semantic representation of the image. The control module trained with reinforcement learning takes the latent vector as input to predict the correct steering angle. The experimental results have showed that our method is capable of learning to maneuver the car without any human control signals.",0
"Recently, it has been demonstrated that navigation policies can be successfully learned from raw sensor data using model-free reinforcement learning. Our study aims to tackle the challenge of teaching an autonomous agent how to drive in a high-fidelity simulator. We expand on previous work that applies deep reinforcement learning to navigation problems by presenting a modular approach that uses deep reinforcement learning to predict the steering angle of the car from raw images. The first module extracts a low-dimensional latent semantic representation of the image, while the control module, trained with reinforcement learning, uses the latent vector to accurately predict the steering angle. Our experimental results demonstrate that our method can effectively teach the car how to maneuver without any human control signals.",1
"Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS",0
"The use of deep reinforcement learning shows potential for controlling real-world robots through vision. However, the success of this technique relies heavily on the amount and diversity of data available for training. Obtaining such data can be challenging for certain types of robots, such as fragile and small-scale quadrotors. While simulated rendering and physics can provide larger datasets, the quality of such data is inferior due to poor modeling of complex physics and air currents, along with other discrepancies between simulation and reality. This study explores a hybrid deep reinforcement learning approach that combines both real-world and simulated data. Real-world data is used to learn about the dynamics of the system, while simulated data is used to develop a perception system that enables the robot to avoid collisions using a single camera. The effectiveness of this approach is demonstrated through a collision avoidance task for a nano aerial vehicle, where the quadrotor is able to avoid collisions in new environments with different lighting and geometry using only an hour of real-world data. The code, instructions for building the aerial vehicles, and experiment videos can be accessed at github.com/gkahn13/GtS.",1
"Deep Reinforcement Learning has been shown to be very successful in complex games, e.g. Atari or Go. These games have clearly defined rules, and hence allow simulation. In many practical applications, however, interactions with the environment are costly and a good simulator of the environment is not available. Further, as environments differ by application, the optimal inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent depends on the application. In this work, we propose a multi-arm bandit framework that selects from a set of different reinforcement learning agents to choose the one with the best inductive bias. To alleviate the problem of sparse rewards, the reinforcement learning agents are augmented with surrogate rewards. This helps the bandit framework to select the best agents early, since these rewards are smoother and less sparse than the environment reward. The bandit has the double objective of maximizing the reward while the agents are learning and selecting the best agent after a finite number of learning steps. Our experimental results on standard environments show that the proposed framework is able to consistently select the optimal agent after a finite number of steps, while collecting more cumulative reward compared to selecting a sub-optimal architecture or uniformly alternating between different agents.",0
"While Deep Reinforcement Learning has proven successful in games with clear rules, like Atari and Go, it is not always feasible to simulate interactions with the environment in practical applications. Additionally, the optimal inductive bias for a reinforcement agent varies by application. To address this, we propose a multi-arm bandit framework that selects from a set of reinforcement learning agents with different biases. The agents are augmented with surrogate rewards to mitigate the problem of sparse rewards and facilitate early identification of the best agent. The bandit aims to maximize reward while selecting the best agent after a finite number of learning steps. Our experimental results demonstrate that this framework consistently selects the optimal agent while achieving greater cumulative reward than selecting a suboptimal architecture or alternating between agents uniformly.",1
"Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. (2018)'s analysis of the C51 algorithm in terms of the Cram\'er distance, but their results only apply to the tabular setting and ignore C51's use of a softmax to produce normalized distributions. In this paper we adapt the Cram\'er distance to deal with arbitrary vectors. From it we derive a new distributional algorithm which is fully Cram\'er-based and can be combined to linear function approximation, with formal guarantees in the context of policy evaluation. In allowing the model's prediction to be any real vector, we lose the probabilistic interpretation behind the method, but otherwise maintain the appealing properties of distributional approaches. To the best of our knowledge, ours is the first proof of convergence of a distributional algorithm combined with function approximation. Perhaps surprisingly, our results provide evidence that Cram\'er-based distributional methods may perform worse than directly approximating the value function.",0
"Our understanding of practical distributional reinforcement learning methods is limited despite algorithmic advancements. Rowland et al. (2018) analyzed the C51 algorithm using the Cram\'er distance, but their findings are limited to the tabular setting and disregard C51's utilization of a softmax for normalized distributions. This paper introduces a Cram\'er-based distributional algorithm that can be combined with linear function approximation for policy evaluation. The model's prediction can be any real vector, which results in the loss of the method's probabilistic interpretation. However, the desirable properties of distributional approaches are still maintained. Our proof of convergence for a distributional algorithm combined with function approximation is the first of its kind. Surprisingly, our results suggest that Cram\'er-based distributional methods may perform worse than directly approximating the value function.",1
"Reinforcement learning (RL) agents have traditionally been tasked with maximizing the value function of a Markov decision process (MDP), either in continuous settings, with fixed discount factor $\gamma < 1$, or in episodic settings, with $\gamma = 1$. While this has proven effective for specific tasks with well-defined objectives (e.g., games), it has never been established that fixed discounting is suitable for general purpose use (e.g., as a model of human preferences). This paper characterizes rationality in sequential decision making using a set of seven axioms and arrives at a form of discounting that generalizes traditional fixed discounting. In particular, our framework admits a state-action dependent ""discount"" factor that is not constrained to be less than 1, so long as there is eventual long run discounting. Although this broadens the range of possible preference structures in continuous settings, we show that there exists a unique ""optimizing MDP"" with fixed $\gamma < 1$ whose optimal value function matches the true utility of the optimal policy, and we quantify the difference between value and utility for suboptimal policies. Our work can be seen as providing a normative justification for (a slight generalization of) Martha White's RL task formalism (2017) and other recent departures from the traditional RL, and is relevant to task specification in RL, inverse RL and preference-based RL.",0
"In traditional reinforcement learning (RL), the objective has been to maximize the value function of a Markov decision process (MDP), either in continuous settings with a fixed discount factor of $\gamma < 1$, or in episodic settings where $\gamma = 1$. While this approach has been effective for specific tasks with defined goals, such as games, it has not been established that fixed discounting is suitable for general use, such as modeling human preferences. This paper presents a framework for rational decision making based on seven axioms, which leads to a generalized form of discounting that is not limited to a discount factor of less than 1. This approach allows for a broader range of preference structures in continuous settings. However, we show that there is a unique ""optimizing MDP"" with fixed $\gamma < 1$ that matches the true utility of the optimal policy, and we quantify the difference between value and utility for suboptimal policies. Our work provides a normative justification for recent departures from traditional RL, such as Martha White's RL task formalism (2017), and is relevant to task specification in RL, inverse RL, and preference-based RL.",1
"Multi-step methods such as Retrace($\lambda$) and $n$-step $Q$-learning have become a crucial component of modern deep reinforcement learning agents. These methods are often evaluated as a part of bigger architectures and their evaluations rarely include enough samples to draw statistically significant conclusions about their performance. This type of methodology makes it difficult to understand how particular algorithmic details of multi-step methods influence learning. In this paper we combine the $n$-step action-value algorithms Retrace, $Q$-learning, Tree Backup, Sarsa, and $Q(\sigma)$ with an architecture analogous to DQN. We test the performance of all these algorithms in the mountain car environment; this choice of environment allows for faster training times and larger sample sizes. We present statistical analyses on the effects of the off-policy correction, the backup length parameter $n$, and the update frequency of the target network on the performance of these algorithms. Our results show that (1) using off-policy correction can have an adverse effect on the performance of Sarsa and $Q(\sigma)$; (2) increasing the backup length $n$ consistently improved performance across all the different algorithms; and (3) the performance of Sarsa and $Q$-learning was more robust to the effect of the target network update frequency than the performance of Tree Backup, $Q(\sigma)$, and Retrace in this particular task.",0
"Modern deep reinforcement learning agents rely heavily on multi-step methods like Retrace($\lambda$) and $n$-step $Q$-learning. However, these methods are often evaluated as part of larger architectures, with limited sample sizes that make it difficult to draw significant conclusions about their performance. This approach hinders our understanding of how specific algorithmic details impact learning. To address this, we use an architecture similar to DQN and combine Retrace, $Q$-learning, Tree Backup, Sarsa, and $Q(\sigma)$ to test their performance in the mountain car environment, which allows for faster training times and larger sample sizes. We conduct statistical analyses on the effects of off-policy correction, backup length parameter $n$, and target network update frequency on algorithm performance. Our results indicate that using off-policy correction can negatively impact Sarsa and $Q(\sigma)$, increasing backup length consistently improved performance across all algorithms, and Sarsa and $Q$-learning were more robust to target network updates than Tree Backup, $Q(\sigma)$, and Retrace in this task.",1
"Multi-armed bandit(MAB) problem is a reinforcement learning framework where an agent tries to maximise her profit by proper selection of actions through absolute feedback for each action. The dueling bandits problem is a variation of MAB problem in which an agent chooses a pair of actions and receives relative feedback for the chosen action pair. The dueling bandits problem is well suited for modelling a setting in which it is not possible to provide quantitative feedback for each action, but qualitative feedback for each action is preferred as in the case of human feedback. The dueling bandits have been successfully applied in applications such as online rank elicitation, information retrieval, search engine improvement and clinical online recommendation. We propose a new method called Sup-KLUCB for K-armed dueling bandit problem specifically Copeland bandit problem by converting it into a standard MAB problem. Instead of using MAB algorithm independently for each action in a pair as in Sparring and in Self-Sparring algorithms, we combine a pair of action and use it as one action. Previous UCB algorithms such as Relative Upper Confidence Bound(RUCB) can be applied only in case of Condorcet dueling bandits, whereas this algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. Our empirical results outperform state of the art Double Thompson Sampling(DTS) in case of Copeland dueling bandits.",0
"The Multi-armed bandit (MAB) problem involves an agent using feedback to select actions that will maximize their profit. The dueling bandits problem is a variation of the MAB problem where an agent selects a pair of actions and receives relative feedback for that pair. This problem is useful for modeling situations where it is not possible to provide quantitative feedback, but qualitative feedback is preferred, such as in cases where humans provide feedback. Dueling bandits have been applied successfully in various applications, such as online rank elicitation, information retrieval, search engine improvement, and clinical online recommendation. Our proposed method, Sup-KLUCB, is designed specifically for the K-armed dueling bandit problem, specifically the Copeland bandit problem. It converts this problem into a standard MAB problem and combines a pair of actions into one action, unlike Sparring and Self-Sparring algorithms, which use MAB algorithms independently for each action in a pair. This algorithm is suitable for general Copeland dueling bandits, including Condorcet dueling bandits as a special case, unlike prior UCB algorithms like Relative Upper Confidence Bound (RUCB), which only apply to Condorcet dueling bandits. Our empirical results show that Sup-KLUCB outperforms the state-of-the-art Double Thompson Sampling (DTS) in Copeland dueling bandits.",1
"Training intelligent agents through reinforcement learning is a notoriously unstable procedure. Massive parallelization on GPUs and distributed systems has been exploited to generate a large amount of training experiences and consequently reduce instabilities, but the success of training remains strongly influenced by the choice of the hyperparameters. To overcome this issue, we introduce HyperTrick, a new metaoptimization algorithm, and show its effective application to tune hyperparameters in the case of deep reinforcement learning, while learning to play different Atari games on a distributed system. Our analysis provides evidence of the interaction between the identification of the optimal hyperparameters and the learned policy, that is typical of the case of metaoptimization for deep reinforcement learning. When compared with state-of-the-art metaoptimization algorithms, HyperTrick is characterized by a simpler implementation and it allows learning similar policies, while making a more effective use of the computational resources in a distributed system.",0
"The process of training intelligent agents using reinforcement learning is known for being unstable. To combat this, researchers have utilized massive parallelization on GPUs and distributed systems to generate a vast amount of training experiences and reduce instabilities. However, the success of training still heavily relies on the selection of hyperparameters. To address this issue, we developed HyperTrick, a new metaoptimization algorithm that effectively tunes hyperparameters for deep reinforcement learning. We applied HyperTrick to learn how to play various Atari games on a distributed system and found that it interacts with the identification of optimal hyperparameters and the learned policy. Our analysis shows that HyperTrick is simpler to implement than state-of-the-art metaoptimization algorithms, allowing for the learning of similar policies while making more effective use of computational resources.",1
"Machine learning approaches hold great potential for the automated detection of lung nodules in chest radiographs, but training the algorithms requires vary large amounts of manually annotated images, which are difficult to obtain. Weak labels indicating whether a radiograph is likely to contain pulmonary nodules are typically easier to obtain at scale by parsing historical free-text radiological reports associated to the radiographs. Using a repositotory of over 700,000 chest radiographs, in this study we demonstrate that promising nodule detection performance can be achieved using weak labels through convolutional neural networks for radiograph classification. We propose two network architectures for the classification of images likely to contain pulmonary nodules using both weak labels and manually-delineated bounding boxes, when these are available. Annotated nodules are used at training time to deliver a visual attention mechanism informing the model about its localisation performance. The first architecture extracts saliency maps from high-level convolutional layers and compares the estimated position of a nodule against the ground truth, when this is available. A corresponding localisation error is then back-propagated along with the softmax classification error. The second approach consists of a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning. When a nodule annotation is available at training time, the reward function is modified accordingly so that exploring portions of the radiographs away from a nodule incurs a larger penalty. Our empirical results demonstrate the potential advantages of these architectures in comparison to competing methodologies.",0
"The detection of lung nodules in chest radiographs can be automated using machine learning. However, training the algorithms requires a large number of manually annotated images, which can be difficult to obtain. Weak labels that indicate whether a radiograph is likely to contain pulmonary nodules are easier to obtain by parsing historical radiological reports associated with the radiographs. This study uses over 700,000 chest radiographs to demonstrate that weak labels can be used with convolutional neural networks for radiograph classification. Two network architectures are proposed for classifying images likely to contain pulmonary nodules using both weak labels and manually-delineated bounding boxes. Annotated nodules are used at training time to provide a visual attention mechanism that informs the model about its localization performance. The first architecture extracts saliency maps and compares the estimated position of a nodule against the ground truth. The second approach is a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning. Results show the potential advantages of these architectures in comparison to other methodologies.",1
"In the NeurIPS 2018 Artificial Intelligence for Prosthetics challenge, participants were tasked with building a controller for a musculoskeletal model with a goal of matching a given time-varying velocity vector. Top participants were invited to describe their algorithms. In this work, we describe the challenge and present thirteen solutions that used deep reinforcement learning approaches. Many solutions use similar relaxations and heuristics, such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending. However, each team implemented different modifications of the known algorithms by, for example, dividing the task into subtasks, learning low-level control, or by incorporating expert knowledge and using imitation learning.",0
"The NeurIPS 2018 Artificial Intelligence for Prosthetics challenge required participants to construct a controller for a musculoskeletal model that matched a specific time-varying velocity vector. The top performers were offered the opportunity to detail their algorithms. This paper discusses the challenge and showcases thirteen solutions that utilized deep reinforcement learning methodologies. While many of the solutions implemented similar relaxations and heuristics (such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending), each team made unique modifications to the known algorithms. These adaptations included dividing the task into subtasks, learning low-level control, incorporating expert knowledge, and utilizing imitation learning.",1
"In this paper, we present a new class of Markov decision processes (MDPs), called Tsallis MDPs, with Tsallis entropy maximization, which generalizes existing maximum entropy reinforcement learning (RL). A Tsallis MDP provides a unified framework for the original RL problem and RL with various types of entropy, including the well-known standard Shannon-Gibbs (SG) entropy, using an additional real-valued parameter, called an entropic index. By controlling the entropic index, we can generate various types of entropy, including the SG entropy, and a different entropy results in a different class of the optimal policy in Tsallis MDPs. We also provide a full mathematical analysis of Tsallis MDPs, including the optimality condition, performance error bounds, and convergence. Our theoretical result enables us to use any positive entropic index in RL. To handle complex and large-scale problems, we propose a model-free actor-critic RL method using Tsallis entropy maximization. We evaluate the regularization effect of the Tsallis entropy with various values of entropic indices and show that the entropic index controls the exploration tendency of the proposed method. For a different type of RL problems, we find that a different value of the entropic index is desirable. The proposed method is evaluated using the MuJoCo simulator and achieves the state-of-the-art performance.",0
"This paper introduces Tsallis MDPs, a new type of Markov decision processes that utilize Tsallis entropy maximization to generalize existing maximum entropy reinforcement learning. By introducing an entropic index, a Tsallis MDP can incorporate various types of entropy, including the standard Shannon-Gibbs entropy. The entropic index can be adjusted to generate different types of entropy, resulting in different optimal policies for the Tsallis MDP. The paper provides a mathematical analysis of Tsallis MDPs, including optimality conditions, performance error bounds, and convergence. To address complex and large-scale problems, the paper proposes a model-free actor-critic RL method that utilizes Tsallis entropy maximization. The method's exploration tendency can be controlled by adjusting the entropic index, which is shown to have a significant impact on the method's performance. The paper also evaluates the proposed method using the MuJoCo simulator, achieving state-of-the-art performance.",1
"This paper provides an analysis of the tradeoff between asymptotic bias (suboptimality with unlimited data) and overfitting (additional suboptimality due to limited data) in the context of reinforcement learning with partial observability. Our theoretical analysis formally characterizes that while potentially increasing the asymptotic bias, a smaller state representation decreases the risk of overfitting. This analysis relies on expressing the quality of a state representation by bounding L1 error terms of the associated belief states. Theoretical results are empirically illustrated when the state representation is a truncated history of observations, both on synthetic POMDPs and on a large-scale POMDP in the context of smartgrids, with real-world data. Finally, similarly to known results in the fully observable setting, we also briefly discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting in the partially observable context.",0
"The article examines the balance between asymptotic bias (imperfection with unlimited data) and overfitting (additional imperfection due to limited data) in reinforcement learning with partial observability. The study establishes that although a smaller state representation may increase asymptotic bias, it reduces the risk of overfitting. The quality of the state representation is evaluated by bounding L1 error terms of the associated belief states. Theoretical findings are demonstrated in synthetic POMDPs and a real-world smart grid POMDP with truncated observation histories. Additionally, the article briefly discusses and demonstrates how using function approximators and adjusting the discount factor may improve the tradeoff between asymptotic bias and overfitting in the partially observable setting, similar to fully observable environments.",1
"The transfer of knowledge from one policy to another is an important tool in Deep Reinforcement Learning. This process, referred to as distillation, has been used to great success, for example, by enhancing the optimisation of agents, leading to stronger performance faster, on harder domains [26, 32, 5, 8]. Despite the widespread use and conceptual simplicity of distillation, many different formulations are used in practice, and the subtle variations between them can often drastically change the performance and the resulting objective that is being optimised. In this work, we rigorously explore the entire landscape of policy distillation, comparing the motivations and strengths of each variant through theoretical and empirical analysis. Our results point to three distillation techniques, that are preferred depending on specifics of the task. Specifically a newly proposed expected entropy regularised distillation allows for quicker learning in a wide range of situations, while still guaranteeing convergence.",0
"Deep Reinforcement Learning relies heavily on the transfer of knowledge between policies, which is achieved through a process called distillation. This technique, which has been successfully used to improve agent optimization and performance on difficult domains, is widely used but its many formulations can have a significant impact on performance and objective optimization. This study thoroughly examines the landscape of policy distillation, comparing the strengths and motivations of each variant through both theoretical and empirical analysis. The findings suggest that there are three preferred distillation techniques, each suited to specific tasks. Among these, the newly proposed expected entropy regularised distillation offers faster learning across a broad range of scenarios while still ensuring convergence.",1
"Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms however have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This paper addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multi-agent deep RL (MADRL) is presented, including non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, multi-agent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed, with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to future development of more robust and highly useful multi-agent learning methods for solving real-world problems.",0
"For decades, reinforcement learning (RL) algorithms have been utilized to resolve sequential decision-making problems. However, these algorithms have struggled when dealing with high-dimensional environments. The emergence of deep learning has allowed RL approaches to generate optimal policies for sophisticated agents that can effectively operate in challenging environments. This article focuses on an essential aspect of deep RL, which pertains to scenarios where multiple agents must communicate and coordinate to tackle complex tasks. The article provides a survey of diverse approaches to multi-agent deep RL (MADRL) issues, such as non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, and multi-agent transfer learning. The benefits and drawbacks of these methods will be scrutinized and discussed, and their relevant applications explored. This review aims to shed light on various MADRL approaches and potentially inspire the development of more resilient and practical multi-agent learning techniques to address real-world problems.",1
"Despite the notable successes in video games such as Atari 2600, current AI is yet to defeat human champions in the domain of real-time strategy (RTS) games. One of the reasons is that an RTS game is a multi-agent game, in which single-agent reinforcement learning methods cannot simply be applied because the environment is not a stationary Markov Decision Process. In this paper, we present a first step toward finding a game-theoretic solution to RTS games by applying Neural Fictitious Self-Play (NFSP), a game-theoretic approach for finding Nash equilibria, to Mini-RTS, a small but nontrivial RTS game provided on the ELF platform. More specifically, we show that NFSP can be effectively combined with policy gradient reinforcement learning and be applied to Mini-RTS. Experimental results also show that the scalability of NFSP can be substantially improved by pretraining the models with simple self-play using policy gradients, which by itself gives a strong strategy despite its lack of theoretical guarantee of convergence.",0
"Human champions remain unbeaten by current AI in real-time strategy (RTS) games, despite the successes achieved in video games like Atari 2600. This is due to the fact that RTS games are multi-agent games, which makes it difficult to apply single-agent reinforcement learning methods as the environment is not a stationary Markov Decision Process. This paper presents the first step towards developing a game-theoretic solution to RTS games by using Neural Fictitious Self-Play (NFSP), a game-theoretic approach that finds Nash equilibria, on Mini-RTS, a challenging RTS game available on the ELF platform. The study demonstrates that NFSP can be effectively combined with policy gradient reinforcement learning and applied to Mini-RTS. Additionally, the experiment shows that the scalability of NFSP can be significantly enhanced by pretraining the models with simple self-play using policy gradients, which yields a powerful strategy despite the lack of theoretical guarantee of convergence.",1
"Backpropagation and the chain rule of derivatives have been prominent; however, the total derivative rule has not enjoyed the same amount of attention. In this work we show how the total derivative rule leads to an intuitive visual framework for creating gradient estimators on graphical models. In particular, previous ""policy gradient theorems"" are easily derived. We derive new gradient estimators based on density estimation, as well as a likelihood ratio gradient, which ""jumps"" to an intermediate node, not directly to the objective function. We evaluate our methods on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm.",0
"While backpropagation and the chain rule of derivatives have received significant attention, the total derivative rule has been overlooked. Our research demonstrates how this rule can provide a clear and intuitive visual framework for developing gradient estimators on graphical models. Our work includes the derivation of previous ""policy gradient theorems"" and the development of new gradient estimators based on density estimation and a likelihood ratio gradient that jumps to an intermediate node rather than directly to the objective function. We evaluate our methods on model-based policy gradient algorithms and achieve strong performance, shedding light on the success of the widely used PILCO algorithm.",1
"Uplift modeling aims to directly model the incremental impact of a treatment on an individual response. In this work, we address the problem from a new angle and reformulate it as a Markov Decision Process (MDP). We conducted extensive experiments on both a synthetic dataset and real-world scenarios, and showed that our method can achieve significant improvement over previous methods.",0
"The objective of uplift modeling is to model the additional effect of a treatment on an individual's response. To tackle this issue, we propose a novel approach by transforming it into a Markov Decision Process (MDP). Our approach was tested on a synthetic dataset and real-life situations, and the results demonstrate its superiority over prior methods.",1
"Recent efforts on training visual navigation agents conditioned on language using deep reinforcement learning have been successful in learning policies for different multimodal tasks, such as semantic goal navigation and embodied question answering. In this paper, we propose a multitask model capable of jointly learning these multimodal tasks, and transferring knowledge of words and their grounding in visual objects across the tasks. The proposed model uses a novel Dual-Attention unit to disentangle the knowledge of words in the textual representations and visual concepts in the visual representations, and align them with each other. This disentangled task-invariant alignment of representations facilitates grounding and knowledge transfer across both tasks. We show that the proposed model outperforms a range of baselines on both tasks in simulated 3D environments. We also show that this disentanglement of representations makes our model modular, interpretable, and allows for transfer to instructions containing new words by leveraging object detectors.",0
"Efforts to train visual navigation agents with language using deep reinforcement learning have yielded successful results in learning policies for various multimodal tasks, such as semantic goal navigation and embodied question answering. In this study, we propose a multitask model that can jointly learn these tasks and transfer knowledge of words and their association with visual objects across them. The model employs a unique Dual-Attention unit to separate the knowledge of words in textual representations from visual concepts in visual representations and align them with one another. This disentangled and task-invariant alignment of representations facilitates grounding and knowledge transfer across both tasks. Our study demonstrates that the proposed model outperforms a range of baselines on both tasks in simulated 3D environments. Additionally, our disentanglement of representations makes our model modular, interpretable, and allows for transfer to instructions containing new words by utilizing object detectors.",1
"We introduce Dynamic Planning Networks (DPN), a novel architecture for deep reinforcement learning, that combines model-based and model-free aspects for online planning. Our architecture learns to dynamically construct plans using a learned state-transition model by selecting and traversing between simulated states and actions to maximize information before acting. In contrast to model-free methods, model-based planning lets the agent efficiently test action hypotheses without performing costly trial-and-error in the environment. DPN learns to efficiently form plans by expanding a single action-conditional state transition at a time instead of exhaustively evaluating each action, reducing the required number of state-transitions during planning by up to 96%. We observe various emergent planning patterns used to solve environments, including classical search methods such as breadth-first and depth-first search. DPN shows improved data efficiency, performance, and generalization to new and unseen domains in comparison to several baselines.",0
"We present a new deep reinforcement learning architecture, called Dynamic Planning Networks (DPN), which merges model-based and model-free features to enable online planning. Our architecture achieves this by learning to generate plans dynamically using a state-transition model and selecting and moving between simulated states and actions to optimize information before taking action. Unlike model-free techniques, model-based planning allows the agent to test action hypotheses efficiently without requiring extensive trial-and-error in the environment. DPN efficiently creates plans by progressively expanding a single action-conditional state transition instead of evaluating each action exhaustively, resulting in up to 96% reduction in the number of state-transitions required for planning. We observe that DPN employs different planning strategies to address various environments, including classical search methods such as breadth-first and depth-first search. Compared to several baselines, DPN exhibits enhanced data efficiency, performance, and generalization to new and unexplored domains.",1
"Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by $10^6$ times.",0
"The exploding gradient problem has long been known to be a major issue in deep learning and model-based reinforcement learning, leading to numerical problems and instability in optimization. However, our experiments in model-based reinforcement learning suggest that this problem may be more than just a numerical issue and could be caused by the chaotic nature of long chains of nonlinear computations. In addition to the magnitudes of the gradients becoming large, their direction becomes essentially random. While reparameterization gradients are affected by this problem, likelihood ratio gradients are more reliable. Using these insights, we have developed a model-based policy search framework called Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is highly flexible and can accommodate almost any model and policy while achieving the same performance as previous data-efficient learning algorithms. Finally, we have created a new algorithm called the total propagation algorithm, which can efficiently compute a union over all pathwise derivative depths during a single backwards pass, giving more weight to estimators with lower variance and sometimes outperforming reparameterization gradients by a factor of $10^6$.",1
"The effectiveness of model-based versus model-free methods is a long-standing question in reinforcement learning (RL). Motivated by recent empirical success of RL on continuous control tasks, we study the sample complexity of popular model-based and model-free algorithms on the Linear Quadratic Regulator (LQR). We show that for policy evaluation, a simple model-based plugin method requires asymptotically less samples than the classical least-squares temporal difference (LSTD) estimator to reach the same quality of solution; the sample complexity gap between the two methods can be at least a factor of state dimension. For policy evaluation, we study a simple family of problem instances and show that nominal (certainty equivalence principle) control also requires several factors of state and input dimension fewer samples than the policy gradient method to reach the same level of control performance on these instances. Furthermore, the gap persists even when employing commonly used baselines. To the best of our knowledge, this is the first theoretical result which demonstrates a separation in the sample complexity between model-based and model-free methods on a continuous control task.",0
"The question of whether model-based or model-free methods are more effective in reinforcement learning (RL) has been a topic of discussion for a long time. Our study is motivated by recent RL successes in continuous control tasks, and focuses on the sample complexity of popular model-based and model-free algorithms in the Linear Quadratic Regulator (LQR). We discovered that, for policy evaluation, a simple model-based plugin method requires significantly fewer samples than the classical least-squares temporal difference (LSTD) estimator to achieve the same quality of solution. The gap between the two methods can be at least a factor of state dimension. We also investigated a simple family of problem instances for policy evaluation and found that nominal (certainty equivalence principle) control needs several factors of state and input dimension fewer samples than the policy gradient method to achieve the same level of control performance on these instances. Additionally, the gap persists even when commonly used baselines are employed. This is the first theoretical result to demonstrate a difference in sample complexity between model-based and model-free methods in a continuous control task.",1
"StarCraft II poses a grand challenge for reinforcement learning. The main difficulties of it include huge state and action space and a long-time horizon. In this paper, we investigate a hierarchical reinforcement learning approach for StarCraft II. The hierarchy involves two levels of abstraction. One is the macro-action automatically extracted from expert's trajectories, which reduces the action space in an order of magnitude yet remains effective. The other is a two-layer hierarchical architecture which is modular and easy to scale, enabling a curriculum transferring from simpler tasks to more complex tasks. The reinforcement training algorithm for this architecture is also investigated. On a 64x64 map and using restrictive units, we achieve a winning rate of more than 99\% against the difficulty level-1 built-in AI. Through the curriculum transfer learning algorithm and a mixture of combat model, we can achieve over 93\% winning rate of Protoss against the most difficult non-cheating built-in AI (level-7) of Terran, training within two days using a single machine with only 48 CPU cores and 8 K40 GPUs. It also shows strong generalization performance, when tested against never seen opponents including cheating levels built-in AI and all levels of Zerg and Protoss built-in AI. We hope this study could shed some light on the future research of large-scale reinforcement learning.",0
"The task of mastering StarCraft II presents significant challenges for reinforcement learning due to its vast state and action space, as well as its long-term horizon. This research explores a hierarchical approach to reinforcement learning for StarCraft II, utilizing two levels of abstraction. The first level involves macro-actions extracted from expert trajectories, which effectively reduces the action space. The second level employs a modular, scalable, two-layer hierarchical architecture that facilitates curriculum transfer learning from simpler to more complex tasks. The study also investigates the reinforcement training algorithm for this architecture. Results show that on a 64x64 map with restrictive units, the approach achieved a winning rate of over 99% against difficulty level-1 built-in AI. By utilizing curriculum transfer learning and a mixture of combat models, the approach achieved a winning rate of over 93% against the most challenging non-cheating built-in AI (level-7) of Terran, training on a single machine with 48 CPU cores and 8 K40 GPUs within two days. The approach also demonstrated strong generalization performance when tested against never-seen-before opponents, including cheating levels built-in AI, and all levels of Zerg and Protoss built-in AI. The study aims to contribute to the future research of large-scale reinforcement learning.",1
"We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.",0
"Implicit Policy is a versatile type of policy that has the ability to represent intricate action distributions in reinforcement learning. It comes with effective algorithms for calculating policy gradients with entropy regularization. Our experiments demonstrate that, even though it is straightforward to implement, combining entropy regularization with a diverse policy class can achieve desirable characteristics commonly observed in maximum entropy reinforcement learning, such as resilience and multi-modality.",1
In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (a sequence of Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. We argue that previous experience with similar problems can provide an agent with information about how it should explore when facing a new but related problem. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself and demonstrate that such strategy can leverage patterns found in the structure of related problems. We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed approach.,0
"This paper delves into the issue of how a reinforcement learning agent can enhance its capacity to solve new problems by utilizing knowledge gained from previous experiences with a sequence of Markov decision processes. The authors assert that familiarity with analogous problems can furnish the agent with insights on how to explore when confronted with a new but related problem. They propose a reinforcement learning problem as a means of seeking an optimal exploration strategy and present evidence that such a strategy can take advantage of patterns discovered in the structure of analogous problems. Finally, the authors provide experimental results that illustrate the advantages of optimizing an exploration strategy based on their suggested method.",1
"In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video results are available at https://sites.google.com/view/actionablerepresentations .",0
"The focus of our research is exploring a novel method for robots to learn from their environment solely through observation. Specifically, we investigate the effectiveness of developing task-agnostic representations for continuous control tasks. Our research builds upon Time-Contrastive Networks (TCN), which utilize visual observations to learn. However, we extend TCN by embedding multiple frames together in the embedding space, rather than a single frame, resulting in more accurate encoding of both position and velocity attributes. We conduct experiments using a reinforcement learning setting and demonstrate that the self-supervised approach, where agents observe themselves or other agents performing tasks successfully, can facilitate the learning of continuous control policies using only the learned embeddings as input. Our results also show significant improvements on the real-world Pouring dataset, with relative error reductions of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. We have provided video results at https://sites.google.com/view/actionablerepresentations.",1
"This paper presents a novel Subject-dependent Deep Aging Path (SDAP), which inherits the merits of both Generative Probabilistic Modeling and Inverse Reinforcement Learning to model the facial structures and the longitudinal face aging process of a given subject. The proposed SDAP is optimized using tractable log-likelihood objective functions with Convolutional Neural Networks (CNNs) based deep feature extraction. Instead of applying a fixed aging development path for all input faces and subjects, SDAP is able to provide the most appropriate aging development path for individual subject that optimizes the reward aging formulation. Unlike previous methods that can take only one image as the input, SDAP further allows multiple images as inputs, i.e. all information of a subject at either the same or different ages, to produce the optimal aging path for the given subject. Finally, SDAP allows efficiently synthesizing in-the-wild aging faces. The proposed model is experimented in both tasks of face aging synthesis and cross-age face verification. The experimental results consistently show SDAP achieves the state-of-the-art performance on numerous face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). Furthermore, we also evaluate the performance of SDAP on large-scale Megaface challenge to demonstrate the advantages of the proposed solution.",0
"In this paper, a new method called Subject-dependent Deep Aging Path (SDAP) is introduced. The technique combines the benefits of Generative Probabilistic Modeling and Inverse Reinforcement Learning to model the facial structures and aging process of an individual. SDAP is optimized using Convolutional Neural Networks (CNNs) for deep feature extraction and tractable log-likelihood objective functions. Unlike previous approaches that use a fixed aging development path for all subjects, SDAP can provide a personalized aging development path for each individual, optimizing the reward aging formulation. Additionally, SDAP can take multiple images as inputs, allowing the use of all information of a subject at different ages to produce the optimal aging path. The proposed model can efficiently synthesize in-the-wild aging faces and has been evaluated in face aging synthesis and cross-age face verification tasks. The experimental results show that SDAP outperforms existing methods on various face aging databases, including FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). The proposed technique has also been evaluated on the large-scale Megaface challenge, demonstrating its advantages over other solutions.",1
"Interactive Machine Learning is concerned with creating systems that operate in environments alongside humans to achieve a task. A typical use is to extend or amplify the capabilities of a human in cognitive or physical ways, requiring the machine to adapt to the users' intentions and preferences. Often, this takes the form of a human operator providing some type of feedback to the user, which can be explicit feedback, implicit feedback, or a combination of both. Explicit feedback, such as through a mouse click, carries a high cognitive load. The focus of this study is to extend the current state of the art in interactive machine learning by demonstrating that agents can learn a human user's behavior and adapt to preferences with a reduced amount of explicit human feedback in a mixed feedback setting. The learning agent perceives a value of its own behavior from hand gestures given via a spatial interface. This feedback mechanism is termed Spatial Interface Valuing. This method is evaluated experimentally in a simulated environment for a grasping task using a robotic arm with variable grip settings. Preliminary results indicate that learning agents using spatial interface valuing can learn a value function mapping spatial gestures to expected future rewards much more quickly as compared to those same agents just receiving explicit feedback, demonstrating that an agent perceiving feedback from a human user via a spatial interface can serve as an effective complement to existing approaches.",0
"Interactive Machine Learning involves developing systems that work alongside humans to achieve goals in various environments. These systems can enhance human abilities in cognitive or physical ways, and they need to adapt to users' preferences and intentions. Human operators usually provide feedback to the user, which can be explicit, implicit, or a combination of both. Explicit feedback, such as a mouse click, can be mentally taxing. This study aims to advance the field of interactive machine learning by demonstrating that agents can learn a user's behavior and preferences with less explicit feedback. The learning agent perceives feedback through a Spatial Interface Valuing mechanism, which involves hand gestures. This method was tested in a simulated environment for a grasping task using a robotic arm with variable grip settings. The results showed that agents using Spatial Interface Valuing can learn much faster than those receiving only explicit feedback. This finding suggests that a spatial interface can complement existing approaches to interactive machine learning.",1
"Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.",0
"Automakers are currently developing or producing automated driving systems (ADS) that provide freeway-pilot functions. These ADS are typically limited to restricted-access freeways and require manual completion of the ramp merging process before transitioning to automated modes. The challenge of extending automation to ramp merging lies in the need for the automated vehicle to optimize long-term objectives while safely executing near-term actions and interacting with other unpredictable vehicles. To address this complex control problem, Deep Reinforcement Learning (DRL) techniques are proposed to find an optimal driving policy that maximizes long-term rewards in an interactive environment. The proposed architecture uses a Long Short-Term Memory (LSTM) model to capture the historical impact of the environment on the reward and convey it to a Deep Q-Network (DQN) to generate Q-values for action selection. This DRL architecture can be applied to other autonomous driving scenarios, such as navigating complex intersections or changing lanes in varying traffic conditions.",1
"Due to the capability of deep learning to perform well in high dimensional problems, deep reinforcement learning agents perform well in challenging tasks such as Atari 2600 games. However, clearly explaining why a certain action is taken by the agent can be as important as the decision itself. Deep reinforcement learning models, as other deep learning models, tend to be opaque in their decision-making process. In this work, we propose to make deep reinforcement learning more transparent by visualizing the evidence on which the agent bases its decision. In this work, we emphasize the importance of producing a justification for an observed action, which could be applied to a black-box decision agent.",0
"Deep reinforcement learning agents excel in difficult tasks like Atari 2600 games due to the effectiveness of deep learning in high-dimensional problems. Nonetheless, explaining why an agent chooses a particular action is just as crucial as the decision itself. The decision-making process of deep reinforcement learning models, like other deep learning models, has a tendency to be opaque. We suggest a solution to enhance the transparency of deep reinforcement learning by presenting the evidence that forms the basis for an agent's decision. This paper highlights the significance of providing a rationale for an observed action, which could be employed for a black-box decision agent.",1
"We describe TF-Replicator, a framework for distributed machine learning designed for DeepMind researchers and implemented as an abstraction over TensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel research code. The same models can be effortlessly deployed to different cluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU accelerators) using synchronous or asynchronous training regimes. To demonstrate the generality and scalability of TF-Replicator, we implement and benchmark three very different models: (1) A ResNet-50 for ImageNet classification, (2) a SN-GAN for class-conditional ImageNet image generation, and (3) a D4PG reinforcement learning agent for continuous control. Our results show strong scalability performance without demanding any distributed systems expertise of the user. The TF-Replicator programming model will be open-sourced as part of TensorFlow 2.0 (see https://github.com/tensorflow/community/pull/25).",0
"TF-Replicator is a framework created for DeepMind researchers that enables distributed machine learning. It functions as an abstraction layer over TensorFlow and greatly simplifies the writing of data-parallel and model-parallel research code. The same models can be deployed to various cluster architectures with ease, including machines containing CPUs, GPUs, or TPU accelerators, using either synchronous or asynchronous training methods. To demonstrate its versatility and scalability, we tested TF-Replicator on three different models: a ResNet-50 for ImageNet classification, an SN-GAN for class-conditional ImageNet image generation, and a D4PG reinforcement learning agent for continuous control. Our results show that TF-Replicator performs well without requiring any expertise in distributed systems from the user. The programming model for TF-Replicator will be made available as part of TensorFlow 2.0, and can be found at https://github.com/tensorflow/community/pull/25.",1
"In dynamic environments, learned controllers are supposed to take motion into account when selecting the action to be taken. However, in existing reinforcement learning works motion is rarely treated explicitly; it is rather assumed that the controller learns the necessary motion representation from temporal stacks of frames implicitly. In this paper, we show that for continuous control tasks learning an explicit representation of motion improves the quality of the learned controller in dynamic scenarios. We demonstrate this on common benchmark tasks (Walker, Swimmer, Hopper), on target reaching and ball catching tasks with simulated robotic arms, and on a dynamic single ball juggling task. Moreover, we find that when equipped with an appropriate network architecture, the agent can, on some tasks, learn motion features also with pure reinforcement learning, without additional supervision. Further we find that using an image difference between the current and the previous frame as an additional input leads to better results than a temporal stack of frames.",0
"When confronted with ever-changing circumstances, controllers that have been learned are expected to factor in motion when deciding on the course of action to take. Nonetheless, in current reinforcement learning research, motion is seldom handled directly; instead, it is assumed that the controller will acquire the necessary motion representation implicitly from a series of frames over time. This paper aims to prove that explicitly learning a motion representation for continuous control tasks can enhance the quality of the learned controller in dynamic scenarios. We demonstrate this by conducting experiments on various benchmark tasks, such as Walker, Swimmer, and Hopper, as well as target reaching and ball catching tasks with simulated robotic arms, and a dynamic single ball juggling task. In addition, we discovered that, on some tasks, the agent can learn motion features through pure reinforcement learning if it is equipped with an appropriate network architecture, without requiring additional supervision. We also discovered that using an image difference between the current and previous frames as an extra input yields better outcomes than a temporal stack of frames.",1
"Many reinforcement learning applications involve the use of data that is sensitive, such as medical records of patients or financial information. However, most current reinforcement learning methods can leak information contained within the (possibly sensitive) data on which they are trained. To address this problem, we present the first differentially private approach for off-policy evaluation. We provide a theoretical analysis of the privacy-preserving properties of our algorithm and analyze its utility (speed of convergence). After describing some results of this theoretical analysis, we show empirically that our method outperforms previous methods (which are restricted to the on-policy setting).",0
"Numerous reinforcement learning applications involve utilizing sensitive data like medical records or financial information. However, most of the current reinforcement learning techniques can reveal the information present in the data on which they are trained, including sensitive data. To overcome this obstacle, we introduce the initial differentially private approach for off-policy evaluation. We offer a theoretical examination of our algorithm's privacy-retaining capabilities and its efficiency (convergence rate). After presenting some findings from our theoretical examination, we demonstrate through experiments that our approach outperforms prior methods (which are limited to the on-policy environment).",1
"Black-box optimizers that explore in parameter space have often been shown to outperform more sophisticated action space exploration methods developed specifically for the reinforcement learning problem. We examine these black-box methods closely to identify situations in which they are worse than action space exploration methods and those in which they are superior. Through simple theoretical analyses, we prove that complexity of exploration in parameter space depends on the dimensionality of parameter space, while complexity of exploration in action space depends on both the dimensionality of action space and horizon length. This is also demonstrated empirically by comparing simple exploration methods on several model problems, including Contextual Bandit, Linear Regression and Reinforcement Learning in continuous control.",0
"The effectiveness of black-box optimizers that explore parameter space has been found to surpass that of more intricate action space exploration techniques created specifically for the reinforcement learning problem. Our study delves into these black-box methods to determine when they are inferior or superior to action space exploration methods. By conducting straightforward theoretical analyses, we establish that the complexity of exploration in parameter space is dependent on the dimensionality of parameter space, while the complexity of exploration in action space is determined by both the dimensionality of action space and horizon length. This is further confirmed through empirical testing of basic exploration techniques on various model problems, such as Contextual Bandit, Linear Regression, and Reinforcement Learning in continuous control.",1
"The ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we extend the SFs & GPI framework in two ways. One of the basic assumptions underlying the original formulation of SFs & GPI is that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features. We relax this constraint and show that the theoretical guarantees supporting the framework can be extended to any set of tasks that only differ in the reward function. Our second contribution is to show that one can use the reward functions themselves as features for future tasks, without any loss of expressiveness, thus removing the need to specify a set of features beforehand. This makes it possible to combine SFs & GPI with deep learning in a more stable way. We empirically verify this claim on a complex 3D environment where observations are images from a first-person perspective. We show that the transfer promoted by SFs & GPI leads to very good policies on unseen tasks almost instantaneously. We also describe how to learn policies specialised to the new tasks in a way that allows them to be added to the agent's set of skills, and thus be reused in the future.",0
"The potential for scaling up reinforcement learning (RL) agents to currently unattainable environments lies in their ability to transfer skills across tasks. A framework based on successor features (SFs) and generalised policy improvement (GPI) has recently been introduced as a principled approach to skill transfer. This paper presents two extensions to the SFs & GPI framework. Firstly, we relax the assumption that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features, demonstrating that the theoretical guarantees supporting the framework can be extended to any set of tasks that differ only in their reward functions. Secondly, we show that reward functions can themselves be used as features for future tasks, eliminating the need to specify a set of features beforehand and enabling a more stable combination with deep learning. Empirical validation is provided on a complex 3D environment, where SFs & GPI transfer leads to strong policies on unseen tasks almost instantly. Additionally, we describe a method for learning specialized policies for new tasks that can be added to the agent's skill set for future reuse.",1
"As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Efficiency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artificial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.",0
"Reinforcement learning techniques can be utilized for photorealistic image synthesis by guiding light transport paths through an approximate solution of the Fredholm integral equation of the second kind, which is common to both fields. This approach can significantly enhance efficiency during rendering. Given the recent progress in reinforcement learning for game playing, the research aims to explore the use of artificial neural networks to represent an approximate solution of the integral equation and develop a corresponding loss function. Monte Carlo and quasi-Monte Carlo methods are employed to train the neural networks, which generate an unlimited number of training samples using standard information. These methods are applicable to light transport simulation and are demonstrated through various examples.",1
"In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.",0
"Deep reinforcement learning (RL) has made significant strides in solving complex problems across various domains in recent years. However, reproducing existing work and accurately assessing improvements offered by new methods are crucial for sustaining this progress. Unfortunately, reproducing state-of-the-art deep RL results is often challenging due to non-determinism in standard benchmark environments and intrinsic variance in the methods. This makes reported results difficult to interpret without significance metrics and tighter standardization of experimental reporting. This paper explores the challenges of reproducibility, proper experimental techniques, and reporting procedures. It highlights the variability in reported metrics and results when compared to common baselines and proposes guidelines to improve reproducibility in future deep RL results. The goal is to stimulate discussion on how to minimize wasted effort caused by non-reproducible and easily misinterpreted results, and ensure continued progress in the field.",1
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",0
"Traditionally, behavioral skills or policies for autonomous agents are acquired through reinforcement learning using reward functions, or through imitation learning using demonstrations. However, both methods have their limitations: manual engineering is required for reward functions, and human expertise is needed for generating demonstrations. Consequently, using natural language instructions to train machines has become an attractive alternative. Nonetheless, a single instruction may not be enough to convey the full task or may not be sufficient for the autonomous agent to understand how to carry out the task. Therefore, we propose an interactive approach to task specification, where an autonomous agent receives iterative language corrections to acquire the desired skill efficiently. Our language-guided policy learning algorithm integrates instructions with a sequence of corrections to quickly acquire new skills. Our experiments demonstrate that this method outperforms traditional non-interactive instruction following for simulated navigation and manipulation tasks.",1
"Despite remarkable successes, Deep Reinforcement Learning (DRL) is not robust to hyperparameterization, implementation details, or small environment changes (Henderson et al. 2017, Zhang et al. 2018). Overcoming such sensitivity is key to making DRL applicable to real world problems. In this paper, we identify sensitivity to time discretization in near continuous-time environments as a critical factor; this covers, e.g., changing the number of frames per second, or the action frequency of the controller. Empirically, we find that Q-learning-based approaches such as Deep Q- learning (Mnih et al., 2015) and Deep Deterministic Policy Gradient (Lillicrap et al., 2015) collapse with small time steps. Formally, we prove that Q-learning does not exist in continuous time. We detail a principled way to build an off-policy RL algorithm that yields similar performances over a wide range of time discretizations, and confirm this robustness empirically.",0
"Deep Reinforcement Learning (DRL) has achieved impressive results, but its sensitivity to hyperparameterization, implementation details, and minor environmental changes has hindered its applicability to real-world problems (Henderson et al., 2017; Zhang et al., 2018). To address this issue, we focus on the sensitivity to time discretization in near continuous-time environments, which involves altering the number of frames per second or the controller's action frequency. Our empirical findings reveal that Q-learning-based approaches, such as Deep Q-learning (Mnih et al., 2015) and Deep Deterministic Policy Gradient (Lillicrap et al., 2015), become unstable with small time steps. Moreover, we provide a formal proof that Q-learning is not viable in continuous time. To overcome this challenge, we propose an off-policy RL algorithm that maintains consistent performance across various time discretizations, and we verify its robustness through experimentation.",1
"Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.",0
"Successful application of model-free deep reinforcement learning (RL) algorithms has been observed in challenging sequential decision making and control tasks. However, high sample complexity and hyperparameter brittleness pose significant challenges that restrict the application of these methods in real-world domains. Recently, we have introduced Soft Actor-Critic (SAC), an off-policy actor-critic algorithm based on the maximum entropy RL framework. SAC aims to maximize both the expected return and entropy of the actor, allowing it to act as randomly as possible while succeeding in the task. We have made modifications to SAC to improve training acceleration and hyperparameter stability, including a constrained formulation that automatically tunes the temperature hyperparameter. We have evaluated SAC on various benchmark tasks and real-world challenges such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC has achieved state-of-the-art performance, surpassing prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Moreover, our approach is remarkably stable, achieving similar performance across different random seeds, unlike other off-policy algorithms. These results indicate that SAC is a promising algorithm for learning in real-world robotics tasks.",1
"Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are ""actionable."" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",0
"Across various areas of machine learning, representation learning remains a significant challenge. In reinforcement learning, the development of effective and functional representations has the potential to accelerate learning progress significantly and solve more complex problems. While past research on representation learning has primarily focused on generative approaches that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner, our paper aims to learn functionally salient representations instead. These representations do not necessarily capture all factors of variation in the observation space but rather focus on those elements that are critical for decision making. They are aware of the environment's dynamics and capture only the necessary factors of variation for decision making, without explicit observation reconstruction. We demonstrate how such representations can be useful in improving exploration for sparse reward problems, enabling long-horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. Our method is evaluated on various simulated environments and compared to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",1
"In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and sample for which the (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks.",0
"In certain situations, only a small number of states result in high rewards, making it difficult to learn from interactions with the environment. Therefore, it may be beneficial to focus on training on those high-reward states and the likely paths that lead to them. To accomplish this, we suggest using a backtracking model that predicts the preceding states that lead to a given high-reward state. By training a model to predict and sample (state, action) pairs that may have led to a high-value state, we can generate Recall Traces that terminate in good states, which can be used to improve a policy. We offer a variational interpretation of this concept and present a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories that lead to large rewards. Our approach enhances the sample efficiency of both on- and off-policy RL algorithms in various environments and tasks.",1
"Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.",0
"Complex predictive models can be learned by humans and animals, enabling them to reason about real-world phenomena with accuracy and reliability. However, deep neural network models lack the ability to adapt quickly in response to unexpected changes. This paper aims to develop a method for continual online learning using deep neural network models to address this limitation. The proposed approach involves an online learning procedure that utilizes stochastic gradient descent to update model parameters and an expectation maximization algorithm with a Chinese restaurant process prior to maintain a mixture of models for non-stationary task distributions. Meta-learning is used to meta-train a model to enable effective direct online adaptation with SGD, which is otherwise not possible for large function approximators. The meta-learning for online learning (MOLe) approach is applied to model-based reinforcement learning, where adapting the predictive model is crucial for control. The results demonstrate that MOLe outperforms alternative methods and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.",1
"Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\% on average, going as high as 36\% for some images, while maintaining the same 76.4\% top-1 accuracy on ImageNet.",0
"Although very deep convolutional neural networks have the potential for excellent recognition results, their computational cost often limits their practical use. However, we have developed an approach called BlockDrop that addresses this issue by dynamically choosing which layers of a deep network to execute during inference. By exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework can select which residual blocks to evaluate for a given image. We achieve this by training a policy network using associative reinforcement learning, which rewards the use of a minimal number of blocks while maintaining recognition accuracy. Our experiments on CIFAR and ImageNet show that these learned policies not only improve inference speed but also encode meaningful visual information. With a ResNet-101 model, our method achieves an average speedup of 20%, reaching up to 36% for some images, while maintaining a top-1 accuracy of 76.4% on ImageNet.",1
"There are two halves to RL systems: experience collection time and policy learning time. For a large number of samples in rollouts, experience collection time is the major bottleneck. Thus, it is necessary to speed up the rollout generation time with multi-process architecture support. Our work, dubbed WALL-E, utilizes multiple rollout samplers running in parallel to rapidly generate experience. Due to our parallel samplers, we experience not only faster convergence times, but also higher average reward thresholds. For example, on the MuJoCo HalfCheetah-v2 task, with $N = 10$ parallel sampler processes, we are able to achieve much higher average return than those from using only a single process architecture.",0
"RL systems consist of two parts, namely experience collection time and policy learning time. The experience collection time is the primary bottleneck when dealing with a vast number of samples in rollouts. Therefore, to accelerate the rollout generation time, it is essential to implement multi-process architecture support. Our project, WALL-E, employs multiple parallel rollout samplers to generate experience rapidly. With our parallel samplers, we achieve faster convergence times and higher average reward thresholds. For instance, on the MuJoCo HalfCheetah-v2 task, using $N = 10$ parallel sampler processes, we obtain significantly higher average return than with a single process architecture.",1
"In this paper we revisit the method of off-policy corrections for reinforcement learning (COP-TD) pioneered by Hallak et al. (2017). Under this method, online updates to the value function are reweighted to avoid divergence issues typical of off-policy learning. While Hallak et al.'s solution is appealing, it cannot easily be transferred to nonlinear function approximation. First, it requires a projection step onto the probability simplex; second, even though the operator describing the expected behavior of the off-policy learning algorithm is convergent, it is not known to be a contraction mapping, and hence, may be more unstable in practice. We address these two issues by introducing a discount factor into COP-TD. We analyze the behavior of discounted COP-TD and find it better behaved from a theoretical perspective. We also propose an alternative soft normalization penalty that can be minimized online and obviates the need for an explicit projection step. We complement our analysis with an empirical evaluation of the two techniques in an off-policy setting on the game Pong from the Atari domain where we find discounted COP-TD to be better behaved in practice than the soft normalization penalty. Finally, we perform a more extensive evaluation of discounted COP-TD in 5 games of the Atari domain, where we find performance gains for our approach.",0
"This paper reexamines the COP-TD off-policy correction method for reinforcement learning, which was first introduced by Hallak et al. (2017). The method involves reweighting online updates to the value function to counteract the common divergence issues that arise in off-policy learning. While Hallak et al.'s approach is effective, it cannot be easily applied to nonlinear function approximation due to the need for a projection step and possible instability of the expected behavior operator. To overcome these issues, we propose a discounted version of COP-TD, which we demonstrate to be theoretically better behaved. Additionally, we introduce a soft normalization penalty that eliminates the need for an explicit projection step. We evaluate the two techniques in an off-policy setting on the Atari game Pong, finding that discounted COP-TD performs better than the soft normalization penalty. We further evaluate our approach in five other Atari games, observing performance gains.",1
"Reward shaping is one of the most effective methods to tackle the crucial yet challenging problem of credit assignment in Reinforcement Learning (RL). However, designing shaping functions usually requires much expert knowledge and hand-engineering, and the difficulties are further exacerbated given multiple similar tasks to solve. In this paper, we consider reward shaping on a distribution of tasks, and propose a general meta-learning framework to automatically learn the efficient reward shaping on newly sampled tasks, assuming only shared state space but not necessarily action space. We first derive the theoretically optimal reward shaping in terms of credit assignment in model-free RL. We then propose a value-based meta-learning algorithm to extract an effective prior over the optimal reward shaping. The prior can be applied directly to new tasks, or provably adapted to the task-posterior while solving the task within few gradient updates. We demonstrate the effectiveness of our shaping through significantly improved learning efficiency and interpretable visualizations across various settings, including notably a successful transfer from DQN to DDPG.",0
"Reward shaping is a useful method for addressing the challenge of allocating credit in Reinforcement Learning (RL). However, creating shaping functions typically requires considerable expertise and manual engineering, which becomes even more difficult when dealing with multiple similar tasks. This paper investigates reward shaping on a range of tasks and proposes a meta-learning framework that automatically learns efficient reward shaping for newly sampled tasks, presuming that the state space is shared but not necessarily the action space. The authors derive the theoretically optimal reward shaping for model-free RL and introduce a value-based meta-learning algorithm that extracts an effective prior over the optimal reward shaping. This prior can be applied directly to new tasks or adapted to the task-posterior while solving the task with only a few gradient updates. The authors demonstrate the effectiveness of this shaping through improved learning efficiency and interpretable visualizations across various settings, including a successful transfer from DQN to DDPG.",1
"Complex environments and tasks pose a difficult problem for holistic end-to-end learning approaches. Decomposition of an environment into interacting controllable and non-controllable objects allows supervised learning for non-controllable objects and universal value function approximator learning for controllable objects. Such decomposition should lead to a shorter learning time and better generalisation capability. Here, we consider arcade-game environments as sets of interacting objects (controllable, non-controllable) and propose a set of functional modules that are specialized on mastering different types of interactions in a broad range of environments. The modules utilize regression, supervised learning, and reinforcement learning algorithms. Results of this case study in different Atari games suggest that human-level performance can be achieved by a learning agent within a human amount of game experience (10-15 minutes game time) when a proper decomposition of an environment or a task is provided. However, automatization of such decomposition remains a challenging problem. This case study shows how a model of a causal structure underlying an environment or a task can benefit learning time and generalization capability of the agent, and argues in favor of exploiting modular structure in contrast to using pure end-to-end learning approaches.",0
"When dealing with complex environments and tasks, holistic end-to-end learning approaches can be challenging. To address this issue, breaking down the environment into controllable and non-controllable objects can allow for supervised learning for non-controllable objects and universal value function approximator learning for controllable objects. This approach can result in a shorter learning time and better generalisation capability. In this study, arcade-game environments were examined, and a set of functional modules was proposed that specializes in mastering various interactions in different environments. These modules use regression, supervised learning, and reinforcement learning algorithms. The results of this study indicate that a learning agent can achieve human-level performance within a short amount of game experience (10-15 minutes game time) when a suitable environment or task decomposition is provided. However, automating this process remains a challenging task. This study highlights the benefits of using a model of the causal structure of an environment or task to improve learning time and generalization capability, and suggests the use of modular structures instead of pure end-to-end learning approaches.",1
"Dynamic portfolio optimization is the process of sequentially allocating wealth to a collection of assets in some consecutive trading periods, based on investors' return-risk profile. Automating this process with machine learning remains a challenging problem. Here, we design a deep reinforcement learning (RL) architecture with an autonomous trading agent such that, investment decisions and actions are made periodically, based on a global objective, with autonomy. In particular, without relying on a purely model-free RL agent, we train our trading agent using a novel RL architecture consisting of an infused prediction module (IPM), a generative adversarial data augmentation module (DAM) and a behavior cloning module (BCM). Our model-based approach works with both on-policy or off-policy RL algorithms. We further design the back-testing and execution engine which interact with the RL agent in real time. Using historical {\em real} financial market data, we simulate trading with practical constraints, and demonstrate that our proposed model is robust, profitable and risk-sensitive, as compared to baseline trading strategies and model-free RL agents from prior work.",0
"The concept of dynamic portfolio optimization involves distributing wealth among various assets over multiple trading periods, according to an investor's return-risk preferences. However, automating this process through machine learning is a difficult task. In this study, we introduce a deep reinforcement learning (RL) framework that utilizes an autonomous trading agent to make investment decisions based on a global objective and with autonomy. To train our trading agent, we developed a unique RL architecture that incorporates an infused prediction module (IPM), a generative adversarial data augmentation module (DAM), and a behavior cloning module (BCM), without relying solely on a model-free RL agent. Our model-based approach is compatible with both on-policy and off-policy RL algorithms. We also designed a real-time back-testing and execution engine to interact with the RL agent. By testing our approach using actual financial market data, we demonstrated that our model is robust, profitable, and risk-sensitive compared to previous model-free RL agents and baseline trading strategies.",1
"Discrete-action algorithms have been central to numerous recent successes of deep reinforcement learning. However, applying these algorithms to high-dimensional action tasks requires tackling the combinatorial increase of the number of possible actions with the number of action dimensions. This problem is further exacerbated for continuous-action tasks that require fine control of actions via discretization. In this paper, we propose a novel neural architecture featuring a shared decision module followed by several network branches, one for each action dimension. This approach achieves a linear increase of the number of network outputs with the number of degrees of freedom by allowing a level of independence for each individual action dimension. To illustrate the approach, we present a novel agent, called Branching Dueling Q-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN). We evaluate the performance of our agent on a set of challenging continuous control tasks. The empirical results show that the proposed agent scales gracefully to environments with increasing action dimensionality and indicate the significance of the shared decision module in coordination of the distributed action branches. Furthermore, we show that the proposed agent performs competitively against a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).",0
"Recent achievements in deep reinforcement learning have relied heavily on discrete-action algorithms. However, when applied to high-dimensional action tasks, the challenge of dealing with the combinatorial increase of possible actions with the number of action dimensions becomes evident. This issue is further aggravated in continuous-action tasks that require fine control of actions through discretization. To address this, we introduce a novel neural architecture that incorporates a shared decision module and multiple network branches, one for each action dimension. This approach enables a level of independence for each individual action dimension, resulting in a linear increase of network outputs with degrees of freedom. We present a new agent, called Branching Dueling Q-Network (BDQ), which is a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the efficacy of our approach. Our agent performs well on challenging continuous control tasks, exhibiting graceful scaling in environments with increasing action dimensionality. Additionally, we demonstrate the importance of the shared decision module in coordinating the distributed action branches. Finally, our results show that our proposed agent performs competitively with a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).",1
"Deep Reinforcement Learning (DeepRL) agents surpass human-level performances in a multitude of tasks. However, the direct mapping from states to actions makes it hard to interpret the rationale behind the decision making of agents. In contrast to previous a-posteriori methods of visualizing DeepRL policies, we propose an end-to-end trainable framework based on Rainbow, a representative Deep Q-Network (DQN) agent. Our method automatically learns important regions in the input domain, which enables characterizations of the decision making and interpretations for non-intuitive behaviors. Hence we name it Region Sensitive Rainbow (RS-Rainbow). RS-Rainbow utilizes a simple yet effective mechanism to incorporate visualization ability into the learning model, not only improving model interpretability, but leading to improved performance. Extensive experiments on the challenging platform of Atari 2600 demonstrate the superiority of RS-Rainbow. In particular, our agent achieves state of the art at just 25% of the training frames. Demonstrations and code are available at https://github.com/yz93/Learn-to-Interpret-Atari-Agents.",0
"A multitude of tasks have been surpassed by Deep Reinforcement Learning (DeepRL) agents, even surpassing human-level performance. However, the direct mapping from states to actions makes it difficult to understand the decision making process of agents. Our proposed Region Sensitive Rainbow (RS-Rainbow) framework is an end-to-end trainable mechanism based on Rainbow, a representative Deep Q-Network (DQN) agent. Our method automatically learns important regions in the input domain, enabling characterizations of the decision making process and interpretations for non-intuitive behaviors. This not only improves model interpretability but also leads to improved performance. RS-Rainbow incorporates a simple yet effective mechanism to incorporate visualization ability into the learning model. Extensive experiments on the challenging platform of Atari 2600 show the superiority of RS-Rainbow. Our agent achieves state of the art at just 25% of the training frames. Demonstrations and code are available at https://github.com/yz93/Learn-to-Interpret-Atari-Agents.",1
"Deep neural networks are data hungry models and thus face difficulties when attempting to train on small text datasets. Transfer learning is a potential solution but their effectiveness in the text domain is not as explored as in areas such as image analysis. In this paper, we study the problem of transfer learning for text summarization and discuss why existing state-of-the-art models fail to generalize well on other (unseen) datasets. We propose a reinforcement learning framework based on a self-critic policy gradient approach which achieves good generalization and state-of-the-art results on a variety of datasets. Through an extensive set of experiments, we also show the ability of our proposed framework to fine-tune the text summarization model using only a few training samples. To the best of our knowledge, this is the first work that studies transfer learning in text summarization and provides a generic solution that works well on unseen data.",0
"The challenge of training deep neural networks on small text datasets is well-known due to their high data requirements. While transfer learning has been used successfully in areas such as image analysis, its effectiveness in the text domain remains relatively unexplored. This paper explores the issue of transfer learning for text summarization and examines why current state-of-the-art models fail to generalize when applied to new datasets. To address this problem, we propose a reinforcement learning framework that employs a self-critic policy gradient approach, which achieves state-of-the-art results on various datasets. Our experiments demonstrate the framework's ability to fine-tune text summarization models using minimal training samples. This is the first study to investigate transfer learning in text summarization and provide a universal solution that performs well on unseen data.",1
"It has recently been shown that if feedback effects of decisions are ignored, then imposing fairness constraints such as demographic parity or equality of opportunity can actually exacerbate unfairness. We propose to address this challenge by modeling feedback effects as the dynamics of a Markov decision processes (MDPs). First, we define analogs of fairness properties that have been proposed for supervised learning. Second, we propose algorithms for learning fair decision-making policies for MDPs. We also explore extensions to reinforcement learning, where parts of the dynamical system are unknown and must be learned without violating fairness. Finally, we demonstrate the need to account for dynamical effects using simulations on a loan applicant MDP.",0
"Ignoring feedback effects in decision-making can lead to increased unfairness when fairness constraints like demographic parity or equality of opportunity are imposed. To overcome this challenge, we suggest incorporating feedback effects through Markov decision processes (MDPs). Our approach involves defining fairness properties similar to those used in supervised learning and developing algorithms for learning fair decision-making policies for MDPs. We also investigate extensions to reinforcement learning, where some parts of the system are unknown and must be learned without violating fairness. In addition, we use simulations on a loan applicant MDP to demonstrate the importance of considering dynamical effects.",1
"Although exploration in reinforcement learning is well understood from a theoretical point of view, provably correct methods remain impractical. In this paper we study the interplay between exploration and approximation, what we call approximate exploration. Our main goal is to further our theoretical understanding of pseudo-count based exploration bonuses (Bellemare et al., 2016), a practical exploration scheme based on density modelling. As a warm-up, we quantify the performance of an exploration algorithm, MBIE-EB (Strehl and Littman, 2008), when explicitly combined with state aggregation. This allows us to confirm that, as might be expected, approximation allows the agent to trade off between learning speed and quality of the learned policy. Next, we show how a given density model can be related to an abstraction and that the corresponding pseudo-count bonus can act as a substitute in MBIE-EB combined with this abstraction, but may lead to either under- or over-exploration. Then, we show that a given density model also defines an implicit abstraction, and find a surprising mismatch between pseudo-counts derived either implicitly or explicitly. Finally we derive a new pseudo-count bonus alleviating this issue.",0
"Theoretical knowledge on reinforcement learning exploration is well-established, but practical implementation of correct methods remains challenging. In this article, we delve into the relationship between exploration and approximation, which we refer to as approximate exploration. Our primary objective is to enhance our comprehension of the exploration bonus technique known as pseudo-count based exploration bonuses (Bellemare et al., 2016), which is based on density modelling. Initially, we evaluate the effectiveness of an exploration algorithm, MBIE-EB (Strehl and Littman, 2008), when combined with state aggregation. This allows us to confirm that approximation enables the agent to balance between the speed of learning and the quality of the policy learned. Next, we demonstrate how a specific density model can be connected to an abstraction, and how the corresponding pseudo-count bonus can serve as a substitute in MBIE-EB when combined with this abstraction, but may cause either under- or over-exploration. We then uncover a surprising mismatch between pseudo-counts derived either implicitly or explicitly, as a given density model also establishes an implicit abstraction. Finally, we present a new pseudo-count bonus that resolves this issue.",1
"Exploration bonus derived from the novelty of the states in an environment has become a popular approach to motivate exploration for deep reinforcement learning agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. Due to the capacity limitation of the models and difficulty of performing next-frame prediction, however, these methods typically fail to balance between exploration and exploitation in high-dimensional observation tasks, resulting in the agents forgetting the visited paths and exploring those states repeatedly. Such inefficient exploration behavior causes significant performance drops, especially in large environments with sparse reward signals. In this paper, we propose to introduce the concept of optical flow estimation from the field of computer vision to deal with the above issue. We propose to employ optical flow estimation errors to examine the novelty of new observations, such that agents are able to memorize and understand the visited states in a more comprehensive fashion. We compare our method against the previous approaches in a number of experimental experiments. Our results indicate that the proposed method appears to deliver superior and long-lasting performance than the previous methods. We further provide a set of comprehensive ablative analysis of the proposed method, and investigate the impact of optical flow estimation on the learning curves of the DRL agents.",0
"In recent years, the use of exploration bonuses to motivate deep reinforcement learning agents has gained popularity. Curiosity-driven exploration methods estimate the novelty of new observations by predicting errors in their system dynamics models. However, these methods tend to struggle with balancing exploration and exploitation in high-dimensional observation tasks due to model capacity limitations and next-frame prediction difficulties. This causes agents to forget visited paths and repeatedly explore the same states, leading to poor performance in large environments with sparse reward signals. To address this issue, we propose using optical flow estimation from computer vision to examine the novelty of new observations. This allows agents to better understand and remember visited states. We conduct several experiments comparing our approach to previous methods and find that our approach delivers superior and long-lasting performance. We also perform comprehensive ablative analysis and investigate the impact of optical flow estimation on the agents' learning curves.",1
"While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.",0
"Although deep reinforcement learning has successfully tackled many challenging control tasks, its practical use has been limited by the inability to guarantee the safety of learned policies. To address this issue, we propose a verifiable reinforcement learning approach that utilizes decision tree policies. These policies can handle complex scenarios due to their nonparametric nature, while also being easily verifiable with current techniques since they are highly structured. However, training decision tree policies is difficult. Therefore, we introduce VIPER, an algorithm that combines model compression and imitation learning ideas to learn decision tree policies guided by a DNN policy (the oracle) and its Q-function. Our results show that VIPER outperforms two baselines. We apply VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves equal performance to the original DNN policy.",1
"Reinforcement learning (RL) algorithms allow agents to learn skills and strategies to perform complex tasks without detailed instructions or expensive labelled training examples. That is, RL agents can learn, as we learn. Given the importance of learning in our intelligence, RL has been thought to be one of key components to general artificial intelligence, and recent breakthroughs in deep reinforcement learning suggest that neural networks (NN) are natural platforms for RL agents. However, despite the efficiency and versatility of NN-based RL agents, their decision-making remains incomprehensible, reducing their utilities. To deploy RL into a wider range of applications, it is imperative to develop explainable NN-based RL agents. Here, we propose a method to derive a secondary comprehensible agent from a NN-based RL agent, whose decision-makings are based on simple rules. Our empirical evaluation of this secondary agent's performance supports the possibility of building a comprehensible and transparent agent using a NN-based RL agent.",0
"Reinforcement learning (RL) algorithms enable agents to acquire the skills and strategies necessary to perform complex tasks without detailed instructions or expensive labeled training examples. This means that RL agents are capable of learning in the same way that we do. Given that learning is a crucial aspect of intelligence, RL has been identified as a key component of general artificial intelligence, and recent advancements in deep reinforcement learning suggest that neural networks (NN) are a natural fit for RL agents. Despite the efficiency and versatility of NN-based RL agents, however, their decision-making processes remain incomprehensible, limiting their usefulness. To extend the applications of RL, it is essential to create NN-based RL agents that are explicable. In this study, we propose a technique for generating a secondary comprehensible agent from an NN-based RL agent whose decision-making is based on simple rules. Our experimental analysis of this secondary agent's performance indicates that it is feasible to construct a comprehensible and transparent agent utilizing an NN-based RL agent.",1
"Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether causal reasoning can emerge via meta-reinforcement learning. We train a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure. We find that the trained agent can perform causal reasoning in novel situations in order to obtain rewards. The agent can select informative interventions, draw causal inferences from observational data, and make counterfactual predictions. Although established formal causal reasoning algorithms also exist, in this paper we show that such reasoning can arise from model-free reinforcement learning, and suggest that causal reasoning in complex settings may benefit from the more end-to-end learning-based approaches presented here. This work also offers new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform -- and interpret -- experiments.",0
"The ability to detect and make use of the underlying causal structure in the environment is a critical obstacle for intelligent agents. This study aims to investigate whether meta-reinforcement learning can lead to the emergence of causal reasoning. We utilize a recurrent network trained with model-free reinforcement learning to tackle a series of problems that feature causal structures. Our findings indicate that the trained agent can apply causal reasoning in unfamiliar situations to acquire rewards. The agent can choose valuable interventions, deduce causal implications from observational data, and generate counterfactual predictions. Although established formal causal reasoning algorithms exist, we demonstrate that this kind of reasoning can arise from model-free reinforcement learning. Furthermore, we suggest that the more end-to-end, learning-based approaches presented here may benefit causal reasoning in complicated scenarios. This research also provides new techniques for structured exploration in reinforcement learning by equipping agents with the capacity to perform and interpret experiments.",1
"Policy evaluation is a key process in reinforcement learning. It assesses a given policy using estimation of the corresponding value function. When using a parameterized function to approximate the value, it is common to optimize the set of parameters by minimizing the sum of squared Bellman Temporal Differences errors. However, this approach ignores certain distributional properties of both the errors and value parameters. Taking these distributions into account in the optimization process can provide useful information on the amount of confidence in value estimation. In this work we propose to optimize the value by minimizing a regularized objective function which forms a trust region over its parameters. We present a novel optimization method, the Kalman Optimization for Value Approximation (KOVA), based on the Extended Kalman Filter. KOVA minimizes the regularized objective function by adopting a Bayesian perspective over both the value parameters and noisy observed returns. This distributional property provides information on parameter uncertainty in addition to value estimates. We provide theoretical results of our approach and analyze the performance of our proposed optimizer on domains with large state and action spaces.",0
"Reinforcement learning involves policy evaluation, which evaluates a policy by estimating its corresponding value function. To approximate the value using a parameterized function, minimizing the sum of squared Bellman Temporal Differences errors is often employed. However, this method overlooks important distributional properties of both the errors and value parameters. Incorporating these properties in the optimization process can yield valuable information about the confidence in value estimation. In this study, we suggest minimizing a regularized objective function that creates a trust region over its parameters to optimize the value. We introduce a new optimization technique, KOVA, which uses the Extended Kalman Filter and a Bayesian approach to minimize the regularized objective function. This approach considers both the value parameters and observed returns' uncertainties, providing information on parameter uncertainty beyond value estimates. We present theoretical results and performance analysis of our proposed optimizer in domains with large state and action spaces.",1
"A reinforcement learning agent tries to maximize its cumulative payoff by interacting in an unknown environment. It is important for the agent to explore suboptimal actions as well as to pick actions with highest known rewards. Yet, in sensitive domains, collecting more data with exploration is not always possible, but it is important to find a policy with a certain performance guaranty. In this paper, we present a brief survey of methods available in the literature for balancing exploration-exploitation trade off and computing robust solutions from fixed samples in reinforcement learning.",0
"The objective of a reinforcement learning agent is to achieve the maximum cumulative payoff while interacting with an unfamiliar environment. The agent should focus on both selecting actions with the highest known rewards and exploring suboptimal actions. However, in domains that require sensitivity, exploration may not always be feasible, and it is essential to create a policy that guarantees a certain level of performance. This paper provides a concise overview of existing approaches in the literature that balance trade-offs between exploration and exploitation and generate strong solutions using limited data in reinforcement learning.",1
"The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.",0
"Understanding videos requires the task of video grounding, which involves localizing a natural language description within a video. Current approaches slide a window over the entire video or exhaustively rank all possible clip-sentence pairs, resulting in a large number of candidates. To address this issue, we present a novel approach that formulates this task as a sequential decision-making problem by training an agent to regulate temporal grounding boundaries based on its policy. Our reinforcement learning-based framework, combined with multi-task learning, improves performance by incorporating supervised boundary information during training. Remarkably, our proposed framework achieves state-of-the-art results on ActivityNet'18 DenseCaption and Charades-STA datasets, requiring only 10 clips or less per video.",1
"This paper addresses the question of how a previously available control policy $\pi_s$ can be used as a supervisor to more quickly and safely train a new learned control policy $\pi_L$ for a robot. A weighted average of the supervisor and learned policies is used during trials, with a heavier weight initially on the supervisor, in order to allow safe and useful physical trials while the learned policy is still ineffective. During the process, the weight is adjusted to favor the learned policy. As weights are adjusted, the learned network must compensate so as to give safe and reasonable outputs under the different weights. A pioneer network is introduced that pre-learns a policy that performs similarly to the current learned policy under the planned next step for new weights; this pioneer network then replaces the currently learned network in the next set of trials. Experiments in OpenAI Gym demonstrate the effectiveness of the proposed method.",0
"The focus of this article is on utilizing an existing control policy, denoted as $\pi_s$, as a supervisor to expedite and ensure the safety of training a new learned control policy, $\pi_L$, for a robot. A combination of the supervisor and learned policies, where the supervisor initially carries more weight, is implemented during trials to enable secure and worthwhile physical trials while the learned policy is still insufficient. Throughout the process, the weight distribution is altered to favor the learned policy, and as a result, the learned network must compensate to generate secure and reasonable outputs under the varying weights. To address this issue, a pioneer network is introduced that pre-learns a policy that functions similarly to the current learned policy for the anticipated next step in weights. The pioneer network then replaces the current learned network for the subsequent set of trials. The effectiveness of this approach is demonstrated through experiments conducted in OpenAI Gym.",1
"The problem of retrosynthetic planning can be framed as one player game, in which the chemist (or a computer program) works backwards from a molecular target to simpler starting materials though a series of choices regarding which reactions to perform. This game is challenging as the combinatorial space of possible choices is astronomical, and the value of each choice remains uncertain until the synthesis plan is completed and its cost evaluated. Here, we address this problem using deep reinforcement learning to identify policies that make (near) optimal reaction choices during each step of retrosynthetic planning. Using simulated experience or self-play, we train neural networks to estimate the expected synthesis cost or value of any given molecule based on a representation of its molecular structure. We show that learned policies based on this value network outperform heuristic approaches in synthesizing unfamiliar molecules from available starting materials using the fewest number of reactions. We discuss how the learned policies described here can be incorporated into existing synthesis planning tools and how they can be adapted to changes in the synthesis cost objective or material availability.",0
"To plan retrosynthesis, the chemist or a computer program plays a one-player game, tracing back from a complex molecular target to simpler starting materials by choosing reactions. However, determining the optimal choice is challenging due to the vast number of possibilities, and the actual cost is unknown until the synthesis plan is completed. To address this issue, we employ deep reinforcement learning to discover policies that make optimal reaction choices during each retrosynthetic planning step. By training neural networks to estimate the synthesis cost or value of a molecule based on its molecular structure, we establish that policies learned from this value network outperform heuristic methods in synthesizing unfamiliar molecules from available starting materials using the minimum number of reactions. We also consider how these learned policies can be integrated into existing synthesis planning tools and adapted to changes in the synthesis cost objective or material availability.",1
"We give a simple optimistic algorithm for which it is easy to derive regret bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly ergodic Markov decision processes with $S$ states, $A$ actions, and mixing time parameter $t_{\rm mix}$. These bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. They could only be improved by using an alternative mixing time parameter.",0
"In Markov decision processes that are uniformly ergodic and have $S$ states, $A$ actions, and a mixing time parameter of $t_{\rm mix}$, we offer a straightforward optimistic algorithm. After $T$ steps, regret bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ can be derived with ease. These are the initial regret bounds in the non-episodic context with an optimal reliance on all the given parameters. The only way to enhance these bounds is by utilizing a different mixing time parameter.",1
"We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM",0
"The aim of our study is to teach the Shadow Dexterous Hand to perform vision-based object reorientation through the use of reinforcement learning (RL). We conduct the training in a simulated environment where we randomize physical properties such as friction coefficients and object appearance. Interestingly, our policies successfully transfer to the physical robot despite being solely trained in simulation, and without any human demonstrations. Our approach enables the emergence of human-like manipulation behaviors, including finger gaiting, multi-finger coordination, and controlled use of gravity. Our results are achieved using the same distributed RL system used for training OpenAI Five, and a video showcasing our findings can be found at https://youtu.be/jwSbzNHGflM.",1
"Machine learning models have become more and more complex in order to better approximate complex functions. Although fruitful in many domains, the added complexity has come at the cost of model interpretability. The once popular k-nearest neighbors (kNN) approach, which finds and uses the most similar data for reasoning, has received much less attention in recent decades due to numerous problems when compared to other techniques. We show that many of these historical problems with kNN can be overcome, and our contribution has applications not only in machine learning but also in online learning, data synthesis, anomaly detection, model compression, and reinforcement learning, without sacrificing interpretability. We introduce a synthesis between kNN and information theory that we hope will provide a clear path towards models that are innately interpretable and auditable. Through this work we hope to gather interest in combining kNN with information theory as a promising path to fully auditable machine learning and artificial intelligence.",0
"To better approximate complex functions, machine learning models have become increasingly complex, but this has resulted in reduced model interpretability. The k-nearest neighbors (kNN) approach, which uses similar data for reasoning, has been less popular due to its numerous problems compared to other techniques. However, we demonstrate that many of these historical issues with kNN can be resolved, and our contribution has applications in various areas of machine learning while maintaining interpretability. We propose a synthesis of kNN and information theory, which we believe is a promising path towards fully auditable machine learning and artificial intelligence, without sacrificing interpretability. Our aim is to generate interest in combining kNN with information theory to achieve this goal.",1
"Building upon the recent success of deep reinforcement learning methods, we investigate the possibility of on-policy reinforcement learning improvement by reusing the data from several consecutive policies. On-policy methods bring many benefits, such as ability to evaluate each resulting policy. However, they usually discard all the information about the policies which existed before. In this work, we propose adaptation of the replay buffer concept, borrowed from the off-policy learning setting, to create the method, combining advantages of on- and off-policy learning. To achieve this, the proposed algorithm generalises the $Q$-, value and advantage functions for data from multiple policies. The method uses trust region optimisation, while avoiding some of the common problems of the algorithms such as TRPO or ACKTR: it uses hyperparameters to replace the trust region selection heuristics, as well as the trainable covariance matrix instead of the fixed one. In many cases, the method not only improves the results comparing to the state-of-the-art trust region on-policy learning algorithms such as PPO, ACKTR and TRPO, but also with respect to their off-policy counterpart DDPG.",0
"We aim to improve on-policy reinforcement learning by reusing data from multiple consecutive policies, building on the success of recent deep reinforcement learning methods. While on-policy methods offer benefits such as the ability to evaluate each policy, they often discard information from previous policies. To address this, we propose a method that combines the advantages of on- and off-policy learning by adapting the replay buffer concept from off-policy learning. Our algorithm generalises the $Q$-, value and advantage functions for data from multiple policies and uses trust region optimisation with hyperparameters replacing trust region selection heuristics and a trainable covariance matrix replacing a fixed one. This method outperforms state-of-the-art trust region on-policy learning algorithms such as PPO, ACKTR, and TRPO, as well as their off-policy counterpart DDPG in many cases.",1
"We propose a hierarchically structured reinforcement learning approach to address the challenges of planning for generating coherent multi-sentence stories for the visual storytelling task. Within our framework, the task of generating a story given a sequence of images is divided across a two-level hierarchical decoder. The high-level decoder constructs a plan by generating a semantic concept (i.e., topic) for each image in sequence. The low-level decoder generates a sentence for each image using a semantic compositional network, which effectively grounds the sentence generation conditioned on the topic. The two decoders are jointly trained end-to-end using reinforcement learning. We evaluate our model on the visual storytelling (VIST) dataset. Empirical results from both automatic and human evaluations demonstrate that the proposed hierarchically structured reinforced training achieves significantly better performance compared to a strong flat deep reinforcement learning baseline.",0
"To tackle the difficulties of generating coherent multi-sentence stories for visual storytelling, we present a reinforcement learning method with hierarchical structure. Our approach involves a two-level decoder system that divides the task of generating a story from a sequence of images. The high-level decoder develops a plan by generating a semantic concept or topic for each image in the sequence, while the low-level decoder generates a sentence for each image using a semantic compositional network that grounds the sentence generation on the topic. Both decoders are jointly trained using reinforcement learning. We evaluate our model using the VIST dataset and show that our hierarchically structured reinforcement learning approach outperforms a strong flat deep reinforcement learning baseline, as demonstrated by empirical results from both automatic and human evaluations.",1
"Central Pattern Generators (CPGs) are biological neural circuits capable of producing coordinated rhythmic outputs in the absence of rhythmic input. As a result, they are responsible for most rhythmic motion in living organisms. This rhythmic control is broadly applicable to fields such as locomotive robotics and medical devices. In this paper, we explore the possibility of creating a self-sustaining CPG network for reinforcement learning that learns rhythmic motion more efficiently and across more general environments than the current multilayer perceptron (MLP) baseline models. Recent work introduces the Structured Control Net (SCN), which maintains linear and nonlinear modules for local and global control, respectively. Here, we show that time-sequence architectures such as Recurrent Neural Networks (RNNs) model CPGs effectively. Combining previous work with RNNs and SCNs, we introduce the Recurrent Control Net (RCN), which adds a linear component to the, RCNs match and exceed the performance of baseline MLPs and SCNs across all environment tasks. Our findings confirm existing intuitions for RNNs on reinforcement learning tasks, and demonstrate promise of SCN-like structures in reinforcement learning.",0
"Biological neural circuits known as Central Pattern Generators (CPGs) can generate coordinated rhythmic outputs without rhythmic input and are responsible for most rhythmic motion in living organisms. This rhythmic control has broad applications in fields such as locomotive robotics and medical devices. The purpose of this paper is to investigate the creation of a self-sustaining CPG network for reinforcement learning that can learn rhythmic motion more efficiently and across more diverse environments than current multilayer perceptron (MLP) baseline models. The Structured Control Net (SCN) has been introduced as a linear and nonlinear module for local and global control, respectively. This paper demonstrates that Recurrent Neural Networks (RNNs) can effectively model CPGs. By combining previous work with RNNs and SCNs, the Recurrent Control Net (RCN) is introduced, which adds a linear component. The RCNs outperform baseline MLPs and SCNs in all environmental tasks, confirming existing intuitions for RNNs in reinforcement learning tasks and demonstrating the potential of SCN-like structures in reinforcement learning.",1
"This paper proposes a new reinforcement learning (RL) algorithm that enhances exploration by amplifying the imitation effect (AIE). This algorithm consists of self-imitation learning and random network distillation algorithms. We argue that these two algorithms complement each other and that combining these two algorithms can amplify the imitation effect for exploration. In addition, by adding an intrinsic penalty reward to the state that the RL agent frequently visits and using replay memory for learning the feature state when using an exploration bonus, the proposed approach leads to deep exploration and deviates from the current converged policy. We verified the exploration performance of the algorithm through experiments in a two-dimensional grid environment. In addition, we applied the algorithm to a simulated environment of unmanned combat aerial vehicle (UCAV) mission execution, and the empirical results show that AIE is very effective for finding the UCAV's shortest flight path to avoid an enemy's missiles.",0
"The paper presents a novel reinforcement learning (RL) technique that boosts exploration by intensifying the imitation effect (AIE). The technique integrates self-imitation learning and random network distillation algorithms to complement each other and enhance the imitation effect for exploration. Furthermore, the proposed approach includes an intrinsic penalty reward for frequently visited states of the RL agent and the use of replay memory for learning feature states, which facilitates deep exploration and deviation from the current policy. The algorithm's exploration performance was evaluated through experiments in a two-dimensional grid environment, and it was also applied to a simulated unmanned combat aerial vehicle (UCAV) mission execution environment, where the empirical results reveal that AIE is highly efficient in discovering the shortest flight path to evade enemy missiles.",1
"In this work, we study value function approximation in reinforcement learning (RL) problems with high dimensional state or action spaces via a generalized version of representation policy iteration (RPI). We consider the limitations of proto-value functions (PVFs) at accurately approximating the value function in low dimensions and we highlight the importance of features learning for an improved low-dimensional value function approximation. Then, we adopt different representation learning algorithm on graphs to learn the basis functions that best represent the value function. We empirically show that node2vec, an algorithm for scalable feature learning in networks, and the Variational Graph Auto-Encoder constantly outperform the commonly used smooth proto-value functions in low-dimensional feature space.",0
"The focus of our research is to explore the use of a generalized version of representation policy iteration (RPI) to approximate value functions in reinforcement learning (RL) problems that involve high dimensional state or action spaces. Our investigation highlights the limitations of proto-value functions (PVFs) in accurately approximating the value function in situations where dimensions are low, and underscores the importance of feature learning for improved low-dimensional value function approximation. To achieve this, we explore different representation learning algorithms on graphs to identify the basis functions that best represent the value function. Our empirical analysis shows that when compared to the commonly used smooth proto-value functions, node2vec, an algorithm for scalable feature learning in networks, and the Variational Graph Auto-Encoder consistently outperform in low-dimensional feature space.",1
"This paper proposes CodeX, an end-to-end framework that facilitates encoding, bitwidth customization, fine-tuning, and implementation of neural networks on FPGA platforms. CodeX incorporates nonlinear encoding to the computation flow of neural networks to save memory. The encoded features demand significantly lower storage compared to the raw full-precision activation values; therefore, the execution flow of CodeX hardware engine is completely performed within the FPGA using on-chip streaming buffers with no access to the off-chip DRAM. We further propose a fully-automated algorithm inspired by reinforcement learning which determines the customized encoding bitwidth across network layers. CodeX full-stack framework comprises of a compiler which takes a high-level Python description of an arbitrary neural network architecture. The compiler then instantiates the corresponding elements from CodeX Hardware library for FPGA implementation. Proof-of-concept evaluations on MNIST, SVHN, and CIFAR-10 datasets demonstrate an average of 4.65x throughput improvement compared to stand-alone weight encoding. We further compare CodeX with six existing full-precision DNN accelerators on ImageNet, showing an average of 3.6x and 2.54x improvement in throughput and performance-per-watt, respectively.",0
"The article introduces CodeX as a comprehensive system for optimizing neural network implementation on FPGA platforms. It employs nonlinear encoding to reduce memory usage and improve efficiency, while a custom algorithm determines the optimal encoding bitwidth for each network layer. The system includes a compiler that uses Python to describe network architecture and instantiate CodeX Hardware library elements for FPGA implementation. CodeX outperforms other DNN accelerators, with a 4.65x increase in throughput for MNIST, SVHN, and CIFAR-10 datasets and 3.6x and 2.54x improvement in throughput and performance-per-watt, respectively, compared to six existing full-precision DNN accelerators on ImageNet. The FPGA hardware engine executes entirely within the FPGA, using on-chip streaming buffers without accessing off-chip DRAM.",1
"We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation -- accessing policy and value function classes exclusively through standard optimization primitives -- and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE, cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.",0
"Our focus is on the computational feasibility of PAC reinforcement learning when dealing with rich observations. We introduce novel algorithms that are guaranteed to be sample-efficient when handling environments with deterministic hidden state dynamics and stochastic rich observations. These algorithms operate within an oracle model of computation, accessing policy and value function classes solely through standard optimization techniques. This makes them computationally efficient alternatives to previous algorithms that necessitate enumeration. When it comes to stochastic hidden state dynamics, we demonstrate that OLIVE, the only sample-efficient algorithm known, cannot be implemented within the oracle model. Additionally, we provide several examples that highlight the fundamental difficulties of achieving tractable PAC reinforcement learning in such general scenarios.",1
"Deep Reinforcement Learning (DRL) has become increasingly powerful in recent years, with notable achievements such as Deepmind's AlphaGo. It has been successfully deployed in commercial vehicles like Mobileye's path planning system. However, a vast majority of work on DRL is focused on toy examples in controlled synthetic car simulator environments such as TORCS and CARLA. In general, DRL is still at its infancy in terms of usability in real-world applications. Our goal in this paper is to encourage real-world deployment of DRL in various autonomous driving (AD) applications. We first provide an overview of the tasks in autonomous driving systems, reinforcement learning algorithms and applications of DRL to AD systems. We then discuss the challenges which must be addressed to enable further progress towards real-world deployment.",0
"In recent years, Deep Reinforcement Learning (DRL) has gained immense power and has been responsible for notable achievements such as Deepmind's AlphaGo. DRL has even been successfully implemented in commercial vehicles like Mobileye's path planning system. However, the majority of DRL research has been focused on small-scale examples in controlled synthetic car simulator environments like TORCS and CARLA. As a result, DRL is still in its early stages of development when it comes to practical applications in the real world. Our objective in this paper is to promote the integration of DRL in various autonomous driving (AD) applications. We begin by providing a comprehensive overview of the tasks involved in autonomous driving systems, reinforcement learning algorithms, and how DRL can be applied to AD systems. We then address the obstacles that need to be resolved to facilitate further progress towards real-world deployment.",1
"Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted heuristics and rule-based policies that require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model compression policy. This learning-based compression policy outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved 1.81x speedup of measured inference latency on an Android phone and 1.43x speedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.",0
"Efficient deployment of neural network models on mobile devices with limited computation resources and tight power budgets is critical, and model compression is a key technique to achieve this. However, traditional model compression techniques rely on hand-crafted heuristics and rule-based policies that are sub-optimal and time-consuming. In this paper, we introduce AutoML for Model Compression (AMC), which employs reinforcement learning to provide an automated model compression policy that outperforms traditional rule-based compression policies by achieving higher compression ratios, better accuracy preservation, and freeing up human labor. Our push-the-button compression pipeline achieved 2.7% better accuracy than handcrafted compression policies for VGG-16 on ImageNet, with a 4x reduction in FLOPs. We also applied this automated pipeline to MobileNet and achieved a 1.81x speedup of measured inference latency on an Android phone and a 1.43x speedup on the Titan XP GPU, with only a 0.1% loss of ImageNet Top-1 accuracy.",1
"Representation learning becomes especially important for complex systems with multimodal data sources such as cameras or sensors. Recent advances in reinforcement learning and optimal control make it possible to design control algorithms on these latent representations, but the field still lacks a large-scale standard dataset for unified comparison. In this work, we present a large-scale dataset and evaluation framework for representation learning for the complex task of landing an airplane. We implement and compare several approaches to representation learning on this dataset in terms of the quality of simple supervised learning tasks and disentanglement scores. The resulting representations can be used for further tasks such as anomaly detection, optimal control, model-based reinforcement learning, and other applications.",0
"Representation learning plays a critical role in dealing with intricate systems that involve multiple data sources such as cameras or sensors. Though there have been recent advancements in reinforcement learning and optimal control, which facilitate the development of control algorithms based on these concealed representations, the field still lacks a standardized and vast dataset for comparative assessment. Accordingly, we introduce a comprehensive dataset and assessment framework in this study that focuses on representation learning for the intricate task of landing an airplane. We compare several approaches to representation learning on this dataset in terms of their efficiency in simple supervised learning tasks and disentanglement scores. The resulting representations can be utilized for various other applications, including anomaly detection, optimal control, model-based reinforcement learning, and more.",1
"Sepsis is the leading cause of mortality in the ICU. It is challenging to manage because individual patients respond differently to treatment. Thus, tailoring treatment to the individual patient is essential for the best outcomes. In this paper, we take steps toward this goal by applying a mixture-of-experts framework to personalize sepsis treatment. The mixture model selectively alternates between neighbor-based (kernel) and deep reinforcement learning (DRL) experts depending on patient's current history. On a large retrospective cohort, this mixture-based approach outperforms physician, kernel only, and DRL-only experts.",0
"The ICU's primary cause of death is sepsis, which poses a management challenge due to varying patient responses to treatment. Therefore, customizing treatment to each patient is imperative for optimal outcomes. In this study, we move closer to this objective by utilizing a blend-of-experts framework to personalize sepsis treatment. The model selectively switches between neighbor-based (kernel) and deep reinforcement learning (DRL) experts, depending on the patient's current history. This approach's superiority over physician-only, kernel-only, and DRL-only experts was demonstrated in a large retrospective cohort.",1
"We study the global convergence of generative adversarial imitation learning for linear quadratic regulators, which is posed as minimax optimization. To address the challenges arising from non-convex-concave geometry, we analyze the alternating gradient algorithm and establish its Q-linear rate of convergence to a unique saddle point, which simultaneously recovers the globally optimal policy and reward function. We hope our results may serve as a small step towards understanding and taming the instability in imitation learning as well as in more general non-convex-concave alternating minimax optimization that arises from reinforcement learning and generative adversarial learning.",0
"Our research focuses on examining the worldwide convergence of generative adversarial imitation learning for linear quadratic regulators. This process involves minimizing and maximizing certain aspects. To overcome the challenges that arise from non-convex-concave geometry, we analyze the effectiveness of the alternating gradient algorithm and establish its Q-linear rate of convergence to a unique saddle point. This point allows for the recovery of both the globally optimal policy and reward function. We believe that our findings can contribute to a better understanding of and solution to the instability that occurs in imitation learning and other non-convex-concave alternating minimax optimization processes in reinforcement and generative adversarial learning.",1
"Automatically generating the descriptions of an image, i.e., image captioning, is an important and fundamental topic in artificial intelligence, which bridges the gap between computer vision and natural language processing. Based on the successful deep learning models, especially the CNN model and Long Short-Term Memories (LSTMs) with attention mechanism, we propose a hierarchical attention model by utilizing both of the global CNN features and the local object features for more effective feature representation and reasoning in image captioning. The generative adversarial network (GAN), together with a reinforcement learning (RL) algorithm, is applied to solve the exposure bias problem in RNN-based supervised training for language problems. In addition, through the automatic measurement of the consistency between the generated caption and the image content by the discriminator in the GAN framework and RL optimization, we make the finally generated sentences more accurate and natural. Comprehensive experiments show the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference.",0
"The task of generating descriptions for images, or image captioning, is a crucial area of study in artificial intelligence, as it connects computer vision and natural language processing. Our proposed hierarchical attention model, which incorporates both global CNN features and local object features, leverages successful deep learning models such as CNN and Long Short-Term Memories (LSTMs) with attention mechanisms to improve feature representation and reasoning in image captioning. We also use a generative adversarial network (GAN) and a reinforcement learning (RL) algorithm to address the exposure bias problem in RNN-based supervised training for language tasks. By measuring the consistency between generated captions and image content automatically through the GAN framework and RL optimization, we achieve more accurate and natural sentences. Comprehensive experiments demonstrate the effectiveness of our RL-based optimization method and the improved performance of the hierarchical attention mechanism. Our model achieves state-of-the-art results on multiple important metrics in the MSCOCO dataset, using only greedy inference.",1
"Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire DGX-1 to learn successful strategies in Atari games in mere minutes, using both synchronous and asynchronous algorithms.",0
"Despite its recent successes, deep reinforcement learning (RL) still faces a major obstacle in terms of experiment turn-around time, both in research and practice. Our study explores ways to enhance existing deep RL algorithms for modern computer systems, particularly for a combination of CPUs and GPUs. We have verified that policy gradient and Q-value learning algorithms can be adjusted to learn from numerous parallel simulator instances. Additionally, we have discovered that it is possible to train using batch sizes that are notably larger than the standard, without having an adverse effect on sample complexity or final performance. By incorporating these findings, we have developed a comprehensive framework for parallelization that significantly accelerates experiments in both algorithm categories. All neural network computations leverage GPUs, enabling faster data collection and training. Our findings demonstrate that we can use an entire DGX-1 to learn successful strategies in Atari games in mere minutes, using both synchronous and asynchronous algorithms.",1
"Algorithmic assurances from advanced autonomous systems assist human users in understanding, trusting, and using such systems appropriately. Designing these systems with the capacity of assessing their own capabilities is one approach to creating an algorithmic assurance. The idea of `machine self-confidence' is introduced for autonomous systems. Using a factorization based framework for self-confidence assessment, one component of self-confidence, called `solver-quality', is discussed in the context of Markov decision processes for autonomous systems. Markov decision processes underlie much of the theory of reinforcement learning, and are commonly used for planning and decision making under uncertainty in robotics and autonomous systems. A `solver quality' metric is formally defined in the context of decision making algorithms based on Markov decision processes. A method for assessing solver quality is then derived, drawing inspiration from empirical hardness models. Finally, numerical experiments for an unmanned autonomous vehicle navigation problem under different solver, parameter, and environment conditions indicate that the self-confidence metric exhibits the desired properties. Discussion of results, and avenues for future investigation are included.",0
"Advanced autonomous systems provide algorithmic assurances that aid human users in comprehending, relying on, and utilizing these systems suitably. One approach to creating such assurances is by designing these systems to be capable of evaluating their own abilities. The concept of ""machine self-confidence"" has been introduced for autonomous systems. In this respect, a factorization-based framework has been proposed for assessing self-confidence, with one of its components called ""solver-quality"" being discussed in the context of Markov decision processes, which are fundamental to reinforcement learning theory and are often employed in decision-making and planning under ambiguity in robotics and autonomous systems. The ""solver quality"" metric is formally defined in terms of decision-making algorithms based on Markov decision processes, and a method for evaluating solver quality is derived based on empirical hardness models. Finally, numerical experiments on an unmanned autonomous vehicle navigation problem under varying solver, parameter, and environment conditions demonstrate that the self-confidence metric displays the desired characteristics. The findings are discussed, and potential areas for future research are identified.",1
"Despite the success of single-agent reinforcement learning, multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in MARL, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy. In this paper, we propose a double averaging scheme, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular, such an algorithm is built upon a primal-dual reformulation of the mean squared projected Bellman error minimization problem, which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge, the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems.",0
"Although single-agent reinforcement learning has been successful, multi-agent reinforcement learning (MARL) is still difficult due to complex interactions between agents. Our focus is on policy evaluation in MARL, where agents work together to learn the value of a given policy using jointly observed state-action pairs and private local rewards. We propose a double averaging scheme, where each agent incorporates neighboring gradient information and local reward information through iterative averaging over space and time, respectively. Our algorithm is proven to converge to the optimal solution at a global geometric rate. This algorithm is based on a primal-dual reformulation of the mean squared projected Bellman error minimization problem, resulting in a decentralized convex-concave saddle-point problem. Our double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems, as far as we know.",1
"We consider the problem of detecting out-of-distribution (OOD) samples in deep reinforcement learning. In a value based reinforcement learning setting, we propose to use uncertainty estimation techniques directly on the agent's value estimating neural network to detect OOD samples. The focus of our work lies in analyzing the suitability of approximate Bayesian inference methods and related ensembling techniques that generate uncertainty estimates. Although prior work has shown that dropout-based variational inference techniques and bootstrap-based approaches can be used to model epistemic uncertainty, the suitability for detecting OOD samples in deep reinforcement learning remains an open question. Our results show that uncertainty estimation can be used to differentiate in- from out-of-distribution samples. Over the complete training process of the reinforcement learning agents, bootstrap-based approaches tend to produce more reliable epistemic uncertainty estimates, when compared to dropout-based approaches.",0
"Our research examines the issue of identifying out-of-distribution (OOD) samples in deep reinforcement learning. In a value-based reinforcement learning context, we suggest utilizing uncertainty estimation techniques on the neural network responsible for estimating the agent's value to detect OOD samples. Our study centers on assessing the appropriateness of approximate Bayesian inference methods and related ensembling techniques that generate uncertainty estimates. While previous research has indicated that dropout-based variational inference techniques and bootstrap-based approaches can model epistemic uncertainty, their suitability for detecting OOD samples in deep reinforcement learning remains uncertain. Our findings demonstrate that uncertainty estimation can differentiate between in-distribution and out-of-distribution samples. As the reinforcement learning agents progress through their training, bootstrap-based approaches generally provide more dependable epistemic uncertainty estimates than dropout-based approaches.",1
"In real-world scenarios, it is appealing to learn a model carrying out stochastic operations internally, known as stochastic computation graphs (SCGs), rather than learning a deterministic mapping. However, standard backpropagation is not applicable to SCGs. We attempt to address this issue from the angle of cost propagation, with local surrogate costs, called Q-functions, constructed and learned for each stochastic node in an SCG. Then, the SCG can be trained based on these surrogate costs using standard backpropagation. We propose the entire framework as a solution to generalize backpropagation for SCGs, which resembles an actor-critic architecture but based on a graph. For broad applicability, we study a variety of SCG structures from one cost to multiple costs. We utilize recent advances in reinforcement learning (RL) and variational Bayes (VB), such as off-policy critic learning and unbiased-and-low-variance gradient estimation, and review them in the context of SCGs. The generalized backpropagation extends transported learning signals beyond gradients between stochastic nodes while preserving the benefit of backpropagating gradients through deterministic nodes. Experimental suggestions and concerns are listed to help design and test any specific model using this framework.",0
"In practical situations, it is desirable to acquire a model that performs stochastic operations internally known as stochastic computation graphs (SCGs) instead of a deterministic mapping. However, standard backpropagation is not suitable for SCGs. To tackle this problem, we approach it from the cost propagation perspective by constructing and learning local surrogate costs, called Q-functions, for each stochastic node in an SCG. Using these surrogate costs, the SCG can be trained using standard backpropagation. We present this entire framework as a solution to generalize backpropagation for SCGs, which is similar to an actor-critic architecture but based on a graph. We investigate a variety of SCG structures from one cost to multiple costs to ensure broad applicability. We utilize recent advancements in reinforcement learning (RL) and variational Bayes (VB), such as off-policy critic learning and unbiased-and-low-variance gradient estimation, and review them in the context of SCGs. The generalized backpropagation extends transported learning signals beyond gradients between stochastic nodes while maintaining the advantages of backpropagating gradients through deterministic nodes. Experimental recommendations and concerns are provided to assist in designing and testing any specific model using this framework.",1
"Stochastic computation graphs (SCGs) provide a formalism to represent structured optimization problems arising in artificial intelligence, including supervised, unsupervised, and reinforcement learning. Previous work has shown that an unbiased estimator of the gradient of the expected loss of SCGs can be derived from a single principle. However, this estimator often has high variance and requires a full model evaluation per data point, making this algorithm costly in large graphs. In this work, we address these problems by generalizing concepts from the reinforcement learning literature. We introduce the concepts of value functions, baselines and critics for arbitrary SCGs, and show how to use them to derive lower-variance gradient estimates from partial model evaluations, paving the way towards general and efficient credit assignment for gradient-based optimization. In doing so, we demonstrate how our results unify recent advances in the probabilistic inference and reinforcement learning literature.",0
"Stochastic computation graphs (SCGs) offer a formal framework for representing structured optimization problems that arise in artificial intelligence, encompassing supervised, unsupervised, and reinforcement learning. Previous research has indicated that a single principle can generate an unbiased gradient estimator for the expected loss of SCGs. However, this estimator often exhibits high variance and necessitates a full model evaluation for each data point, rendering the algorithm expensive for large graphs. In this study, we tackle these issues by extending concepts from the reinforcement learning literature. We introduce the notions of value functions, critics, and baselines for arbitrary SCGs and illustrate how they can be employed to derive lower-variance gradient estimates from partial model evaluations. This breakthrough paves the way for general and efficient credit assignment for gradient-based optimization, and unifies recent developments in the probabilistic inference and reinforcement learning literature.",1
"Long-term planning poses a major difficulty to many reinforcement learning algorithms. This problem becomes even more pronounced in dynamic visual environments. In this work we propose Hierarchical Planning and Reinforcement Learning (HIP-RL), a method for merging the benefits and capabilities of Symbolic Planning with the learning abilities of Deep Reinforcement Learning. We apply HIPRL to the complex visual tasks of interactive question answering and visual semantic planning and achieve state-of-the-art results on three challenging datasets all while taking fewer steps at test time and training in fewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI",0
"Numerous reinforcement learning algorithms face a major obstacle in executing long-term planning, particularly in dynamic visual environments. To address this issue, we propose a solution called Hierarchical Planning and Reinforcement Learning (HIP-RL) that combines the strengths of Symbolic Planning with the learning capabilities of Deep Reinforcement Learning. We demonstrate the effectiveness of HIP-RL in complex visual tasks such as interactive question answering and visual semantic planning, achieving superior performance on three challenging datasets with fewer steps during testing and training. See youtu.be/0TtWJ_0mPfI for sample results.",1
"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",0
"The study of inferring intent from observed behavior has been extensively researched in the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods involve inferring a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator, which can then be used by another agent to predict, imitate, or assist the human user. However, inverse reinforcement learning assumes that the demonstrator is close to optimal, and suboptimal behavior is typically attributed to random noise or known cognitive biases. In this paper, the authors propose an alternative approach, modeling suboptimal behavior as a result of internal model misspecification. This means that the user's actions may deviate from near-optimal actions because they have an incorrect set of beliefs about the rules governing how actions affect the environment. By estimating these internal beliefs from observed behavior, the authors arrive at a new method for inferring intent, which has been demonstrated to be more accurate in simulation and in a user study with 12 participants. This approach has potential applications in offering assistance in a shared autonomy framework and inferring human preferences.",1
"Data in real-world application often exhibit skewed class distribution which poses an intense challenge for machine learning. Conventional classification algorithms are not effective in the case of imbalanced data distribution, and may fail when the data distribution is highly imbalanced. To address this issue, we propose a general imbalanced classification model based on deep reinforcement learning. We formulate the classification problem as a sequential decision-making process and solve it by deep Q-learning network. The agent performs a classification action on one sample at each time step, and the environment evaluates the classification action and returns a reward to the agent. The reward from minority class sample is larger so the agent is more sensitive to the minority class. The agent finally finds an optimal classification policy in imbalanced data under the guidance of specific reward function and beneficial learning environment. Experiments show that our proposed model outperforms the other imbalanced classification algorithms, and it can identify more minority samples and has great classification performance.",0
"The skewed distribution of data in real-world scenarios can pose a significant obstacle for machine learning. Traditional classification algorithms may not be effective when dealing with imbalanced data distribution, especially when it is highly skewed. To tackle this challenge, we present a novel imbalanced classification model that utilizes deep reinforcement learning. Our approach frames the classification problem as a sequential decision-making process, which is solved using a deep Q-learning network. At each time step, the agent performs a classification action on a single sample, and the environment evaluates the action and returns a reward to the agent. Notably, rewards for minority class samples are larger, making the agent more sensitive to these samples. By utilizing a specific reward function and a favorable learning environment, our approach enables the agent to identify an optimal classification policy for imbalanced data. Experimental results demonstrate that our proposed model outperforms other imbalanced classification algorithms, with better performance in identifying minority samples and achieving higher classification accuracy.",1
"Optimal decision making with limited or no information in stochastic environments where multiple agents interact is a challenging topic in the realm of artificial intelligence. Reinforcement learning (RL) is a popular approach for arriving at optimal strategies by predicating stimuli, such as the reward for following a strategy, on experience. RL is heavily explored in the single-agent context, but is a nascent concept in multiagent problems. To this end, I propose several principled model-free and partially model-based reinforcement learning approaches for several multiagent settings. In the realm of normative reinforcement learning, I introduce scalable extensions to Monte Carlo exploring starts for partially observable Markov Decision Processes (POMDP), dubbed MCES-P, where I expand the theory and algorithm to the multiagent setting. I first examine MCES-P with probably approximately correct (PAC) bounds in the context of multiagent setting, showing MCESP+PAC holds in the presence of other agents. I then propose a more sample-efficient methodology for antagonistic settings, MCESIP+PAC. For cooperative settings, I extend MCES-P to the Multiagent POMDP, dubbed MCESMP+PAC. I then explore the use of reinforcement learning as a methodology in searching for optima in realistic and latent model environments. First, I explore a parameterized Q-learning approach in modeling humans learning to reason in an uncertain, multiagent environment. Next, I propose an implementation of MCES-P, along with image segmentation, to create an adaptive team-based reinforcement learning technique to positively identify the presence of phenotypically-expressed water and pathogen stress in crop fields.",0
"Artificial intelligence faces a difficult challenge when it comes to making optimal decisions in stochastic environments where multiple agents interact, especially when there is limited or no information available. Reinforcement learning is a widely used method for determining optimal strategies by relying on experience to predict stimuli, such as the reward for following a strategy. However, while reinforcement learning has been extensively researched in the single-agent context, it is still a relatively new concept in multiagent problems. Therefore, I propose several principled approaches to reinforcement learning that are model-free or partially model-based for various multiagent settings. In normative reinforcement learning, I introduce scalable extensions to Monte Carlo exploring starts for partially observable Markov decision processes, or POMDP, that I call MCES-P, and expand the theory and algorithm to the multiagent setting. I first examine MCES-P with probably approximately correct bounds in the multiagent context, demonstrating that MCESP+PAC holds even in the presence of other agents. I then propose a more sample-efficient methodology for antagonistic settings, MCESIP+PAC, and for cooperative settings, I extend MCES-P to the Multiagent POMDP, or MCESMP+PAC. I also explore the use of reinforcement learning as a methodology for searching for optima in both realistic and latent model environments. Specifically, I investigate a parameterized Q-learning approach for modeling human reasoning in an uncertain multiagent environment, as well as an implementation of MCES-P and image segmentation to create an adaptive team-based reinforcement learning technique for identifying the presence of water and pathogen stress in crop fields.",1
"Consider mutli-goal tasks that involve static environments and dynamic goals. Examples of such tasks, such as goal-directed navigation and pick-and-place in robotics, abound. Two types of Reinforcement Learning (RL) algorithms are used for such tasks: model-free or model-based. Each of these approaches has limitations. Model-free RL struggles to transfer learned information when the goal location changes, but achieves high asymptotic accuracy in single goal tasks. Model-based RL can transfer learned information to new goal locations by retaining the explicitly learned state-dynamics, but is limited by the fact that small errors in modelling these dynamics accumulate over long-term planning. In this work, we improve upon the limitations of model-free RL in multi-goal domains. We do this by adapting the Floyd-Warshall algorithm for RL and call the adaptation Floyd-Warshall RL (FWRL). The proposed algorithm learns a goal-conditioned action-value function by constraining the value of the optimal path between any two states to be greater than or equal to the value of paths via intermediary states. Experimentally, we show that FWRL is more sample-efficient and learns higher reward strategies in multi-goal tasks as compared to Q-learning, model-based RL and other relevant baselines in a tabular domain.",0
"Tasks that have multiple goals and involve both static environments and dynamic goals, such as goal-directed navigation and pick-and-place in robotics, are common. Two types of Reinforcement Learning (RL) algorithms, namely model-free and model-based, are used for such tasks, but each approach has its own limitations. Model-free RL is unable to transfer learned information when the goal location changes, but performs well in single goal tasks. In contrast, model-based RL can transfer learned information to new goal locations by retaining the explicitly learned state-dynamics, but is limited by the accumulation of small errors in modelling these dynamics over long-term planning. This study proposes an algorithm called Floyd-Warshall RL (FWRL), which adapts the Floyd-Warshall algorithm for RL to overcome the limitations of model-free RL in multi-goal domains. The proposed algorithm learns a goal-conditioned action-value function by constraining the value of the optimal path between any two states to be greater than or equal to the value of paths via intermediary states. Experimental results show that FWRL is more efficient in terms of sample usage and learns higher reward strategies in multi-goal tasks compared to Q-learning, model-based RL, and other relevant baselines in a tabular domain.",1
"We propose a hybrid approach aimed at improving the sample efficiency in goal-directed reinforcement learning. We do this via a two-step mechanism where firstly, we approximate a model from Model-Free reinforcement learning. Then, we leverage this approximate model along with a notion of reachability using Mean First Passage Times to perform Model-Based reinforcement learning. Built on such a novel observation, we design two new algorithms - Mean First Passage Time based Q-Learning (MFPT-Q) and Mean First Passage Time based DYNA (MFPT-DYNA), that have been fundamentally modified from the state-of-the-art reinforcement learning techniques. Preliminary results have shown that our hybrid approaches converge with much fewer iterations than their corresponding state-of-the-art counterparts and therefore requiring much fewer samples and much fewer training trials to converge.",0
"Our proposed method seeks to enhance the efficiency of goal-directed reinforcement learning through a hybrid approach. This entails utilizing a two-step process, wherein we first approximate a model using Model-Free reinforcement learning. Subsequently, we utilize this approximation in conjunction with Mean First Passage Times to enable Model-Based reinforcement learning. Based on this unique approach, we have developed two new algorithms - Mean First Passage Time based Q-Learning (MFPT-Q) and Mean First Passage Time based DYNA (MFPT-DYNA). These algorithms have been significantly modified from existing reinforcement learning techniques. Our initial findings indicate that our hybrid approach requires fewer iterations to converge compared to traditional methods, thereby necessitating fewer samples and training trials to achieve the same results.",1
"Operating directly from raw high dimensional sensory inputs like images is still a challenge for robotic control. Recently, Reinforcement Learning methods have been proposed to solve specific tasks end-to-end, from pixels to torques. However, these approaches assume the access to a specified reward which may require specialized instrumentation of the environment. Furthermore, the obtained policy and representations tend to be task specific and may not transfer well. In this work we investigate completely self-supervised learning of a general image embedding and control primitives, based on finding the shortest time to reach any state. We also introduce a new structure for the state-action value function that builds a connection between model-free and model-based methods, and improves the performance of the learning algorithm. We experimentally demonstrate these findings in three simulated robotic tasks.",0
"A difficulty faced by robotic control is operating directly from raw high dimensional sensory inputs such as images. Despite recent proposals using Reinforcement Learning methods to solve specific end-to-end tasks from pixels to torques, these approaches require access to a specified reward and may not transfer well between tasks. Moreover, the policy and representations obtained tend to be task-specific. This study explores the possibility of completely self-supervised learning of a general image embedding and control primitives by finding the shortest time to reach any state. In addition, a new structure for the state-action value function is introduced to connect model-free and model-based methods and enhance the learning algorithm's performance. The findings are experimentally demonstrated in three simulated robotic tasks.",1
"This work examines the role of reinforcement learning in reducing the severity of on-road collisions by controlling velocity and steering in situations in which contact is imminent. We construct a model, given camera images as input, that is capable of learning and predicting the dynamics of obstacles, cars and pedestrians, and train our policy using this model. Two policies that control both braking and steering are compared against a baseline where the only action taken is (conventional) braking in a straight line. The two policies are trained using two distinct reward structures, one where any and all collisions incur a fixed penalty, and a second one where the penalty is calculated based on already established delta-v models of injury severity. The results show that both policies exceed the performance of the baseline, with the policy trained using injury models having the highest performance.",0
"This study investigates how reinforcement learning can be used to minimize the impact of on-road collisions by regulating velocity and steering in situations where a collision is unavoidable. A model is developed to learn and anticipate the behavior of obstacles, cars, and pedestrians based on camera images. We then train our policy using this model and compare two policies that control both braking and steering against a baseline that only uses conventional braking in a straight line. Our policies are trained using two different reward structures: one where any collision incurs a fixed penalty, and another where the penalty is calculated based on pre-established delta-v models of injury severity. The results demonstrate that both policies outperform the baseline, with the policy trained using injury models achieving the highest performance.",1
"This study proposes a framework for human-like autonomous car-following planning based on deep reinforcement learning (deep RL). Historical driving data are fed into a simulation environment where an RL agent learns from trial and error interactions based on a reward function that signals how much the agent deviates from the empirical data. Through these interactions, an optimal policy, or car-following model that maps in a human-like way from speed, relative speed between a lead and following vehicle, and inter-vehicle spacing to acceleration of a following vehicle is finally obtained. The model can be continuously updated when more data are fed in. Two thousand car-following periods extracted from the 2015 Shanghai Naturalistic Driving Study were used to train the model and compare its performance with that of traditional and recent data-driven car-following models. As shown by this study results, a deep deterministic policy gradient car-following model that uses disparity between simulated and observed speed as the reward function and considers a reaction delay of 1s, denoted as DDPGvRT, can reproduce human-like car-following behavior with higher accuracy than traditional and recent data-driven car-following models. Specifically, the DDPGvRT model has a spacing validation error of 18% and speed validation error of 5%, which are less than those of other models, including the intelligent driver model, models based on locally weighted regression, and conventional neural network-based models. Moreover, the DDPGvRT demonstrates good capability of generalization to various driving situations and can adapt to different drivers by continuously learning. This study demonstrates that reinforcement learning methodology can offer insight into driver behavior and can contribute to the development of human-like autonomous driving algorithms and traffic-flow models.",0
"The aim of this study is to propose a framework for autonomous car-following planning that mimics human behavior using deep reinforcement learning (deep RL). The approach involves feeding historical driving data into a simulation environment where an RL agent learns from trial and error interactions based on a reward function that measures how much the agent deviates from the empirical data. The resulting car-following model maps from speed, relative speed, and inter-vehicle spacing to acceleration in a human-like way. The model is continuously updated with more data and was trained using 2000 car-following periods from the 2015 Shanghai Naturalistic Driving Study. Results show that the proposed DDPGvRT model outperforms traditional and recent data-driven car-following models, with a spacing validation error of 18% and speed validation error of 5%. The model also demonstrates good generalization to various driving situations and can adapt to different drivers by continuously learning. The study concludes that reinforcement learning methodology can improve the development of human-like autonomous driving algorithms and traffic-flow models.",1
"To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.",0
"In order for agents to effectively navigate complex environments, we suggest that they possess a mental simulator of the world that possesses three key traits. Firstly, it must construct an abstract state that represents the current condition of the world. Secondly, it must form a belief that acknowledges uncertainty about the world. Lastly, it must incorporate temporal abstraction, going beyond a simple step-by-step simulation. However, since there is currently no existing model that fulfills all these requirements, we introduce TD-VAE - a generative sequence model that can learn representations with explicit beliefs about states multiple steps ahead, and can be directly implemented without single-step transitions. TD-VAE is trained on pairs of time points that are separated in time, utilizing a form of temporal difference learning that is commonly used in reinforcement learning.",1
"Although end-to-end (E2E) learning has led to impressive progress on a variety of visual understanding tasks, it is often impeded by hardware constraints (e.g., GPU memory) and is prone to overfitting. When it comes to video captioning, one of the most challenging benchmark tasks in computer vision, those limitations of E2E learning are especially amplified by the fact that both the input videos and output captions are lengthy sequences. Indeed, state-of-the-art methods for video captioning process video frames by convolutional neural networks and generate captions by unrolling recurrent neural networks. If we connect them in an E2E manner, the resulting model is both memory-consuming and data-hungry, making it extremely hard to train. In this paper, we propose a multitask reinforcement learning approach to training an E2E video captioning model. The main idea is to mine and construct as many effective tasks (e.g., attributes, rewards, and the captions) as possible from the human captioned videos such that they can jointly regulate the search space of the E2E neural network, from which an E2E video captioning model can be found and generalized to the testing phase. To the best of our knowledge, this is the first video captioning model that is trained end-to-end from the raw video input to the caption output. Experimental results show that such a model outperforms existing ones to a large margin on two benchmark video captioning datasets.",0
"Despite the impressive progress that end-to-end (E2E) learning has achieved in various visual understanding tasks, it frequently faces limitations due to hardware constraints, such as GPU memory, and is susceptible to overfitting. For video captioning, one of the most challenging tasks in computer vision, these limitations of E2E learning are amplified as both input videos and output captions are lengthy sequences. The current state-of-the-art video captioning methods process video frames using convolutional neural networks and generate captions using unrolled recurrent neural networks. However, connecting these networks in an E2E manner results in a memory-consuming and data-hungry model that is tough to train. In this study, we propose a multitask reinforcement learning approach to train an E2E video captioning model, where we mine and construct multiple effective tasks, such as attributes, rewards, and captions, from human-captioned videos to jointly regulate the E2E neural network's search space. This approach enables us to find an E2E video captioning model that can be generalized to testing phases. To the best of our knowledge, this is the first video captioning model that is trained end-to-end from raw video input to caption output. Experimental results indicate that this model outperforms existing ones by a considerable margin on two benchmark video captioning datasets.",1
"Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.",0
"To learn a good policy through model-free reinforcement learning (RL), particularly in environments with sparse rewards, a large number of trials are required. However, we have discovered a method that enhances sampling efficiency when there is access to demonstrations. Our technique, called Backplay, utilizes a solitary demonstration to create a curriculum for a given task. Instead of commencing every training episode at the fixed initial state of the environment, we initiate the agent near the end of the demonstration and gradually move the starting point backward during training until we reach the initial state. We have evaluated the effectiveness of Backplay in large grid worlds and a complex four-player zero-sum game (Pommerman) and have demonstrated that this approach is superior to other competitive methods that have been known to improve sample efficiency, such as reward shaping, behavioral cloning, and reverse curriculum generation. Furthermore, we have analytically characterized the types of environments where Backplay can improve training speed.",1
"Deploying the idea of long-term cumulative return, reinforcement learning has shown remarkable performance in various fields. We propose a formulation of the landmark localization in 3D medical images as a reinforcement learning problem. Whereas value-based methods have been widely used to solve similar problems, we adopt an actor-critic based direct policy search method framed in a temporal difference learning approach. Successful behavior learning is challenging in large state and/or action spaces, requiring many trials. We introduce a partial policy-based reinforcement learning to enable solving the large problem of localization by learning the optimal policy on smaller partial domains. Independent actors efficiently learn the corresponding partial policies, each utilizing their own independent critic. The proposed policy reconstruction from the partial policies ensures a robust and efficient localization utilizing the sub-agents solving simple binary decision problems in their corresponding partial action spaces. The proposed reinforcement learning requires a small number of trials to learn the optimal behavior compared with the original behavior learning scheme.",0
"Reinforcement learning has demonstrated impressive results in diverse fields by leveraging the concept of long-term cumulative return. In this study, we present a novel approach to address the problem of landmark localization in 3D medical images using reinforcement learning. Although value-based methods have been commonly utilized for similar tasks, we opted for an actor-critic based direct policy search technique that employs a temporal difference learning framework. Learning optimal behavior in large state and/or action spaces can be challenging and requires numerous trials. To overcome this hurdle, we introduce a partial policy-based reinforcement learning strategy that enables us to solve the localization problem by training optimal policies on smaller partial domains. Independent actors are used to learn the corresponding partial policies, each with its own independent critic. Combining the partial policies ensures robust and efficient localization by leveraging sub-agents that solve simple binary decision problems in their respective partial action spaces. Our proposed reinforcement learning method requires fewer trials to learn optimal behavior compared to the original learning scheme.",1
"This paper deals with the geometric multi-model fitting from noisy, unstructured point set data (e.g., laser scanned point clouds). We formulate multi-model fitting problem as a sequential decision making process. We then use a deep reinforcement learning algorithm to learn the optimal decisions towards the best fitting result. In this paper, we have compared our method against the state-of-the-art on simulated data. The results demonstrated that our approach significantly reduced the number of fitting iterations.",0
"The focus of this paper is on fitting geometric multi-models using imprecise, disorganized point sets like those derived from laser scanning. To accomplish this, we frame the multi-model fitting issue as a step-by-step decision-making process and utilize a deep reinforcement learning algorithm to determine the ideal decisions for the most favorable fitting outcome. Our methodology was pitted against current techniques using simulated data, and the findings demonstrated that our approach considerably decreased the number of fitting iterations required.",1
"We address the problem of person re-identification from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our first contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances re-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.",0
"Our focus is on solving the issue of person re-identification using standard depth sensors. The primary challenge with depth-based recognition is the lack of data available. To address this, we introduce a split-rate RGB-to-Depth transfer method that utilizes large RGB datasets more effectively than traditional fine-tuning methods. Our approach involves sharing model parameters at the bottom layers of a deep convolutional neural network between RGB and depth data, while fine-tuning the remaining layers rapidly. Additionally, we enhance re-identification for video by implementing temporal attention using a Bernoulli-Sigmoid unit that acts upon frame-level features. To train the stochastic temporal attention parameters, we employ reinforcement learning. We conduct extensive experiments to validate the accuracy of our method in person re-identification from depth sequences. Finally, we demonstrate significant performance improvements for our approach compared to a state-of-the-art model that relies on RGB data, particularly in scenarios where subjects are wearing new clothing.",1
"We propose a general formulation for addressing reinforcement learning (RL) problems in settings with observational data. That is, we consider the problem of learning good policies solely from historical data in which unobserved factors (confounders) affect both observed actions and rewards. Our formulation allows us to extend a representative RL algorithm, the Actor-Critic method, to its deconfounding variant, with the methodology for this extension being easily applied to other RL algorithms. In addition to this, we develop a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Using this benchmark, we demonstrate that the proposed algorithms are superior to traditional RL methods in confounded environments with observational data. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full RL problems with observational data. Code is available at https://github.com/CausalRL/DRL.",0
"Our proposal puts forward a general approach for tackling reinforcement learning (RL) problems in situations where only observational data is available. Specifically, we focus on the challenge of constructing effective policies based on historical data that is influenced by unobserved factors (known as confounders), which impact both the actions and rewards observed. Our formulation enables us to adapt the Actor-Critic method, a commonly used RL algorithm, to its deconfounding version. This method can also be easily applied to other RL algorithms. Furthermore, we introduce a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Our results demonstrate that our proposed algorithms outperform traditional RL methods in confounded environments with observational data. This marks the first time that confounders have been considered in addressing complete RL problems with observational data. The code can be accessed at https://github.com/CausalRL/DRL.",1
"Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work we present a novel multi-timescale approach for constrained policy optimization, called `Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.",0
"It is difficult to solve tasks in Reinforcement Learning, as the agent's objective is to maximize the reward, which can lead to undesired behavior due to loopholes and errors in the reward signal. Although constraints can address this problem, a general solution for constraints does not exist. Our study proposes a new approach for constrained policy optimization called 'Reward Constrained Policy Optimization' (RCPO), which employs an alternative penalty signal to guide the policy towards satisfying the constraint. We demonstrate the effectiveness of our approach through empirical evidence and prove its convergence.",1
"In this paper, a new deep reinforcement learning based augmented general sequence tagging system is proposed. The new system contains two parts: a deep neural network (DNN) based sequence tagging model and a deep reinforcement learning (DRL) based augmented tagger. The augmented tagger helps improve system performance by modeling the data with minority tags. The new system is evaluated on SLU and NLU sequence tagging tasks using ATIS and CoNLL-2003 benchmark datasets, to demonstrate the new system's outstanding performance on general tagging tasks. Evaluated by F1 scores, it shows that the new system outperforms the current state-of-the-art model on ATIS dataset by 1.9% and that on CoNLL-2003 dataset by 1.4%.",0
"This paper presents a novel system for general sequence tagging that utilizes deep reinforcement learning. The system comprises of two components: a deep neural network-based sequence tagging model and a deep reinforcement learning-based augmented tagger. The augmented tagger enhances system performance by taking into account data with minor tags. Performance evaluation of the new system is conducted on SLU and NLU sequence tagging tasks using benchmark datasets such as ATIS and CoNLL-2003, which demonstrates its exceptional performance in general tagging tasks. The evaluation, measured by F1 scores, reveals that the new system surpasses the current state-of-the-art model on the ATIS dataset by 1.9% and the CoNLL-2003 dataset by 1.4%.",1
"We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data. We show that the improved performance is due to the increased diversity of reformulation strategies.",0
"Our proposed approach aims to efficiently teach varied strategies in reinforcement learning for query reformulation. This approach is applicable to document retrieval and question answering tasks. The agent comprises of multiple specialized sub-agents along with a meta-agent that learns to combine answers from sub-agents to generate the final answer. The sub-agents are trained on different partitions of the training data, whereas the meta-agent is trained on the entire training set. Our method is highly parallelizable, making the learning process quicker, and it offers better generalization performance compared to strong baselines like a full-data trained agent ensemble. Our approach's enhanced performance is attributed to the increased diversity of reformulation strategies.",1
"Recent breakthroughs in Go play and strategic games have witnessed the great potential of reinforcement learning in intelligently scheduling in uncertain environment, but some bottlenecks are also encountered when we generalize this paradigm to universal complex tasks. Among them, the low efficiency of data utilization in model-free reinforcement algorithms is of great concern. In contrast, the model-based reinforcement learning algorithms can reveal underlying dynamics in learning environments and seldom suffer the data utilization problem. To address the problem, a model-based reinforcement learning algorithm with attention mechanism embedded is proposed as an extension of World Models in this paper. We learn the environment model through Mixture Density Network Recurrent Network(MDN-RNN) for agents to interact, with combinations of variational auto-encoder(VAE) and attention incorporated in state value estimates during the process of learning policy. In this way, agent can learn optimal policies through less interactions with actual environment, and final experiments demonstrate the effectiveness of our model in control problem.",0
"Reinforcement learning has shown great potential in scheduling tasks in uncertain environments, as seen in recent advancements in Go play and strategic games. However, there are challenges in applying this paradigm to complex tasks. The data utilization problem, which affects the efficiency of model-free reinforcement algorithms, is a significant concern. Model-based reinforcement learning algorithms, on the other hand, can identify underlying dynamics in learning environments and are not affected by this problem. To address this issue, this paper proposes an extension of World Models, a model-based reinforcement learning algorithm with an embedded attention mechanism. The environment model is learned using a Mixture Density Network Recurrent Network (MDN-RNN), with state value estimates incorporating a combination of variational auto-encoder (VAE) and attention during policy learning. With this approach, the agent can learn optimal policies with fewer interactions with the actual environment. The final experiments demonstrate the effectiveness of this model in control problems.",1
"Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients' alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy's stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control.",0
"To avoid getting stuck in local optima, reinforcement learning agents require exploratory behaviors. These behaviors can involve immediate perturbation and consistent exploration over time. A stochastic policy model that maintains consistency over time is particularly useful for tasks with sparse rewards or long-term information. This paper introduces a new exploration strategy for deep reinforcement learning agents called Neural Adaptive Dropout Policy Exploration (NADPEx). NADPEx uses dropout as a global random variable in the conditional distribution to provide temporal consistency to reinforcement learning policies, even when rewards are sparse. The paper discusses two factors, namely gradients' alignment with the objective and KL constraint in policy space, to ensure stable improvement of the NADPEx policy. The experiments conducted in this research show that NADPEx is effective in solving tasks with sparse rewards, outperforming naive exploration and parameter noise methods. Additionally, NADPEx achieves comparable or faster convergence in the standard mujoco benchmark for continuous control.",1
"We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.",0
"A new methodology called Supervised Policy Update (SPU) is proposed for deep reinforcement learning, which is efficient in terms of sample usage. The SPU methodology generates data from the current policy and then solves a constrained optimization problem in the non-parameterized proximal policy space. The optimal non-parameterized policy is then converted into a parameterized policy using supervised regression, from which new samples are drawn. This methodology is versatile, as it can be applied to both discrete and continuous action spaces and can handle various proximity constraints for the non-parameterized optimization problem. The SPU methodology can address problems such as the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem. Compared to TRPO, SPU implementation is much simpler and more sample-efficient. Extensive experiments have shown that SPU outperforms TRPO in Mujoco simulated robotic tasks and PPO in Atari video game tasks.",1
"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by learning directly from image input. A deep neural network is used as a function approximator and requires no specific state information. However, one drawback of using only images as input is that this approach requires a prohibitively large amount of training time and data for the model to learn the state feature representation and approach reasonable performance. This is not feasible in real-world applications, especially when the data are expansive and training phase could introduce disasters that affect human safety. In this work, we use a human demonstration approach to speed up training for learning features and use the resulting pre-trained model to replace the neural network in the deep RL Deep Q-Network (DQN), followed by human interaction to further refine the model. We empirically evaluate our approach by using only a human demonstration model and modified DQN with human demonstration model included in the Microsoft AirSim car simulator. Our results show that (1) pre-training with human demonstration in a supervised learning approach is better and much faster at discovering features than DQN alone, (2) initializing the DQN with a pre-trained model provides a significant improvement in training time and performance even with limited human demonstration, and (3) providing the ability for humans to supply suggestions during DQN training can speed up the network's convergence on an optimal policy, as well as allow it to learn more complex policies that are harder to discover by random exploration.",0
"By learning directly from image input, deep reinforcement learning (deep RL) has shown remarkable success in complex sequential tasks. A deep neural network is employed as a function approximator, which eliminates the need for specific state information. However, relying solely on images as input poses a significant challenge as it requires a substantial amount of training time and data for the model to learn the state feature representation and achieve reasonable performance. This is not practical in real-world applications, especially when the data is extensive, and the training phase could result in disasters that endanger human safety. To tackle this issue, we propose a human demonstration approach to accelerate the training process for learning features. We use the resulting pre-trained model to replace the neural network in the deep RL Deep Q-Network (DQN) and further refine the model through human interaction. Our empirical evaluation, which uses only a human demonstration model and a modified DQN with the human demonstration model included in the Microsoft AirSim car simulator, demonstrates that pre-training with human demonstration in a supervised learning approach is better and much faster at discovering features than DQN alone. In addition, initializing the DQN with a pre-trained model significantly enhances training time and performance, even with limited human demonstration. Finally, we show that enabling humans to provide suggestions during DQN training can accelerate the network's convergence on an optimal policy and enable it to learn more complex policies that are challenging to discover by random exploration.",1
"We consider a framework involving behavioral economics and machine learning. Rationally inattentive Bayesian agents make decisions based on their posterior distribution, utility function and information acquisition cost Renyi divergence which generalizes Shannon mutual information). By observing these decisions, how can an observer estimate the utility function and information acquisition cost? Using deep learning, we estimate framing information (essential extrinsic features) that determines the agent's attention strategy. Then we present a preference based inverse reinforcement learning algorithm to test for rational inattention: is the agent an utility maximizer, attention maximizer, and does an information cost function exist that rationalizes the data? The test imposes a Renyi mutual information constraint which impacts how the agent can select attention strategies to maximize their expected utility. The test provides constructive estimates of the utility function and information acquisition cost of the agent. We illustrate these methods on a massive YouTube dataset for characterizing the commenting behavior of users.",0
"Our approach combines behavioral economics and machine learning to form a framework. Our focus is on rationally inattentive Bayesian agents who base their decisions on their posterior distribution, utility function, and information acquisition cost. To estimate the utility function and information acquisition cost, we examine the agent's decisions and use deep learning to estimate framing information. This framing information determines the agent's attention strategy. We then use a preference-based inverse reinforcement learning algorithm to test for rational inattention and determine if the agent is a utility maximizer, attention maximizer, and if there exists an information cost function that rationalizes the data. The test takes into account a Renyi mutual information constraint that impacts the agent's selection of attention strategies to maximize their expected utility. Our method provides constructive estimates of the utility function and information acquisition cost of the agent. We demonstrate the effectiveness of our approach using a vast YouTube dataset to analyze user commenting behavior.",1
"Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",0
"Reinforcement Learning (RL) agents may encounter difficulties in learning when presented with environments that have large state and action spaces and sparse rewards. For example, natural language instructions on the Web, like booking a flight ticket, can lead to RL settings with a vast input vocabulary and a high number of actionable elements on a page. Although recent approaches have improved the success rate on simpler environments with human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. Our proposed guided RL approaches aim to overcome these challenges by generating an unbounded amount of experience for an agent to learn from. Instead of learning from a complex instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum that gradually increases the subset of these relatively easier sub-instructions. Additionally, when expert demonstrations are unavailable, our novel meta-learning framework generates new instruction-following tasks and trains the agent more effectively. We train a DQN deep reinforcement learning agent using a QWeb neural network architecture to approximate the Q-value function on these smaller, synthetic instructions. Our agent's ability to generalize to new instructions is evaluated on the World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without any human demonstration, achieving a 100% success rate on several challenging environments.",1
"Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during meta-training as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance.",0
"The understanding of credit assignment in Meta-reinforcement learning (Meta-RL) is inadequate, with existing methods overlooking credit assignment for pre-adaptation behavior or implementing it in a simplistic manner. This results in low sample-efficiency during meta-training and ineffective task identification strategies. This article presents a theoretical examination of credit assignment in gradient-based Meta-RL. Based on the acquired insights, a novel meta-learning algorithm is developed that addresses the issues of poor credit assignment and previous difficulties in estimating meta-policy gradients. By regulating the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm enables efficient and stable meta-learning. Our approach results in superior pre-adaptation policy behavior, outperforming previous Meta-RL algorithms in terms of sample-efficiency, wall-clock time, and asymptotic performance on a consistent basis.",1
"This paper proposes an efficient neural network (NN) architecture design methodology called Chameleon that honors given resource constraints. Instead of developing new building blocks or using computationally-intensive reinforcement learning algorithms, our approach leverages existing efficient network building blocks and focuses on exploiting hardware traits and adapting computation resources to fit target latency and/or energy constraints. We formulate platform-aware NN architecture search in an optimization framework and propose a novel algorithm to search for optimal architectures aided by efficient accuracy and resource (latency and/or energy) predictors. At the core of our algorithm lies an accuracy predictor built atop Gaussian Process with Bayesian optimization for iterative sampling. With a one-time building cost for the predictors, our algorithm produces state-of-the-art model architectures on different platforms under given constraints in just minutes. Our results show that adapting computation resources to building blocks is critical to model performance. Without the addition of any bells and whistles, our models achieve significant accuracy improvements against state-of-the-art hand-crafted and automatically designed architectures. We achieve 73.8% and 75.3% top-1 accuracy on ImageNet at 20ms latency on a mobile CPU and DSP. At reduced latency, our models achieve up to 8.5% (4.8%) and 6.6% (9.3%) absolute top-1 accuracy improvements compared to MobileNetV2 and MnasNet, respectively, on a mobile CPU (DSP), and 2.7% (4.6%) and 5.6% (2.6%) accuracy gains over ResNet-101 and ResNet-152, respectively, on an Nvidia GPU (Intel CPU).",0
"The Chameleon methodology proposed in this paper offers an efficient approach for designing neural network (NN) architectures that meet specific resource constraints. Unlike other methods that require the development of new building blocks or rely on computationally-intensive reinforcement learning algorithms, Chameleon leverages existing network building blocks and focuses on adapting computation resources to fit target latency and/or energy constraints. The proposed algorithm employs an optimization framework to search for optimal architectures while also utilizing efficient accuracy and resource predictors. The algorithm includes an accuracy predictor that uses Gaussian Process with Bayesian optimization for iterative sampling. The algorithm is able to produce state-of-the-art model architectures on different platforms under given constraints in just minutes, with a one-time building cost for the predictors. Results indicate that adapting computation resources to building blocks is crucial for model performance, as the models produced by Chameleon achieve significant accuracy improvements against both hand-crafted and automatically designed architectures. For example, on ImageNet at 20ms latency on a mobile CPU and DSP, the models achieve top-1 accuracy of 73.8% and 75.3%, respectively. At reduced latency, the models achieve up to 8.5% (4.8%) and 6.6% (9.3%) absolute top-1 accuracy improvements compared to MobileNetV2 and MnasNet, respectively, on a mobile CPU (DSP), and 2.7% (4.6%) and 5.6% (2.6%) accuracy gains over ResNet-101 and ResNet-152, respectively, on an Nvidia GPU (Intel CPU).",1
"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using deep neural networks as function approximators to learn directly from raw input images. However, learning directly from raw images is data inefficient. The agent must learn feature representation of complex states in addition to learning a policy. As a result, deep RL typically suffers from slow learning speeds and often requires a prohibitively large amount of training time and data to reach reasonable performance, making it inapplicable to real-world settings where data is expensive. In this work, we improve data efficiency in deep RL by addressing one of the two learning goals, feature learning. We leverage supervised learning to pre-train on a small set of non-expert human demonstrations and empirically evaluate our approach using the asynchronous advantage actor-critic algorithms (A3C) in the Atari domain. Our results show significant improvements in learning speed, even when the provided demonstration is noisy and of low quality.",0
"The use of deep neural networks as function approximators in deep reinforcement learning (deep RL) has led to exceptional performance in complex sequential tasks by learning directly from raw input images. However, this approach is not efficient in terms of data utilization as the agent must learn complex state feature representation in addition to policy learning. Consequently, deep RL is hindered by slow learning and requires a large amount of training time and data to achieve satisfactory performance, making it impractical in real-world scenarios where data is expensive. This study aims to enhance data efficiency in deep RL by focusing on feature learning, one of the two learning goals. To achieve this, we utilize supervised learning to pre-train on a small set of non-expert human demonstrations and apply the asynchronous advantage actor-critic algorithms (A3C) in the Atari domain to evaluate our approach empirically. Our findings show significant improvements in learning speed, even when the demonstrations provided are noisy and of low quality.",1
"Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",0
"Current interactive image retrieval methods have shown that incorporating user feedback can enhance retrieval outcomes. However, most existing systems rely on limited forms of feedback, such as binary relevance responses or fixed sets of relative attributes, which restricts their effectiveness. To address this issue, we introduce a new approach to interactive image search that allows users to provide feedback through natural language, enabling more natural and efficient interaction. We frame the task of dialog-based interactive image retrieval as a reinforcement learning problem, where the dialog system is rewarded for improving the rank of the target image in each turn. To overcome the challenges and costs of collecting human-machine conversations during the system's learning, we train our approach with a user simulator that can differentiate between target and candidate images. We demonstrate the effectiveness of our approach in a footwear retrieval application. Both simulated and real-world experiments show that our learning framework outperforms other supervised and reinforcement learning baselines, and that user feedback based on natural language produces more effective retrieval results and a more natural and expressive communication interface.",1
"The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment.",0
"Learning multiple reward functions simultaneously through reinforcement learning (RL) offers several advantages, such as breaking down complex tasks into simpler ones, sharing information between tasks, and reusing skills. Our focus is on the ability of RL agents to generalize to new tasks. One method of parametric generalization involves using a function approximator, such as universal value function approximators (UVFAs), which can interpolate based on the task description. Another approach is to leverage the structure of the RL problem itself through generalised policy improvement (GPI), which combines solutions from previous tasks to create a policy for the new task. This is made possible by using successor features (SFs) for instantaneous policy evaluation. Our proposed universal successor features approximators (USFAs) combine the benefits of both UVFAs and SFs, offering scalability, instant inference, and strong generalization. We discuss the challenges associated with training a USFA, its generalization properties, and demonstrate its practical benefits and transfer abilities in a large-scale 3D navigation task.",1
"Deep reinforcement learning agents have recently been successful across a variety of discrete and continuous control tasks; however, they can be slow to train and require a large number of interactions with the environment to learn a suitable policy. This is borne out by the fact that a reinforcement learning agent has no prior knowledge of the world, no pre-existing data to depend on and so must devote considerable time to exploration. Transfer learning can alleviate some of the problems by leveraging learning done on some source task to help learning on some target task. Our work presents an algorithm for initialising the hidden feature representation of the target task. We propose a domain adaptation method to transfer state representations and demonstrate transfer across domains, tasks and action spaces. We utilise adversarial domain adaptation ideas combined with an adversarial autoencoder architecture. We align our new policies' representation space with a pre-trained source policy, taking target task data generated from a random policy. We demonstrate that this initialisation step provides significant improvement when learning a new reinforcement learning task, which highlights the wide applicability of adversarial adaptation methods; even as the task and label/action space also changes.",0
"Although deep reinforcement learning agents have shown success in both discrete and continuous control tasks, they can be slow to train and require extensive interactions with the environment to learn a suitable policy. This is due to the fact that they have no prior knowledge or pre-existing data, and therefore require a considerable amount of exploration time. To mitigate these issues, transfer learning can be used to leverage learning from a source task to aid in learning a target task. Our research introduces an algorithm for initializing the hidden feature representation of the target task and proposes a domain adaptation method to transfer state representations. We demonstrate transfer across domains, tasks, and action spaces by utilizing adversarial domain adaptation ideas with an adversarial autoencoder architecture. Our approach aligns the representation space of the new policies with a pre-trained source policy, using target task data generated from a random policy. We show that this initialization step significantly improves the learning of new reinforcement learning tasks, highlighting the broad applicability of adversarial adaptation methods even as the task and label/action space changes.",1
"Gradient-based meta-learners such as MAML are able to learn a meta-prior from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. One important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML algorithm that is able to modulate its meta-learned prior according to the identified task, allowing faster adaptation. We evaluate the proposed model on a diverse set of problems including regression, few-shot image classification, and reinforcement learning. The results demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks sampled from a multimodal distribution.",0
"Meta-learners like MAML use gradients to learn a meta-prior from similar tasks, enabling them to adapt to new tasks from the same distribution with minimal gradient updates. However, these frameworks are limited in their ability to learn from diverse task distributions because they rely on a common initialization shared across the entire distribution. In this study, we introduce a multimodal MAML algorithm that can identify tasks from a multimodal distribution and quickly adapt using gradient updates. Our algorithm modulates the meta-learned prior based on the identified task, resulting in faster adaptation. We evaluate our model on various problems, including regression, few-shot image classification, and reinforcement learning, and show that it effectively modulates the meta-learned prior in response to the characteristics of tasks from a multimodal distribution.",1
"Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and doesn't always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.",0
"Efficient and flexible, Deep Neural Networks excel in various tasks, including image and speech recognition, as well as natural language understanding. Researchers, in particular, show great interest in Convolutional Neural Networks (CNN) for computer vision and classification tasks. The CNN architecture and its hyperparameters depend on the task at hand, with the network extracting complex and relevant features for optimal convergence. However, designing such architectures requires significant expertise and computation time, and may not always result in an optimal network. Despite extensive research in machine learning, there is no standardized automatic method for model configuration. This survey reviews and discusses the ongoing progress in automating CNN architecture search.",1
"Recognition of human environment with computer systems always was a big deal in artificial intelligence. In this area handwriting recognition and conceptualization of it to computer is an important area in it. In the past years with growth of machine learning in artificial intelligence, efforts to using this technique increased. In this paper is tried to using fuzzy controller, to optimizing amount of reward of reinforcement learning for recognition of handwritten digits. For this aim first a sample of every digit with 10 standard computer fonts, given to actor and then actor is trained. In the next level is tried to test the actor with dataset and then results show improvement of recognition when using fuzzy controller of reinforcement learning.",0
"Artificial intelligence has always faced challenges in recognizing the human environment using computer systems. One key area of focus has been handwriting recognition and its conceptualization in computers. With the rise of machine learning in recent years, there has been an increased effort to leverage this technique. This paper explores the use of fuzzy controllers to optimize the reward of reinforcement learning for handwritten digit recognition. The process involved providing the actor with samples of each digit using 10 standard computer fonts, followed by training. The actor was then tested with a dataset, and the results showed improved recognition when using the fuzzy controller for reinforcement learning.",1
"Mobile edge computing (MEC) emerges recently as a promising solution to relieve resource-limited mobile devices from computation-intensive tasks, which enables devices to offload workloads to nearby MEC servers and improve the quality of computation experience. Nevertheless, by considering a MEC system consisting of multiple mobile users with stochastic task arrivals and wireless channels in this paper, the design of computation offloading policies is challenging to minimize the long-term average computation cost in terms of power consumption and buffering delay. A deep reinforcement learning (DRL) based decentralized dynamic computation offloading strategy is investigated to build a scalable MEC system with limited feedback. Specifically, a continuous action space-based DRL approach named deep deterministic policy gradient (DDPG) is adopted to learn efficient computation offloading policies independently at each mobile user. Thus, powers of both local execution and task offloading can be adaptively allocated by the learned policies from each user's local observation of the MEC system. Numerical results are illustrated to demonstrate that efficient policies can be learned at each user, and performance of the proposed DDPG based decentralized strategy outperforms the conventional deep Q-network (DQN) based discrete power control strategy and some other greedy strategies with reduced computation cost. Besides, the power-delay tradeoff is also analyzed for both the DDPG based and DQN based strategies.",0
"Recently, Mobile Edge Computing (MEC) has emerged as a potential solution for relieving resource-limited mobile devices from computation-heavy tasks. This allows devices to offload their workloads to nearby MEC servers, improving the quality of the computation experience. However, this paper considers a MEC system with multiple mobile users, who experience stochastic task arrivals and wireless channels. The challenge is to design computation offloading policies to minimize the long-term average computation cost based on power consumption and buffering delay. To build a scalable MEC system with limited feedback, this paper investigates a Deep Reinforcement Learning (DRL) based decentralized dynamic computation offloading strategy. Specifically, a continuous action space-based DRL approach called Deep Deterministic Policy Gradient (DDPG) is adopted to learn efficient computation offloading policies independently at each mobile user. This approach allows powers of both local execution and task offloading to be adaptively allocated by the learned policies from each user's local observation of the MEC system. Numerical results demonstrate that efficient policies can be learned at each user, and the proposed DDPG-based decentralized strategy outperforms the conventional Deep Q-Network (DQN) based discrete power control strategy and some other greedy strategies with reduced computation cost. Additionally, the power-delay tradeoff is analyzed for both DDPG-based and DQN-based strategies.",1
"Proximal policy optimization(PPO) has been proposed as a first-order optimization method for reinforcement learning. We should notice that an exterior penalty method is used in it. Often, the minimizers of the exterior penalty functions approach feasibility only in the limits as the penalty parameter grows increasingly large. Therefore, it may result in the low level of sampling efficiency. This method, which we call proximal policy optimization with barrier method (PPO-B), keeps almost all advantageous spheres of PPO such as easy implementation and good generalization. Specifically, a new surrogate objective with interior penalty method is proposed to avoid the defect arose from exterior penalty method. Conclusions can be draw that PPO-B is able to outperform PPO in terms of sampling efficiency since PPO-B achieved clearly better performance on Atari and Mujoco environment than PPO.",0
"Proximal policy optimization (PPO) has been suggested as a primary optimization approach for reinforcement learning. It should be noted that PPO uses an external penalty method, which may result in low sampling efficiency as the minimizers of external penalty functions only approach feasibility when the penalty parameter grows increasingly large. However, a new method called PPO with barrier method (PPO-B) preserves the advantageous aspects of PPO, such as easy implementation and good generalization. PPO-B introduces a new surrogate objective with an internal penalty method to avoid the limitations of the external penalty method. Results show that PPO-B outperforms PPO in terms of sampling efficiency, as evidenced by its superior performance in Atari and Mujoco environments.",1
"We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.",0
"We have created a method for coordinated exploration that is practical for a team of reinforcement learning agents operating in a shared environment. Our method uses seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016), and is suitable for larger problems. In our tests, the approach performed similarly to other tabular model learning methods for simple contexts. However, when applied to more complex problems with a neural network value function representation, our approach was able to learn quickly with fewer agents than other exploration schemes.",1
"Deep Reinforcement Learning has shown tremendous success in solving several games and tasks in robotics. However, unlike humans, it generally requires a lot of training instances. Trajectories imitating to solve the task at hand can help to increase sample-efficiency of deep RL methods. In this paper, we present a simple approach to use such trajectories, applied to the challenging Ball-in-Maze Games, recently introduced in the literature. We show that in spite of not using human-generated trajectories and just using the simulator as a model to generate a limited number of trajectories, we can get a speed-up of about 2-3x in the learning process. We also discuss some challenges we observed while using trajectory-based learning for very sparse reward functions.",0
"Although Deep Reinforcement Learning has proven to be extremely effective in tackling various games and robotic tasks, it typically requires an extensive amount of training compared to humans. The utilization of trajectories that mimic the resolution of a given task can enhance the sample efficiency of deep RL techniques. This article presents a straightforward approach for implementing such trajectories in the Ball-in-Maze Games, which are notoriously difficult and were recently introduced in literature. Despite not relying on human-generated trajectories and instead utilizing a simulator to generate only a limited number of trajectories, we were able to expedite the learning process by approximately 2-3x. Additionally, we address some of the challenges encountered when using trajectory-based learning for highly sparse reward functions.",1
"In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.",0
"It has been proven that deep learning models can be trained with large batch sizes without losing data efficiency in many fields. However, the limits of this data parallelism vary from domain to domain, with ImageNet using batches of tens of thousands and Dota 2 RL agents using batches of millions. It is unclear why these limits vary and how to choose the correct batch size for a new domain. This paper introduces the gradient noise scale, a simple measure that predicts the largest useful batch size across various domains (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word, Atari, Dota) and generative model training. The noise scale increases as the loss decreases and depends on the model size. The theory also explains the tradeoff between compute-efficiency and time-efficiency and provides a rough model of adaptive batch-size training benefits.",1
"Deep reinforcement learning (deep RL) research has grown significantly in recent years. A number of software offerings now exist that provide stable, comprehensive implementations for benchmarking. At the same time, recent deep RL research has become more diverse in its goals. In this paper we introduce Dopamine, a new research framework for deep RL that aims to support some of that diversity. Dopamine is open-source, TensorFlow-based, and provides compact and reliable implementations of some state-of-the-art deep RL agents. We complement this offering with a taxonomy of the different research objectives in deep RL research. While by no means exhaustive, our analysis highlights the heterogeneity of research in the field, and the value of frameworks such as ours.",0
"In recent years, there has been significant growth in the field of deep reinforcement learning (deep RL) research. There are now several software options available that offer reliable, comprehensive implementations for benchmarking purposes. Additionally, deep RL research has become more varied in its objectives. This paper introduces Dopamine, a new research framework for deep RL that seeks to support this diversity. It is open-source, based on TensorFlow, and includes dependable implementations of some of the latest deep RL agents. We also provide a classification of the various research goals in deep RL research, which demonstrates the wide range of research in the field and the usefulness of frameworks like Dopamine. While not exhaustive, our analysis highlights the heterogeneity of the research and the importance of such frameworks.",1
"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.",0
"Homomorphic encryption and secure multiparty computation are currently the only methods available for sharing governance of a deep learning model. However, their practicality is limited due to their significant computational and communication overheads, rendering them unsuitable for the training of large neural networks. To address this issue, we propose a scalable technique for shared model governance by dividing the deep learning model among multiple parties. This paper explores the security guarantee of this approach by introducing the model completion problem, which determines how much training is required to restore a trained deep learning model's original performance using a subset of its parameters and the entire training data set or an environment simulator. We establish a metric for evaluating the difficulty of the model completion problem and conduct experiments on supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our findings indicate that the model completion problem is more challenging in reinforcement learning than in supervised learning due to the absence of the trained agent's trajectories. Furthermore, the degree of difficulty is predominantly determined by the type and location of the missing parameters rather than the number of parameters. Our results suggest that model splitting may be a viable technique for shared model governance in settings where training is prohibitively expensive.",1
"Deep reinforcement-learning methods have achieved remarkable performance on challenging control tasks. Observations of the resulting behavior give the impression that the agent has constructed a generalized representation that supports insightful action decisions. We re-examine what is meant by generalization in RL, and propose several definitions based on an agent's performance in on-policy, off-policy, and unreachable states. We propose a set of practical methods for evaluating agents with these definitions of generalization. We demonstrate these techniques on a common benchmark task for deep RL, and we show that the learned networks make poor decisions for states that differ only slightly from on-policy states, even though those states are not selected adversarially. Taken together, these results call into question the extent to which deep Q-networks learn generalized representations, and suggest that more experimentation and analysis is necessary before claims of representation learning can be supported.",0
"Impressive results have been achieved in challenging control tasks through deep reinforcement-learning methods. The behavior observed indicates that the agent has developed a comprehensive representation that enables it to make insightful decisions. In this study, we revisit the concept of generalization in RL and put forward various definitions based on the agent's performance in on-policy, off-policy, and unreachable states. We offer practical techniques for assessing the agents using these definitions of generalization. Using a standard deep RL benchmark task, we show that the learned networks make poor decisions for states that are only slightly different from on-policy states, even when those states are not deliberately chosen to be adversarial. These outcomes highlight the need for further experimentation and analysis to support claims of representation learning and raise concerns regarding the extent to which deep Q-networks learn generalized representations.",1
"We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, i.e. the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.",0
"Our focus is on developing a continuous state representation model that can adapt to changes in the environment. Our goal is to compress the sensory state's information in a way that preserves previous knowledge while accommodating new information efficiently. To achieve this, we propose a method that utilizes Variational Auto-Encoders for state representation and Generative Replay to retain past knowledge through generated samples. Additionally, we introduce an effective and reliable approach to detecting changes in the environment automatically. Our model enables efficient state representation, forward transfer, and prevents catastrophic forgetting, allowing for incremental learning without relying on prior data and with a bounded system size.",1
"Efficient Reinforcement Learning usually takes advantage of demonstration or good exploration strategy. By applying posterior sampling in model-free RL under the hypothesis of GP, we propose Gaussian Process Posterior Sampling Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving theoretical justifications and empirical results. We also provide theoretical and empirical results that various demonstration could lower expected uncertainty and benefit posterior sampling exploration. In this way, we combined the demonstration and exploration process together to achieve a more efficient reinforcement learning.",0
"A more efficient way of implementing Reinforcement Learning is achieved through the utilization of demonstration or a well-planned exploration strategy. Our proposed algorithm, Gaussian Process Posterior Sampling Reinforcement Learning (GPPSTD), utilizes posterior sampling in model-free RL, based on the GP hypothesis, for continuous state space. We have provided both theoretical and empirical evidence that different types of demonstration can decrease expected uncertainty and improve posterior sampling exploration. By combining these processes, we have achieved a more efficient Reinforcement Learning approach.",1
"A key challenge for gradient based optimization methods in model-free reinforcement learning is to develop an approach that is sample efficient and has low variance. In this work, we apply Kronecker-factored curvature estimation technique (KFAC) to a recently proposed gradient estimator for control variate optimization, RELAX, to increase the sample efficiency of using this gradient estimation method in reinforcement learning. The performance of the proposed method is demonstrated on a synthetic problem and a set of three discrete control task Atari games.",0
"Developing a sample-efficient and low variance approach is a significant hurdle for gradient-based optimization methods in model-free reinforcement learning. This study employs the Kronecker-factored curvature estimation technique (KFAC) to enhance the sample efficiency of the gradient estimator for control variate optimization, RELAX, in reinforcement learning. The performance of this method is evaluated on a synthetic problem and a series of three discrete control task Atari games.",1
"We propose a method for modeling and learning turn-taking behaviors for accessing a shared resource. We model the individual behavior for each agent in an interaction and then use a multi-agent fusion model to generate a summary over the expected actions of the group to render the model independent of the number of agents. The individual behavior models are weighted finite state transducers (WFSTs) with weights dynamically updated during interactions, and the multi-agent fusion model is a logistic regression classifier.   We test our models in a multi-agent tower-building environment, where a Q-learning agent learns to interact with rule-based agents. Our approach accurately models the underlying behavior patterns of the rule-based agents with accuracy ranging between 0.63 and 1.0 depending on the stochasticity of the other agent behaviors. In addition we show using KL-divergence that the model accurately captures the distribution of next actions when interacting with both a single agent (KL-divergence < 0.1) and with multiple agents (KL-divergence < 0.37). Finally, we demonstrate that our behavior model can be used by a Q-learning agent to take turns in an interactive turn-taking environment.",0
"Our proposed method involves modeling and learning turn-taking behaviors for accessing a shared resource. We create individual behavior models for each agent involved, and then use a multi-agent fusion model to generate a summary of expected actions from the group. This makes the model independent of the number of agents involved. The individual behavior models are WFSTs with weights that update dynamically during interactions, while the multi-agent fusion model is a logistic regression classifier. To test our models, we use a multi-agent tower-building environment where a Q-learning agent learns to interact with rule-based agents. Our approach accurately models the behavior patterns of the rule-based agents, with accuracy ranging from 0.63 to 1.0 depending on the stochasticity of the other agent behaviors. We also demonstrate that our model captures the distribution of next actions when interacting with both a single agent (KL-divergence < 0.1) and multiple agents (KL-divergence < 0.37). Finally, we show that our behavior model can be used by a Q-learning agent to take turns in an interactive turn-taking environment.",1
"We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma's Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma's Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.",0
"Our proposed approach offers a novel solution to tackling challenging exploration tasks like Montezuma's Revenge in Atari games. Unlike other recent works that rely on imitating human demonstrations, our method directly maximizes rewards. We leverage off-the-shelf reinforcement learning to train our agent, which begins each episode by resetting to a state from a demonstration. This reduces the amount of exploration required to learn the game compared to starting from scratch every time. We demonstrate the effectiveness of our method in a simple toy environment, where we highlight the exponential scaling of standard RL methods for sparse reward tasks. Our approach reduces this scaling to quadratic, making previously infeasible tasks more accessible. We then apply our method to Montezuma's Revenge, achieving a high-score of 74,500, surpassing all previously published results.",1
"Although deep reinforcement learning (deep RL) methods have lots of strengths that are favorable if applied to autonomous driving, real deep RL applications in autonomous driving have been slowed down by the modeling gap between the source (training) domain and the target (deployment) domain. Unlike current policy transfer approaches, which generally limit to the usage of uninterpretable neural network representations as the transferred features, we propose to transfer concrete kinematic quantities in autonomous driving. The proposed robust-control-based (RC) generic transfer architecture, which we call RL-RC, incorporates a transferable hierarchical RL trajectory planner and a robust tracking controller based on disturbance observer (DOB). The deep RL policies trained with known nominal dynamics model are transfered directly to the target domain, DOB-based robust tracking control is applied to tackle the modeling gap including the vehicle dynamics errors and the external disturbances such as side forces. We provide simulations validating the capability of the proposed method to achieve zero-shot transfer across multiple driving scenarios such as lane keeping, lane changing and obstacle avoidance.",0
"The implementation of deep reinforcement learning (deep RL) methods for autonomous driving has been hindered by the disparity between the training and deployment domains. While existing policy transfer methods rely on impenetrable neural network representations for feature transfer, we suggest transferring tangible kinematic quantities in autonomous driving. Our proposed transfer framework, RL-RC, utilizes a transferable hierarchical RL trajectory planner and robust tracking controller based on disturbance observer (DOB). The deep RL policies trained using nominal dynamics model are transferred directly to the target domain, and DOB-based robust tracking control is used to overcome the modeling gap, including vehicle dynamics errors and external disturbances such as side forces. Our simulations demonstrate the effectiveness of the proposed method in achieving zero-shot transfer across multiple driving scenarios, including lane keeping, lane changing, and obstacle avoidance.",1
"Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimisation problems, such as the travelling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward (R2) algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds.",0
"The use of adversarial self-play in two-player games has yielded impressive outcomes when paired with reinforcement learning methods that incorporate deep neural networks and tree search. These algorithms, such as AlphaZero and Expert Iteration, learn from scratch, generating highly informative training data on the spot. However, this self-play training strategy cannot be directly applied to single-player games. Recently, various combinatorial optimization problems, like the bin packing problem and the traveling salesman problem, have been redefined as reinforcement learning problems, which emphasizes the need to extend the advantages of self-play beyond two-player games. Our solution to this is the Ranked Reward (R2) algorithm, which creates a relative performance metric by ranking the rewards acquired by a single agent across multiple games. Our results indicate that the R2 algorithm outperforms generic Monte Carlo tree search, heuristic algorithms, and integer programming solvers when applied to two-dimensional and three-dimensional bin packing problem instances. Furthermore, we present an analysis of the ranked reward mechanism, particularly its impact on problem instances with varying levels of difficulty and different ranking thresholds.",1
"Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ""victim"" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ""knockoff"" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.",0
"The use of Machine Learning (ML) models has become more common for various tasks, and our research examines the ability of an adversary to steal the functionality of these ""victim"" models through blackbox interactions: input of an image and output of a prediction. Our approach differs from previous studies as we assume the adversary lacks knowledge of the model's training and testing data, its internal workings, and semantics over output values. We propose a two-step process for stealing the model's functionality: (i) sending a set of input images to the blackbox model and obtaining predictions; (ii) training a ""knockoff"" model using the image-prediction pairs obtained from the queries. Our research reveals several interesting findings: (a) even when using a different architecture, the knockoff performs well when queried with random images from a different distribution than the blackbox training data; (b) and (c) our reinforcement learning method improves query sample efficiency and performance gains in certain settings. Our validation of model functionality stealing includes various datasets and tasks, as well as an image analysis API where a reasonable knockoff can be created for as little as $30.",1
"Recent research has shown that although Reinforcement Learning (RL) can benefit from expert demonstration, it usually takes considerable efforts to obtain enough demonstration. The efforts prevent training decent RL agents with expert demonstration in practice. In this work, we propose Active Reinforcement Learning with Demonstration (ARLD), a new framework to streamline RL in terms of demonstration efforts by allowing the RL agent to query for demonstration actively during training. Under the framework, we propose Active Deep Q-Network, a novel query strategy which adapts to the dynamically-changing distributions during the RL training process by estimating the uncertainty of recent states. The expert demonstration data within Active DQN are then utilized by optimizing supervised max-margin loss in addition to temporal difference loss within usual DQN training. We propose two methods of estimating the uncertainty based on two state-of-the-art DQN models, namely the divergence of bootstrapped DQN and the variance of noisy DQN. The empirical results validate that both methods not only learn faster than other passive expert demonstration methods with the same amount of demonstration and but also reach super-expert level of performance across four different tasks.",0
"Recent studies indicate that while Reinforcement Learning (RL) can benefit from expert demonstration, acquiring sufficient demonstration requires significant effort, hampering the training of effective RL agents with expert demonstration. To address this issue, we introduce Active Reinforcement Learning with Demonstration (ARLD), a novel framework that simplifies RL by enabling the RL agent to actively request for demonstration during training. Within this framework, we introduce Active Deep Q-Network, an innovative query strategy that adjusts to the prevalent dynamic distributions during the RL training process by assessing the uncertainty of recent states. The expert demonstration data within Active DQN are then employed by optimizing the supervised max-margin loss, in addition to the temporal difference loss in traditional DQN training. We propose two methods for estimating uncertainty based on two state-of-the-art DQN models, that is, the divergence of bootstrapped DQN and the variance of noisy DQN. Empirical results indicate that both methods not only facilitate quicker learning than other passive expert demonstration methods with the same level of demonstration, but also achieve super-expert performance across four distinct tasks.",1
"We present an off-policy actor-critic algorithm for Reinforcement Learning (RL) that combines ideas from gradient-free optimization via stochastic search with learned action-value function. The result is a simple procedure consisting of three steps: i) policy evaluation by estimating a parametric action-value function; ii) policy improvement via the estimation of a local non-parametric policy; and iii) generalization by fitting a parametric policy. Each step can be implemented in different ways, giving rise to several algorithm variants. Our algorithm draws on connections to existing literature on black-box optimization and 'RL as an inference' and it can be seen either as an extension of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997] to a policy iteration scheme. Our comparison on 31 continuous control tasks from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al., 2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited amount of compute and a single set of hyperparameters, demonstrate the effectiveness of our method and the state of art results. Videos, summarizing results, can be found at goo.gl/HtvJKR .",0
"We introduce an off-policy actor-critic algorithm for Reinforcement Learning (RL) that blends gradient-free optimization through stochastic search with a learned action-value function. This results in a straightforward three-step process: i) policy evaluation by approximating a parametric action-value function; ii) policy improvement through estimating a local non-parametric policy; and iii) generalization by fitting a parametric policy. Each step can be executed in various ways, leading to multiple algorithmic variations. Our algorithm is grounded in previous research on black-box optimization and 'RL as an inference.' It can be viewed as an extension of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997] to a policy iteration scheme. Our approach is compared to 31 continuous control tasks from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al., 2018] and OpenAI Gym [Brockman et al., 2016] with a limited amount of computing power and a single set of hyperparameters, demonstrating its effectiveness and producing state-of-the-art results. Videos summarizing the findings can be found at goo.gl/HtvJKR.",1
"The capacity of meta-learning algorithms to quickly adapt to a variety of tasks, including ones they did not experience during meta-training, has been a key factor in the recent success of these methods on few-shot learning problems. This particular advantage of using meta-learning over standard supervised or reinforcement learning is only well founded under the assumption that the adaptation phase does improve the performance of our model on the task of interest. However, in the classical framework of meta-learning, this constraint is only mildly enforced, if not at all, and we only see an improvement on average over a distribution of tasks. In this paper, we show that the adaptation in an algorithm like MAML can significantly decrease the performance of an agent in a meta-reinforcement learning setting, even on a range of meta-training tasks.",0
"Meta-learning algorithms have the ability to quickly adapt to various tasks, even ones not encountered during meta-training, which has contributed to their success in few-shot learning problems. This advantage over standard supervised or reinforcement learning is based on the assumption that the adaptation phase improves the model's performance on the target task. However, the traditional meta-learning framework only mildly enforces this constraint, resulting in an average improvement over a task distribution. In this paper, we demonstrate that in a meta-reinforcement learning scenario, adaptation in an algorithm such as MAML can significantly reduce an agent's performance, even across a range of meta-training tasks.",1
"We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q&A. Our visual context tree model, dubbed VCTree, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., ""clothes"" and ""pants"" are usually co-occur and belong to ""person""; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTree, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q&A, show that VCTree outperforms state-of-the-art results while discovering interpretable visual context structures.",0
"Our proposal is to create dynamic tree structures that provide a visual context for objects within an image, aiding visual reasoning tasks such as scene graph generation and visual Q&A. Our model, called VCTree, offers two distinct advantages over existing object representations including chains and fully-connected graphs. Firstly, the binary tree structure efficiently encodes the parallel/hierarchical relationships between objects, such as how ""clothes"" and ""pants"" are typically associated with a ""person"". Secondly, the dynamic structure can vary depending on the image and task, enabling more targeted message passing among objects. To construct a VCTree, we use a score function to determine the validity of object pairs, and then generate a binary tree using the maximum spanning tree from the score matrix. The visual contexts are encoded and decoded using bidirectional TreeLSTM and task-specific models. We employ a hybrid learning approach that combines end-task supervised learning with tree structure reinforcement learning, where the former serves as a self-critic for the latter's structure exploration. Our experimental results on two benchmarks, Visual Genome and VQA2.0, demonstrate that VCTree outperforms existing methods while producing interpretable visual context structures.",1
"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.",0
"This paper deals with the issue of assessing learning systems in safety-critical fields such as autonomous driving, where errors can lead to disastrous outcomes. Our focus is on two problems: identifying scenarios where trained agents fail and determining their likelihood of failure. The conventional reinforcement learning method for assessing agents, Vanilla Monte Carlo, may not detect failures, resulting in the deployment of dangerous agents. We demonstrate that this is a problem even for current agents, where matching the compute used for training is sometimes not enough for evaluation. To address this limitation, we draw on the literature on rare event probability estimation and propose an adversarial evaluation strategy that centers on adversarial situations while still offering unbiased failure probability estimates. The main challenge is identifying these adversarial situations, as failure is infrequent, and there is little signal to guide optimization. To overcome this, we propose a continuation approach that learns failure modes in related but less robust agents. Additionally, our approach allows for the reuse of data already gathered for agent training. We demonstrate the effectiveness of adversarial evaluation in two standard domains: humanoid control and simulated driving. The experimental outcomes show that our methods can detect catastrophic failures and estimate agent failure rates several orders of magnitude faster than traditional evaluation techniques, taking minutes to hours instead of days.",1
"The recently proposed option-critic architecture Bacon et al. provide a stochastic policy gradient approach to hierarchical reinforcement learning. Specifically, they provide a way to estimate the gradient of the expected discounted return with respect to parameters that define a finite number of temporally extended actions, called \textit{options}. In this paper we show how the option-critic architecture can be extended to estimate the natural gradient of the expected discounted return. To this end, the central questions that we consider in this paper are: 1) what is the definition of the natural gradient in this context, 2) what is the Fisher information matrix associated with an option's parameterized policy, 3) what is the Fisher information matrix associated with an option's parameterized termination function, and 4) how can a compatible function approximation approach be leveraged to obtain natural gradient estimates for both the parameterized policy and parameterized termination functions of an option with per-time-step time and space complexity linear in the total number of parameters. Based on answers to these questions we introduce the natural option critic algorithm. Experimental results showcase improvement over the vanilla gradient approach.",0
"Bacon et al. proposed the option-critic architecture for hierarchical reinforcement learning, which employs a stochastic policy gradient approach. This method estimates the gradient of the expected discounted return by defining a finite number of temporally extended actions, or ""options,"" and calculating their associated parameters. In this study, we expand on this approach by introducing the natural gradient estimation for the expected discounted return. We address four key questions: the definition of the natural gradient, the Fisher information matrix for policy and termination functions, and a compatible function approximation method. Using these answers, we introduce the natural option critic algorithm, which demonstrates improvement over the standard gradient approach in our experiments. The time and space complexity of this algorithm is proportional to the total number of parameters.",1
"For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",0
"To perform a variety of user-defined tasks during testing, an independent entity must possess a collection of skills that are widely applicable and adaptable. These skills should also be able to handle raw sensory data, such as images, to ensure they are general enough. In this article, we suggest a method that combines unsupervised representation learning and reinforcement learning of goal-oriented policies to acquire such general-purpose skills. Since the entity is unaware of the specific goals it may need to achieve during testing, it goes through a self-supervised ""practice"" phase where it imagines and attempts to achieve goals. We also teach the entity a visual representation with three objectives: generating goals for self-supervised practice, transforming raw sensory inputs in a structured manner, and calculating a reward signal for goal completion. We propose a retroactive goal relabeling approach to enhance the efficiency of our method. Our off-policy method is powerful enough to learn policies that use raw image data and objectives for a real-world robotic system and outperforms prior methods significantly.",1
"A vine copula model is a flexible high-dimensional dependence model which uses only bivariate building blocks. However, the number of possible configurations of a vine copula grows exponentially as the number of variables increases, making model selection a major challenge in development. In this work, we formulate a vine structure learning problem with both vector and reinforcement learning representation. We use neural network to find the embeddings for the best possible vine model and generate a structure. Throughout experiments on synthetic and real-world datasets, we show that our proposed approach fits the data better in terms of log-likelihood. Moreover, we demonstrate that the model is able to generate high-quality samples in a variety of applications, making it a good candidate for synthetic data generation.",0
"The vine copula model is a highly flexible dependence model that relies solely on bivariate building blocks. However, as the number of variables increases, the possible configurations of a vine copula grow exponentially, posing a significant challenge for model selection. To address this issue, we present a vine structure learning problem that utilizes both vector and reinforcement learning representations. Our approach employs neural networks to identify the best possible vine model and generate a structure. Our experiments on synthetic and real-world datasets demonstrate that our proposed method provides a better fit to the data in terms of log-likelihood. Additionally, our model produces high-quality samples across various applications, making it a promising candidate for synthetic data generation.",1
"Intelligent Transportation Systems (ITSs) are envisioned to play a critical role in improving traffic flow and reducing congestion, which is a pervasive issue impacting urban areas around the globe. Rapidly advancing vehicular communication and edge cloud computation technologies provide key enablers for smart traffic management. However, operating viable real-time actuation mechanisms on a practically relevant scale involves formidable challenges, e.g., policy iteration and conventional Reinforcement Learning (RL) techniques suffer from poor scalability due to state space explosion. Motivated by these issues, we explore the potential for Deep Q-Networks (DQN) to optimize traffic light control policies. As an initial benchmark, we establish that the DQN algorithms yield the ""thresholding"" policy in a single-intersection. Next, we examine the scalability properties of DQN algorithms and their performance in a linear network topology with several intersections along a main artery. We demonstrate that DQN algorithms produce intelligent behavior, such as the emergence of ""greenwave"" patterns, reflecting their ability to learn favorable traffic light actuations.",0
"The use of Intelligent Transportation Systems (ITSs) is crucial in reducing traffic congestion, a common issue in cities worldwide. The development of vehicular communication and edge cloud computation technologies has enabled smart traffic management. However, implementing real-time actuation mechanisms on a large scale is challenging due to issues such as policy iteration and conventional Reinforcement Learning techniques being unable to handle the state space explosion. To address these problems, we investigate the potential of using Deep Q-Networks (DQN) to optimize traffic light control policies. We test the DQN algorithms in a single-intersection scenario and find that they produce the ""thresholding"" policy. Furthermore, we evaluate their scalability and performance in a linear network topology with multiple intersections along a main road. Results show that the DQN algorithms exhibit intelligent behavior, such as the emergence of ""greenwave"" patterns, indicating their ability to learn favorable traffic light actuations.",1
"De novo protein structure prediction from amino acid sequence is one of the most challenging problems in computational biology. As one of the extensively explored mathematical models for protein folding, Hydrophobic-Polar (HP) model enables thorough investigation of protein structure formation and evolution. Although HP model discretizes the conformational space and simplifies the folding energy function, it has been proven to be an NP-complete problem. In this paper, we propose a novel protein folding framework FoldingZero, self-folding a de novo protein 2D HP structure from scratch based on deep reinforcement learning. FoldingZero features the coupled approach of a two-head (policy and value heads) deep convolutional neural network (HPNet) and a regularized Upper Confidence Bounds for Trees (R-UCT). It is trained solely by a reinforcement learning algorithm, which improves HPNet and R-UCT iteratively through iterative policy optimization. Without any supervision and domain knowledge, FoldingZero not only achieves comparable results, but also learns the latent folding knowledge to stabilize the structure. Without exponential computation, FoldingZero shows promising potential to be adopted for real-world protein properties prediction.",0
"The task of predicting protein structure from amino acid sequence is extremely difficult in computational biology. The Hydrophobic-Polar (HP) model is a widely used mathematical model for protein folding which allows for in-depth exploration of protein structure development and evolution. Despite its simplification of the folding energy function and discretization of the conformational space, the HP model has been proven to be an NP-complete problem. In this paper, we introduce FoldingZero, a novel protein folding framework that uses deep reinforcement learning to self-fold a de novo protein 2D HP structure from scratch. FoldingZero combines a two-head deep convolutional neural network (HPNet) and a regularized Upper Confidence Bounds for Trees (R-UCT) to train solely with a reinforcement learning algorithm which iteratively optimizes HPNet and R-UCT. Without any supervision or domain knowledge, FoldingZero produces comparable results and learns latent folding knowledge to stabilize the structure. It shows great potential for real-world protein properties prediction without the need for exponential computation.",1
"This paper explores a simple regularizer for reinforcement learning by proposing Generative Adversarial Self-Imitation Learning (GASIL), which encourages the agent to imitate past good trajectories via generative adversarial imitation learning framework. Instead of directly maximizing rewards, GASIL focuses on reproducing past good trajectories, which can potentially make long-term credit assignment easier when rewards are sparse and delayed. GASIL can be easily combined with any policy gradient objective by using GASIL as a learned shaped reward function. Our experimental results show that GASIL improves the performance of proximal policy optimization on 2D Point Mass and MuJoCo environments with delayed reward and stochastic dynamics.",0
"The objective of this paper is to propose a straightforward regularizer for reinforcement learning referred to as Generative Adversarial Self-Imitation Learning (GASIL). This approach encourages the agent to imitate prior successful trajectories through the use of a generative adversarial imitation learning framework. Instead of solely maximizing rewards, GASIL emphasizes replicating past good trajectories, which may simplify long-term credit assignment when rewards are few and delayed. GASIL can be readily incorporated with any policy gradient objective by using it as a learned shaped reward function. Empirical results demonstrate that GASIL enhances the performance of proximal policy optimization in both 2D Point Mass and MuJoCo environments with stochastic dynamics and delayed reward.",1
"Multi-agent reinforcement learning systems aim to provide interacting agents with the ability to collaboratively learn and adapt to the behaviour of other agents. In many real-world applications, the agents can only acquire a partial view of the world. Here we consider a setting whereby most agents' observations are also extremely noisy, hence only weakly correlated to the true state of the environment. Under these circumstances, learning an optimal policy becomes particularly challenging, even in the unrealistic case that an agent's policy can be made conditional upon all other agents' observations. To overcome these difficulties, we propose a multi-agent deep deterministic policy gradient algorithm enhanced by a communication medium (MADDPG-M), which implements a two-level, concurrent learning mechanism. An agent's policy depends on its own private observations as well as those explicitly shared by others through a communication medium. At any given point in time, an agent must decide whether its private observations are sufficiently informative to be shared with others. However, our environments provide no explicit feedback informing an agent whether a communication action is beneficial, rather the communication policies must also be learned through experience concurrently to the main policies. Our experimental results demonstrate that the algorithm performs well in six highly non-stationary environments of progressively higher complexity, and offers substantial performance gains compared to the baselines.",0
"The objective of multi-agent reinforcement learning systems is to enable agents to learn and adapt to the behavior of their peers. In practical applications, agents often have a limited view of the environment and face the challenge of dealing with noisy observations that are only weakly correlated with the true state of the environment. Even in cases where an agent's policy can be made conditional upon all other agents' observations, learning an optimal policy is difficult. To address this challenge, we propose a multi-agent deep deterministic policy gradient algorithm enhanced by a communication medium (MADDPG-M). Our algorithm implements a two-level, concurrent learning mechanism where an agent's policy depends on its own private observations and those explicitly shared by others. However, communication policies must also be learned through experience concurrently with the main policies, as there is no explicit feedback to inform an agent whether a communication action is beneficial. Our experimental results demonstrate that the algorithm performs well in six highly non-stationary environments of progressively higher complexity and provides substantial performance gains compared to the baselines.",1
"Advances in Deep Reinforcement Learning have led to agents that perform well across a variety of sensory-motor domains. In this work, we study the setting in which an agent must learn to generate programs for diverse scenes conditioned on a given symbolic instruction. Final goals are specified to our agent via images of the scenes. A symbolic instruction consistent with the goal images is used as the conditioning input for our policies. Since a single instruction corresponds to a diverse set of different but still consistent end-goal images, the agent needs to learn to generate a distribution over programs given an instruction. We demonstrate that with simple changes to the reinforced adversarial learning objective, we can learn instruction conditioned policies to achieve the corresponding diverse set of goals. Most importantly, our agent's stochastic policy is shown to more accurately capture the diversity in the goal distribution than a fixed pixel-based reward function baseline. We demonstrate the efficacy of our approach on two domains: (1) drawing MNIST digits with a paint software conditioned on instructions and (2) constructing scenes in a 3D editor that satisfies a certain instruction.",0
"The progress made in Deep Reinforcement Learning has resulted in efficient agents that excel in several sensory-motor areas. In this study, we examine a scenario where an agent must acquire the skill to create programs for a range of scenes, with a specific symbolic command as the precondition. The final objectives are conveyed to the agent through images of the scenes. Our policies rely on a symbolic instruction that corresponds to the goal images. As a single instruction can lead to multiple but compatible end-result images, the agent must learn to generate a program distribution based on the instruction. By making simple modifications to the reinforced adversarial learning objective, we can teach instruction-dependent policies to meet the diverse range of goals. Our crucial finding is that our agent's stochastic policy more effectively captures the variation in the goal distribution than the fixed pixel-based reward function baseline. We exhibit the effectiveness of our method in two domains: (1) using a painting software to draw MNIST digits conditioned on instructions, and (2) creating scenes in a 3D editor that fulfill specific commands.",1
"To solve a text-based game, an agent needs to formulate valid text commands for a given context and find the ones that lead to success. Recent attempts at solving text-based games with deep reinforcement learning have focused on the latter, i.e., learning to act optimally when valid actions are known in advance. In this work, we propose to tackle the first task and train a model that generates the set of all valid commands for a given context. We try three generative models on a dataset generated with Textworld. The best model can generate valid commands which were unseen at training and achieve high $F_1$ score on the test set.",0
"In order to succeed in a text-based game, an agent must create appropriate text commands within a given context and determine which ones will lead to success. Previous attempts to solve these games using deep reinforcement learning have primarily focused on the latter task, which involves learning to act optimally when valid actions have already been determined. This study, however, aims to address the former task by training a model that can generate a complete set of valid commands for any given context. The study tests three different generative models using a dataset created with Textworld. The best performing model was able to generate previously unseen valid commands and achieve a high F1 score on the test set.",1
"Target tracking in a camera network is an important task for surveillance and scene understanding. The task is challenging due to disjoint views and illumination variation in different cameras. In this direction, many graph-based methods were proposed using appearance-based features. However, the appearance information fades with high illumination variation in the different camera FOVs. We, in this paper, use spatial and temporal information as the state of the target to learn a policy that predicts the next camera given the current state. The policy is trained using Q-learning and it does not assume any information about the topology of the camera network. We will show that the policy learns the camera network topology. We demonstrate the performance of the proposed method on the NLPR MCT dataset.",0
"Surveillance and scene comprehension require effective target tracking in camera networks, which is a challenging task due to the varying illumination and disjoint views across multiple cameras. Previous graph-based methods utilized appearance features, but these features degrade in the presence of high illumination variation. This paper proposes a policy that predicts the next camera based on the current target state, using spatial and temporal information. The policy is trained using Q-learning, without assuming any knowledge of the camera network topology. The study shows that the policy can learn the camera network topology, and the proposed method's effectiveness is demonstrated on the NLPR MCT dataset.",1
"Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.",0
"The amalgamation of reinforcement learning (RL) and deep learning has given rise to a new field of study known as deep reinforcement learning. This area of research has been able to tackle a vast array of intricate decision-making tasks that were previously beyond the capabilities of a machine. As a result, deep RL has opened up numerous potential applications in various fields, including healthcare, robotics, smart grids, and finance, among others. This paper aims to provide an overview of deep reinforcement learning models, algorithms, and techniques, with a particular emphasis on generalization and practical applications. It is assumed that the reader has a basic understanding of machine learning concepts.",1
"In urban environments, supply resources have to be constantly matched to the ""right"" locations (where customer demand is present) so as to improve quality of life. For instance, ambulances have to be matched to base stations regularly so as to reduce response time for emergency incidents in EMS (Emergency Management Systems); vehicles (cars, bikes, scooters etc.) have to be matched to docking stations so as to reduce lost demand in shared mobility systems. Such problem domains are challenging owing to the demand uncertainty, combinatorial action spaces (due to allocation) and constraints on allocation of resources (e.g., total resources, minimum and maximum number of resources at locations and regions).   Existing systems typically employ myopic and greedy optimization approaches to optimize allocation of supply resources to locations. Such approaches typically are unable to handle surges or variances in demand patterns well. Recent research has demonstrated the ability of Deep RL methods in adapting well to highly uncertain environments. However, existing Deep RL methods are unable to handle combinatorial action spaces and constraints on allocation of resources. To that end, we have developed three approaches on top of the well known actor critic approach, DDPG (Deep Deterministic Policy Gradient) that are able to handle constraints on resource allocation. More importantly, we demonstrate that they are able to outperform leading approaches on simulators validated on semi-real and real data sets.",0
"Improving quality of life in urban environments requires constantly matching supply resources to areas with high customer demand. This includes allocating ambulances to base stations to reduce emergency response times and assigning vehicles to docking stations in shared mobility systems to minimize lost demand. However, these allocation problems are challenging due to demand uncertainty, constraints on resource allocation, and combinatorial action spaces. Existing optimization methods are myopic and unable to handle demand surges. Deep RL methods have shown promise in uncertain environments, but struggle with combinatorial action spaces and constraints. To address this, we developed three approaches on top of DDPG that can handle resource allocation constraints and outperform leading methods on semi-real and real data sets.",1
"Recent studies on neural architecture search have shown that automatically designed neural networks perform as good as expert-crafted architectures. While most existing works aim at finding architectures that optimize the prediction accuracy, these architectures may have complexity and is therefore not suitable being deployed on certain computing environment (e.g., with limited power budgets). We propose MONAS, a framework for Multi-Objective Neural Architectural Search that employs reward functions considering both prediction accuracy and other important objectives (e.g., power consumption) when searching for neural network architectures. Experimental results showed that, compared to the state-ofthe-arts, models found by MONAS achieve comparable or better classification accuracy on computer vision applications, while satisfying the additional objectives such as peak power.",0
"Recent research on neural architecture search has revealed that automatically designed neural networks perform equally well as architectures crafted by experts. However, most of these studies focus solely on optimizing prediction accuracy, which can lead to complex architectures that are unsuitable for deployment on limited computing environments. To address this issue, we present MONAS, a framework for Multi-Objective Neural Architectural Search that incorporates reward functions for other important objectives, such as power consumption, alongside prediction accuracy. Our experimental results demonstrate that models discovered by MONAS are comparable, if not superior, to state-of-the-art models for computer vision tasks, while also satisfying additional objectives like peak power.",1
"Semi-supervised learning is attracting increasing attention due to the fact that datasets of many domains lack enough labeled data. Variational Auto-Encoder (VAE), in particular, has demonstrated the benefits of semi-supervised learning. The majority of existing semi-supervised VAEs utilize a classifier to exploit label information, where the parameters of the classifier are introduced to the VAE. Given the limited labeled data, learning the parameters for the classifiers may not be an optimal solution for exploiting label information. Therefore, in this paper, we develop a novel approach for semi-supervised VAE without classifier. Specifically, we propose a new model called Semi-supervised Disentangled VAE (SDVAE), which encodes the input data into disentangled representation and non-interpretable representation, then the category information is directly utilized to regularize the disentangled representation via the equality constraint. To further enhance the feature learning ability of the proposed VAE, we incorporate reinforcement learning to relieve the lack of data. The dynamic framework is capable of dealing with both image and text data with its corresponding encoder and decoder networks. Extensive experiments on image and text datasets demonstrate the effectiveness of the proposed framework.",0
"The lack of labeled data in many domains has led to an increase in interest in semi-supervised learning. Variational Auto-Encoder (VAE) has shown promising results in this area. However, most existing semi-supervised VAEs rely on a classifier to exploit label information, which may not be optimal given the limited labeled data. In this paper, we present a new approach for semi-supervised VAE called Semi-supervised Disentangled VAE (SDVAE), which does not use a classifier. SDVAE encodes input data into disentangled and non-interpretable representations and directly utilizes category information to regulate the disentangled representation through the equality constraint. To improve the feature learning ability of the proposed VAE, we incorporate reinforcement learning to overcome data scarcity. The dynamic framework can handle both image and text data using corresponding encoder and decoder networks. Extensive experiments on image and text datasets prove the effectiveness of our proposed framework.",1
"Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.",0
"The investment companies heavily rely on their stock trading strategy, but crafting an optimal plan in the ever-changing and complex stock market is a daunting task. To tackle this issue, we investigate the potential of deep reinforcement learning to maximize investment returns by optimizing the stock trading strategy. Our study involves selecting thirty trading stocks and utilizing their daily prices as the market environment for both training and trading. Through this, we develop an adaptive trading strategy by training a deep reinforcement learning agent. We evaluate the agent's performance by comparing it to the traditional min-variance portfolio allocation strategy and the Dow Jones Industrial Average. The results reveal that our proposed approach of deep reinforcement learning outperforms the two baselines in terms of both the Sharpe ratio and cumulative returns.",1
"Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.",0
"Reinforcement learning employs curriculum learning as a training approach to expedite the learning of a complex target task. This is achieved by first training on a series of simpler tasks and then transferring the acquired knowledge to the target task. However, automatically selecting a sequence of such tasks remains a challenge, and recent studies have focused on resolving this issue. In this research, we enhance a recent curriculum design technique that transforms the curriculum sequencing problem into a Markov Decision Process. We extend this model to manage multiple transfer learning techniques and prove that a curriculum policy over this MDP can be learned through experience. We assess our approach by designing curriculum policies for multiple agents in two distinct domains, using various representations that enable this. Our method yields curricula that can train agents to perform as fast or faster on a target task than existing methods.",1
"Current clinical practice to monitor patients' health follows either regular or heuristic-based lab test (e.g. blood test) scheduling. Such practice not only gives rise to redundant measurements accruing cost, but may even lead to unnecessary patient discomfort. From the computational perspective, heuristic-based test scheduling might lead to reduced accuracy of clinical forecasting models. Computationally learning an optimal clinical test scheduling and measurement collection, is likely to lead to both, better predictive models and patient outcome improvement. We address the scheduling problem using deep reinforcement learning (RL) to achieve high predictive gain and low measurement cost, by scheduling fewer, but strategically timed tests. We first show that in the simulation our policy outperforms heuristic-based measurement scheduling with higher predictive gain or lower cost measured by accumulated reward. We then learn a scheduling policy for mortality forecasting in the real-world clinical dataset (MIMIC3), our learned policy is able to provide useful clinical insights. To our knowledge, this is the first RL application on multi-measurement scheduling problem in the clinical setting.",0
"The current practice of monitoring patients' health involves regular or heuristic-based lab tests, such as blood tests. However, this approach can result in unnecessary costs and patient discomfort due to redundant measurements. Additionally, heuristic-based scheduling may not accurately predict clinical outcomes. Therefore, it is important to computationally learn an optimal test scheduling approach to improve predictive models and patient outcomes. We propose using deep reinforcement learning to schedule strategically timed tests that achieve high predictive gain and low measurement cost. Our simulation results show that our policy outperforms heuristic-based scheduling. Furthermore, we apply our approach to a real-world clinical dataset (MIMIC3) for mortality forecasting and find that our learned policy provides useful clinical insights. This is the first application of reinforcement learning to multi-measurement scheduling in a clinical setting.",1
"Deep reinforcement learning (DRL) has achieved great successes in recent years with the help of novel methods and higher compute power. However, there are still several challenges to be addressed such as convergence to locally optimal policies and long training times. In this paper, firstly, we augment Asynchronous Advantage Actor-Critic (A3C) method with a novel self-supervised auxiliary task, i.e. \emph{Terminal Prediction}, measuring temporal closeness to terminal states, namely A3C-TP. Secondly, we propose a new framework where planning algorithms such as Monte Carlo tree search or other sources of (simulated) demonstrators can be integrated to asynchronous distributed DRL methods. Compared to vanilla A3C, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.",0
"Recent years have seen significant accomplishments in Deep Reinforcement Learning (DRL) due to advanced methods and enhanced computing capabilities. However, there are still obstacles to overcome, such as the attainment of locally optimal policies and extended training periods. In this study, we introduce two novel approaches. Firstly, we enhance the Asynchronous Advantage Actor-Critic (A3C) method with a self-supervised auxiliary task, referred to as Terminal Prediction, which measures proximity to terminal states. Our new method, A3C-TP, is designed to overcome the limitations of vanilla A3C. Secondly, we suggest a new framework that integrates planning algorithms, such as Monte Carlo tree search, or other sources of simulated demonstrators, into asynchronous distributed DRL approaches. Our proposed methods outperform vanilla A3C in terms of faster learning and better convergence to optimal policies, as demonstrated in the two-player mini version of the Pommerman game.",1
"Solving tasks with sparse rewards is a main challenge in reinforcement learning. While hierarchical controllers are an intuitive approach to this problem, current methods often require manual reward shaping, alternating training phases, or manually defined sub tasks. We introduce modulated policy hierarchies (MPH), that can learn end-to-end to solve tasks from sparse rewards. To achieve this, we study different modulation signals and exploration for hierarchical controllers. Specifically, we find that communicating via bit-vectors is more efficient than selecting one out of multiple skills, as it enables mixing between them. To facilitate exploration, MPH uses its different time scales for temporally extended intrinsic motivation at each level of the hierarchy. We evaluate MPH on the robotics tasks of pushing and sparse block stacking, where it outperforms recent baselines.",0
"One of the main challenges in reinforcement learning is addressing tasks with sparse rewards. Although hierarchical controllers are commonly used to solve this problem, current methods often require manual reward shaping, alternating training phases, or manually defined sub tasks. Our solution to this problem is the introduction of modulated policy hierarchies (MPH), which can learn end-to-end and solve tasks with sparse rewards. We conducted a study on different modulation signals and exploration techniques for hierarchical controllers, and discovered that communicating via bit-vectors is more efficient than selecting one out of multiple skills because it allows for mixing between them. MPH uses its different time scales to facilitate exploration through temporally extended intrinsic motivation at each level of the hierarchy. We evaluated MPH on the robotics tasks of pushing and sparse block stacking, and it outperformed recent baselines.",1
"Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.",0
"Tasks with sparse environment rewards have traditionally posed a challenge for deep reinforcement learning methods. One effective solution is to imitate trajectories demonstrated by a human, but these demonstrations are usually collected under artificial conditions with precise access to the agent's environment setup, as well as the demonstrator's action and reward trajectories. To address these limitations, we propose a two-stage approach that utilizes noisy, unaligned footage without such data access. First, we employ self-supervised objectives that encompass time and modality (i.e. vision and sound) to map unaligned videos from various sources to a shared representation. Second, we use a single YouTube video to create a reward function that encourages an agent to mimic human gameplay. This one-shot imitation technique enables our agent to outperform human-level performance on notoriously difficult exploration games such as Montezuma's Revenge, Pitfall!, and Private Eye, even without any environment rewards.",1
"Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.",0
"The field of inverse reinforcement learning (IRL) has seen progress in the development of advanced inference frameworks that challenge the original assumption of modeling an agent's behavior according to a single intention. Recent IRL methods have improved on this assumption by dividing demonstration data into parts that account for different intentions. This work takes it further by introducing the concept of subgoals, which allows for a more efficient explanation of a single trajectory within a specific context. With this idea in mind, an implicit intentional model is created to predict an agent's behavior in new situations. The result is a Bayesian prediction framework superior to existing IRL solutions, providing consistent policy estimates. This approach is well-suited for situations where an agent's intentions change over time, which classical IRL algorithms struggle with. Additionally, the probabilistic nature of this model makes it useful in active learning scenarios to guide experts in the demonstration process.",1
"Microfluidic devices are utilized to control and direct flow behavior in a wide variety of applications, particularly in medical diagnostics. A particularly popular form of microfluidics -- called inertial microfluidic flow sculpting -- involves placing a sequence of pillars to controllably deform an initial flow field into a desired one. Inertial flow sculpting can be formally defined as an inverse problem, where one identifies a sequence of pillars (chosen, with replacement, from a finite set of pillars, each of which produce a specific transformation) whose composite transformation results in a user-defined desired transformation. Endemic to most such problems in engineering, inverse problems are usually quite computationally intractable, with most traditional approaches based on search and optimization strategies. In this paper, we pose this inverse problem as a Reinforcement Learning (RL) problem. We train a DoubleDQN agent to learn from this environment. The results suggest that learning is possible using a DoubleDQN model with the success frequency reaching 90% in 200,000 episodes and the rewards converging. While most of the results are obtained by fixing a particular target flow shape to simplify the learning problem, we later demonstrate how to transfer the learning of an agent based on one target shape to another, i.e. from one design to another and thus be useful for a generic design of a flow shape.",0
"Microfluidic devices are commonly utilized in a variety of fields, especially in medical diagnostics, for controlling and directing flow behavior. One popular type of microfluidics is inertial microfluidic flow sculpting, which involves using a sequence of pillars to manipulate an initial flow field into a desired one. This process is considered an inverse problem, as it requires identifying a sequence of pillars that will produce a specific transformation resulting in the desired flow. Due to the complexity of inverse problems, traditional approaches rely on search and optimization strategies. However, this paper suggests using a Reinforcement Learning (RL) approach to train a DoubleDQN agent to learn from the environment. The results show that the DoubleDQN model is successful, with a 90% success rate in 200,000 episodes and converging rewards. Although most of the results were obtained by fixing a specific target flow shape, the paper also demonstrates how the agent's learning can be transferred from one design to another, making it useful for a generic flow shape design.",1
"Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.",0
"Deep reinforcement learning has proven to be successful in various areas, with the capability to learn directly from high-dimensional sensory streams. However, neural networks that are trained in a fixed environment, such as a single level in a video game, tend to overfit and fail to generalize to new levels. Overfitting in RL models can lead to poor agent performance when even the slightest modifications are made to the environment. To address this issue, this study investigates the potential of procedurally generated levels during training to improve generality. The findings suggest that procedural level generation can enhance generalization to new levels within the same distribution and achieve better performance with less data. The study also evaluates the generality of learned behaviors on a set of human-designed levels, where the ability to generalize is largely dependent on the design of the level generators. To better understand the generators' distributions of levels, the paper applies dimensionality reduction and clustering techniques. The analysis reveals the extent to which the generators can produce levels similar to those designed by humans.",1
"We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state, action) pairs. For any arbitrary structure, we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward functions are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs, the bounds are shown not to scale with the sizes $S$ and $A$ of the state and action spaces, i.e., they are smaller than $c\log T$ where $T$ is the time horizon and the constant $c$ only depends on the Lipschitz structure, the span of the bias function, and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as $SA\log T$. We devise DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs, and show that the simplified version is still able to efficiently exploit the structure.",0
"Our focus is on reinforcement learning problems with finite state and action spaces. In cases where the underlying MDP has a known structure, we aim to minimize exploration rates of suboptimal (state, action) pairs by exploiting this structure. We develop problem-specific regret lower bounds for any arbitrary structure, which apply to all learning algorithms. These lower bounds are explicitly stated for unstructured MDPs, as well as those with Lipschitz continuous transition probabilities and average reward functions relative to state and action. For Lipschitz MDPs, the bounds are not dependent on the sizes of the state and action spaces, but rather on the Lipschitz structure, the bias function span, and the minimal action sub-optimality gap. In contrast, unstructured MDPs typically have a regret lower bound that scales as SA log T. To this end, we introduce DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We also simplify the algorithm for Lipschitz MDPs and demonstrate its efficiency in exploiting the structure.",1
"Initial DR studies mainly adopt model predictive control and thus require accurate models of the control problem (e.g., a customer behavior model), which are to a large extent uncertain for the EV scenario. Hence, model-free approaches, especially based on reinforcement learning (RL) are an attractive alternative. In this paper, we propose a new Markov decision process (MDP) formulation in the RL framework, to jointly coordinate a set of EV charging stations. State-of-the-art algorithms either focus on a single EV, or perform the control of an aggregate of EVs in multiple steps (e.g., aggregate load decisions in one step, then a step translating the aggregate decision to individual connected EVs). On the contrary, we propose an RL approach to jointly control the whole set of EVs at once. We contribute a new MDP formulation, with a scalable state representation that is independent of the number of EV charging stations. Further, we use a batch reinforcement learning algorithm, i.e., an instance of fitted Q-iteration, to learn the optimal charging policy. We analyze its performance using simulation experiments based on a real-world EV charging data. More specifically, we (i) explore the various settings in training the RL policy (e.g., duration of the period with training data), (ii) compare its performance to an oracle all-knowing benchmark (which provides an upper bound for performance, relying on information that is not available or at least imperfect in practice), (iii) analyze performance over time, over the course of a full year to evaluate possible performance fluctuations (e.g, across different seasons), and (iv) demonstrate the generalization capacity of a learned control policy to larger sets of charging stations.",0
"The use of model predictive control in initial DR studies requires precise models of the control problem, which can be uncertain in the EV scenario. Consequently, model-free approaches, particularly those based on reinforcement learning, are gaining popularity. In this study, we propose a new Markov decision process formulation in the RL framework to jointly coordinate a group of EV charging stations. Unlike existing algorithms that either focus on a single EV or control an aggregate of EVs in multiple steps, we suggest an RL approach to control the entire set of EVs simultaneously. Our contribution includes a new MDP formulation with a scalable state representation that is independent of the number of EV charging stations. We use a batch reinforcement learning algorithm to learn the optimal charging policy and analyze its performance through simulation. Specifically, we examine various settings in training the RL policy, compare its performance to an oracle all-knowing benchmark, analyze performance over time, and demonstrate the generalization capacity of learned control policy to larger sets of charging stations.",1
"To overcome the curse of dimensionality and curse of modeling in Dynamic Programming (DP) methods for solving classical Markov Decision Process (MDP) problems, Reinforcement Learning (RL) algorithms are popular. In this paper, we consider an infinite-horizon average reward MDP problem and prove the optimality of the threshold policy under certain conditions. Traditional RL techniques do not exploit the threshold nature of optimal policy while learning. In this paper, we propose a new RL algorithm which utilizes the known threshold structure of the optimal policy while learning by reducing the feasible policy space. We establish that the proposed algorithm converges to the optimal policy. It provides a significant improvement in convergence speed and computational and storage complexity over traditional RL algorithms. The proposed technique can be applied to a wide variety of optimization problems that include energy efficient data transmission and management of queues. We exhibit the improvement in convergence speed of the proposed algorithm over other RL algorithms through simulations.",0
"Reinforcement Learning (RL) algorithms are commonly used to overcome the challenges of the curse of dimensionality and curse of modeling that Dynamic Programming (DP) methods face when solving classical Markov Decision Process (MDP) problems. This paper focuses on an infinite-horizon average reward MDP problem and proves the optimality of the threshold policy under certain conditions. However, traditional RL techniques do not take advantage of the threshold nature of the optimal policy during learning. To address this, a new RL algorithm is proposed that reduces the feasible policy space by utilizing the known threshold structure of the optimal policy. The proposed algorithm is proven to converge to the optimal policy and offers significant improvements in convergence speed, computational complexity, and storage complexity over traditional RL algorithms. This technique can be applied to various optimization problems, such as energy-efficient data transmission and queue management. Simulations demonstrate the superior convergence speed of the proposed algorithm compared to other RL algorithms.",1
"Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. To ask the adequate question, deep learning and reinforcement learning have been recently applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose ""Answerer in Questioner's Mind"" (AQM), a novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks: ""MNIST Counting Dialog"" and ""GuessWhat?!"". In our experiments, AQM outperforms comparative algorithms by a large margin.",0
"The significance of goal-oriented dialog has led to its widespread use in artificial intelligence. This type of dialogue occurs when a questioner poses a question related to an action, and the answerer provides a response that helps the questioner determine the appropriate course of action. In recent years, deep learning and reinforcement learning have been employed to formulate relevant questions. However, these methods face difficulties in developing a proficient recurrent neural questioner due to the complexity of learning a sequence of sentences. Inspired by the theory of mind, we introduce a new information theoretic algorithm for goal-oriented dialog called ""Answerer in Questioner's Mind"" (AQM). With AQM, the questioner makes inquiries and infers using an estimated probabilistic model of the answerer. By explicitly calculating the information gain of candidate intentions and possible answers for each question, the questioner determines the answerer's intention. We applied AQM to two goal-oriented visual dialog tasks: ""MNIST Counting Dialog"" and ""GuessWhat?!"". Our experiments showed that AQM outperforms other algorithms by a significant margin.",1
"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",0
"This paper explores the challenge of acquiring vision-based dynamic manipulation skills through a scalable reinforcement learning technique. The study focuses on the problem of grasping, which has long been a challenge in robotic manipulation. Unlike traditional static learning behaviors that select a grasp point and execute the desired grasp, our approach enables closed-loop vision-based control. This means that the robot continuously updates its grasp strategy based on the latest observations to optimize long-term grasp success. We introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that leverages over 580k real-world grasp attempts. The deep neural network Q-function with over 1.2M parameters can perform closed-loop, real-world grasping with a 96% success rate on unseen objects. Our method not only achieves a high success rate but also exhibits unique behaviors compared to standard grasping systems. Using only RGB vision-based perception from an over-the-shoulder camera, our approach automatically learns regrasping strategies, probes objects to determine the most effective grasps, learns to reposition objects, and performs other non-prehensile pre-grasp manipulations while dynamically responding to disturbances and perturbations.",1
"Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptually-specified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a co-operative game, giving rise to a learned reward function that reflects similarity in controllable aspects of the environment instead of distance in the space of observations. We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains -- Atari, the DeepMind Control Suite and DeepMind Lab.",0
"It is difficult to learn how to control an environment without pre-designed rewards or expert data, and this is a topic of research within reinforcement learning. Our research introduces an algorithm for training agents to achieve specified goals using only observations and actions, without supervision. The agent learns both a policy for achieving goals and a reward function that measures similarity between the current state and the goal state. By optimizing both of these simultaneously, the agent learns to cooperate with itself and generate a reward function that reflects controllable aspects of the environment instead of observations. We demonstrate the effectiveness of our algorithm by showing that it can learn to achieve a diverse set of goals in three domains: Atari, the DeepMind Control Suite, and DeepMind Lab, all without supervision.",1
"With the advent of the Internet of Things (IoT), an increasing number of energy harvesting methods are being used to supplement or supplant battery based sensors. Energy harvesting sensors need to be configured according to the application, hardware, and environmental conditions to maximize their usefulness. As of today, the configuration of sensors is either manual or heuristics based, requiring valuable domain expertise. Reinforcement learning (RL) is a promising approach to automate configuration and efficiently scale IoT deployments, but it is not yet adopted in practice. We propose solutions to bridge this gap: reduce the training phase of RL so that nodes are operational within a short time after deployment and reduce the computational requirements to scale to large deployments. We focus on configuration of the sampling rate of indoor solar panel based energy harvesting sensors. We created a simulator based on 3 months of data collected from 5 sensor nodes subject to different lighting conditions. Our simulation results show that RL can effectively learn energy availability patterns and configure the sampling rate of the sensor nodes to maximize the sensing data while ensuring that energy storage is not depleted. The nodes can be operational within the first day by using our methods. We show that it is possible to reduce the number of RL policies by using a single policy for nodes that share similar lighting conditions.",0
"Due to the Internet of Things (IoT), more energy harvesting methods are being utilized to replace battery-based sensors. These sensors require configuration based on the application, hardware, and environmental conditions to maximize their use, which currently relies on manual or heuristic-based methods. Reinforcement learning (RL) is a promising approach to automate configuration and enhance IoT scalability, but it has not yet been widely adopted. To address this, we propose solutions to shorten the RL training phase and reduce computational requirements to enable scaling. Our focus is on configuring the sampling rate of indoor solar panel-based energy harvesting sensors. We developed a simulator based on 3 months of data from 5 sensor nodes with varying lighting conditions. We demonstrate that RL can learn energy availability patterns and optimize the sampling rate to maximize data while preserving energy storage. Our methods can make nodes operational within a day, and we show that it is possible to reduce the number of RL policies by using a single policy for nodes with similar lighting conditions.",1
"We seek to automate the design of molecules based on specific chemical properties. Our primary contributions are a simpler method for generating SMILES strings guaranteed to be chemically valid, using a combination of a new context-free grammar for SMILES and additional masking logic; and casting the molecular property optimization as a reinforcement learning problem, specifically best-of-batch policy gradient applied to a Transformer model architecture. This approach uses substantially fewer model steps per atom than earlier approaches, thus enabling generation of larger molecules, and beats previous state-of-the art baselines by a significant margin. Applying reinforcement learning to a combination of a custom context-free grammar with additional masking to enforce non-local constraints is applicable to any optimization of a graph structure under a mixture of local and nonlocal constraints.",0
"Our objective is to automate the process of designing molecules based on specific chemical properties. Our primary contributions include a simpler method of generating SMILES strings that are chemically valid by implementing a new context-free grammar for SMILES and using additional masking logic. We also use reinforcement learning to optimize molecular properties, specifically through a best-of-batch policy gradient applied to a Transformer model architecture. This approach requires fewer model steps per atom, allowing for the generation of larger molecules, and outperforms previous state-of-the-art baselines by a significant margin. Additionally, this approach can be applied to optimizing any graph structure under a combination of local and non-local constraints by combining a custom context-free grammar with additional masking to enforce these constraints.",1
"Incorporating various modes of information into the machine learning procedure is becoming a new trend. And data from various source can provide more information than single one no matter they are heterogeneous or homogeneous. Existing deep learning based algorithms usually directly concatenate features from each domain to represent the input data. Seldom of them take the quality of data into consideration which is a key issue in related multimodal problems. In this paper, we propose an efficient quality-aware deep neural network to model the weight of data from each domain using deep reinforcement learning (DRL). Specifically, we take the weighting of each domain as a decision-making problem and teach an agent learn to interact with the environment. The agent can tune the weight of each domain through discrete action selection and obtain a positive reward if the saliency results are improved. The target of the agent is to achieve maximum rewards after finished its sequential action selection. We validate the proposed algorithms on multimodal saliency detection in a coarse-to-fine way. The coarse saliency maps are generated from an encoder-decoder framework which is trained with content loss and adversarial loss. The final results can be obtained via adaptive weighting of maps from each domain. Experiments conducted on two kinds of salient object detection benchmarks validated the effectiveness of our proposed quality-aware deep neural network.",0
"The integration of multiple forms of information in the machine learning process has become a recent trend. Utilizing data from various sources, regardless of whether they are diverse or similar, can provide a more comprehensive understanding. Typically, existing deep learning algorithms concatenate features from each domain to represent the input data, without considering the quality of the data, which is crucial in multimodal problems. In this study, we propose an efficient deep neural network that uses deep reinforcement learning to determine the weight of data from each domain. The approach treats the weighting of each domain as a decision-making problem, and the agent learns to interact with the environment to optimize the weighting of each domain. By selecting discrete actions and obtaining positive rewards for improved saliency results, the agent aims to achieve maximum rewards after sequential action selection. Our proposed algorithm is validated through experiments on multimodal saliency detection, which involves generating coarse saliency maps from an encoder-decoder framework, followed by adaptive weighting of maps from each domain to obtain the final results. Results from experiments on two salient object detection benchmarks illustrate the effectiveness of our proposed quality-aware deep neural network.",1
"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.   Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.",0
"In this article, we provide an overview of the recent accomplishments of deep reinforcement learning (RL). We cover six essential components, six significant mechanisms, and twelve applications. Our discussion begins with the foundation of machine learning, deep learning, and reinforcement learning. The core elements of RL are then examined, including value function, specifically Deep Q-Network (DQN), policy, reward, model, planning, and exploration. We then delve into crucial mechanisms for RL, such as attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Furthermore, we describe diverse RL applications, such as games, AlphaGo, robotics, natural language processing, dialogue systems, machine translation, text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. Lastly, we mention unexplored topics and provide a list of RL resources. Our article concludes with a brief summary and discussions. For a significant update, please refer to Deep Reinforcement Learning, arXiv:1810.06339.",1
"In this work, we present our various contributions to the objective of building a decision support tool for the diagnosis of rare diseases. Our goal is to achieve a state of knowledge where the uncertainty about the patient's disease is below a predetermined threshold. We aim to reach such states while minimizing the average number of medical tests to perform. In doing so, we take into account the need, in many medical applications, to avoid, as much as possible, any misdiagnosis. To solve this optimization task, we investigate several reinforcement learning algorithm and make them operable in our high-dimensional and sparse-reward setting. We also present a way to combine expert knowledge, expressed as conditional probabilities, with real clinical data. This is crucial because the scarcity of data in the field of rare diseases prevents any approach based solely on clinical data. Finally we show that it is possible to integrate the ontological information about symptoms while remaining in our probabilistic reasoning. It enables our decision support tool to process information given at different level of precision by the user.",0
"Our work focuses on creating a decision support tool for diagnosing rare diseases with minimal medical testing while ensuring a predetermined level of certainty in the diagnosis. We prioritize avoiding misdiagnosis and address the challenge of operating in a high-dimensional, sparse-reward environment by exploring various reinforcement learning algorithms. To overcome data scarcity in rare diseases, we combine expert knowledge expressed as conditional probabilities with clinical data. Additionally, we demonstrate the integration of ontological symptom information for flexible processing of user-provided data.",1
"We demonstrate the use of conditional autoregressive generative models (van den Oord et al., 2016a) over a discrete latent space (van den Oord et al., 2017b) for forward planning with MCTS. In order to test this method, we introduce a new environment featuring varying difficulty levels, along with moving goals and obstacles. The combination of high-quality frame generation and classical planning approaches nearly matches true environment performance for our task, demonstrating the usefulness of this method for model-based planning in dynamic environments.",0
"Our study showcases the employment of generative models with a conditional autoregressive nature (van den Oord et al., 2016a) on a latent space that is discrete (van den Oord et al., 2017b) to enable forward planning with MCTS. To assess the efficacy of this approach, we created a fresh environment that has obstacles and goals in motion, and difficulty levels that vary. Our method, which utilizes both classical planning methods and superior quality frame generation, nearly matches authentic environmental performance for our task, thus proving the potential of this technique for model-based planning in dynamic environments.",1
"Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection accuracy of medical terminologies, and improved human evaluation performance.",0
"The task of creating comprehensive reports for medical images is difficult because it requires bridging visual patterns with descriptive language. To address this challenge, we propose a new approach called the Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent). This method combines traditional retrieval-based techniques with modern learning-based methods, allowing for structured, robust, and diverse report generation. The HRGR-Agent uses a hierarchical decision-making process to determine whether to retrieve a sentence template from a pre-existing database or to generate a new sentence. The model is updated through reinforcement learning, guided by sentence-level and word-level rewards. Our experiments demonstrate that the HRGR-Agent achieves state-of-the-art results on two medical report datasets, producing well-balanced sentences and accurately identifying medical terminologies. Additionally, our model improves human evaluation performance.",1
"Recent machine learning models have shown that including attention as a component results in improved model accuracy and interpretability, despite the concept of attention in these approaches only loosely approximating the brain's attention mechanism. Here we extend this work by building a more brain-inspired deep network model of the primate ATTention Network (ATTNet) that learns to shift its attention so as to maximize the reward. Using deep reinforcement learning, ATTNet learned to shift its attention to the visual features of a target category in the context of a search task. ATTNet's dorsal layers also learned to prioritize these shifts of attention so as to maximize success of the ventral pathway classification and receive greater reward. Model behavior was tested against the fixations made by subjects searching images for the same cued category. Both subjects and ATTNet showed evidence for attention being preferentially directed to target goals, behaviorally measured as oculomotor guidance to targets. More fundamentally, ATTNet learned to shift its attention to target like objects and spatially route its visual inputs to accomplish the task. This work makes a step toward a better understanding of the role of attention in the brain and other computational systems.",0
"Recent advancements in machine learning have demonstrated that incorporating attention as a component in models leads to enhanced accuracy and interpretability. However, the attention mechanism used in these approaches only approximates the brain's attention mechanism. This study expands on this knowledge by constructing a deep network model called the primate ATTention Network (ATTNet) that is more brain-inspired. ATTNet uses deep reinforcement learning to learn how to shift attention to maximize rewards in a search task. The dorsal layers of ATTNet prioritize attention shifts to increase the success of the ventral pathway classification and receive more significant rewards. This study compared the behavior of ATTNet to human subjects searching for the same cued category and found that both preferentially directed attention to target goals. ATTNet also learned to shift attention to target-like objects and spatially route its visual inputs to execute the task successfully. These findings contribute to a better understanding of the role of attention in the brain and other computational systems.",1
"Predicting the structure of a protein from its sequence is a cornerstone task of molecular biology. Established methods in the field, such as homology modeling and fragment assembly, appeared to have reached their limit. However, this year saw the emergence of promising new approaches: end-to-end protein structure and dynamics models, as well as reinforcement learning applied to protein folding. For these approaches to be investigated on a larger scale, an efficient implementation of their key computational primitives is required. In this paper we present a library of differentiable mappings from two standard dihedral-angle representations of protein structure (full-atom representation ""$\phi,\psi,\omega,\chi$"" and backbone-only representation ""$\phi,\psi,\omega$"") to atomic Cartesian coordinates. The source code and documentation can be found at https://github.com/lupoglaz/TorchProteinLibrary.",0
"The prediction of a protein's structure from its sequence is a fundamental task in molecular biology, and established methods like homology modeling and fragment assembly have hit a plateau. However, promising new approaches have surfaced this year, including end-to-end protein structure and dynamics models and reinforcement learning techniques applied to protein folding. To explore these approaches further, efficient implementation of their key computational functions is necessary. This paper introduces a library of differentiable mappings that can convert two standard dihedral-angle representations of protein structure (full-atom representation ""$\phi,\psi,\omega,\chi$"" and backbone-only representation ""$\phi,\psi,\omega$"") into atomic Cartesian coordinates. The source code and documentation can be accessed at https://github.com/lupoglaz/TorchProteinLibrary.",1
"Sepsis is a dangerous condition that is a leading cause of patient mortality. Treating sepsis is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. In this work, we explore the use of continuous state-space model-based reinforcement learning (RL) to discover high-quality treatment policies for sepsis patients. Our quantitative evaluation reveals that by blending the treatment strategy discovered with RL with what clinicians follow, we can obtain improved policies, potentially allowing for better medical treatment for sepsis.",0
"Sepsis, which is a major cause of patient death, poses a significant challenge in terms of treatment due to the varying responses of patients to medical interventions and the lack of a widely accepted treatment. This study investigates the potential of using continuous state-space model-based reinforcement learning (RL) to identify effective treatment policies for sepsis patients. Our evaluation demonstrates that by combining the RL-discovered treatment approach with the conventional clinical approach, we can develop superior policies that may enhance the management of sepsis.",1
"In hierarchical reinforcement learning a major challenge is determining appropriate low-level policies. We propose an unsupervised learning scheme, based on asymmetric self-play from Sukhbaatar et al. (2018), that automatically learns a good representation of sub-goals in the environment and a low-level policy that can execute them. A high-level policy can then direct the lower one by generating a sequence of continuous sub-goal vectors. We evaluate our model using Mazebase and Mujoco environments, including the challenging AntGather task. Visualizations of the sub-goal embeddings reveal a logical decomposition of tasks within the environment. Quantitatively, our approach obtains compelling performance gains over non-hierarchical approaches.",0
"Determining appropriate low-level policies is a major challenge in hierarchical reinforcement learning. We propose an unsupervised learning method, which is based on Sukhbaatar et al.'s (2018) asymmetric self-play, to automatically learn a good representation of sub-goals in the environment and a low-level policy that can execute them. By generating a sequence of continuous sub-goal vectors, a high-level policy can direct the lower one. We evaluate our model in Mazebase and Mujoco environments, including the difficult AntGather task. Visualizations of the sub-goal embeddings show a logical decomposition of tasks in the environment. Our approach obtains significant performance gains over non-hierarchical methods according to quantitative measures.",1
"Machine learning applications in medical imaging are frequently limited by the lack of quality labeled data. In this paper, we explore the self training method, a form of semi-supervised learning, to address the labeling burden. By integrating reinforcement learning, we were able to expand the application of self training to complex segmentation networks without any further human annotation. The proposed approach, reinforced self training (ReST), fine tunes a semantic segmentation networks by introducing a policy network that learns to generate pseudolabels. We incorporate an expert demonstration network, based on inverse reinforcement learning, to enhance clinical validity and convergence of the policy network. The model was tested on a pulmonary nodule segmentation task in chest X-rays and achieved the performance of a standard U-Net while using only 50% of the labeled data, by exploiting unlabeled data. When the same number of labeled data was used, a moderate to significant cross validation accuracy improvement was achieved depending on the absolute number of labels used.",0
"The lack of quality labeled data often limits the effectiveness of machine learning applications in medical imaging. In this study, we examine the self training method as a solution to the labeling burden. Through the integration of reinforcement learning, we have extended the use of self training to complex segmentation networks without the need for human annotation. Our proposed method, reinforced self training (ReST), involves fine tuning a semantic segmentation network by introducing a policy network that generates pseudolabels. To enhance the clinical validity and convergence of the policy network, we have incorporated an expert demonstration network based on inverse reinforcement learning. We tested the model on a pulmonary nodule segmentation task in chest X-rays and achieved similar performance to a standard U-Net using only 50% of the labeled data. A moderate to significant improvement in cross-validation accuracy was also achieved when the same number of labeled data was used, depending on the absolute number of labels.",1
"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",0
"Our proposed framework addresses the challenge of ensuring the safe behavior of a reinforcement learning agent in cases where the reward function is difficult to define. To achieve this, we leverage expert policy demonstrations and establish a theoretical framework for the agent to optimize rewards that align with its current knowledge. We present two solutions to tackle the resulting optimization problem: an ellipsoid-based approach and a variation on the ""follow-the-perturbed-leader"" algorithm. Our experiments showcase the effectiveness of our algorithm in both continuous and discrete problems, with the trained agent avoiding hazardous states while emulating the expert's actions in other states.",1
"We consider the problem of high-level strategy selection in the adversarial setting of real-time strategy games from a reinforcement learning perspective, where taking an action corresponds to switching to the respective strategy. Here, a good strategy successfully counters the opponent's current and possible future strategies which can only be estimated using partial observations. We investigate whether we can utilize the full game state information during training time (in the form of an auxiliary prediction task) to increase performance. Experiments carried out within a StarCraft: Brood War bot against strong community bots show substantial win rate improvements over a fixed-strategy baseline and encouraging results when learning with the auxiliary task.",0
"From a reinforcement learning viewpoint, we explore the challenge of selecting high-level strategies in real-time strategy games in an adversarial environment, where each action taken corresponds to a strategy switch. In this context, an effective strategy must be able to counter both the opponent's current and potential future strategies, which can only be approximated based on limited observations. Our objective is to investigate whether the performance of the strategy selection process can be improved by leveraging complete game state information available during training, which can be accomplished through an auxiliary prediction task. Our experiments, which were carried out using a StarCraft: Brood War bot against well-established community bots, demonstrate significant enhancements in win rates over a fixed-strategy baseline and promising outcomes when the auxiliary task is employed for learning.",1
"We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.",0
"We have developed Intelligent Annotation Dialogs that utilize a trained agent to guide a human annotator in producing bounding boxes quickly and accurately. Two actions are considered: box verification and manual box drawing. Our agents are of two types, one that predicts the probability of positive verification and the other based on reinforcement learning. Our experiments show that our agents can learn efficient annotation strategies in different scenarios, adapting to the image difficulty, quality of boxes, and detector strength. The annotation dialogs speed up annotation and outperform any fixed combination of verification and drawing. Finally, in a realistic scenario, our agents evolve strategies that reflect the changing trade-off between verification and drawing as the detector becomes stronger.",1
"This paper proposes a new optimization objective for value-based deep reinforcement learning. We extend conventional Deep Q-Networks (DQNs) by adding a model-learning component yielding a transcoder network. The prediction errors for the model are included in the basic DQN loss as additional regularizers. This augmented objective leads to a richer training signal that provides feedback at every time step. Moreover, because learning an environment model shares a common structure with the RL problem, we hypothesize that the resulting objective improves both sample efficiency and performance. We empirically confirm our hypothesis on a range of 20 games from the Atari benchmark attaining superior results over vanilla DQN without model-based regularization.",0
"In this paper, a new optimization objective is suggested for value-based deep reinforcement learning. The conventional Deep Q-Networks (DQNs) are expanded by the addition of a model-learning component, creating a transcoder network. The prediction errors for the model are used as additional regularizers in the basic DQN loss. This updated objective provides a more comprehensive training signal, giving feedback at every time step. As learning an environment model shares a common structure with the RL problem, the resulting objective is expected to improve both sample efficiency and performance. Our hypothesis is supported by empirical confirmation on a range of 20 games from the Atari benchmark, with superior results obtained over vanilla DQN without model-based regularization.",1
"Representation learning of pedestrian trajectories transforms variable-length timestamp-coordinate tuples of a trajectory into a fixed-length vector representation that summarizes spatiotemporal characteristics. It is a crucial technique to connect feature-based data mining with trajectory data. Trajectory representation is a challenging problem, because both environmental constraints (e.g., wall partitions) and temporal user dynamics should be meticulously considered and accounted for. Furthermore, traditional sequence-to-sequence autoencoders using maximum log-likelihood often require dataset covering all the possible spatiotemporal characteristics to perform well. This is infeasible or impractical in reality. We propose TREP, a practical pedestrian trajectory representation learning algorithm which captures the environmental constraints and the pedestrian dynamics without the need of any training dataset. By formulating a sequence-to-sequence autoencoder with a spatial-aware objective function under the paradigm of actor-critic reinforcement learning, TREP intelligently encodes spatiotemporal characteristics of trajectories with the capability of handling diverse trajectory patterns. Extensive experiments on both synthetic and real datasets validate the high fidelity of TREP to represent trajectories.",0
"The process of representation learning for pedestrian trajectories involves converting trajectory data, which includes varying lengths of timestamp-coordinate pairs, into a condensed, fixed-length vector representation that summarizes the spatiotemporal features. This technique is crucial to connect feature-based data mining with trajectory data. The problem of trajectory representation is challenging due to the need to account for environmental constraints, such as wall partitions, as well as temporal user dynamics. Traditional sequence-to-sequence autoencoders that rely on maximum log-likelihood often require datasets covering all possible spatiotemporal characteristics, which is impractical. To address this issue, we propose TREP, a pedestrian trajectory representation learning algorithm that captures both environmental constraints and pedestrian dynamics without the need for training datasets. TREP uses a sequence-to-sequence autoencoder with a spatial-aware objective function under the actor-critic reinforcement learning paradigm to intelligently encode spatiotemporal characteristics of trajectories, making it capable of handling diverse trajectory patterns. Extensive experiments on synthetic and real datasets demonstrate TREP's ability to accurately represent trajectories.",1
"The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.",0
"Significant progress has been made with the broad range of deep generative models (DGMs); however, integrating complex structured domain knowledge into end-to-end DGMs can often be challenging. Posterior regularization (PR) is a systematic approach that enables the imposition of structured constraints on probabilistic models but has limited applicability in the case of diverse DGMs that may lack a Bayesian formulation or explicit density evaluation. Furthermore, PR necessitates the complete specification of constraints a priori, which may be impractical or suboptimal for complex knowledge with uncertain parts that can be learned. In this paper, we establish a mathematical correspondence between PR and reinforcement learning (RL), and using this connection, we expand PR to learn constraints similar to extrinsic rewards in RL. This algorithm is model-agnostic, making it usable across any DGMs, and it is flexible enough to allow the adaptation of arbitrary constraints with the model jointly. Our experiments, which involve generating human images and templated sentences, demonstrate that models with learned knowledge constraints using our algorithm significantly improve upon base generative models.",1
"Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.",0
"Policy gradient methods are commonly used reinforcement learning algorithms that are model-free. These methods utilize a state-dependent baseline to reduce the variance of the gradient estimator. Recently, some papers have suggested that introducing an action-dependent baseline alongside the state-dependent baseline can further reduce variance and improve sample efficiency without introducing bias into the gradient estimates. In this study, we sought to examine this development by breaking down the variance of the policy gradient estimator. We conducted numerical analyses and found that learned state-action-dependent baselines do not decrease variance compared to a state-dependent baseline in commonly tested benchmark domains. We validated this unexpected result by reviewing the open-source code accompanying the prior papers and discovered that subtle implementation decisions deviated from the methods presented in the papers, explaining the previously observed empirical gains. Moreover, our variance decomposition revealed areas for improvement, which we demonstrated by illustrating a simple change to the typical value function parameterization that can significantly enhance performance.",1
"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",0
"The challenge of utilizing reinforcement learning algorithms in real-world scenarios stems from the absence of appropriate reward functions. The task of constructing these functions is complicated because users possess only an implicit grasp of the objective. This results in the agent alignment issue, which concerns creating agents that operate in accordance with the user's intentions. We propose a research direction to tackle this problem by concentrating on reward modeling, which involves learning a reward function through user interaction and optimizing it with reinforcement learning. We also address the primary hurdles that we may encounter while scaling reward modeling to complex and universal domains, suggest practical methods to mitigate these challenges, and explore ways to establish confidence in the agents generated.",1
"A* is a popular path-finding algorithm, but it can only be applied to those domains where a good heuristic function is known. Inspired by recent methods combining Deep Neural Networks (DNNs) and trees, this study demonstrates how to train a heuristic represented by a DNN and combine it with A*. This new algorithm which we call aleph-star can be used efficiently in domains where the input to the heuristic could be processed by a neural network. We compare aleph-star to N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving simulation with pixel-based input, and demonstrate significantly better performance in this scenario.",0
"The A* algorithm is widely used for path-finding, but its application is limited to domains where a reliable heuristic function exists. This study takes inspiration from recent techniques that merge Deep Neural Networks (DNNs) with trees. It shows how to teach a DNN-based heuristic and integrate it with A*, resulting in a new algorithm called aleph-star. This approach is effective in domains where the neural network can process the heuristic input. We compare aleph-star with N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving simulation that uses pixel-based input. Our results indicate that aleph-star performs significantly better in this scenario.",1
"Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.",0
"An important aspect of achieving strong meta-learning is the ability to infer Bayesian posterior from a few-shot dataset, as model uncertainty is inherent in the problem. This paper introduces a novel Bayesian model-agnostic meta-learning approach that combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, this method can learn complex uncertainty structures beyond a point estimate or a simple Gaussian approximation. Additionally, a new meta-loss is introduced to prevent overfitting during meta-update, resulting in a robust Bayesian meta-update mechanism. As an efficient gradient-based meta-learner, the approach is also model-agnostic and straightforward to implement. Experiment results demonstrate the proposed method's accuracy and robustness across various tasks, such as sinusoidal regression, image classification, active learning, and reinforcement learning.",1
"In the quest for efficient and robust reinforcement learning methods, both model-free and model-based approaches offer advantages. In this paper we propose a new way of explicitly bridging both approaches via a shared low-dimensional learned encoding of the environment, meant to capture summarizing abstractions. We show that the modularity brought by this approach leads to good generalization while being computationally efficient, with planning happening in a smaller latent state space. In addition, this approach recovers a sufficient low-dimensional representation of the environment, which opens up new strategies for interpretable AI, exploration and transfer learning.",0
"Efficient and strong reinforcement learning methods can be achieved through both model-free and model-based approaches. This paper presents a novel method of combining both approaches by utilizing a shared low-dimensional encoding of the environment to capture abstract summaries. The modular nature of this approach results in excellent generalization and computational efficiency, with planning taking place in a smaller latent state space. Furthermore, this method produces a satisfactory low-dimensional representation of the environment, providing opportunities for interpretable AI, exploration, and transfer learning.",1
"Model-free reinforcement learning methods such as the Proximal Policy Optimization algorithm (PPO) have successfully applied in complex decision-making problems such as Atari games. However, these methods suffer from high variances and high sample complexity. On the other hand, model-based reinforcement learning methods that learn the transition dynamics are more sample efficient, but they often suffer from the bias of the transition estimation. How to make use of both model-based and model-free learning is a central problem in reinforcement learning. In this paper, we present a new technique to address the trade-off between exploration and exploitation, which regards the difference between model-free and model-based estimations as a measure of exploration value. We apply this new technique to the PPO algorithm and arrive at a new policy optimization method, named Policy Optimization with Model-based Explorations (POME). POME uses two components to predict the actions' target values: a model-free one estimated by Monte-Carlo sampling and a model-based one which learns a transition model and predicts the value of the next state. POME adds the error of these two target estimations as the additional exploration value for each state-action pair, i.e, encourages the algorithm to explore the states with larger target errors which are hard to estimate. We compare POME with PPO on Atari 2600 games, and it shows that POME outperforms PPO on 33 games out of 49 games.",0
"Successful application of model-free reinforcement learning methods like the Proximal Policy Optimization algorithm (PPO) in complex decision-making problems like Atari games is known. However, these methods are prone to high variances and sample complexity. Conversely, model-based reinforcement learning methods that learn transition dynamics are more sample efficient, but transition estimation bias is a common issue. Hence, combining model-based and model-free learning is a central challenge in reinforcement learning. This paper presents a new exploration-exploitation trade-off technique that measures exploration value by the difference between model-free and model-based estimations. The technique is applied to PPO to create a new policy optimization method called Policy Optimization with Model-based Explorations (POME). POME predicts action target values using two components: a model-free component estimated by Monte-Carlo sampling and a model-based component that learns a transition model and predicts the value of the next state. POME adds the error of these two target estimations as additional exploration value for each state-action pair, encouraging exploration of states with larger target errors that are difficult to estimate. POME outperforms PPO on 33 out of 49 Atari 2600 games, as per the comparison.",1
"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries. In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",0
"Industry is increasingly using Deep Learning for computer vision on embedded devices, but Convolutional Neural Networks' accuracy has plateaued, and inference latency and throughput are major concerns, especially for low-cost and low-power devices. This bottleneck may hinder Deep Learning's adoption, and CNN deployment across different platforms faces issues due to vendor-specific technology and acceleration libraries. In this study, we introduce QS-DNN, an automatic search using Reinforcement Learning that, with an inference engine optimizer, explores the design space and finds the best combinations of libraries and primitives to speed up CNN inference on heterogeneous embedded devices. We demonstrate that an optimized combination achieves a 45x speedup in inference latency on CPU and a 2x speedup on average on GPGPU compared to the best vendor library. Moreover, our method yields better results and shorter ""to-solution"" time than Random Search, with up to 15x improvement in a short-time search.",1
"Multi-agent learning provides a potential framework for learning and simulating traffic behaviors. This paper proposes a novel architecture to learn multiple driving behaviors in a traffic scenario. The proposed architecture can learn multiple behaviors independently as well as simultaneously. We take advantage of the homogeneity of agents and learn in a parameter sharing paradigm. To further speed up the training process asynchronous updates are employed into the architecture. While learning different behaviors simultaneously, the given framework was also able to learn cooperation between the agents, without any explicit communication. We applied this framework to learn two important behaviors in driving: 1) Lane-Keeping and 2) Over-Taking. Results indicate faster convergence and learning of a more generic behavior, that is scalable to any number of agents. When compared the results with existing approaches, our results indicate equal and even better performance in some cases.",0
"The possibility of learning and simulating traffic behaviors using multi-agent learning is explored in this paper. A new architecture is proposed to learn various driving behaviors in a traffic scenario, which can learn multiple behaviors both independently and simultaneously. The homogeneity of agents is taken advantage of, and a parameter sharing paradigm is used to learn. To speed up the training process, asynchronous updates are employed. Cooperation between agents is learned implicitly without explicit communication. Two significant driving behaviors, Lane-Keeping and Over-Taking, are learned using this framework. The results show that the proposed approach has faster convergence and can learn more generic behavior that is scalable to any number of agents. Compared to existing approaches, our results demonstrate similar or better performance.",1
"The class of Gaussian Process (GP) methods for Temporal Difference learning has shown promise for data-efficient model-free Reinforcement Learning. In this paper, we consider a recent variant of the GP-SARSA algorithm, called Sparse Pseudo-input Gaussian Process SARSA (SPGP-SARSA), and derive recursive formulas for its predictive moments. This extension promotes greater memory efficiency, since previous computations can be reused and, interestingly, it provides a technique for updating value estimates on a multiple timescales",0
"Data-efficient model-free Reinforcement Learning has shown potential through the Gaussian Process (GP) methods class, particularly for Temporal Difference learning. The paper examines the Sparse Pseudo-input Gaussian Process SARSA (SPGP-SARSA), a recent variant of the GP-SARSA algorithm, and develops recursive formulas for its predictive moments. This approach enhances memory efficiency by reusing past computations and, notably, offers a method for updating value estimates at various timescales.",1
"We investigate sparse representations for control in reinforcement learning. While these representations are widely used in computer vision, their prevalence in reinforcement learning is limited to sparse coding where extracting representations for new data can be computationally intensive. Here, we begin by demonstrating that learning a control policy incrementally with a representation from a standard neural network fails in classic control domains, whereas learning with a representation obtained from a neural network that has sparsity properties enforced is effective. We provide evidence that the reason for this is that the sparse representation provides locality, and so avoids catastrophic interference, and particularly keeps consistent, stable values for bootstrapping. We then discuss how to learn such sparse representations. We explore the idea of Distributional Regularizers, where the activation of hidden nodes is encouraged to match a particular distribution that results in sparse activation across time. We identify a simple but effective way to obtain sparse representations, not afforded by previously proposed strategies, making it more practical for further investigation into sparse representations for reinforcement learning.",0
"Our research delves into the use of sparse representations in reinforcement learning, a technique commonly employed in computer vision but not as much in reinforcement learning due to its computational complexity in extracting representations for new data. Our study shows that learning a control policy incrementally with a representation from a standard neural network is ineffective in classic control domains, while using a representation obtained from a neural network with sparsity properties enforced yields better results. This is because the sparse representation provides locality, preventing catastrophic interference and ensuring consistent, stable values for bootstrapping. We also explore methods for learning such sparse representations, including Distributional Regularizers that encourage activation of hidden nodes to match a specific distribution resulting in sparse activation across time. Our approach offers a novel and practical strategy for further investigation into sparse representations for reinforcement learning.",1
"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",0
"In order to tackle complicated real-life issues through reinforcement learning, it is not feasible to depend on manually defined reward functions. Instead, the objective can be directly communicated to the agent by humans. This study combines two methods of learning from human feedback, namely expert demonstrations and trajectory preferences. A deep neural network is trained to create a model of the reward function, which is then used to train a deep reinforcement learning agent based on DQN for 9 Atari games. Our technique outperforms the imitation learning baseline in 7 games and accomplishes strictly superior performance in 2 games, without relying on game rewards. Moreover, we examine the suitability of the reward model, present some reward hacking issues, and explore the effects of noise in human labels.",1
"Robustness is important for sequential decision making in a stochastic dynamic environment with uncertain probabilistic parameters. We address the problem of using robust MDPs (RMDPs) to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution is determined by its ambiguity set. Existing methods construct ambiguity sets that lead to impractically conservative solutions. In this paper, we propose RSVF, which achieves less conservative solutions with the same worst-case guarantees by 1) leveraging a Bayesian prior, 2) optimizing the size and location of the ambiguity set, and, most importantly, 3) relaxing the requirement that the set is a confidence interval. Our theoretical analysis shows the safety of RSVF, and the empirical results demonstrate its practical promise.",0
"In a dynamic environment with uncertain probabilities, making sequential decisions requires robustness. To ensure that reinforcement learning policies have provable worst-case guarantees, we explore the use of robust MDPs (RMDPs). However, the quality and robustness of RMDP solutions depend on their ambiguity sets, which can be constructed by existing methods in a manner that is overly conservative. To address this issue, we present RSVF, which achieves less conservative solutions while maintaining the same worst-case guarantees. By leveraging a Bayesian prior, optimizing ambiguity set size and location, and relaxing the requirement that the set be a confidence interval, RSVF offers practical promise. Our theoretical analysis confirms the safety of RSVF, and empirical results support its effectiveness.",1
"Dealing with uncertainty is essential for efficient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from fixed datasets, but many of the most popular approaches are poorly-suited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable `prior' network to each ensemble member. We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.",0
"Efficient reinforcement learning requires the ability to deal with uncertainty. While there is a growing body of literature on uncertainty estimation for deep learning from fixed datasets, many of the most popular approaches are not suitable for sequential decision problems. Bootstrap sampling, for example, offers no means of accounting for uncertainty beyond the observed data. This can be a significant limitation, which we address by proposing the addition of a randomized untrainable 'prior' network to each ensemble member. Our approach is effective with linear representations, and we provide examples of its efficacy with nonlinear representations. Moreover, our approach scales to large-scale problems much better than previous attempts.",1
"Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.",0
"Reinforcement learning algorithms require large amounts of real experience, which can be costly to acquire. Learning policies on data synthesized by models can theoretically solve this problem, but simulating plausible experience can be difficult in complex environments and can lead to biases for model-based policy evaluation and search. Instead, this paper proposes the Counterfactually-Guided Policy Search (CF-GPS) algorithm, which assumes logged, real experience and models alternative outcomes of this experience under counterfactual actions. CF-GPS leverages structural causal models to evaluate arbitrary policies on individual off-policy episodes and improve on vanilla model-based RL algorithms by debiasing model predictions. Unlike off-policy algorithms based on Importance Sampling, which re-weight data, CF-GPS explicitly considers alternative outcomes and makes better use of experience data. Empirical results show that CF-GPS improves policy evaluation and search results on a non-trivial grid-world task, and it generalizes the previously proposed Guided Policy Search. The paper also suggests that reparameterization-based algorithms like Stochastic Value Gradient can be interpreted as counterfactual methods.",1
"Policy gradient methods are very attractive in reinforcement learning due to their model-free nature and convergence guarantees. These methods, however, suffer from high variance in gradient estimation, resulting in poor sample efficiency. To mitigate this issue, a number of variance-reduction approaches have been proposed. Unfortunately, in the challenging problems with delayed rewards, these approaches either bring a relatively modest improvement or do reduce variance at expense of introducing a bias and undermining convergence. The unbiased methods of gradient estimation, in general, only partially reduce variance, without eliminating it completely even in the limit of exact knowledge of the value functions and problem dynamics, as one might have wished. In this work we propose an unbiased method that does completely eliminate variance under some, commonly encountered, conditions. Of practical interest is the limit of deterministic dynamics and small policy stochasticity. In the case of a quadratic value function, as in linear quadratic Gaussian models, the policy randomness need not be small. We use such a model to analyze performance of the proposed variance-elimination approach and compare it with standard variance-reduction methods. The core idea behind the approach is to use control variates at all future times down the trajectory. We present both a model-based and model-free formulations.",0
"Reinforcement learning favors policy gradient methods for their model-free nature and convergence guarantees. However, these methods suffer from high variance in gradient estimation, leading to poor sample efficiency. Although variance-reduction approaches have been proposed, they often introduce bias and undermine convergence, providing only a modest improvement in challenging problems with delayed rewards. Even unbiased methods of gradient estimation only partially reduce variance, failing to eliminate it entirely even with exact knowledge of value functions and problem dynamics. Our work proposes an unbiased method that completely eliminates variance under commonly encountered conditions, particularly in the limit of small policy stochasticity and deterministic dynamics. We use a quadratic value function model to analyze the proposed variance-elimination approach and compare it with standard variance-reduction methods. The approach uses control variates throughout the trajectory and is presented in both model-based and model-free formulations.",1
"Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",0
"A wide range of environments have shown that deep reinforcement learning techniques perform better. However, research on understanding the networks' learning has not kept up with the fast-paced advancements in training algorithms. This paper suggests an interpretable neural network architecture for Q-learning that explains the model's behavior using key-value memories, attention, and reconstructible embeddings. The model has a directed exploration strategy and can reach training rewards equivalent to state-of-the-art deep Q-learning models. Nevertheless, the features extracted by the neural network are shallow, and it is simple for the agent to overfit to training trajectories, as evidenced by out-of-sample testing.",1
"Deep neural networks have shown superior performance in many regimes to remember familiar patterns with large amounts of data. However, the standard supervised deep learning paradigm is still limited when facing the need to learn new concepts efficiently from scarce data. In this paper, we present a memory-augmented neural network which is motivated by the process of human concept learning. The training procedure, imitating the concept formation course of human, learns how to distinguish samples from different classes and aggregate samples of the same kind. In order to better utilize the advantages originated from the human behavior, we propose a sequential process, during which the network should decide how to remember each sample at every step. In this sequential process, a stable and interactive memory serves as an important module. We validate our model in some typical one-shot learning tasks and also an exploratory outlier detection problem. In all the experiments, our model gets highly competitive to reach or outperform those strong baselines.",0
"Although deep neural networks have been proven to excel at recognizing common patterns using vast amounts of data, the conventional supervised deep learning approach still has limitations when it comes to efficiently learning new concepts with limited data. This paper introduces a neural network with memory augmentation, inspired by the human concept learning process. By simulating human concept formation, the training process learns to differentiate between samples from different classes and combine samples of the same type. To optimize the advantages of human behavior, a sequential process is proposed where the network decides how to remember each sample at each step, utilizing a stable and interactive memory as a crucial module. The model is validated through various one-shot learning tasks and an exploratory outlier detection problem, where it performs competitively or outperforms strong baselines.",1
"While current benchmark reinforcement learning (RL) tasks have been useful to drive progress in the field, they are in many ways poor substitutes for learning with real-world data. By testing increasingly complex RL algorithms on low-complexity simulation environments, we often end up with brittle RL policies that generalize poorly beyond the very specific domain. To combat this, we propose three new families of benchmark RL domains that contain some of the complexity of the natural world, while still supporting fast and extensive data acquisition. The proposed domains also permit a characterization of generalization through fair train/test separation, and easy comparison and replication of results. Through this work, we challenge the RL research community to develop more robust algorithms that meet high standards of evaluation.",0
"Although current benchmark reinforcement learning (RL) tasks have driven progress in the field, they cannot replace learning from real-world data. The low-complexity simulation environments used to test complex RL algorithms often lead to policies that do not generalize well beyond specific domains. To address this issue, we introduce three new benchmark RL domains that mimic the complexity of the natural world but still enable quick and extensive data acquisition. These domains allow for fair train/test separation, easy comparison, and replication of results, and facilitate the evaluation of generalization. Our aim is to encourage the RL research community to develop more robust algorithms that meet high evaluation standards.",1
"Reinforcement learning (RL) has recently been introduced to interactive recommender systems (IRS) because of its nature of learning from dynamic interactions and planning for long-run performance. As IRS is always with thousands of items to recommend (i.e., thousands of actions), most existing RL-based methods, however, fail to handle such a large discrete action space problem and thus become inefficient. The existing work that tries to deal with the large discrete action space problem by utilizing the deep deterministic policy gradient framework suffers from the inconsistency between the continuous action representation (the output of the actor network) and the real discrete action. To avoid such inconsistency and achieve high efficiency and recommendation effectiveness, in this paper, we propose a Tree-structured Policy Gradient Recommendation (TPGR) framework, where a balanced hierarchical clustering tree is built over the items and picking an item is formulated as seeking a path from the root to a certain leaf of the tree. Extensive experiments on carefully-designed environments based on two real-world datasets demonstrate that our model provides superior recommendation performance and significant efficiency improvement over state-of-the-art methods.",0
"Interactive recommender systems (IRS) have started incorporating reinforcement learning (RL) due to its ability to learn from dynamic interactions and plan for long-term performance. However, existing RL-based methods struggle to handle the large discrete action space problem that arises when recommending from thousands of items. This inefficiency is compounded by the inconsistency between the continuous action representation and the real discrete action in the deep deterministic policy gradient framework. To address these issues, we introduce the Tree-structured Policy Gradient Recommendation (TPGR) framework, which utilizes a hierarchical clustering tree to select items. Our experiments on two real-world datasets show that TPGR outperforms state-of-the-art methods in both recommendation effectiveness and efficiency.",1
"This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.",0
"This paper introduces a fresh approach to analyzing wireheading in intelligent agents. Drawing inspiration from natural examples of wireheading and their widespread occurrences, we suggest treating this phenomenon in Reinforcement Learning (RL) agents as a type of psychological disorder. To test this idea, we investigate the possibility of emergent addictive policies in Q-learning agents in the game of Snake. We adjust the game by adding a ""drug"" seed, and use an RL-based model of natural addiction to derive conditions for the emergence of addictive behavior in these agents. We then conduct three simulation-based experiments to validate our theoretical analysis, which shows the potential for further research on the modeling of complex AI safety issues related to psychopathology.",1
"Deep learning models are vulnerable to external attacks. In this paper, we propose a Reinforcement Learning (RL) based approach to generate adversarial examples for the pre-trained (target) models. We assume a semi black-box setting where the only access an adversary has to the target model is the class probabilities obtained for the input queries. We train a Deep Q Network (DQN) agent which, with experience, learns to attack only a small portion of image pixels to generate non-targeted adversarial images. Initially, an agent explores an environment by sequentially modifying random sets of image pixels and observes its effect on the class probabilities. At the end of an episode, it receives a positive (negative) reward if it succeeds (fails) to alter the label of the image. Experimental results with MNIST, CIFAR-10 and Imagenet datasets demonstrate that our RL framework is able to learn an effective attack policy.",0
"The vulnerability of deep learning models to outside attacks is a concern. This paper proposes an approach based on Reinforcement Learning (RL) to create adversarial examples for pre-trained (target) models. The setting assumed is semi-black-box, with adversaries only having access to class probabilities for input queries. A Deep Q Network (DQN) agent is trained to attack a small portion of image pixels, generating non-targeted adversarial images through experience. The agent explores the environment by sequentially modifying random sets of image pixels and receives positive (negative) rewards at the end of an episode if it succeeds (fails) to alter the image label. Experimental results using MNIST, CIFAR-10, and Imagenet datasets show that our RL framework is effective in learning an attack policy.",1
"We present a novel algorithm to train a deep Q-learning agent using natural-gradient techniques. We compare the original deep Q-network (DQN) algorithm to its natural-gradient counterpart, which we refer to as NGDQN, on a collection of classic control domains. Without employing target networks, NGDQN significantly outperforms DQN without target networks, and performs no worse than DQN with target networks, suggesting that NGDQN stabilizes training and can help reduce the need for additional hyperparameter tuning. We also find that NGDQN is less sensitive to hyperparameter optimization relative to DQN. Together these results suggest that natural-gradient techniques can improve value-function optimization in deep reinforcement learning.",0
"A new method for training a deep Q-learning agent using natural-gradient techniques is presented in this study. The original deep Q-network (DQN) algorithm is compared to its natural-gradient counterpart, called NGDQN, on a variety of classic control domains. NGDQN outperforms DQN without requiring target networks, and it performs just as well as DQN with target networks. This indicates that NGDQN can stabilize training and minimize the need for additional hyperparameter tuning. Moreover, the study found that NGDQN is less sensitive to hyperparameter optimization compared to DQN. Overall, these findings demonstrate that natural-gradient techniques can enhance value-function optimization in deep reinforcement learning.",1
"This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep RL for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. We apply the proposed method to three image processing tasks: image denoising, image restoration, and local color enhancement. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning.",0
"This article deals with a novel challenge: using reinforcement learning with pixel-wise rewards (pixelRL) for image processing. Although deep RL has been highly successful since the introduction of the deep Q-network, its applications in image processing are still limited. In order to overcome this, we propose extending deep RL to pixelRL for various image processing purposes. In pixelRL, each pixel has its own agent that alters its value by taking an action. We have also devised an effective learning approach for pixelRL that greatly enhances performance by considering the future states of both the pixel and its neighboring pixels. This method can be used for image processing tasks that require pixel-wise manipulation, where deep RL has never been utilized. We have applied this method to three image processing tasks: image denoising, image restoration, and local color enhancement. Our experiments have demonstrated that our proposed method achieves comparable or superior performance when compared to state-of-the-art supervised learning methods.",1
"Modelling and exploiting teammates' policies in cooperative multi-agent systems have long been an interest and also a big challenge for the reinforcement learning (RL) community. The interest lies in the fact that if the agent knows the teammates' policies, it can adjust its own policy accordingly to arrive at proper cooperations; while the challenge is that the agents' policies are changing continuously due to they are learning concurrently, which imposes difficulty to model the dynamic policies of teammates accurately. In this paper, we present \emph{ATTention Multi-Agent Deep Deterministic Policy Gradient} (ATT-MADDPG) to address this challenge. ATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two special designs. First, in order to model the teammates' policies, the agent should get access to the observations and actions of teammates. ATT-MADDPG adopts a centralized critic to collect such information. Second, to model the teammates' policies using the collected information in an effective way, ATT-MADDPG enhances the centralized critic with an attention mechanism. This attention mechanism introduces a special structure to explicitly model the dynamic joint policy of teammates, making sure that the collected information can be processed efficiently. We evaluate ATT-MADDPG on both benchmark tasks and the real-world packet routing tasks. Experimental results show that it not only outperforms the state-of-the-art RL-based methods and rule-based methods by a large margin, but also achieves better performance in terms of scalability and robustness.",0
"For the reinforcement learning (RL) community, modelling and utilizing the policies of teammates in cooperative multi-agent systems has been a long-standing area of interest and a significant challenge. This is because knowing teammates' policies enables an agent to adjust its policy to achieve optimal cooperation, but the continuous learning of agents results in dynamic policy changes, making it difficult to accurately model teammates' policies. To overcome this challenge, we propose ATTention Multi-Agent Deep Deterministic Policy Gradient (ATT-MADDPG), an extension of DDPG, a single-agent actor-critic RL method. ATT-MADDPG features two special designs: a centralized critic that provides the agent access to teammates' observations and actions, and an attention mechanism that enhances the centralized critic by explicitly modelling the dynamic joint policy of teammates to ensure efficient processing of collected information. We evaluate ATT-MADDPG on benchmark tasks and real-world packet routing tasks, and our experimental results demonstrate that it outperforms the state-of-the-art RL-based and rule-based methods in terms of scalability, robustness, and overall performance.",1
"Hierarchical reinforcement learning (HRL) has recently shown promising advances on speeding up learning, improving the exploration, and discovering intertask transferable skills. Most recent works focus on HRL with two levels, i.e., a master policy manipulates subpolicies, which in turn manipulate primitive actions. However, HRL with multiple levels is usually needed in many real-world scenarios, whose ultimate goals are highly abstract, while their actions are very primitive. Therefore, in this paper, we propose a diversity-driven extensible HRL (DEHRL), where an extensible and scalable framework is built and learned levelwise to realize HRL with multiple levels. DEHRL follows a popular assumption: diverse subpolicies are useful, i.e., subpolicies are believed to be more useful if they are more diverse. However, existing implementations of this diversity assumption usually have their own drawbacks, which makes them inapplicable to HRL with multiple levels. Consequently, we further propose a novel diversity-driven solution to achieve this assumption in DEHRL. Experimental studies evaluate DEHRL with five baselines from four perspectives in two domains; the results show that DEHRL outperforms the state-of-the-art baselines in all four aspects.",0
"Recently, there have been promising advancements in Hierarchical Reinforcement Learning (HRL) that have led to faster learning, improved exploration, and the discovery of transferable skills between tasks. Most studies have focused on HRL with two levels, where a master policy controls subpolicies that manipulate primitive actions. However, in many real-world scenarios, multiple levels of HRL are necessary, as the ultimate goals are abstract while the actions are primitive. To address this, we propose Diversity-Driven Extensible HRL (DEHRL), which uses an extensible and scalable framework to learn HRL at multiple levels. DEHRL assumes that diverse subpolicies are useful, but previous implementations have limitations that make them unsuitable for multiple levels of HRL. Therefore, we propose a novel diversity-driven solution to address this issue. Experimental studies show that DEHRL outperforms state-of-the-art baselines in all four aspects evaluated across two domains.",1
"A crucial and time-sensitive task when any disaster occurs is to rescue victims and distribute resources to the right groups and locations. This task is challenging in populated urban areas, due to the huge burst of help requests generated in a very short period. To improve the efficiency of the emergency response in the immediate aftermath of a disaster, we propose a heuristic multi-agent reinforcement learning scheduling algorithm, named as ResQ, which can effectively schedule the rapid deployment of volunteers to rescue victims in dynamic settings. The core concept is to quickly identify victims and volunteers from social network data and then schedule rescue parties with an adaptive learning algorithm. This framework performs two key functions: 1) identify trapped victims and rescue volunteers, and 2) optimize the volunteers' rescue strategy in a complex time-sensitive environment. The proposed ResQ algorithm can speed up the training processes through a heuristic function which reduces the state-action space by identifying the set of particular actions over others. Experimental results showed that the proposed heuristic multi-agent reinforcement learning based scheduling outperforms several state-of-art methods, in terms of both reward rate and response times.",0
"When a disaster occurs, rescuing victims and distributing resources quickly and efficiently is crucial. This can be particularly challenging in densely populated urban areas where there is a high volume of help requests in a short period of time. To address this issue, we propose a heuristic multi-agent reinforcement learning scheduling algorithm called ResQ. ResQ uses social network data to identify victims and volunteers and then schedules rescue parties with an adaptive learning algorithm. This algorithm has two main functions: identifying trapped victims and rescue volunteers, and optimizing rescue strategies for volunteers in a complex, time-sensitive environment. ResQ speeds up the training process through a heuristic function that reduces the state-action space by identifying specific actions over others. Our experimental results demonstrate that ResQ outperforms several state-of-the-art methods in terms of reward rate and response times.",1
"The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.",0
"Real-world applications of reinforcement learning often face a practical challenge in designing a reward function. While approaches like inverse reinforcement learning aim to solve this problem, obtaining expert demonstrations can be difficult and expensive. To address this issue, we introduce variational inverse control with events (VICE) which extends inverse reinforcement learning to scenarios where full demonstrations are not required, such as when only goal state samples are available. Our approach is grounded in a different perspective on control and reinforcement learning, where the agent's objective is to maximize the likelihood of one or more future events occurring, rather than cumulative rewards. We demonstrate the efficacy of our method on continuous control tasks, particularly those with high-dimensional observations like images where defining rewards can be challenging or even impossible.",1
"Previous attempts for data augmentation are designed manually, and the augmentation policies are dataset-specific. Recently, an automatic data augmentation approach, named AutoAugment, is proposed using reinforcement learning. AutoAugment searches for the augmentation polices in the discrete search space, which may lead to a sub-optimal solution. In this paper, we employ the Augmented Random Search method (ARS) to improve the performance of AutoAugment. Our key contribution is to change the discrete search space to continuous space, which will improve the searching performance and maintain the diversities between sub-policies. With the proposed method, state-of-the-art accuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without additional data). Our code is available at https://github.com/gmy2013/ARS-Aug.",0
"Manual design of data augmentation techniques has been the norm in the past, with such policies being specific to the dataset. However, a new approach called AutoAugment, which uses reinforcement learning, has been introduced to automate this process. The search for augmentation policies in the discrete search space used by AutoAugment may result in sub-optimal solutions. In this paper, we present the Augmented Random Search method (ARS) to enhance the performance of AutoAugment. Our approach involves changing the search space from discrete to continuous, which improves search performance and maintains diversity between sub-policies. Our method outperforms existing techniques on CIFAR-10, CIFAR-100, and ImageNet (without additional data). Our code is available at https://github.com/gmy2013/ARS-Aug.",1
"Evolution Strategies (ES) emerged as a scalable alternative to popular Reinforcement Learning (RL) techniques, providing an almost perfect speedup when distributed across hundreds of CPU cores thanks to a reduced communication overhead. Despite providing large improvements in wall-clock time, ES is data inefficient when compared to competing RL methods. One of the main causes of such inefficiency is the collection of large batches of experience, which are discarded after each policy update. In this work, we study how to perform more than one update per batch of experience by means of Importance Sampling while preserving the scalability of the original method. The proposed method, Importance Weighted Evolution Strategies (IW-ES), shows promising results and is a first step towards designing efficient ES algorithms.",0
"Evolution Strategies (ES) have emerged as a scalable alternative to Reinforcement Learning (RL) techniques, offering almost perfect speedup when distributed across numerous CPU cores due to minimal communication overhead. Despite significant improvements in wall-clock time, ES falls short in terms of data efficiency when compared to competing RL methods. This inefficiency stems mainly from the collection of large batches of experience, which are discarded after each policy update. In this study, we explore the possibility of performing more than one update per batch of experience through Importance Sampling, without compromising the scalability of the original technique. Our proposed method, Importance Weighted Evolution Strategies (IW-ES), displays promising results and represents a crucial first step towards developing efficient ES algorithms.",1
"I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",0
"In this paragraph, I present an ideal perspective on adversarial machine learning that involves viewing the machine learner as a dynamic system, the input as adversarial actions, and the control costs as the adversary's objectives to cause damage and avoid detection. This approach encompasses various forms of adversarial machine learning, such as test-item assaults, training-data contamination, and adversarial reward shaping. By adopting this viewpoint, researchers in adversarial machine learning are encouraged to take advantage of developments in control theory and reinforcement learning.",1
"We tackle the blackbox issue of deep neural networks in the settings of reinforcement learning (RL) where neural agents learn towards maximizing reward gains in an uncontrollable way. Such learning approach is risky when the interacting environment includes an expanse of state space because it is then almost impossible to foresee all unwanted outcomes and penalize them with negative rewards beforehand. Unlike reverse analysis of learned neural features from previous works, our proposed method \nj{tackles the blackbox issue by encouraging} an RL policy network to learn interpretable latent features through an implementation of a disentangled representation learning method. Toward this end, our method allows an RL agent to understand self-efficacy by distinguishing its influences from uncontrollable environmental factors, which closely resembles the way humans understand their scenes. Our experimental results show that the learned latent factors not only are interpretable, but also enable modeling the distribution of entire visited state space with a specific action condition. We have experimented that this characteristic of the proposed structure can lead to ex post facto governance for desired behaviors of RL agents.",0
"In the context of reinforcement learning, we address the problem of blackbox deep neural networks, where agents aim to maximize rewards in an unpredictable environment. This approach is risky in complex environments, where it is difficult to anticipate and penalize unwanted outcomes. Rather than reverse engineering previous work, we propose a disentangled representation learning method that encourages interpretable latent features for an RL policy network. This allows the agent to distinguish between controllable and uncontrollable factors, similar to human perception. Our experiments show that the learned factors are interpretable and can model the entire state space with specific actions, leading to better control over agent behavior.",1
"In this paper, we develop a reinforcement learning (RL) based system to learn an effective policy for carpooling that maximizes transportation efficiency so that fewer cars are required to fulfill the given amount of trip demand. For this purpose, first, we develop a deep neural network model, called ST-NN (Spatio-Temporal Neural Network), to predict taxi trip time from the raw GPS trip data. Secondly, we develop a carpooling simulation environment for RL training, with the output of ST-NN and using the NYC taxi trip dataset. In order to maximize transportation efficiency and minimize traffic congestion, we choose the effective distance covered by the driver on a carpool trip as the reward. Therefore, the more effective distance a driver achieves over a trip (i.e. to satisfy more trip demand) the higher the efficiency and the less will be the traffic congestion. We compared the performance of RL learned policy to a fixed policy (which always accepts carpool) as a baseline and obtained promising results that are interpretable and demonstrate the advantage of our RL approach. We also compare the performance of ST-NN to that of state-of-the-art travel time estimation methods and observe that ST-NN significantly improves the prediction performance and is more robust to outliers.",0
"The aim of this study was to develop a carpooling policy that would maximize transportation efficiency and reduce the number of cars needed to meet trip demand. To achieve this, a reinforcement learning (RL) based system was created, with a focus on minimizing traffic congestion and maximizing the distance covered by drivers during carpool trips. The study used the NYC taxi trip dataset and developed a deep neural network model, ST-NN, to predict taxi trip time from raw GPS data. The RL training was done in a carpooling simulation environment using the output of ST-NN. Results showed that the RL learned policy outperformed a fixed policy in terms of efficiency and traffic congestion. Additionally, ST-NN was found to significantly improve travel time prediction performance compared to state-of-the-art methods and was more robust to outliers.",1
"Direct policy search is one of the most important algorithm of reinforcement learning. However, learning from scratch needs a large amount of experience data and can be easily prone to poor local optima. In addition to that, a partially trained policy tends to perform dangerous action to agent and environment. In order to overcome these challenges, this paper proposed a policy initialization algorithm called Policy Learning based on Completely Behavior Cloning (PLCBC). PLCBC first transforms the Model Predictive Control (MPC) controller into a piecewise affine (PWA) function using multi-parametric programming, and uses a neural network to express this function. By this way, PLCBC can completely clone the MPC controller without any performance loss, and is totally training-free. The experiments show that this initialization strategy can help agent learn at the high reward state region, and converge faster and better.",0
"Reinforcement learning relies heavily on the direct policy search algorithm, which is crucial. However, starting from scratch can pose challenges as it requires a vast amount of experience data and may result in suboptimal results. Additionally, an incompletely trained policy may cause harm to the agent and the environment. To address these issues, the authors propose a policy initialization algorithm called Policy Learning based on Completely Behavior Cloning (PLCBC). This algorithm transforms the Model Predictive Control (MPC) controller into a piecewise affine (PWA) function using multi-parametric programming and expresses it using a neural network. The PLCBC can clone the MPC controller without any loss of performance and requires no training. The experiments demonstrate that this initialization approach can help the agent learn in high reward state regions and achieve faster and better convergence.",1
"Correlation filter has been proven to be an effective tool for a number of approaches in visual tracking, particularly for seeking a good balance between tracking accuracy and speed. However, correlation filter based models are susceptible to wrong updates stemming from inaccurate tracking results. To date, little effort has been devoted towards handling the correlation filter update problem. In this paper, we propose a novel approach to address the correlation filter update problem. In our approach, we update and maintain multiple correlation filter models in parallel, and we use deep reinforcement learning for the selection of an optimal correlation filter model among them. To facilitate the decision process in an efficient manner, we propose a decision-net to deal target appearance modeling, which is trained through hundreds of challenging videos using proximal policy optimization and a lightweight learning network. An exhaustive evaluation of the proposed approach on the OTB100 and OTB2013 benchmarks show that the approach is effective enough to achieve the average success rate of 62.3% and the average precision score of 81.2%, both exceeding the performance of traditional correlation filter based trackers.",0
"The correlation filter has been a useful tool in visual tracking, balancing accuracy and speed. However, it is prone to errors caused by inaccurate tracking results. Little has been done to address this update problem. This paper proposes a new method by maintaining multiple correlation filter models and using deep reinforcement learning to select the best one. A decision-net is also proposed to efficiently model target appearance. Through evaluation on challenging videos, the approach achieved an average success rate of 62.3% and an average precision score of 81.2%, surpassing traditional correlation filter based trackers.",1
"Reinforcement Learning (RL) agents require the specification of a reward signal for learning behaviours. However, introduction of corrupt or stochastic rewards can yield high variance in learning. Such corruption may be a direct result of goal misspecification, randomness in the reward signal, or correlation of the reward with external factors that are not known to the agent. Corruption or stochasticity of the reward signal can be especially problematic in robotics, where goal specification can be particularly difficult for complex tasks. While many variance reduction techniques have been studied to improve the robustness of the RL process, handling such stochastic or corrupted reward structures remains difficult. As an alternative for handling this scenario in model-free RL methods, we suggest using an estimator for both rewards and value functions. We demonstrate that this improves performance under corrupted stochastic rewards in both the tabular and non-linear function approximation settings for a variety of noise types and environments. The use of reward estimation is a robust and easy-to-implement improvement for handling corrupted reward signals in model-free RL.",0
"For Reinforcement Learning (RL) agents to learn behaviours, a reward signal needs to be specified. However, if the rewards are corrupt or stochastic, the learning process can have high variance. Such corruption can arise from goal misspecification, reward signal randomness or correlation with external factors unknown to the agent. This issue is particularly challenging in robotics where complex tasks make goal specification difficult. While there are several variance reduction techniques to enhance the RL process's robustness, handling stochastic or corrupt reward structures remains daunting. To address this scenario in model-free RL methods, we propose using an estimator for rewards and value functions. Our findings show that this approach enhances performance under corrupted stochastic rewards across different noise types and environments in both tabular and non-linear function approximation settings. Reward estimation is an easy-to-implement and robust approach to handling corrupt reward signals in model-free RL.",1
"In this paper, we propose the Quantile Option Architecture (QUOTA) for exploration based on recent advances in distributional reinforcement learning (RL). In QUOTA, decision making is based on quantiles of a value distribution, not only the mean. QUOTA provides a new dimension for exploration via making use of both optimism and pessimism of a value distribution. We demonstrate the performance advantage of QUOTA in both challenging video games and physical robot simulators.",0
"The Quantile Option Architecture (QUOTA) is introduced in this paper as a method for exploration that takes advantage of progress in distributional reinforcement learning (RL). With QUOTA, decision making is grounded in the quantiles of a value distribution, rather than solely on the mean. QUOTA allows for exploration to occur through the utilization of both the optimism and pessimism of a value distribution. Our study showcases the performance benefits of QUOTA in difficult video games and physical robot simulators.",1
"In this paper, we propose an actor ensemble algorithm, named ACE, for continuous control with a deterministic policy in reinforcement learning. In ACE, we use actor ensemble (i.e., multiple actors) to search the global maxima of the critic. Besides the ensemble perspective, we also formulate ACE in the option framework by extending the option-critic architecture with deterministic intra-option policies, revealing a relationship between ensemble and options. Furthermore, we perform a look-ahead tree search with those actors and a learned value prediction model, resulting in a refined value estimation. We demonstrate a significant performance boost of ACE over DDPG and its variants in challenging physical robot simulators.",0
"The paper introduces ACE, an algorithm for continuous control that utilizes an actor ensemble approach with a deterministic policy in reinforcement learning. ACE aims to search for the global maxima of the critic by using multiple actors. Additionally, ACE is formulated in the option framework by extending the option-critic architecture with deterministic intra-option policies, which reveals a connection between ensemble and options. Furthermore, a look-ahead tree search is conducted with the actors and a learned value prediction model, which improves the value estimation. The results demonstrate a substantial performance improvement of ACE over DDPG and its variants in physically demanding robot simulators.",1
"This work investigates continual learning of two segmentation tasks in brain MRI with neural networks. To explore in this context the capabilities of current methods for countering catastrophic forgetting of the first task when a new one is learned, we investigate elastic weight consolidation, a recently proposed method based on Fisher information, originally evaluated on reinforcement learning of Atari games. We use it to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions. Our findings show this recent method reduces catastrophic forgetting, while large room for improvement exists in these challenging settings for continual learning.",0
"The focus of this study is to examine the use of neural networks for continual learning of two segmentation tasks in brain MRI. Specifically, we investigate the efficacy of elastic weight consolidation, a method based on Fisher information and previously tested in reinforcement learning of Atari games, in countering catastrophic forgetting of the first task when a new one is learned. Our experiment involves sequentially learning segmentation of normal brain structures followed by segmentation of white matter lesions. Our results indicate that this method can reduce catastrophic forgetting; however, there is still significant room for improvement in the challenging context of continual learning.",1
"In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.",0
"Humans interact with complex environments through various asynchronous stochastic discrete events in continuous time. The question is whether we can create online interventions that aid humans in achieving specific objectives in this type of setting. Our paper tackles this issue through the lens of deep reinforcement learning of marked temporal point processes, where an agent's actions and the feedback from the environment are asynchronous stochastic discrete events characterized by marked temporal point processes. We establish the agent's policy by utilizing the intensity and mark distribution of the corresponding process and then develop a flexible policy gradient technique that incorporates the agent's actions and feedback into real-valued vectors via deep recurrent neural networks. Our method does not assume any functional form of the intensity and mark distribution of the feedback, and it can handle intricate reward functions. We apply our approach to personalized teaching and viral marketing, using data from Duolingo and Twitter, and demonstrate that it can identify interventions that help learners and marketers achieve their objectives more efficiently than other methods.",1
"Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a simple and explicit finite time analysis of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. Final sections of the paper show how all of our main results extend to the study of TD learning with eligibility traces, known as TD($\lambda$), and to Q-learning applied in high-dimensional optimal stopping problems.",0
"Temporal difference learning (TD) is an iterative algorithm that estimates the value function of a given policy in a Markov decision process. Despite being widely used in reinforcement learning, TD's theoretical analysis is difficult, and there are few guarantees on its statistical efficiency. This study presents a simple and explicit finite time analysis of TD with linear function approximation. Our analysis follows standard techniques for analyzing stochastic gradient descent algorithms, with a few key insights. Additionally, our main results extend to TD learning with eligibility traces (TD($\lambda$)) and Q-learning in high-dimensional optimal stopping problems.",1
"With the growing prevalence of smart grid technology, short-term load forecasting (STLF) becomes particularly important in power system operations. There is a large collection of methods developed for STLF, but selecting a suitable method under varying conditions is still challenging. This paper develops a novel reinforcement learning based dynamic model selection (DMS) method for STLF. A forecasting model pool is first built, including ten state-of-the-art machine learning based forecasting models. Then a Q-learning agent learns the optimal policy of selecting the best forecasting model for the next time step, based on the model performance. The optimal DMS policy is applied to select the best model at each time step with a moving window. Numerical simulations on two-year load and weather data show that the Q-learning algorithm converges fast, resulting in effective and efficient DMS. The developed STLF model with Q-learning based DMS improves the forecasting accuracy by approximately 50%, compared to the state-of-the-art machine learning based STLF models.",0
"The importance of short-term load forecasting (STLF) in power system operations has increased with the growing prevalence of smart grid technology. Although many methods have been developed for STLF, it remains a challenge to select a suitable method under varying conditions. This study proposes a novel reinforcement learning based dynamic model selection (DMS) method for STLF. A forecasting model pool is created, comprising ten state-of-the-art machine learning based forecasting models. The Q-learning agent learns the optimal policy for selecting the best forecasting model for the next time step, based on model performance. The best model is selected at each time step with a moving window, using the optimal DMS policy. Numerical simulations on two-year load and weather data demonstrate that the Q-learning algorithm converges quickly, resulting in effective and efficient DMS. The developed STLF model with Q-learning based DMS improves forecasting accuracy by approximately 50% compared to state-of-the-art machine learning based STLF models.",1
"Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space. Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method. In DSO-NAS, we provide a novel model pruning view to NAS problem. In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations. Next, we impose sparse regularizations to prune useless connections in the architecture. Lastly, we derive an efficient and theoretically sound optimization method to solve it. Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet. Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84\%, while on the ImageNet dataset DSO-NAS achieves 25.4\% test error under 600M FLOPs with 8 GPUs in 18 hours.",0
"NAS has become a topic of interest for both academia and industry, but its non-continuous and vast search space makes it a challenging task. This paper suggests a novel approach called Direct Sparse Optimization NAS (DSO-NAS) that differs from previous methods that used evolutionary algorithm or reinforcement learning. DSO-NAS adopts a model pruning perspective where a completely connected block is pruned using scaling factors to regulate information flow between operations and sparse regularizations to eliminate useless connections. An efficient and sound optimization method is derived to solve the problem, making it differentiable and efficient for use in large datasets like ImageNet. DSO-NAS achieves an average test error of 2.84% on CIFAR-10 dataset and 25.4% test error on the ImageNet dataset with 600M FLOPs using 8 GPUs in 18 hours.",1
"Sparse reward problems are one of the biggest challenges in Reinforcement Learning. Goal-directed tasks are one such sparse reward problems where a reward signal is received only when the goal is reached. One promising way to train an agent to perform goal-directed tasks is to use Hindsight Learning approaches. In these approaches, even when an agent fails to reach the desired goal, the agent learns to reach the goal it achieved instead. Doing this over multiple trajectories while generalizing the policy learned from the achieved goals, the agent learns a goal conditioned policy to reach any goal. One such approach is Hindsight Experience replay which uses an off-policy Reinforcement Learning algorithm to learn a goal conditioned policy. In this approach, a replay of the past transitions happens in a uniformly random fashion. Another approach is to use a Hindsight version of the policy gradients to directly learn a policy. In this work, we discuss different ways to replay past transitions to improve learning in hindsight experience replay focusing on prioritized variants in particular. Also, we implement the Hindsight Policy gradient methods to robotic tasks.",0
"Reinforcement Learning faces a major obstacle in sparse reward problems, particularly in goal-directed tasks where rewards are only given upon reaching the goal. Hindsight Learning approaches offer a promising solution by training the agent to achieve the goals it has reached, even if it failed to reach the desired goal. This is done through multiple trajectories and generalizing the policy learned from the achieved goals. One such approach is the Hindsight Experience replay, which uses an off-policy Reinforcement Learning algorithm to learn a goal-conditioned policy. It randomly replays past transitions, while another approach uses a Hindsight version of the policy gradients to directly learn a policy. This study delves into various ways to replay past transitions, prioritizing variants to improve learning in hindsight experience replay. Additionally, we apply the Hindsight Policy gradient methods to robotic tasks.",1
"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).",0
"Although model-based reinforcement learning (RL) algorithms are highly efficient with minimal samples, they often struggle to match the asymptotic performance of the best model-free algorithms, especially when using high-capacity parametric function approximators like deep networks. To address this gap, this study investigates the use of uncertainty-aware dynamics models to improve results. The proposed algorithm, Probabilistic Ensembles with Trajectory Sampling (PETS), integrates deep network dynamics models with sampling-based uncertainty propagation. Comparisons to current model-based and model-free deep RL algorithms demonstrate that PETS achieves similar asymptotic performance as model-free algorithms for challenging benchmark tasks, while using significantly fewer samples (e.g., PETS required 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization, respectively, for the half-cheetah task).",1
"Dantzig Selector (DS) is widely used in compressed sensing and sparse learning for feature selection and sparse signal recovery. Since the DS formulation is essentially a linear programming optimization, many existing linear programming solvers can be simply applied for scaling up. The DS formulation can be explained as a basis pursuit denoising problem, wherein the data matrix (or measurement matrix) is employed as the denoising matrix to eliminate the observation noise. However, we notice that the data matrix may not be the optimal denoising matrix, as shown by a simple counter-example. This motivates us to pursue a better denoising matrix for defining a general DS formulation. We first define the optimal denoising matrix through a minimax optimization, which turns out to be an NPhard problem. To make the problem computationally tractable, we propose a novel algorithm, termed as Optimal Denoising Dantzig Selector (ODDS), to approximately estimate the optimal denoising matrix. Empirical experiments validate the proposed method. Finally, a novel sparse reinforcement learning algorithm is formulated by extending the proposed ODDS algorithm to temporal difference learning, and empirical experimental results demonstrate to outperform the conventional vanilla DS-TD algorithm.",0
"The Dantzig Selector (DS) is a commonly used technique for feature selection and sparse signal recovery in compressed sensing and sparse learning. The DS approach is based on linear programming optimization and can be easily scaled up using existing solvers. Essentially, the DS formulation is a basis pursuit denoising problem where the denoising matrix, typically the data or measurement matrix, is used to remove observation noise. However, the data matrix may not always be the best denoising matrix, as demonstrated by a counter-example. To address this limitation, we aim to find a better denoising matrix by minimizing the maximization problem, which is unfortunately an NPhard problem. To overcome this, we introduce a new algorithm called Optimal Denoising Dantzig Selector (ODDS) that can approximately estimate the optimal denoising matrix. Empirical experiments support the effectiveness of our proposed approach. Additionally, we extend the ODDS algorithm to temporal difference learning to formulate a novel sparse reinforcement learning algorithm that outperforms traditional DS-TD techniques.",1
"The problem-solving in automated theorem proving (ATP) can be interpreted as a search problem where the prover constructs a proof tree step by step. In this paper, we propose a deep reinforcement learning algorithm for proof search in intuitionistic propositional logic. The most significant challenge in the application of deep learning to the ATP is the absence of large, public theorem database. We, however, overcame this issue by applying a novel data augmentation procedure at each iteration of the reinforcement learning. We also improve the efficiency of the algorithm by representing the syntactic structure of formulas by a novel compact graph representation. Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Our method is also cost-efficient in terms of computational time. We will show that our prover outperforms Coq's $\texttt{tauto}$ tactic, a prover based on human-engineered heuristics. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while $\texttt{tauto}$ was able to solve only 52%.",0
"Automated theorem proving (ATP) involves constructing a proof tree through a search process. We propose a deep reinforcement learning algorithm for proof search in intuitionistic propositional logic, which faces challenges due to the absence of a large, public theorem database. To overcome this issue, we apply a novel data augmentation procedure during each iteration of the reinforcement learning process and represent formula syntax using a compact graph structure to improve algorithm efficiency. Our method is cost-efficient and outperforms Coq's $\texttt{tauto}$ tactic, solving 84% of benchmark library theorems within the specified time limit, while $\texttt{tauto}$ solves only 52%. We achieve this high accuracy through training graph neural networks on a large volume of augmented data that approximate the value function for the set of syntactic structures of formulas.",1
"Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences -- and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.",0
"To learn representations for high-dimensional discrete spaces, deep generative models have been successfully employed. They represent discrete objects as sequences and use powerful sequence-based deep models. However, these models often generate invalid sequences that do not represent any underlying discrete structure. This hinders the utility of the models. To address this issue, we propose the use of a deep recurrent validator model that can estimate whether a partial sequence can function as the beginning of a full and valid sequence. This validator provides insights into how individual sequence elements influence the validity of the overall sequence. It can also be used to constrain sequence-based models to generate valid sequences, thus enabling them to faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle that can evaluate the validity of complete sequences provides a sparse reward signal. We demonstrate the effectiveness of our approach as a generative model of Python 3 source code for mathematical expressions. Additionally, we show how it improves the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.",1
"When environmental interaction is expensive, model-based reinforcement learning offers a solution by planning ahead and avoiding costly mistakes. Model-based agents typically learn a single-step transition model. In this paper, we propose a multi-step model that predicts the outcome of an action sequence with variable length. We show that this model is easy to learn, and that the model can make policy-conditional predictions. We report preliminary results that show a clear advantage for the multi-step model compared to its one-step counterpart.",0
"Model-based reinforcement learning can be useful in cases where environmental interaction is costly as it allows for planning ahead and minimizing errors. Traditionally, model-based agents only learn single-step transition models. However, our paper proposes a multi-step model that can predict the outcome of action sequences with varying lengths. This model is simple to learn and can make predictions based on policies. Our preliminary results demonstrate that the multi-step model outperforms the single-step model.",1
"We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.",0
"TensorFlow Agents is a new infrastructure paradigm that enables the development of parallel reinforcement learning algorithms in TensorFlow. By simulating multiple environments in parallel and grouping them to perform neural network computation on a batch, the TensorFlow execution engine can parallelize computation without requiring manual synchronization. Environments are stepped in separate Python processes to allow for parallel progression without interference from the global interpreter lock. As part of this initiative, BatchPPO, a highly efficient implementation of the proximal policy optimization algorithm, has been introduced. Our goal in open sourcing TensorFlow Agents is to provide a flexible foundation for future research projects that will accelerate advancements in the field.",1
"Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.",0
"To address continuous control tasks, policy optimization is a reinforcement learning approach that has proven effective. Recent advancements demonstrate that alternating online and offline optimization is a successful approach for efficient trajectory reuse. However, determining the optimal time to stop optimizing and gather new trajectories is challenging due to the need to consider the variance of the objective function estimate. This paper proposes a novel policy search algorithm, POIS, that is model-free and applicable in both action-based and parameter-based settings. The algorithm derives a high-confidence bound for importance sampling estimation and defines a surrogate objective function that is offline-optimized when a new batch of trajectories is collected. The algorithm is tested on various continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.",1
"Recently, with convolutional neural networks gaining significant achievements in many challenging machine learning fields, hand-crafted neural networks no longer satisfy our requirements as designing a network will cost a lot, and automatically generating architectures has attracted increasingly more attention and focus. Some research on auto-generated networks has achieved promising results. However, they mainly aim at picking a series of single layers such as convolution or pooling layers one by one. There are many elegant and creative designs in the carefully hand-crafted neural networks, such as Inception-block in GoogLeNet, residual block in residual network and dense block in dense convolutional network. Based on reinforcement learning and taking advantages of the superiority of these networks, we propose a novel automatic process to design a multi-block neural network, whose architecture contains multiple types of blocks mentioned above, with the purpose to do structure learning of deep neural networks and explore the possibility whether different blocks can be composed together to form a well-behaved neural network. The optimal network is created by the Q-learning agent who is trained to sequentially pick different types of blocks. To verify the validity of our proposed method, we use the auto-generated multi-block neural network to conduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image classification task with restricted computational resources. The results demonstrate that our method is very effective, achieving comparable or better performance than hand-crafted networks and advanced auto-generated neural networks.",0
"Convolutional neural networks have made significant progress in many challenging machine learning fields, rendering hand-crafted neural networks inadequate due to high design costs. Consequently, there is a growing interest in automatically generating architectures by selecting individual layers such as convolution or pooling layers. However, this approach has limitations as it fails to incorporate the elegant and creative designs found in carefully hand-crafted neural networks. Therefore, we propose a novel approach that uses reinforcement learning to design a multi-block neural network that contains various types of blocks, including Inception-block, residual block, and dense block, to explore the possibility of creating a well-behaved neural network. Our Q-learning agent sequentially selects different types of blocks to create an optimal network, which we test on image benchmark datasets with limited computational resources. Our results demonstrate that our approach is highly effective, achieving comparable or better performance than both hand-crafted and advanced auto-generated neural networks.",1
"This work provides a thorough study on how reward scaling can affect performance of deep reinforcement learning agents. In particular, we would like to answer the question that how does reward scaling affect non-saturating ReLU networks in RL? This question matters because ReLU is one of the most effective activation functions for deep learning models. We also propose an Adaptive Network Scaling framework to find a suitable scale of the rewards during learning for better performance. We conducted empirical studies to justify the solution.",0
"The focus of this study is to investigate the impact of reward scaling on the performance of deep reinforcement learning agents. Specifically, the inquiry centers around how non-saturating ReLU networks in RL are influenced by reward scaling. This query is significant as ReLU is a highly efficient activation function for deep learning models. Additionally, we introduce the Adaptive Network Scaling framework as a means of identifying an optimal reward scale for improved performance during the learning process. Our proposed solution is supported by empirical research.",1
"Attention modules connecting encoder and decoders have been widely applied in the field of object recognition, image captioning, visual question answering and neural machine translation, and significantly improves the performance. In this paper, we propose a bottom-up gated hierarchical attention (GHA) mechanism for image captioning. Our proposed model employs a CNN as the decoder which is able to learn different concepts at different layers, and apparently, different concepts correspond to different areas of an image. Therefore, we develop the GHA in which low-level concepts are merged into high-level concepts and simultaneously low-level attended features pass to the top to make predictions. Our GHA significantly improves the performance of the model that only applies one level attention, for example, the CIDEr score increases from 0.923 to 0.999, which is comparable to the state-of-the-art models that employ attributes boosting and reinforcement learning (RL). We also conduct extensive experiments to analyze the CNN decoder and our proposed GHA, and we find that deeper decoders cannot obtain better performance, and when the convolutional decoder becomes deeper the model is likely to collapse during training.",0
"The use of attention modules that connect encoders and decoders has become prevalent in the fields of object recognition, image captioning, visual question answering, and neural machine translation. This has led to significant improvements in performance. In this study, we introduce a new bottom-up gated hierarchical attention (GHA) mechanism for image captioning. Our approach employs a CNN as the decoder, which can learn various concepts at different layers with each concept corresponding to specific areas of an image. The GHA merges low-level concepts into high-level ones while passing low-level attended features to the top for predictions. Our GHA model significantly outperforms one-level attention models, as evidenced by the increase in CIDEr score from 0.923 to 0.999. This performance is competitive with state-of-the-art models that use attributes boosting and reinforcement learning. We also conducted extensive experiments to analyze the CNN decoder and our proposed GHA and discovered that deeper decoders do not necessarily lead to better performance. In fact, when the convolutional decoder is too deep, the model is prone to collapse during training.",1
"We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",0
"To enhance deep reinforcement learning techniques, we propose an easy-to-implement exploration bonus with minimal computational overhead. This bonus is determined by the difference between the error of a fixed randomly initialized neural network and that of a neural network predicting observations' features. Additionally, we introduce a method that allows for the flexible combination of intrinsic and extrinsic rewards. By using the random network distillation (RND) bonus in conjunction with this increased flexibility, we achieve significant progress in several challenging exploration Atari games, including Montezuma's Revenge. Our approach sets a new benchmark for this game, which is notoriously difficult for deep reinforcement learning methods, surpassing average human performance without requiring demonstrations or access to the game's underlying state. Moreover, our method occasionally completes the first level of the game.",1
"This paper investigates the vision-based autonomous driving with deep learning and reinforcement learning methods. Different from the end-to-end learning method, our method breaks the vision-based lateral control system down into a perception module and a control module. The perception module which is based on a multi-task learning neural network first takes a driver-view image as its input and predicts the track features. The control module which is based on reinforcement learning then makes a control decision based on these features. In order to improve the data efficiency, we propose visual TORCS (VTORCS), a deep reinforcement learning environment which is based on the open racing car simulator (TORCS). By means of the provided functions, one can train an agent with the input of an image or various physical sensor measurement, or evaluate the perception algorithm on this simulator. The trained reinforcement learning controller outperforms the linear quadratic regulator (LQR) controller and model predictive control (MPC) controller on different tracks. The experiments demonstrate that the perception module shows promising performance and the controller is capable of controlling the vehicle drive well along the track center with visual input.",0
"This article explores the use of deep learning and reinforcement learning techniques in autonomous driving that relies on vision. Our approach differs from the end-to-end learning method by breaking down the vision-based lateral control system into two modules: a perception and a control module. The perception module uses a multi-task learning neural network to analyze driver-view images and predict track features. The control module then uses reinforcement learning to make control decisions based on these features. To enhance data efficiency, we introduce visual TORCS (VTORCS), a deep reinforcement learning environment based on the open racing car simulator (TORCS). This allows for training an agent using image inputs or various physical sensor measurements, as well as evaluating the perception algorithm on the simulator. Our trained reinforcement learning controller outperforms the linear quadratic regulator (LQR) controller and model predictive control (MPC) controller on different tracks. The results demonstrate the effectiveness of our perception module and the controller's ability to successfully drive the vehicle along the track center using visual input.",1
"Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.",0
"Learning dynamics models in model-based reinforcement learning has been a significant challenge due to the issue of generalization. Prior research on action-conditioned dynamics prediction has focused on learning pixel-level motion, which does not generalize effectively to new environments with different object arrangements. Our paper introduces a novel object-oriented framework, the object-oriented dynamics predictor (OODP), which breaks down the environment into objects and predicts their dynamics based on both actions and object-to-object relationships. OODP is an end-to-end neural network that can be trained unsupervisedly. To promote the generalization capabilities of dynamics learning, we have developed a unique CNN-based relation mechanism that is class-specific and utilizes the locality principle. Our experimental results indicate that OODP outperforms previous methods in terms of generalization to various object layouts in new environments. OODP can learn from only a few environments and accurately forecast dynamics in numerous unseen environments. Furthermore, OODP learns dynamics models that are semantically and visually interpretable.",1
"Multi-objective optimizations are frequently encountered in engineering practices. The solution techniques and parametric selections however are usually problem-specific. In this study we formulate a reinforcement learning hyper-heuristic scheme, and propose four low-level heuristics which can work coherently with the single point search algorithm MOSA/R (Multi-Objective Simulated Annealing Algorithm based on Re-seed) towards multi-objective optimization problems of general applications. Making use of the domination amount, crowding distance and hypervolume calculations, the proposed hyper-heuristic scheme can meet various optimization requirements adaptively and autonomously. The approach developed not only exhibits improved and more robust performance compared to AMOSA, NSGA-II and MOEA/D when applied to benchmark test cases, but also shows promising results when applied to a generic structural fault identification problem. The outcome of this research can be extended to a variety of design and manufacturing optimization applications.",0
"In engineering practices, multi-objective optimizations are common, but the techniques and parameters used are typically specific to the problem at hand. To address this issue, we have developed a reinforcement learning hyper-heuristic scheme that employs four low-level heuristics that work in tandem with the MOSA/R algorithm to solve multi-objective optimization problems of general applicability. Our hyper-heuristic scheme employs domination amount, crowding distance, and hypervolume calculations to adaptively and autonomously meet various optimization requirements. Our approach outperforms AMOSA, NSGA-II, and MOEA/D when applied to benchmark test cases, and also demonstrates promising results when applied to a generic structural fault identification problem. This research has broad implications for design and manufacturing optimization applications.",1
"Collecting training data from the physical world is usually time-consuming and even dangerous for fragile robots, and thus, recent advances in robot learning advocate the use of simulators as the training platform. Unfortunately, the reality gap between synthetic and real visual data prohibits direct migration of the models trained in virtual worlds to the real world. This paper proposes a modular architecture for tackling the virtual-to-real problem. The proposed architecture separates the learning model into a perception module and a control policy module, and uses semantic image segmentation as the meta representation for relating these two modules. The perception module translates the perceived RGB image to semantic image segmentation. The control policy module is implemented as a deep reinforcement learning agent, which performs actions based on the translated image segmentation. Our architecture is evaluated in an obstacle avoidance task and a target following task. Experimental results show that our architecture significantly outperforms all of the baseline methods in both virtual and real environments, and demonstrates a faster learning curve than them. We also present a detailed analysis for a variety of variant configurations, and validate the transferability of our modular architecture.",0
"Gathering training data from the physical world can be a time-consuming and risky process for delicate robots. Therefore, recent robot learning advancements suggest using simulators as a training platform. However, the gap between synthetic and real visual data makes it difficult to transfer models trained in virtual worlds to the real world. This study proposes a modular architecture to address the virtual-to-real problem. The architecture divides the learning model into a perception module and a control policy module, using semantic image segmentation as the meta representation to relate the two modules. The perception module converts RGB images to semantic image segmentation, while the control policy module functions as a deep reinforcement learning agent, taking action based on the translated image segmentation. In two tasks, obstacle avoidance and target following, our architecture outperforms all baseline methods in virtual and real environments, with a faster learning curve. A thorough analysis of various configurations confirms the transferability of our modular architecture.",1
"Dynamic spectrum access (DSA) is regarded as an effective and efficient technology to share radio spectrum among different networks. As a secondary user (SU), a DSA device will face two critical problems: avoiding causing harmful interference to primary users (PUs), and conducting effective interference coordination with other secondary users. These two problems become even more challenging for a distributed DSA network where there is no centralized controllers for SUs. In this paper, we investigate communication strategies of a distributive DSA network under the presence of spectrum sensing errors. To be specific, we apply the powerful machine learning tool, deep reinforcement learning (DRL), for SUs to learn ""appropriate"" spectrum access strategies in a distributed fashion assuming NO knowledge of the underlying system statistics. Furthermore, a special type of recurrent neural network (RNN), called the reservoir computing (RC), is utilized to realize DRL by taking advantage of the underlying temporal correlation of the DSA network. Using the introduced machine learning-based strategy, SUs could make spectrum access decisions distributedly relying only on their own current and past spectrum sensing outcomes. Through extensive experiments, our results suggest that the RC-based spectrum access strategy can help the SU to significantly reduce the chances of collision with PUs and other SUs. We also show that our scheme outperforms the myopic method which assumes the knowledge of system statistics, and converges faster than the Q-learning method when the number of channels is large.",0
"Dynamic spectrum access (DSA) is considered an efficient technology for sharing radio spectrum among various networks. Secondary users (SUs) using DSA devices face two critical issues: avoiding harmful interference to primary users (PUs) and coordinating with other SUs to manage interference. These challenges become even more complex in a distributed DSA network without centralized controllers for SUs. In this research, we explore communication strategies for a distributed DSA network in the presence of spectrum sensing errors. Our approach employs deep reinforcement learning (DRL), a powerful machine learning tool, for SUs to learn appropriate spectrum access strategies in a distributed manner without knowledge of underlying system statistics. We use a type of recurrent neural network (RNN) called reservoir computing (RC) to facilitate DRL and leverage the temporal correlation of the DSA network. Our machine learning-based strategy allows SUs to make spectrum access decisions based solely on their current and past spectrum sensing outcomes. Our experimental results demonstrate that the RC-based spectrum access strategy significantly reduces the likelihood of collision with PUs and other SUs. Our approach outperforms the myopic method that assumes knowledge of system statistics and converges faster than the Q-learning method for a large number of channels.",1
"Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.",0
"Challenging control tasks have been successfully tackled using Deep Reinforcement Learning (DRL) algorithms, but these methods typically face three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are highly sensitive to hyperparameters. Consequently, these challenges limit the applicability of these approaches in real-world scenarios. Evolutionary Algorithms (EAs), which are optimization techniques inspired by natural evolution, can address these challenges, but they struggle with large parameters and high sample complexity. This paper presents Evolutionary Reinforcement Learning (ERL), which is a hybrid algorithm that uses the population of an EA to train an RL agent with diversified data and injects gradient information into the EA by periodically reinserting the RL agent into the population. ERL combines EA's temporal credit assignment, effective exploration, and stability with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments on a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.",1
"We present a reinforcement learning approach for detecting objects within an image. Our approach performs a step-wise deformation of a bounding box with the goal of tightly framing the object. It uses a hierarchical tree-like representation of predefined region candidates, which the agent can zoom in on. This reduces the number of region candidates that must be evaluated so that the agent can afford to compute new feature maps before each step to enhance detection quality. We compare an approach that is based purely on zoom actions with one that is extended by a second refinement stage to fine-tune the bounding box after each zoom step. We also improve the fitting ability by allowing for different aspect ratios of the bounding box. Finally, we propose different reward functions to lead to a better guidance of the agent while following its search trajectories. Experiments indicate that each of these extensions leads to more correct detections. The best performing approach comprises a zoom stage and a refinement stage, uses aspect-ratio modifying actions and is trained using a combination of three different reward metrics.",0
"Our method employs reinforcement learning to identify objects in images, by iteratively adjusting the bounding box to tightly enclose the object. To achieve this, we utilize a hierarchical representation of region candidates that can be selectively zoomed into by the agent. This reduces the number of regions that need to be evaluated, allowing the agent to compute new feature maps to enhance detection quality. Our study compares the performance of two approaches - one purely based on zoom actions and the other extended with a refinement stage to fine-tune the bounding box. We also introduce the ability to modify bounding box aspect ratios and propose various reward functions to guide the agent's search. Our experiments demonstrate that these extensions lead to more accurate detections. The best approach involves both zoom and refinement stages, aspect-ratio modifications, and is trained using a combination of three reward metrics.",1
"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.",0
"Our focus is on the issue of multiset prediction, which involves creating a predictor that can map an input to a multiset of multiple items. Unlike other supervised learning problems like classification, ranking, and sequence generation, there is no established order among the items in the multiset, and each item may recur, making this a particularly difficult challenge. We propose a new multiset loss function in this paper, which is based on sequential decision making. We evaluate this function empirically on two types of datasets with varying levels of complexity, one synthetic and the other real, and compare it to reinforcement learning, sequence, and aggregated distribution matching loss functions. The results indicate that our proposed function outperforms the others.",1
"Variable speed limits (VSL) control is a flexible way to improve traffic condition,increase safety and reduce emission. There is an emerging trend of using reinforcement learning technique for VSL control and recent studies have shown promising results. Currently, deep learning is enabling reinforcement learning to develope autonomous control agents for problems that were previously intractable. In this paper, we propose a more effective deep reinforcement learning (DRL) model for differential variable speed limits (DVSL) control, in which the dynamic and different speed limits among lanes can be imposed. The proposed DRL models use a novel actor-critic architecture which can learn a large number of discrete speed limits in a continues action space. Different reward signals, e.g. total travel time, bottleneck speed, emergency braking, and vehicular emission are used to train the DVSL controller, and comparison between these reward signals are conducted. We test proposed DRL baased DVSL controllers on a simulated freeway recurrent bottleneck. Results show that the efficiency, safety and emissions can be improved by the proposed method. We also show some interesting findings through the visulization of the control policies generated from DRL models.",0
"A flexible method to enhance traffic conditions, safety, and reduce emissions is variable speed limits (VSL) control. There is a growing trend of using reinforcement learning techniques for VSL control, which have shown promising results. The use of deep learning is enabling reinforcement learning to develop autonomous control agents for previously unsolvable problems. This paper proposes a more effective deep reinforcement learning (DRL) model for differential variable speed limits (DVSL) control, where dynamic and different speed limits can be imposed among lanes. The proposed DRL model uses a novel actor-critic architecture, which can learn a large number of discrete speed limits in a continuous action space. Various reward signals, such as total travel time, bottleneck speed, emergency braking, and vehicular emissions, are used to train the DVSL controller, and comparisons are made between these reward signals. The proposed DRL-based DVSL controllers are tested on a simulated freeway recurrent bottleneck, where the efficiency, safety, and emissions are improved. Additionally, interesting findings are shown through the visualization of the control policies generated from DRL models.",1
"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to an advantage-like function and is robust to partially observed state. We demonstrate that this new algorithm can substantially outperform strong baseline methods on several partially observed reinforcement learning tasks: learning first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.",0
"Various challenging domains have benefitted from the effectiveness of deep reinforcement learning algorithms that evaluate state and state-action value functions, such as learning control strategies from raw image pixels. However, these algorithms typically assume a fully observable state and must compensate for partial observations through the use of finite length observation histories or recurrent networks. This study introduces a novel deep reinforcement learning algorithm that employs counterfactual regret minimization to iteratively update an approximation of an advantage-like function while remaining robust to partially observed states. Through experimentation, we demonstrate that this new algorithm surpasses strong baseline methods in numerous partially observed reinforcement learning tasks, including first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.",1
"This paper presents a new meta-modeling framework to employ deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are conceptualized as information flow in directed graphs. The process of writing constitutive models are simplified as a sequence of forming graph edges with the goal of maximizing the model score (a function of accuracy, robustness and forward prediction quality). Thus meta-modeling can be formulated as a Markov decision process with well-defined states, actions, rules, objective functions, and rewards. By using neural networks to estimate policies and state values, the computer agent is able to efficiently self-improve the constitutive model it generated through self-playing, in the same way AlphaGo Zero (the algorithm that outplayed the world champion in the game of Go)improves its gameplay. Our numerical examples show that this automated meta-modeling framework not only produces models which outperform existing cohesive models on benchmark traction-separation data but is also capable of detecting hidden mechanisms among micro-structural features and incorporating them in constitutive models to improve the forward prediction accuracy, which are difficult tasks to do manually.",0
"A novel framework for meta-modeling is introduced in this paper, which utilizes deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are represented as directed graphs, with the process of creating these models simplified as a sequence of forming edges in the graph. The objective of this process is to maximize the model score, which is determined by accuracy, robustness, and forward prediction quality. This framework can be viewed as a Markov decision process, with well-defined states, actions, rules, objective functions, and rewards. By using neural networks to estimate policies and state values, the computer agent can self-improve the generated constitutive model through self-playing, similar to AlphaGo Zero. Numerical examples demonstrate that this automated meta-modeling framework outperforms existing cohesive models on benchmark traction-separation data. Furthermore, it can identify hidden mechanisms among micro-structural features and incorporate them into constitutive models to improve the forward prediction accuracy - tasks that are difficult to do manually.",1
"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.",0
"Achieving superhuman performance in video game environments using deep reinforcement learning is possible, but it requires a designer to manually specify a reward function. In contrast, providing demonstrations of a target behavior is often simpler. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but applying IRL to high-dimensional video games has received little attention. Our CNN-AIRL baseline modifies the state-of-the-art adversarial IRL (AIRL) algorithm by using CNNs for the generator and discriminator to improve training stability. Additionally, we learn a low-dimensional state representation using a novel autoencoder architecture that is optimized for video game environments. This embedding is utilized as input to the reward network, improving the sample efficiency of expert demonstrations. Our approach achieves high-level performance on the Catcher video game, surpassing the CNN-AIRL baseline. However, on the Enduro Atari racing game, we score points but do not match expert performance, indicating the need for further research.",1
"Robust Reinforcement Learning aims to derive optimal behavior that accounts for model uncertainty in dynamical systems. However, previous studies have shown that by considering the worst case scenario, robust policies can be overly conservative. Our soft-robust framework is an attempt to overcome this issue. In this paper, we present a novel Soft-Robust Actor-Critic algorithm (SR-AC). It learns an optimal policy with respect to a distribution over an uncertainty set and stays robust to model uncertainty but avoids the conservativeness of robust strategies. We show the convergence of SR-AC and test the efficiency of our approach on different domains by comparing it against regular learning methods and their robust formulations.",0
"The objective of Robust Reinforcement Learning is to determine the best possible behavior that takes into account the uncertainty of models in dynamic systems. However, prior research has demonstrated that robust policies can be excessively cautious when they contemplate worst-case scenarios. To address this issue, we have developed a soft-robust framework. Our paper introduces a new Soft-Robust Actor-Critic algorithm (SR-AC) that learns an optimal policy based on a distribution of uncertainty and remains robust to model uncertainty while avoiding the overly-conservative nature of robust strategies. We demonstrate the convergence of SR-AC and compare its effectiveness against conventional learning approaches and their robust counterparts in various domains.",1
"Since the inception of Deep Reinforcement Learning (DRL) algorithms, there has been a growing interest in both research and industrial communities in the promising potentials of this paradigm. The list of current and envisioned applications of deep RL ranges from autonomous navigation and robotics to control applications in the critical infrastructure, air traffic control, defense technologies, and cybersecurity. While the landscape of opportunities and the advantages of deep RL algorithms are justifiably vast, the security risks and issues in such algorithms remain largely unexplored. To facilitate and motivate further research on these critical challenges, this paper presents a foundational treatment of the security problem in DRL. We formulate the security requirements of DRL, and provide a high-level threat model through the classification and identification of vulnerabilities, attack vectors, and adversarial capabilities. Furthermore, we present a review of current literature on security of deep RL from both offensive and defensive perspectives. Lastly, we enumerate critical research venues and open problems in mitigation and prevention of intentional attacks against deep RL as a roadmap for further research in this area.",0
"The potential of Deep Reinforcement Learning (DRL) algorithms has captured the attention of both the research and industrial communities. Applications of deep RL are numerous, ranging from autonomous navigation to air traffic control and cybersecurity. Despite the vast opportunities and advantages of deep RL algorithms, security risks and issues in such algorithms have not been explored thoroughly. This paper aims to encourage further research on these critical challenges by providing a foundational treatment of the security problem in DRL. We outline the security requirements of DRL and present a high-level threat model that identifies vulnerabilities, attack vectors, and adversarial capabilities. We also review current literature on the security of deep RL from both offensive and defensive perspectives. Finally, we enumerate critical research areas and open problems in the mitigation and prevention of intentional attacks against deep RL, providing a roadmap for further research in this field.",1
"We explore Deep Reinforcement Learning in a parameterized action space. Specifically, we investigate how to achieve sample-efficient end-to-end training in these tasks. We propose a new compact architecture for the tasks where the parameter policy is conditioned on the output of the discrete action policy. We also propose two new methods based on the state-of-the-art algorithms Trust Region Policy Optimization (TRPO) and Stochastic Value Gradient (SVG) to train such an architecture. We demonstrate that these methods outperform the state of the art method, Parameterized Action DDPG, on test domains.",0
"Our focus is on Deep Reinforcement Learning in an action space that is parameterized. We are particularly interested in discovering ways to attain efficient training from beginning to end in these tasks. To achieve this, we suggest a new, streamlined structure for tasks that rely on a policy parameter which is based on the outcome of the action policy. We also present two novel methods that build upon the latest algorithms, Trust Region Policy Optimization (TRPO) and Stochastic Value Gradient (SVG), to train this kind of architecture. Our results indicate that these approaches are more effective than the current state-of-the-art technique, Parameterized Action DDPG, in test domains.",1
"We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a $d$-dimensional state space and the discounted factor $\gamma \in (0,1)$, given an arbitrary sample path with ""covering time"" $ L $, we establish that the algorithm is guaranteed to output an $\varepsilon$-accurate estimate of the optimal Q-function using $\tilde{O}\big(L/(\varepsilon^3(1-\gamma)^7)\big)$ samples. For instance, for a well-behaved MDP, the covering time of the sample path under the purely random policy scales as $ \tilde{O}\big(1/\varepsilon^d\big),$ so the sample complexity scales as $\tilde{O}\big(1/\varepsilon^{d+3}\big).$ Indeed, we establish a lower bound that argues that the dependence of $ \tilde{\Omega}\big(1/\varepsilon^{d+2}\big)$ is necessary.",0
"We explore the use of model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel. In this scenario, only a single sample path of the system under an arbitrary policy is available. To solve this problem, we utilize the Nearest Neighbor Q-Learning (NNQL) algorithm and employ a nearest neighbor regression method to learn the optimal Q function. Our main contribution is to provide a thorough analysis of the convergence rate with a tight finite sample bound. For MDPs with a state space of dimension $d$ and a discounted factor $\gamma \in (0,1)$, we guarantee that the algorithm will produce an $\varepsilon$-accurate estimate of the optimal Q-function using $\tilde{O}\big(L/(\varepsilon^3(1-\gamma)^7)\big)$ samples, where $L$ represents the covering time of the sample path. For well-behaved MDPs, the covering time scales as $\tilde{O}\big(1/\varepsilon^d\big),$ which results in a sample complexity of $\tilde{O}\big(1/\varepsilon^{d+3}\big).$ However, we prove that the dependence of $ \tilde{\Omega}\big(1/\varepsilon^{d+2}\big)$ is necessary by establishing a lower bound.",1
"We propose a practical non-episodic PSRL algorithm that unlike recent state-of-the-art PSRL algorithms uses a deterministic, model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove a Bayesian regret bound under mild assumptions. Our result is more generally applicable to multiple parameters and continuous state action problems. We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature. Finally, we show how the assumptions of our algorithm satisfy a sensible parametrization for a large class of problems in sequential recommendations.",0
"Our proposed non-episodic PSRL algorithm, called deterministic schedule PSRL (DS-PSRL), utilizes a deterministic, model-independent episode switching schedule, which sets it apart from recent state-of-the-art PSRL algorithms. Our algorithm is efficient in terms of time, sample, and space complexity, and we provide a Bayesian regret bound under mild assumptions. Our approach is applicable to multiple parameters and continuous state action problems and we compare it with other PSRL algorithms on standard discrete and continuous problems from the literature. Furthermore, we demonstrate how the assumptions of our algorithm satisfy a sensible parametrization for a wide range of problems in sequential recommendations.",1
"Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this work, we show that the \textsc{Tree Backup} and \textsc{Retrace} algorithms are unstable with linear function approximation, both in theory and in practice with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation. By exploiting the problem structure proper to these algorithms, we are able to provide convergence guarantees and finite-sample bounds. The applicability of our new analysis also goes beyond \textsc{Tree Backup} and \textsc{Retrace} and allows us to provide new convergence rates for the GTD and GTD2 algorithms without having recourse to projections or Polyak averaging.",0
"Scaling up reinforcement learning relies on off-policy learning, which involves learning about a target policy through the experience generated by a different behavior policy. However, combining off-policy learning with function approximation and multi-step bootstrapping has been problematic in terms of achieving both stability and efficiency. This study demonstrates the instability of the \textsc{Tree Backup} and \textsc{Retrace} algorithms with linear function approximation and provides a solution in the form of stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation. The analysis also extends to the GTD and GTD2 algorithms, offering new convergence rates without requiring projections or Polyak averaging.",1
"Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",0
"Reinforcement learning is a field that utilizes policy gradient methods to optimize the cumulative reward and can be used with nonlinear function approximators like neural networks. However, there are two main challenges that arise: the large number of samples required and the difficulty of obtaining stable improvement due to nonstationarity. To address the first challenge, value functions are used to reduce variance while introducing bias, with an exponentially-weighted estimator of the advantage function that resembles TD(lambda). The second challenge is tackled through trust region optimization procedures for both the policy and value function, which are represented by neural networks. Our method produced strong empirical results in challenging 3D locomotion tasks, such as learning running gaits for bipedal and quadrupedal simulated robots, and developing a policy for getting a biped to stand up from lying on the ground. Unlike previous work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",1
"Transportation and traffic are currently undergoing a rapid increase in terms of both scale and complexity. At the same time, an increasing share of traffic participants are being transformed into agents driven or supported by artificial intelligence resulting in mixed-intelligence traffic. This work explores the implications of distributed decision-making in mixed-intelligence traffic. The investigations are carried out on the basis of an online-simulated highway scenario, namely the MIT \emph{DeepTraffic} simulation. In the first step traffic agents are trained by means of a deep reinforcement learning approach, being deployed inside an elitist evolutionary algorithm for hyperparameter search. The resulting architectures and training parameters are then utilized in order to either train a single autonomous traffic agent and transfer the learned weights onto a multi-agent scenario or else to conduct multi-agent learning directly. Both learning strategies are evaluated on different ratios of mixed-intelligence traffic. The strategies are assessed according to the average speed of all agents driven by artificial intelligence. Traffic patterns that provoke a reduction in traffic flow are analyzed with respect to the different strategies.",0
"The size and complexity of transportation and traffic are rapidly increasing, and more participants are being transformed into agents supported by artificial intelligence, leading to mixed-intelligence traffic. This study examines the implications of distributed decision-making in mixed-intelligence traffic, using an online-simulated highway scenario called MIT DeepTraffic. The first step is to train traffic agents using deep reinforcement learning within an elitist evolutionary algorithm for hyperparameter search. The resulting architectures and parameters are then used to train a single autonomous traffic agent or conduct multi-agent learning directly. Both strategies are evaluated on various ratios of mixed-intelligence traffic, with the average speed of AI-driven agents as the assessment criterion. Different traffic patterns that cause a reduction in traffic flow are analyzed in terms of the two strategies.",1
"We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer. EVA shifts the value predicted by a neural network with an estimate of the value function found by planning over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning. We show that EVAis performant on a demonstration task and Atari games.",0
"Our proposal is Ephemeral Value Adjustments (EVA), which enables deep reinforcement learning agents to swiftly adjust to their experience in the replay buffer. EVA involves altering the predicted value by a neural network with a value function estimate derived from planning over experience tuples that are close to the current state. By integrating slot-based storage, content-based retrieval, and memory-based planning, EVA combines several recent concepts on incorporating episodic memory-like structures into reinforcement learning agents. We demonstrate the effectiveness of EVA on a demonstration task and Atari games.",1
"Reinforcement learning methods carry a well known bias-variance trade-off in n-step algorithms for optimal control. Unfortunately, this has rarely been addressed in current research. This trade-off principle holds independent of the choice of the algorithm, such as n-step SARSA, n-step Expected SARSA or n-step Tree backup. A small n results in a large bias, while a large n leads to large variance. The literature offers no straightforward recipe for the best choice of this value. While currently all n-step algorithms use a fixed value of n over the state space we extend the framework of n-step updates by allowing each state to have its specific n.   We propose a solution to this problem within the context of human aided reinforcement learning. Our approach is based on the observation that a human can learn more efficiently if she receives input regarding the criticality of a given state and thus the amount of attention she needs to invest into the learning in that state. This observation is related to the idea that each state of the MDP has a certain measure of criticality which indicates how much the choice of the action in that state influences the return. In our algorithm the RL agent utilizes the criticality measure, a function provided by a human trainer, in order to locally choose the best stepnumber n for the update of the Q function.",0
"The use of reinforcement learning methods for optimal control in n-step algorithms is well-known to have a trade-off between bias and variance. Despite this, current research has not adequately addressed this issue. This principle applies regardless of the algorithm used, including n-step SARSA, n-step Expected SARSA, and n-step Tree backup. The choice of n can result in either a large bias with a small n or a large variance with a large n, and there is no clear guidance on the best value to use. While current algorithms use a fixed n value across all states, we propose a solution by allowing each state to have its own specific n value. Our approach is based on the observation that humans can learn more efficiently when they understand the criticality of a given state and how much attention they need to give to learning in that state. This criticality measure is related to how much the action in that state influences the return. Our algorithm uses this criticality measure, provided by a human trainer, to choose the best step number n for updating the Q function on a local level.",1
"Deep reinforcement learning enables algorithms to learn complex behavior, deal with continuous action spaces and find good strategies in environments with high dimensional state spaces. With deep reinforcement learning being an active area of research and many concurrent inventions, we decided to focus on a relatively simple robotic task to evaluate a set of ideas that might help to solve recent reinforcement learning problems. We test a newly created combination of two commonly used reinforcement learning methods, whether it is able to learn more effectively than a baseline. We also compare different ideas to preprocess information before it is fed to the reinforcement learning algorithm. The goal of this strategy is to reduce training time and eventually help the algorithm to converge. The concluding evaluation proves the general applicability of the described concepts by testing them using a simulated environment. These concepts might be reused for future experiments.",0
"By utilizing deep reinforcement learning, algorithms can acquire intricate behaviors, cope with continuous action spaces, and identify effective strategies in environments with high dimensional state spaces. As deep reinforcement learning currently remains a dynamic field of research with numerous concurrent inventions, we opted to concentrate on a rather uncomplicated robotic task to evaluate a range of concepts that may aid in resolving recent reinforcement learning challenges. We examined a recently developed combination of two commonly utilized reinforcement learning techniques to determine if it could learn more efficiently than a baseline. Additionally, we compared various methods for preprocessing data before feeding it into the reinforcement learning algorithm, aiming to lessen training time and ultimately promote algorithm convergence. The final assessment validates the general applicability of the aforementioned concepts, as we tested them in a simulated environment, and suggests that they may be repurposed for future experiments.",1
"How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from the knowledge graph to predict the actions. For evaluation, we use the AI2-THOR framework. Our experiments show how semantic knowledge improves performance significantly. More importantly, we show improvement in generalization to unseen scenes and/or objects. The supplementary video can be accessed at the following link: https://youtu.be/otKjuO805dE .",0
"Is the way humans navigate to new objects in unfamiliar settings guided by the semantic and functional expectations that we have acquired over time? For instance, when searching for cups, we typically check the cupboards by the coffee maker, whereas we look in the refrigerator for fruits. Our research concentrates on integrating semantic priors in semantic navigation tasks. Our approach proposes to employ Graph Convolutional Networks in a deep reinforcement learning system to incorporate prior knowledge. The agent employs the knowledge graph's features to predict actions. We assess our technique using the AI2-THOR framework and demonstrate that incorporating semantic knowledge enhances performance significantly. Furthermore, we illustrate how our method improves generalization in previously unencountered environments and/or objects. You can watch the supplementary video at the following link: https://youtu.be/otKjuO805dE.",1
"We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",0
"There are two concerns regarding the set of algorithms based on the Adversarial Imitation Learning framework. Firstly, the reward functions used in these algorithms have an inherent bias, which can work well in certain situations but lead to suboptimal behavior in others. Secondly, although these algorithms can learn from a limited number of expert demonstrations, they necessitate an excessive number of interactions with the environment to imitate the expert for practical uses. To tackle these problems, we suggest a novel approach called Discriminator-Actor-Critic that employs off-policy Reinforcement Learning to reduce the complexity of policy-environment interaction sampling by an average of 10. Moreover, our unbiased reward function enables us to apply this algorithm to various problems without any task-specific customization.",1
"We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.",0
"An overview of deep reinforcement learning is presented, in which a detailed big picture is drawn. Contemporary and historical contexts are examined, and six core elements, six important mechanisms, and twelve applications are discussed. The background of artificial intelligence, machine learning, deep learning, and reinforcement learning is explored, along with resources. The core elements of RL, such as policy, reward, and exploration vs. exploitation, are then analyzed, as are important mechanisms like attention and memory, unsupervised learning, and relational RL. Finally, RL applications in areas like robotics, NLP, and finance are discussed, along with challenges and opportunities, before ending with an epilogue.",1
"Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that ""soft"" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\epsilon$-greedy exploration schedule.",0
"Policy gradient methods and $Q$-learning methods are two prominent model-free reinforcement learning approaches. While $Q$-learning methods can be efficient and effective, their ability to estimate $Q$-values accurately is not well understood. This may be because they implement policy gradient updates, as we demonstrate in our study of entropy-regularized reinforcement learning. We establish a precise equivalence between $Q$-learning and policy gradient methods in this setting, revealing that ""soft"" $Q$-learning is equivalent to a policy gradient method. We also identify a link between $Q$-learning and natural policy gradient methods. Our experimental investigation of entropy-regularized versions of $Q$-learning and policy gradients shows them to perform as well as or slightly better than standard variants on the Atari benchmark. We further prove the practical applicability of our findings by constructing a $Q$-learning method that replicates A3C's learning dynamics without resorting to a target network or $\epsilon$-greedy exploration schedule.",1
"Reinforcement learning methods require careful design involving a reward function to obtain the desired action policy for a given task. In the absence of hand-crafted reward functions, prior work on the topic has proposed several methods for reward estimation by using expert state trajectories and action pairs. However, there are cases where complete or good action information cannot be obtained from expert demonstrations. We propose a novel reinforcement learning method in which the agent learns an internal model of observation on the basis of expert-demonstrated state trajectories to estimate rewards without completely learning the dynamics of the external environment from state-action pairs. The internal model is obtained in the form of a predictive model for the given expert state distribution. During reinforcement learning, the agent predicts the reward as a function of the difference between the actual state and the state predicted by the internal model. We conducted multiple experiments in environments of varying complexity, including the Super Mario Bros and Flappy Bird games. We show our method successfully trains good policies directly from expert game-play videos.",0
"To achieve the desired action policy, reinforcement learning methods require careful design that includes a reward function. Some prior work in this area has suggested methods for reward estimation that use expert state trajectories and action pairs when hand-crafted reward functions are not available. However, there are situations where expert demonstrations do not provide complete or sufficient action information. In response, we propose a new reinforcement learning approach that allows the agent to learn an internal model of observation based on expert-demonstrated state trajectories. This allows for reward estimation without having to learn the external environment dynamics from state-action pairs. The internal model takes the form of a predictive model for the given expert state distribution. During reinforcement learning, the agent predicts the reward based on the difference between the actual state and the state predicted by the internal model. We conducted several experiments in different environments, including Super Mario Bros and Flappy Bird games, and our method successfully trained good policies directly from expert game-play videos.",1
"Sample efficiency is critical in solving real-world reinforcement learning problems, where agent-environment interactions can be costly. Imitation learning from expert advice has proved to be an effective strategy for reducing the number of interactions required to train a policy. Online imitation learning, which interleaves policy evaluation and policy optimization, is a particularly effective technique with provable performance guarantees. In this work, we seek to further accelerate the convergence rate of online imitation learning, thereby making it more sample efficient. We propose two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based on solving variational inequalities and MoBIL-Prox based on stochastic first-order updates. These two methods leverage a model to predict future gradients to speed up policy learning. When the model oracle is learned online, these algorithms can provably accelerate the best known convergence rate up to an order. Our algorithms can be viewed as a generalization of stochastic Mirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style analysis of performance.",0
"When dealing with reinforcement learning problems in the real world, it's important to be efficient in order to avoid costly interactions between agents and their environment. To reduce the number of interactions needed to train a policy, imitation learning from an expert is a useful strategy. Online imitation learning, which combines policy evaluation and optimization, is a particularly effective technique with guaranteed performance. The aim of this study is to further improve the efficiency of online imitation learning by proposing two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction. MoBIL-VI solves variational inequalities, while MoBIL-Prox uses stochastic first-order updates. Both methods use a model to predict future gradients and accelerate policy learning. When the model oracle is learned online, these algorithms can improve the convergence rate up to an order. Our algorithms are similar to stochastic Mirror-Prox and can be easily analyzed for performance using FTL-style analysis.",1
"Predicting movement of objects while the action of learning agent interacts with the dynamics of the scene still remains a key challenge in robotics. We propose a multi-layer Long Short Term Memory (LSTM) autoendocer network that predicts future frames for a robot navigating in a dynamic environment with moving obstacles. The autoencoder network is composed of a state and action conditioned decoder network that reconstructs the future frames of video, conditioned on the action taken by the agent. The input image frames are first transformed into low dimensional feature vectors with a pre-trained encoder network and then reconstructed with the LSTM autoencoder network to generate the future frames. A virtual environment, based on the OpenAi-Gym framework for robotics, is used to gather training data and test the proposed network. The initial experiments show promising results indicating that these predicted frames can be used by an appropriate reinforcement learning framework in future to navigate around dynamic obstacles.",0
"The challenge of forecasting the movement of objects in the presence of a learning agent that interacts with the scene dynamics remains a major obstacle in robotics. Our proposed solution involves a multi-layered Long Short-Term Memory (LSTM) autoencoder network that can predict future frames for a robot navigating through a dynamic environment with obstacles that move. This autoencoder network includes a state and action-conditioned decoder network that can reconstruct video frames of the future based on the actions taken by the agent. Initially, the input image frames are transformed into low-dimensional feature vectors using a pre-trained encoder network, which are then reconstructed using the LSTM autoencoder network to generate the future frames. We gather training data and test our proposed network using a virtual environment based on the OpenAI-Gym framework for robotics. Our initial experiments show that these predicted frames can be used by an appropriate reinforcement learning framework in the future to navigate around dynamic obstacles with promising results.",1
"E-learning systems are capable of providing more adaptive and efficient learning experiences for students than the traditional classroom setting. A key component of such systems is the learning strategy, the algorithm that designs the learning paths for students based on information such as the students' current progresses, their skills, learning materials, and etc. In this paper, we address the problem of finding the optimal learning strategy for an E-learning system. To this end, we first develop a model for students' hierarchical skills in the E-learning system. Based on the hierarchical skill model and the classical cognitive diagnosis model, we further develop a framework to model various proficiency levels of hierarchical skills. The optimal learning strategy on top of the hierarchical structure is found by applying a model-free reinforcement learning method, which does not require information on students' learning transition process. The effectiveness of the proposed framework is demonstrated via numerical experiments.",0
"E-learning systems offer students a more adaptive and efficient learning experience compared to traditional classrooms. One of the crucial components of these systems is the learning strategy, which designs the learning paths for students based on their progress, skills, learning materials, and other relevant information. This paper focuses on finding the best learning strategy for an E-learning system. To achieve this, we first introduce a model for students' hierarchical skills within the system. We then use this model, along with the classical cognitive diagnosis model, to develop a framework that models various levels of hierarchical skills proficiency. The optimal learning strategy is determined by using a model-free reinforcement learning method, which does not require information on students' learning transition process. Numerical experiments demonstrate the effectiveness of our proposed framework.",1
"Exploration is a difficult challenge in reinforcement learning and is of prime importance in sparse reward environments. However, many of the state of the art deep reinforcement learning algorithms, that rely on epsilon-greedy, fail on these environments. In such cases, empowerment can serve as an intrinsic reward signal to enable the agent to maximize the influence it has over the near future. We formulate empowerment as the channel capacity between states and actions and is calculated by estimating the mutual information between the actions and the following states. The mutual information is estimated using Mutual Information Neural Estimator and a forward dynamics model. We demonstrate that an empowerment driven agent is able to improve significantly the score of a baseline DQN agent on the game of Montezuma's Revenge.",0
"The challenge of exploration is a significant obstacle in reinforcement learning, particularly when dealing with sparse reward environments. Despite this, many advanced deep reinforcement learning algorithms that rely on epsilon-greedy techniques struggle in these settings. In such instances, empowerment can function as an intrinsic reward signal, allowing the agent to maximize its influence over the near future. Our approach defines empowerment as the channel capacity between states and actions, which is calculated by estimating the mutual information between actions and subsequent states. We use a Mutual Information Neural Estimator and a forward dynamics model to estimate mutual information. Our results demonstrate that an empowerment-based agent significantly improves upon a baseline DQN agent's scores in the game of Montezuma's Revenge.",1
"The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent.",0
"The geometry of a weighted graph can be represented succinctly by the smallest eigenvectors of the graph Laplacian. This approach shows promise in state representation learning for reinforcement learning (RL), where the weighted graph can be interpreted as the state transition process induced by a behavior policy in the environment. However, the existing methods for approximating the eigenvectors of the Laplacian are not suitable for general RL settings due to their computational cost and lack of justification beyond finite-state settings. This paper presents a scalable and fully general method for approximating the eigenvectors of the Laplacian in a model-free RL context. The method is evaluated systematically and shows better performance than previous proposals even in tabular, finite-state settings. Furthermore, the Laplacian representation learned using this method has potential benefits in goal-achieving RL tasks, demonstrating significant improvement in the performance of an RL agent.",1
"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.",0
"In robotics and control applications, the goal of state representation learning is to acquire condensed representations from unprocessed observations. Various techniques such as auto-encoders, inverse dynamics, learning forward models, and utilizing general priors on state attributes are employed for this purpose. Nonetheless, due to the broad range of practices and strategies used in this domain, there is a lack of standardized benchmarks, evaluation criteria, and assignments. This paper presents a collection of settings, data generators, metrics, and utilities to simplify the iterative process of state representation learning and evaluation in reinforcement learning contexts.",1
"We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep Quality-Value (DQV) Learning. DQV uses temporal-difference learning to train a Value neural network and uses this network for training a second Quality-value network that learns to estimate state-action values. We first test DQV's update rules with Multilayer Perceptrons as function approximators on two classic RL problems, and then extend DQV with the use of Deep Convolutional Neural Networks, `Experience Replay' and `Target Neural Networks' for tackling four games of the Atari Arcade Learning environment. Our results show that DQV learns significantly faster and better than Deep Q-Learning and Double Deep Q-Learning, suggesting that our algorithm can potentially be a better performing synchronous temporal difference algorithm than what is currently present in DRL.",0
"The Deep Quality-Value (DQV) Learning algorithm is a new approach to Deep Reinforcement Learning (DRL). It utilizes temporal-difference learning to train a Value neural network, which is then used to train a second network that estimates state-action values. To test DQV's efficacy, we applied it to two classic RL problems using Multilayer Perceptrons as function approximators. We then extended DQV using Deep Convolutional Neural Networks, 'Experience Replay', and 'Target Neural Networks' to tackle four games of the Atari Arcade Learning environment. Our findings demonstrate that DQV outperforms Deep Q-Learning and Double Deep Q-Learning in terms of speed and accuracy, indicating that it could potentially become the leading synchronous temporal difference algorithm in DRL.",1
"Most existing deep reinforcement learning (DRL) frameworks consider either discrete action space or continuous action space solely. Motivated by applications in computer games, we consider the scenario with discrete-continuous hybrid action space. To handle hybrid action space, previous works either approximate the hybrid space by discretization, or relax it into a continuous set. In this paper, we propose a parametrized deep Q-network (P- DQN) framework for the hybrid action space without approximation or relaxation. Our algorithm combines the spirits of both DQN (dealing with discrete action space) and DDPG (dealing with continuous action space) by seamlessly integrating them. Empirical results on a simulation example, scoring a goal in simulated RoboCup soccer and the solo mode in game King of Glory (KOG) validate the efficiency and effectiveness of our method.",0
"Current deep reinforcement learning (DRL) frameworks typically focus on either a discrete action space or a continuous action space. However, our study is inspired by computer game applications and explores the potential of a scenario with a hybrid action space that is both discrete and continuous. Past research on hybrid action space has involved approximating it through discretization or relaxation into a continuous set. Our paper proposes a parametrized deep Q-network (P-DQN) framework that deals with the hybrid action space without approximation or relaxation. Our algorithm draws on the strengths of both DQN (for discrete action space) and DDPG (for continuous action space) and seamlessly integrates them. The effectiveness and efficiency of our method are validated through empirical results from a simulation example, scoring a goal in a simulated RoboCup soccer match, and the solo mode in King of Glory (KOG) game.",1
"The enactive approach to cognition is typically proposed as a viable alternative to traditional cognitive science. Enactive cognition displaces the explanatory focus from the internal representations of the agent to the direct sensorimotor interaction with its environment. In this paper, we investigate enactive learning through means of artificial agent simulations. We compare the performances of the enactive agent to an agent operating on classical reinforcement learning in foraging tasks within maze environments. The characteristics of the agents are analysed in terms of the accessibility of the environmental states, goals, and exploration/exploitation tradeoffs. We confirm that the enactive agent can successfully interact with its environment and learn to avoid unfavourable interactions using intrinsically defined goals. The performance of the enactive agent is shown to be limited by the number of affordable actions.",0
"The enactive approach to cognition is often suggested as an alternative to traditional cognitive science, as it shifts the focus away from an agent's internal representations to its direct sensorimotor interaction with the environment. This study examines enactive learning using artificial agent simulations and compares its performance to classical reinforcement learning in foraging tasks within maze environments. The agents' characteristics are evaluated based on the accessibility of environmental states, goals, and exploration/exploitation tradeoffs. The results show that the enactive agent can successfully interact with its environment and avoid unfavorable interactions using intrinsically defined goals. However, the agent's performance is limited by the number of affordable actions.",1
"Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt the instructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model-based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.",0
"An assistive system is being considered to guide visually impaired users to their destination through speech and haptic feedback. However, existing navigation technologies operate in a generic manner, which may lead to confusion and errors. To address this issue, a novel model-based reinforcement learning framework is proposed to personalize the system's interaction with different end-users with diverse mobility skills. The proposed approach uses a weighted experts model to address data-efficiency limitations in transfer learning with deep models when incrementally adapting the system to new users. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for more accurate long-term human behavior prediction and quick adaptation at the onset of learning when data is limited.",1
"In statistical modelling the biggest threat is concept drift which makes the model gradually showing deteriorating performance over time. There are state of the art methodologies to detect the impact of concept drift, however general strategy considered to overcome the issue in performance is to rebuild or re-calibrate the model periodically as the variable patterns for the model changes significantly due to market change or consumer behavior change etc. Quantitative research is the most widely spread application of data science in Marketing or financial domain where applicability of state of the art reinforcement learning for auto-learning is less explored paradigm. Reinforcement learning is heavily dependent on having a simulated environment which is majorly available for gaming or online systems, to learn from the live feedback. However, there are some research happened on the area of online advertisement, pricing etc where due to the nature of the online learning environment scope of reinforcement learning is explored. Our proposed solution is a reinforcement learning based, true self-learning algorithm which can adapt to the data change or concept drift and auto learn and self-calibrate for the new patterns of the data solving the problem of concept drift.   Keywords - Reinforcement learning, Genetic Algorithm, Q-learning, Classification modelling, CMA-ES, NES, Multi objective optimization, Concept drift, Population stability index, Incremental learning, F1-measure, Predictive Modelling, Self-learning, MCTS, AlphaGo, AlphaZero",0
"The primary concern in statistical modelling is concept drift, which causes the model's performance to gradually decline over time. Although there are advanced techniques to detect the impact of concept drift, the general solution to mitigate performance issues is to periodically rebuild or re-calibrate the model when variable patterns change significantly due to market or consumer behavior changes. Quantitative research is the most popular application of data science in financial and marketing domains, where the applicability of reinforcement learning for auto-learning is less explored. Reinforcement learning relies on a simulated environment, primarily available for gaming or online systems, to learn from live feedback. However, some research has been conducted on online advertisement, pricing, and other areas where the scope of reinforcement learning is explored due to the nature of the online learning environment. Our proposed solution is a true self-learning algorithm based on reinforcement learning that can adapt to data changes or concept drift, auto-learn, and self-calibrate for new data patterns, solving the problem of concept drift. The proposed algorithm employs genetic algorithms, Q-learning, classification modelling, CMA-ES, NES, multi-objective optimization, population stability index, incremental learning, F1-measure, predictive modelling, MCTS, AlphaGo, and AlphaZero.",1
"In this paper we obtain several informative error bounds on function approximation for the policy evaluation algorithm proposed by Basu et al. when the aim is to find the risk-sensitive cost represented using exponential utility. The main idea is to use classical Bapat's inequality and to use Perron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to get the new bounds. The novelty of our approach is that we use the irreduciblity of Markov chain to get the new bounds whereas the earlier work by Basu et al. used spectral variation bound which is true for any matrix. We also give examples where all our bounds achieve the ""actual error"" whereas the earlier bound given by Basu et al. is much weaker in comparison. We show that this happens due to the absence of difference term in the earlier bound which is always present in all our bounds when the state space is large. Additionally, we discuss how all our bounds compare with each other. As a corollary of our main result we provide a bound between largest eigenvalues of two irreducibile matrices in terms of the matrix entries.",0
"This paper presents informative error bounds for function approximation in the policy evaluation algorithm proposed by Basu et al. Specifically, we aim to find the risk-sensitive cost represented using exponential utility. Our approach involves using classical Bapat's inequality and Perron-Frobenius eigenvectors, which are only available if we assume an irreducible Markov chain. This is a novel approach, as previous work by Basu et al. used spectral variation bound, which is true for any matrix. Our bounds achieve the ""actual error"" in certain examples, whereas the earlier bound given by Basu et al. is weaker. This is due to the absence of a difference term in their bound, which is present in all of ours when the state space is large. We also compare all of our bounds with each other, and as a corollary of our main result, we provide a bound between the largest eigenvalues of two irreducible matrices in terms of the matrix entries.",1
"Model-free approaches for reinforcement learning (RL) and continuous control find policies based only on past states and rewards, without fitting a model of the system dynamics. They are appealing as they are general purpose and easy to implement; however, they also come with fewer theoretical guarantees than model-based RL. In this work, we present a new model-free algorithm for controlling linear quadratic (LQ) systems, and show that its regret scales as $O(T^{\xi+2/3})$ for any small $\xi>0$ if time horizon satisfies $T>C^{1/\xi}$ for a constant $C$. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem. In practice, it corresponds to a variant of policy iteration with forced exploration, where the policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that provably achieves sublinear regret and has a polynomial computation cost. Empirically, our algorithm dramatically outperforms standard policy iteration, but performs worse than a model-based approach.",0
"Reinforcement learning (RL) and continuous control can be achieved using model-free approaches that rely solely on past states and rewards, without the need for a model of the system dynamics. Although these methods are desirable due to their versatility and ease of implementation, they do not offer as many theoretical guarantees as model-based RL. This study presents a novel model-free algorithm for controlling linear quadratic (LQ) systems, which demonstrates regret scaling as $O(T^{\xi+2/3})$ for any small $\xi>0$ if the time horizon satisfies $T>C^{1/\xi}$ for a constant $C$. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem, and in practice, it corresponds to a policy iteration variant with forced exploration. The policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that can achieve sublinear regret and has a polynomial computation cost. Our algorithm outperforms standard policy iteration in empirical testing, but is not as effective as a model-based approach.",1
"Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.",0
"The use of hierarchical reinforcement learning (HRL) is a promising method to solve more complex tasks beyond traditional reinforcement learning (RL) techniques. However, current HRL methods require task-specific design and on-policy training, making them challenging to apply in real-world situations. In this study, we explore how we can develop HRL algorithms that are both general and efficient, making them suitable for real-world problems like robotic control. Our approach involves supervising lower-level controllers with automatically learned and proposed goals from higher-level controllers, and using off-policy experience for both higher and lower-level training. This presents a significant challenge, but we introduce an off-policy correction to address it, allowing us to learn both higher- and lower-level policies using fewer environment interactions than on-policy algorithms. Our resulting HRL agent, HIRO, is generally applicable and highly sample-efficient, as demonstrated through experiments that show it can learn complex behaviors for simulated robots with only a few million samples, equivalent to a few days of real-time interaction. Comparisons with prior HRL methods reveal that our approach outperforms previous state-of-the-art techniques.",1
"Recent analyses of certain gradient descent optimization methods have shown that performance can degrade in some settings - such as with stochasticity or implicit momentum. In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient. As an agent improves over time, the optimization target changes and thus the loss landscape (and local optima) change. Due to the failure modes of those methods, the ideal choice of optimizer for Deep RL remains unclear. As such, we provide an empirical analysis of the effects that a wide range of gradient descent optimizers and their hyperparameters have on policy gradient methods, a subset of Deep RL algorithms, for benchmark continuous control tasks. We find that adaptive optimizers have a narrow window of effective learning rates, diverging in other cases, and that the effectiveness of momentum varies depending on the properties of the environment. Our analysis suggests that there is significant interplay between the dynamics of the environment and Deep RL algorithm properties which aren't necessarily accounted for by traditional adaptive gradient methods. We provide suggestions for optimal settings of current methods and further lines of research based on our findings.",0
"Analyses have revealed that certain gradient descent optimization techniques may not perform well in certain scenarios, such as those involving stochasticity or implicit momentum. These methods are commonly employed in Deep RL to train neural networks through temporal difference error or policy gradient. However, the optimization target changes as the agent improves over time, leading to a shift in the loss landscape and local optima. Due to the limitations of these methods, the best optimizer for Deep RL is uncertain. To address this, we conducted an empirical analysis of how various gradient descent optimizers and their hyperparameters affect policy gradient methods, a type of Deep RL algorithm, in benchmark continuous control tasks. Our results indicate that adaptive optimizers have a limited range of effective learning rates and diverge in other cases. Additionally, the effectiveness of momentum varies depending on the environment's properties. Our findings suggest that traditional adaptive gradient methods do not fully account for the interplay between the environment's dynamics and Deep RL algorithm properties. We provide recommendations for optimal settings and potential areas for further research.",1
"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.",0
"Recent advances in deep learning have led to the success of model-free reinforcement learning (RL) methods in many tasks. However, these methods often have high sample complexity, making them less applicable in real-world domains. On the other hand, model-based reinforcement learning can reduce sample complexity but requires careful tuning and has only been successful in restrictive domains. In this study, we investigate the behavior of model-based RL methods when deep neural networks are used to learn both the model and the policy. Our findings reveal that the learned policy exploits regions where the model has insufficient data, leading to instability during training. To address this issue, we propose the use of an ensemble of models to maintain model uncertainty and regularize the learning process. We also demonstrate that using likelihood ratio derivatives is more stable than backpropagation through time. Overall, our approach, Model-Ensemble Trust-Region Policy Optimization (ME-TRPO), significantly reduces sample complexity compared to model-free deep RL methods in challenging continuous control benchmark tasks.",1
"Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cartpole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation are compared in this context with the standard LQR solution. Further, we propose a novel approach to integrate RL and swing-up controllers.",0
"The task of designing controllers that are optimal remains difficult due to the increasing complexity and inherent nonlinearity of systems. The main benefit of reinforcement learning (RL) is its capacity to extract an optimal control strategy by learning from interactions with the environment. This study investigates how RL can be applied to control the cartpole dynamical system, a recognized benchmark, without prior knowledge of its dynamics. The study compares RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation to the traditional LQR solution. Additionally, the study proposes a new method for integrating RL and swing-up controllers.",1
"In an RF-powered backscatter cognitive radio network, multiple secondary users communicate with a secondary gateway by backscattering or harvesting energy and actively transmitting their data depending on the primary channel state. To coordinate the transmission of multiple secondary transmitters, the secondary gateway needs to schedule the backscattering time, energy harvesting time, and transmission time among them. However, under the dynamics of the primary channel and the uncertainty of the energy state of the secondary transmitters, it is challenging for the gateway to find a time scheduling mechanism which maximizes the total throughput. In this paper, we propose to use the deep reinforcement learning algorithm to derive an optimal time scheduling policy for the gateway. Specifically, to deal with the problem with large state and action spaces, we adopt a Double Deep-Q Network (DDQN) that enables the gateway to learn the optimal policy. The simulation results clearly show that the proposed deep reinforcement learning algorithm outperforms non-learning schemes in terms of network throughput.",0
"A network of secondary users communicates with a secondary gateway in an RF-powered backscatter cognitive radio system. These users backscatter or harvest energy and transmit their data depending on the state of the primary channel. To manage multiple secondary transmitters, the gateway schedules their backscattering, energy harvesting, and transmission times. However, the primary channel's dynamics and uncertainty in the secondary transmitter's energy state make it difficult to maximize throughput with an optimal time scheduling mechanism. This paper proposes using the deep reinforcement learning algorithm to derive an optimal policy for the gateway, using a Double Deep-Q Network (DDQN) to handle large state and action spaces. Simulation results show that this algorithm yields higher network throughput than non-learning schemes.",1
"Many recent algorithms for reinforcement learning are model-free and founded on the Bellman equation. Here we present a method founded on the costate equation and models of the state dynamics. We use the costate -- the gradient of cost with respect to state -- to improve the policy and also to ""focus"" the model, training it to detect and mimic those features of the environment that are most relevant to its task. We show that this method can handle difficult time-optimal control problems, driving deterministic or stochastic mechanical systems quickly to a target. On these tasks it works well compared to deep deterministic policy gradient, a recent Bellman method. And because it creates a model, the costate method can also learn from mental practice.",0
"Several recent reinforcement learning algorithms rely on the Bellman equation and are model-free. However, our approach is based on the costate equation and involves models of state dynamics. By utilizing the costate - the gradient of cost concerning state - we can enhance the policy and direct the model to focus on the most crucial environmental features for the task at hand. Our method can handle challenging time-optimal control problems, efficiently steering deterministic or stochastic mechanical systems towards a set target. Compared to a recent Bellman approach called deep deterministic policy gradient, our method performs equally well. Additionally, since our approach creates a model, it can also learn through mental practice.",1
"Reinforcement learning has shown great potential in generalizing over raw sensory data using only a single neural network for value optimization. There are several challenges in the current state-of-the-art reinforcement learning algorithms that prevent them from converging towards the global optima. It is likely that the solution to these problems lies in short- and long-term planning, exploration and memory management for reinforcement learning algorithms. Games are often used to benchmark reinforcement learning algorithms as they provide a flexible, reproducible, and easy to control environment. Regardless, few games feature a state-space where results in exploration, memory, and planning are easily perceived. This paper presents The Dreaming Variational Autoencoder (DVAE), a neural network based generative modeling architecture for exploration in environments with sparse feedback. We further present Deep Maze, a novel and flexible maze engine that challenges DVAE in partial and fully-observable state-spaces, long-horizon tasks, and deterministic and stochastic problems. We show initial findings and encourage further work in reinforcement learning driven by generative exploration.",0
"Using a single neural network for value optimization, reinforcement learning has demonstrated significant potential in generalizing over raw sensory data. However, there are various challenges in the current state-of-the-art reinforcement learning algorithms that hinder them from reaching the global optima. Addressing these issues may involve short- and long-term planning, memory management, and exploration for reinforcement learning algorithms. Although games are commonly used to benchmark reinforcement learning algorithms due to their flexible, reproducible, and easy-to-control nature, few games feature a state-space that enables easy perception of exploration, memory, and planning outcomes. This paper introduces The Dreaming Variational Autoencoder (DVAE), a generative modeling architecture for exploration in environments with sparse feedback, and Deep Maze, a flexible maze engine that challenges DVAE in partial and fully-observable state-spaces, long-horizon tasks, and deterministic and stochastic problems. The paper presents initial findings and encourages further research in reinforcement learning driven by generative exploration.",1
"We study a reinforcement learning setting, where the state transition function is a convex combination of a stochastic continuous function and a deterministic function. Such a setting generalizes the widely-studied stochastic state transition setting, namely the setting of deterministic policy gradient (DPG).   We firstly give a simple example to illustrate that the deterministic policy gradient may be infinite under deterministic state transitions, and introduce a theoretical technique to prove the existence of the policy gradient in this generalized setting. Using this technique, we prove that the deterministic policy gradient indeed exists for a certain set of discount factors, and further prove two conditions that guarantee the existence for all discount factors. We then derive a closed form of the policy gradient whenever exists. Furthermore, to overcome the challenge of high sample complexity of DPG in this setting, we propose the Generalized Deterministic Policy Gradient (GDPG) algorithm. The main innovation of the algorithm is a new method of applying model-based techniques to the model-free algorithm, the deep deterministic policy gradient algorithm (DDPG). GDPG optimize the long-term rewards of the model-based augmented MDP subject to a constraint that the long-rewards of the MDP is less than the original one.   We finally conduct extensive experiments comparing GDPG with state-of-the-art methods and the direct model-based extension method of DDPG on several standard continuous control benchmarks. Results demonstrate that GDPG substantially outperforms DDPG, the model-based extension of DDPG and other baselines in terms of both convergence and long-term rewards in most environments.",0
"This paper investigates a reinforcement learning scenario where the state transition function is a combination of a stochastic continuous function and a deterministic function, thus generalizing the commonly studied stochastic state transition setting (i.e., deterministic policy gradient or DPG). Initially, we provide a simple example illustrating the infinite nature of DPG under deterministic state transitions, and introduce a theoretical technique to prove the existence of the policy gradient in this extended setting. By applying this technique, we prove that the deterministic policy gradient exists for a specific range of discount factors and establish two conditions that ensure its existence for all discount factors. We also derive a closed-form policy gradient if it exists. However, to address the high sample complexity of DPG in this setting, we propose a new approach called Generalized Deterministic Policy Gradient (GDPG) algorithm that applies model-based techniques to the model-free deep deterministic policy gradient algorithm (DDPG). GDPG optimizes the long-term rewards of the model-based augmented MDP with a constraint that the long-rewards of the MDP are less than the original one. Finally, we perform extensive experiments comparing GDPG with state-of-the-art methods and the direct model-based extension of DDPG on various continuous control benchmarks. Results show that GDPG outperforms DDPG, its model-based extension, and other baselines in terms of both convergence and long-term rewards in most environments.",1
"Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.",0
"In order for reinforcement learning and planning methods to work, an objective or reward function must be established to represent the desired behavior. However, this can be challenging in scenarios where tasks involve visual observations with unknown object positions or deformable objects. Current methods involve creating problem-specific solutions, such as adding sensors to measure a proxy for the objective, but this requires a lot of effort and is not practical for robots to learn complex skills outside of a controlled environment. Our aim is to find a more scalable and general solution for teaching robots in unconstrained environments. We propose a few-shot objective learning problem, where we teach the robot to learn a task objective from a few successful end state images. We suggest a simple solution to this problem by meta-learning a classifier to recognize new goals from just a few examples. We demonstrate how this can be used with both model-free reinforcement learning and visual model-based planning, and provide results from three domains: rope manipulation in a simulated environment, visual navigation in a 3D simulation, and object arrangement on a real robot.",1
"We propose a probabilistic framework to directly insert prior knowledge in reinforcement learning (RL) algorithms by defining the behaviour policy as a Bayesian posterior distribution. Such a posterior combines task specific information with prior knowledge, thus allowing to achieve transfer learning across tasks. The resulting method is flexible and it can be easily incorporated to any standard off-policy and on-policy algorithms, such as those based on temporal differences and policy gradients. We develop a specific instance of this Bayesian transfer RL framework by expressing prior knowledge as general deterministic rules that can be useful in a large variety of tasks, such as navigation tasks. Also, we elaborate more on recent probabilistic and entropy-regularised RL by developing a novel temporal learning algorithm and show how to combine it with Bayesian transfer RL. Finally, we demonstrate our method for solving mazes and show that significant speed ups can be obtained.",0
"Our proposal involves a probabilistic framework that integrates previous knowledge into reinforcement learning (RL) algorithms. We achieve this by defining the behavior policy as a Bayesian posterior distribution, which merges task-specific information with prior knowledge. This approach allows for transfer learning across tasks, making it flexible enough to be incorporated into any standard off-policy or on-policy algorithms. We have created a specific instance of this Bayesian transfer RL framework, where prior knowledge is expressed as general deterministic rules that can be used in various tasks, including navigation tasks. Additionally, we have expanded on recent probabilistic and entropy-regularized RL by developing a new temporal learning algorithm and demonstrating how it can be combined with Bayesian transfer RL. Finally, we showcase the effectiveness of our method in solving mazes and achieving significant speed improvements.",1
"Deep Reinforcement Learning (DRL) has become a powerful strategy to solve complex decision making problems based on Deep Neural Networks (DNNs). However, it is highly data demanding, so unfeasible in physical systems for most applications. In this work, we approach an alternative Interactive Machine Learning (IML) strategy for training DNN policies based on human corrective feedback, with a method called Deep COACH (D-COACH). This approach not only takes advantage of the knowledge and insights of human teachers as well as the power of DNNs, but also has no need of a reward function (which sometimes implies the need of external perception for computing rewards). We combine Deep Learning with the COrrective Advice Communicated by Humans (COACH) framework, in which non-expert humans shape policies by correcting the agent's actions during execution. The D-COACH framework has the potential to solve complex problems without much data or time required. Experimental results validated the efficiency of the framework in three different problems (two simulated, one with a real robot), with state spaces of low and high dimensions, showing the capacity to successfully learn policies for continuous action spaces like in the Car Racing and Cart-Pole problems faster than with DRL.",0
"Utilizing Deep Neural Networks (DNNs), Deep Reinforcement Learning (DRL) has emerged as an effective approach to resolving intricate decision making dilemmas. However, DRL is data-intensive, rendering it impractical for most physical systems. In this study, we propose an Interactive Machine Learning (IML) strategy termed Deep COACH (D-COACH) that leverages human corrective feedback to train DNN policies. This approach capitalizes on both the DNNs' capabilities and the knowledge and insights of human teachers. D-COACH does not require a reward function, which often necessitates external perception for computing rewards. We merge Deep Learning with the COrrective Advice Communicated by Humans (COACH) framework, enabling non-expert humans to correct the agent's actions during execution and shape policies. The D-COACH framework has the potential to solve intricate problems with minimal data or time requirements. In three different problems, including two simulated and one with a real robot, with low and high-dimensional state spaces, the experimental outcomes demonstrated the framework's efficacy, showing faster policy learning in continuous action spaces such as in the Car Racing and Cart-Pole problems, outperforming DRL.",1
"Learning in sparse reward settings remains a challenge in Reinforcement Learning, which is often addressed by using intrinsic rewards. One promising strategy is inspired by human curiosity, requiring the agent to learn to predict the future. In this paper a curiosity-driven agent is extended to use these predictions directly for training. To achieve this, the agent predicts the value function of the next state at any point in time. Subsequently, the consistency of this prediction with the current value function is measured, which is then used as a regularization term in the loss function of the algorithm. Experiments were made on grid-world environments as well as on a 3D navigation task, both with sparse rewards. In the first case the extended agent is able to learn significantly faster than the baselines.",0
"Reinforcement Learning faces a challenge when learning in sparse reward settings, which can be overcome by incorporating intrinsic rewards. A promising approach inspired by human curiosity involves the agent learning to predict the future. This study extends a curiosity-driven agent to incorporate these predictions directly into its training. The agent predicts the value function of the next state at any given time and measures its consistency with the current value function to use as a regularization term in the loss function. Experiments were conducted on grid-world environments and a 3D navigation task, both with sparse rewards. The extended agent showed significantly faster learning than the baselines in the grid-world environment.",1
"Reinforcement learning refers to a group of methods from artificial intelligence where an agent performs learning through trial and error. It differs from supervised learning, since reinforcement learning requires no explicit labels; instead, the agent interacts continuously with its environment. That is, the agent starts in a specific state and then performs an action, based on which it transitions to a new state and, depending on the outcome, receives a reward. Different strategies (e.g. Q-learning) have been proposed to maximize the overall reward, resulting in a so-called policy, which defines the best possible action in each state. Mathematically, this process can be formalized by a Markov decision process and it has been implemented by packages in R; however, there is currently no package available for reinforcement learning. As a remedy, this paper demonstrates how to perform reinforcement learning in R and, for this purpose, introduces the ReinforcementLearning package. The package provides a remarkably flexible framework and is easily applied to a wide range of different problems. We demonstrate its use by drawing upon common examples from the literature (e.g. finding optimal game strategies).",0
"Reinforcement learning is a type of artificial intelligence that utilizes trial and error to facilitate learning. Unlike supervised learning, the agent does not require explicit labels but rather interacts continuously with the environment. Through this process, the agent starts in a specific state, takes an action, transitions to a new state, and depending on the outcome, receives a reward. To maximize the reward, different strategies such as Q-learning have been proposed, which results in a policy that identifies the best possible action in each state. This process can be formalized by a Markov decision process and implemented in R using packages. However, there is currently no package available for reinforcement learning. To address this, the ReinforcementLearning package was introduced, providing a flexible framework for various problems. This paper demonstrates the use of the package through examples from the literature, such as finding optimal game strategies.",1
"Building deep reinforcement learning agents that can generalize and adapt to unseen environments remains a fundamental challenge for AI. This paper describes progresses on this challenge in the context of man-made environments, which are visually diverse but contain intrinsic semantic regularities. We propose a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. We perform experiments in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects. LEAPS outperforms strong baselines that do not explicitly plan using the semantic content.",0
"The challenge of creating deep reinforcement learning agents that can adapt and generalize to unfamiliar environments remains a significant obstacle for AI. This research paper outlines advancements made in addressing this challenge in the context of man-made environments, which feature diverse visual elements but also have regular semantic structures. Our proposed approach, called LEArning and Planning with Semantics (LEAPS), combines both model-based and model-free strategies. LEAPS includes a multi-target sub-policy that operates on visual inputs, as well as a Bayesian model that focuses on semantic structures. When placed in an unfamiliar environment, the agent uses the semantic model to make informed decisions, suggests the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. To test our approach, we conducted experiments in visual navigation tasks using House3D, a 3D environment that features human-designed indoor scenes with real-world objects. Our results show that LEAPS outperforms other strong baselines that do not explicitly factor in semantic content during planning.",1
"This paper proposes an exploration method for deep reinforcement learning based on parameter space noise. Recent studies have experimentally shown that parameter space noise results in better exploration than the commonly used action space noise. Previous methods devised a way to update the diagonal covariance matrix of a noise distribution and did not consider the direction of the noise vector and its correlation. In addition, fast updates of the noise distribution are required to facilitate policy learning. We propose a method that deforms the noise distribution according to the accumulated returns and the noises that have led to the returns. Moreover, this method switches isotropic exploration and directional exploration in parameter space with regard to obtained rewards. We validate our exploration strategy in the OpenAI Gym continuous environments and modified environments with sparse rewards. The proposed method achieves results that are competitive with a previous method at baseline tasks. Moreover, our approach exhibits better performance in sparse reward environments by exploration with the switching strategy.",0
"A technique for deep reinforcement learning exploration utilizing parameter space noise is introduced in this paper. Research has indicated that parameter space noise leads to better exploration than action space noise. However, previous approaches neglected the direction and correlation of the noise vector and only focused on updating the diagonal covariance matrix of a noise distribution. Additionally, rapid updates of the noise distribution are essential for policy learning. To address these issues, we propose a method that alters the noise distribution based on the accumulated returns and the noises that led to the returns. Our method also switches between isotropic and directional exploration in parameter space based on achieved rewards. We demonstrate the effectiveness of our exploration strategy in continuous environments of OpenAI Gym and modified sparse reward environments. The proposed method achieves comparable results to a prior approach in standard tasks and outperforms it in sparse reward environments via the switching strategy.",1
"Modern vision-based reinforcement learning techniques often use convolutional neural networks (CNN) as universal function approximators to choose which action to take for a given visual input. Until recently, CNNs have been treated like black-box functions, but this mindset is especially dangerous when used for control in safety-critical settings. In this paper, we present our extensions of CNN visualization algorithms to the domain of vision-based reinforcement learning. We use a simulated drone environment as an example scenario. These visualization algorithms are an important tool for behavior introspection and provide insight into the qualities and flaws of trained policies when interacting with the physical world. A video may be seen at https://sites.google.com/view/drlvisual .",0
"To select an action based on visual input, contemporary reinforcement learning methods rely on convolutional neural networks (CNNs) as universal function approximators. However, CNNs were previously viewed as black-box functions, a potentially hazardous approach for safety-critical control scenarios. Our study focuses on extending CNN visualization algorithms for vision-based reinforcement learning in a simulated drone environment. These visualization algorithms allow for introspection of behavior and highlight the strengths and weaknesses of trained policies in real-world interactions. A video demonstration is available at https://sites.google.com/view/drlvisual.",1
"Anderson acceleration is an old and simple method for accelerating the computation of a fixed point. However, as far as we know and quite surprisingly, it has never been applied to dynamic programming or reinforcement learning. In this paper, we explain briefly what Anderson acceleration is and how it can be applied to value iteration, this being supported by preliminary experiments showing a significant speed up of convergence, that we critically discuss. We also discuss how this idea could be applied more generally to (deep) reinforcement learning.",0
"Although Anderson acceleration has been a long-standing and straightforward approach to speed up computing a fixed point, it has never been utilized in dynamic programming or reinforcement learning, which is quite unexpected. This paper gives a brief overview of Anderson acceleration and demonstrates how it can be implemented in value iteration. The effectiveness of this approach is supported by initial experiments that show a considerable improvement in convergence speed, which we analyze critically. We also explore the potential application of this method in (deep) reinforcement learning.",1
"Low precision networks in the reinforcement learning (RL) setting are relatively unexplored because of the limitations of binary activations for function approximation. Here, in the discrete action ATARI domain, we demonstrate, for the first time, that low precision policy distillation from a high precision network provides a principled, practical way to train an RL agent. As an application, on 10 different ATARI games, we demonstrate real-time end-to-end game playing on low-power neuromorphic hardware by converting a sequence of game frames into discrete actions.",0
"Due to the constraints of binary activations in function approximation, there has been little investigation into low precision networks for reinforcement learning (RL). However, we have achieved a breakthrough in the discrete action ATARI domain by showing that low precision policy distillation derived from a high precision network is a viable and systematic method for training an RL agent. Our demonstration involves real-time gameplay on low-power neuromorphic hardware, where we convert game frames into discrete actions across 10 different ATARI games.",1
"In the real world, agents often have to operate in situations with incomplete information, limited sensing capabilities, and inherently stochastic environments, making individual observations incomplete and unreliable. Moreover, in many situations it is preferable to delay a decision rather than run the risk of making a bad decision. In such situations it is necessary to aggregate information before taking an action; however, most state of the art reinforcement learning (RL) algorithms are biased towards taking actions \textit{at every time step}, even if the agent is not particularly confident in its chosen action. This lack of caution can lead the agent to make critical mistakes, regardless of prior experience and acclimation to the environment. Motivated by theories of dynamic resolution of uncertainty during decision making in biological brains, we propose a simple accumulator module which accumulates evidence in favor of each possible decision, encodes uncertainty as a dynamic competition between actions, and acts on the environment only when it is sufficiently confident in the chosen action. The agent makes no decision by default, and the burden of proof to make a decision falls on the policy to accrue evidence strongly in favor of a single decision. Our results show that this accumulator module achieves near-optimal performance on a simple guessing game, far outperforming deep recurrent networks using traditional, forced action selection policies.",0
"In situations where agents operate with incomplete information, limited sensing capabilities, and stochastic environments, individual observations can be unreliable. Sometimes, delaying a decision is preferred to making a bad decision. However, most state of the art reinforcement learning algorithms tend to bias towards taking actions at every time step, even if the agent lacks confidence in its chosen action. This recklessness can lead to critical mistakes, despite prior experience and acclimation to the environment. Inspired by theories of dynamic resolution of uncertainty in biological brains, we propose an accumulator module that accumulates evidence for each possible decision, encodes uncertainty as a dynamic competition between actions, and acts on the environment only when sufficiently confident in the chosen action. The agent does not make decisions by default, and the policy must accrue evidence strongly in favor of a single decision. Our results show that this accumulator module performs near-optimally on a simple guessing game, outperforming deep recurrent networks with traditional forced action selection policies.",1
"Epistasis (gene-gene interaction) is crucial to predicting genetic disease. Our work tackles the computational challenges faced by previous works in epistasis detection by modeling it as a one-step Markov Decision Process where the state is genome data, the actions are the interacted genes, and the reward is an interaction measurement for the selected actions. A reinforcement learning agent using policy gradient method then learns to discover a set of highly interacted genes.",0
"The importance of gene-gene interaction, or epistasis, in the prediction of genetic disease is significant. Our research addresses the computational hurdles previously encountered in detecting epistasis by formulating it as a one-step Markov Decision Process. In this process, the genome data represents the state, the interacted genes serve as the actions, and the reward is the measurement of the interaction for the chosen actions. To identify a group of genes with high interaction, our approach involves a reinforcement learning agent that employs the policy gradient method.",1
"Modern information technology services largely depend on cloud infrastructures to provide their services. These cloud infrastructures are built on top of datacenter networks (DCNs) constructed with high-speed links, fast switching gear, and redundancy to offer better flexibility and resiliency. In this environment, network traffic includes long-lived (elephant) and short-lived (mice) flows with partitioned and aggregated traffic patterns. Although SDN-based approaches can efficiently allocate networking resources for such flows, the overhead due to network reconfiguration can be significant. With limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in an OpenFlow enabled switch, it is crucial to determine which forwarding rules should remain in the flow table, and which rules should be processed by the SDN controller in case of a table-miss on the SDN switch. This is needed in order to obtain the flow entries that satisfy the goal of reducing the long-term control plane overhead introduced between the controller and the switches. To achieve this goal, we propose a machine learning technique that utilizes two variations of reinforcement learning (RL) algorithms-the first of which is traditional reinforcement learning algorithm based while the other is deep reinforcement learning based. Emulation results using the RL algorithm show around 60% improvement in reducing the long-term control plane overhead, and around 14% improvement in the table-hit ratio compared to the Multiple Bloom Filters (MBF) method given a fixed size flow table of 4KB.",0
"Cloud infrastructures are integral to modern information technology services and rely on datacenter networks (DCNs) with high-speed links and redundancy for flexibility and resiliency. Traffic on these networks includes both long-lived and short-lived flows, with a mix of partitioned and aggregated traffic patterns. While SDN-based approaches can effectively allocate networking resources for these flows, the overhead of network reconfiguration can be significant. Therefore, it is crucial to determine which forwarding rules should remain in the flow table and which should be processed by the SDN controller. To minimize control plane overhead, we propose a machine learning technique utilizing two variations of reinforcement learning algorithms. The results show a 60% improvement in reducing long-term control plane overhead and a 14% improvement in table-hit ratio compared to the Multiple Bloom Filters method when using a fixed-size flow table of 4KB.",1
"Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.",0
"Our lives are increasingly impacted by autonomous cyber-physical agents and systems. To ensure that these agents act in accordance with societal values, it is necessary to develop techniques that allow them to learn and follow implicit constraints, in addition to maximizing their rewards in a given environment. These constraints may arise from various sources such as ethical principles, social norms, regulations, laws, and business process guidelines. We propose a new method that involves using inverse reinforcement learning to learn unspecified constraints from task demonstrations, and reinforcement learning to maximize environmental rewards. The agent observes behavior traces of society members, but has no explicit access to the set of constraints that govern the behavior. The learned constraints are combined with a value function through a contextual bandit-based orchestrator that selects the most suitable choice between the two policies (constraint-based and environment reward-based) when taking actions. This orchestrator allows the agent to combine policies in innovative ways, selecting the best actions from either a reward-maximizing or constrained policy. Additionally, it is clear which policy is being employed at each time step. We demonstrate the effectiveness of our approach by testing it on a Pac-Man domain, showing that the agent can learn to act optimally while adhering to the demonstrated constraints, and mixing these two functions in complex ways.",1
"In this paper, we investigate a new form of automated curriculum learning based on adaptive selection of accuracy requirements, called accuracy-based curriculum learning. Using a reinforcement learning agent based on the Deep Deterministic Policy Gradient algorithm and addressing the Reacher environment, we first show that an agent trained with various accuracy requirements sampled randomly learns more efficiently than when asked to be very accurate at all times. Then we show that adaptive selection of accuracy requirements, based on a local measure of competence progress, automatically generates a curriculum where difficulty progressively increases, resulting in a better learning efficiency than sampling randomly.",0
"The focus of our study is accuracy-based curriculum learning, a novel form of automated curriculum learning that involves selecting accuracy requirements adaptively. Our research employs a reinforcement learning agent that follows the Deep Deterministic Policy Gradient algorithm and deals with the Reacher environment. Our findings reveal that randomly sampled accuracy requirements facilitate more efficient learning than always striving for high accuracy. We also demonstrate that the selection of accuracy requirements can be adjusted automatically, based on a local measure of competence progress, to create a curriculum in which the level of difficulty escalates progressively. This approach produces better learning efficiency than random sampling.",1
"We consider the problem of reinforcement learning under safety requirements, in which an agent is trained to complete a given task, typically formalized as the maximization of a reward signal over time, while concurrently avoiding undesirable actions or states, associated to lower rewards, or penalties. The construction and balancing of different reward components can be difficult in the presence of multiple objectives, yet is crucial for producing a satisfying policy. For example, in reaching a target while avoiding obstacles, low collision penalties can lead to reckless movements while high penalties can discourage exploration. To circumvent this limitation, we examine the effect of past actions in terms of safety to estimate which are acceptable or should be avoided in the future. We then actively reshape the action space of the agent during reinforcement learning, so that reward-driven exploration is constrained within safety limits. We propose an algorithm enabling the learning of such safety constraints in parallel with reinforcement learning and demonstrate its effectiveness in terms of both task completion and training time.",0
"The problem of reinforcement learning with safety requirements is our focus, where an agent is trained to complete a given task while avoiding undesirable actions or states. The objective is typically to maximize a reward signal over time, but the construction and balancing of different reward components can be challenging with multiple objectives. For example, low collision penalties can lead to reckless movements, while high penalties can discourage exploration. We propose an algorithm that examines the safety of past actions and actively reshapes the action space of the agent during reinforcement learning. This approach constrains reward-driven exploration within safety limits, allowing for effective task completion and reduced training time.",1
"In reinforcement learning (RL) , one of the key components is policy evaluation, which aims to estimate the value function (i.e., expected long-term accumulated reward) of a policy. With a good policy evaluation method, the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous \emph{Gradient-based Temporal Difference(GTD)} policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming, a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated, previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting them into convex-concave saddle point problems. However, it is well-known that, the data are generated from Markov processes rather than i.i.d. in RL problems.. In this paper, in the realistic Markov setting, we derive the finite sample bounds for the general convex-concave saddle point problems, and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size, GTD algorithms converge. (2) The convergence rate is determined by the step size, with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix, the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process. To the best of our knowledge, our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.",0
"Policy evaluation is a crucial element in reinforcement learning (RL), as it aims to estimate the expected long-term accumulated reward of a policy. The accuracy of the value function estimation can impact the effectiveness of the RL algorithms and the policy identified. When the state space is vast or continuous, Gradient-based Temporal Difference (GTD) algorithms using linear function approximation are commonly utilized. However, given the time and reward costs associated with data collection, understanding the finite sample performance of policy evaluation algorithms is critical. Prior studies have provided finite sample analysis of GTD algorithms with constant step size on the assumption of i.i.d. generated data, converted to convex-concave saddle point problems. Nonetheless, data in RL problems are typically generated by Markov processes. This paper introduces finite sample bounds for the general convex-concave saddle point problems, and consequently, for GTD algorithms in a realistic Markov setting. The results indicate that GTD algorithms converge with varying step sizes, and the convergence rate is determined by the step size and mixing time of the Markov process. Furthermore, the study demonstrates the effectiveness of the experience replay trick in enhancing the mixing property of the Markov process and its impact on GTD algorithm convergence. This analysis is the first to provide finite sample bounds for GTD algorithms in a Markov setting.",1
"Q-learning is one of the most popular methods in Reinforcement Learning (RL). Transfer Learning aims to utilize the learned knowledge from source tasks to help new tasks to improve the sample complexity of the new tasks. Considering that data collection in RL is both more time and cost consuming and Q-learning converges slowly comparing to supervised learning, different kinds of transfer RL algorithms are designed. However, most of them are heuristic with no theoretical guarantee of the convergence rate. Therefore, it is important for us to clearly understand when and how will transfer learning help RL method and provide the theoretical guarantee for the improvement of the sample complexity. In this paper, we propose to transfer the Q-function learned in the source task to the target of the Q-learning in the new task when certain safe conditions are satisfied. We call this new transfer Q-learning method target transfer Q-Learning. The safe conditions are necessary to avoid the harm to the new tasks and thus ensure the convergence of the algorithm. We study the convergence rate of the target transfer Q-learning. We prove that if the two tasks are similar with respect to the MDPs, the optimal Q-functions in the source and new RL tasks are similar which means the error of the transferred target Q-function in new MDP is small. Also, the convergence rate analysis shows that the target transfer Q-Learning will converge faster than Q-learning if the error of the transferred target Q-function is smaller than the current Q-function in the new task. Based on our theoretical results, we design the safe condition as the Bellman error of the transferred target Q-function is less than the current Q-function. Our experiments are consistent with our theoretical founding and verified the effectiveness of our proposed target transfer Q-learning method.",0
"Reinforcement Learning (RL) commonly employs Q-learning, while Transfer Learning utilizes previously learned knowledge from source tasks to improve new task sample complexity. This is particularly relevant in RL, where data collection is expensive and time-consuming, and Q-learning convergence is slow relative to supervised learning. Although various transfer RL algorithms exist, most are heuristic with no guaranteed convergence rate. Thus, it is crucial to understand when and how Transfer Learning can enhance RL, and provide theoretical guarantees for sample complexity improvement. This paper proposes a new Transfer Q-learning method, called target transfer Q-Learning, which transfers the Q-function learned from a source task to the target of the Q-learning in the new task under safe conditions. The safe conditions prevent harm to the new tasks and ensure algorithm convergence. The convergence rate analysis indicates that target transfer Q-Learning will converge faster than Q-learning if the transferred target Q-function error is smaller than the current Q-function in the new task. The theory-based safe condition is that the Bellman error of the transferred target Q-function is less than the current Q-function. Empirical results confirm the effectiveness of the proposed method.",1
"Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.",0
"Model-free reinforcement learning has proven to be a promising solution for continuous control robotic tasks, thanks to recent simulation successes. Open source implementations of learning algorithms and simulated benchmark tasks have allowed for reproducibility and rapid analysis of these results. However, to apply these successes to real-world applications, it is important to experiment with physical robots and take advantage of simulation-specific advantages. Unfortunately, reinforcement learning research with physical robots faces resistance due to the lack of benchmark tasks and source code. To address this, we introduce several reinforcement learning tasks using commercially available robots with varying levels of difficulty, setup, and repeatability. We test off-the-shelf implementations of four reinforcement learning algorithms on these tasks and analyze their sensitivity to hyper-parameters to determine their applicability to real-world tasks. Our results show that careful task interface and computation setups can make some implementations applicable to physical robots. We also found that state-of-the-art learning algorithms are sensitive to their hyper-parameters and need to be re-tuned for each task for optimal performance. However, the best hyper-parameter configuration from one task may work well for held-out tasks with different robots. We make these benchmark tasks publicly available to improve reproducibility in real-world reinforcement learning.",1
"In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments. Supplementary videos and discussion can be found at http://frama.link/gep_pg, the code at http://github.com/flowersteam/geppg.",0
"The efficiency of standard deep reinforcement learning algorithms such as DDPG is compromised when dealing with sparse or deceptive reward problems in continuous action domains. In contrast, evolutionary and developmental methods that prioritize exploration, such as Quality-Diversity, Goal Exploration Processes, and Novelty Search, are effective in exploring more robustly, but less efficient in fine-tuning policies through gradient descent. This paper introduces the GEP-PG approach, which combines the benefits of both methods by sequentially integrating a Goal Exploration Process and two DDPG variants. The study examines the learning performance of these components and their combination on a deceptive reward problem with low dimensions and the larger Half-Cheetah benchmark. The results show that DDPG is ineffective in the former, while GEP-PG outperforms the best DDPG variant in both environments. For supplementary videos and discussion, visit http://frama.link/gep_pg, and for the code, visit http://github.com/flowersteam/geppg.",1
"Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work \cite{efroni2018beyond}, multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.",0
"The effectiveness of multiple-step lookahead policies in Reinforcement Learning has been demonstrated through the use of Monte Carlo Tree Search or Model Predictive Control. Recently, a study by Efroni proposed and analyzed the use of multiple-step greedy policies in vanilla Policy Iteration algorithms. In this work, we aim to examine the practical applications of multiple-step greedy algorithms. One challenge we address is the surprising difficulty that arises with soft-policy updates, where monotonic policy improvement cannot be guaranteed without a sufficiently large update stepsize. To overcome this challenge, we develop and analyze online and approximate algorithms that incorporate a multi-step greedy operator.",1
"Temporal difference (TD) learning is an important approach in reinforcement learning, as it combines ideas from dynamic programming and Monte Carlo methods in a way that allows for online and incremental model-free learning. A key idea of TD learning is that it is learning predictive knowledge about the environment in the form of value functions, from which it can derive its behavior to address long-term sequential decision making problems. The agent's horizon of interest, that is, how immediate or long-term a TD learning agent predicts into the future, is adjusted through a discount rate parameter. In this paper, we introduce an alternative view on the discount rate, with insight from digital signal processing, to include complex-valued discounting. Our results show that setting the discount rate to appropriately chosen complex numbers allows for online and incremental estimation of the Discrete Fourier Transform (DFT) of a signal of interest with TD learning. We thereby extend the types of knowledge representable by value functions, which we show are particularly useful for identifying periodic effects in the reward sequence.",0
"Reinforcement learning employs temporal difference (TD) learning, a significant technique that combines the concepts of dynamic programming and Monte Carlo methods. This method facilitates model-free learning in an online and incremental manner by learning predictive knowledge through value functions. Such knowledge is used to arrive at long-term sequential decision-making solutions. The discount rate parameter in TD learning determines the agent's horizon of interest, ranging from immediate to long-term prediction. In this study, we present an alternative view on this parameter, incorporating complex-valued discounting, inspired by digital signal processing. Our findings demonstrate that effective estimation of the Discrete Fourier Transform (DFT) can be achieved through TD learning by setting the discount rate to suitable complex numbers. By broadening the representable knowledge types by value functions, we show how this approach can identify periodic effects in reward sequences.",1
"The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs (SCG), eg, in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order derivatives is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order derivative involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for estimators of higher-order derivatives. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and numerical evaluation of the DiCE derivative estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://www.github.com/alshedivat/lola.",0
"The score function estimator is a commonly used method for estimating stochastic objective gradients in stochastic computation graphs (SCG), such as in reinforcement learning and meta-learning. While deriving first-order gradient estimators by differentiating a surrogate loss (SL) objective is straightforward in terms of computation and concept, it becomes more challenging when calculating higher-order derivatives. Firstly, deriving and implementing such estimators analytically is laborious and does not comply with automatic differentiation. Secondly, constructing new objectives for each order derivative using SL requires increasingly cumbersome graph manipulations. Lastly, SL treats part of the cost as a fixed sample to match the first-order gradient under differentiation, which leads to missing and incorrect terms for estimating higher-order derivatives. To address these issues, we introduce DiCE, which provides a single objective that can be repeatedly differentiated to generate correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing necessary graph manipulations. We prove the correctness of DiCE and evaluate its derivative estimates numerically. Additionally, we use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available on https://www.github.com/alshedivat/lola.",1
"We explore the problem of learning to decompose spatial tasks into segments, as exemplified by the problem of a painting robot covering a large object. Inspired by the ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate the problem of decomposing objects into segments as a parsing approach. We make the insight that the derivation of a parse-tree that decomposes the object into segments closely resembles a decision tree constructed by ID3, which can be done when the ground-truth available. We learn to imitate an expert parsing oracle, such that our neural parser can generalize to parse natural images without ground truth. We introduce a novel deterministic policy gradient update, DRAG (i.e., DeteRministically AGgrevate) in the form of a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. From another perspective, our approach is a variant of the Deterministic Policy Gradient suitable for the imitation learning setting. The deterministic policy representation offered by training our neural parser with DRAG allows it to outperform state of the art imitation and reinforcement learning approaches.",0
"Our focus is on the challenge of breaking down spatial tasks into segments, as illustrated by the task of a painting robot coating a large object. We draw inspiration from classical decision tree algorithms that can construct structured partitions of input spaces. To address the task of segmenting objects, we take a parsing approach. We observe that the process of creating a parse-tree that breaks down an object into its constituent parts is similar to constructing a decision tree using ID3 when ground-truth data is available. We aim to teach our neural parser to imitate an expert parsing oracle, allowing it to generalize to parse natural images without access to ground-truth information. We introduce a new policy gradient update technique, DRAG (DeteRministically AGgrevate), which is a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. Our approach also represents a variation of the Deterministic Policy Gradient designed for imitation learning. Training our neural parser with DRAG results in a deterministic policy representation that outperforms existing imitation and reinforcement learning techniques.",1
"We present an effective technique for training deep learning agents capable of negotiating on a set of clauses in a contract agreement using a simple communication protocol. We use Multi Agent Reinforcement Learning to train both agents simultaneously as they negotiate with each other in the training environment. We also model selfish and prosocial behavior to varying degrees in these agents. Empirical evidence is provided showing consistency in agent behaviors. We further train a meta agent with a mixture of behaviors by learning an ensemble of different models using reinforcement learning. Finally, to ascertain the deployability of the negotiating agents, we conducted experiments pitting the trained agents against human players. Results demonstrate that the agents are able to hold their own against human players, often emerging as winners in the negotiation. Our experiments demonstrate that the meta agent is able to reasonably emulate human behavior.",0
"Our study showcases an effective method for training deep learning agents to negotiate on a contract agreement's set of clauses using a straightforward communication protocol. To achieve this, we utilize Multi Agent Reinforcement Learning to concurrently train both agents while they negotiate in a training environment. These agents exhibit varying degrees of selfish and prosocial behavior, and we provide empirical proof of their consistent actions. Furthermore, we train a meta agent with a blend of behaviors by employing reinforcement learning to learn an ensemble of various models. To test the deployability of the negotiating agents, we conduct experiments that pit them against human players. Remarkably, the agents hold their ground against human players, frequently emerge victorious in negotiations. Our findings demonstrate that the meta agent can effectively mimic human behavior.",1
"We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",0
"Our proposition is to employ boosted regression trees to derive human-understandable resolutions to reinforcement learning predicaments. Boosting merges multiple regression trees to enhance their precision without considerably diminishing their innate comprehensibility. Previous research has concentrated on both reinforcement learning and machine learning that can be interpreted, but making reinforcement learning interpretable has seen little advancement. Our trial outcomes indicate that boosted regression trees produce solutions that are both explainable and equivalent in standard to the top reinforcement learning approaches.",1
"The main focus of this paper is on enhancement of two types of game-theoretic learning algorithms: log-linear learning and reinforcement learning. The standard analysis of log-linear learning needs a highly structured environment, i.e. strong assumptions about the game from an implementation perspective. In this paper, we introduce a variant of log-linear learning that provides asymptotic guarantees while relaxing the structural assumptions to include synchronous updates and limitations in information available to the players. On the other hand, model-free reinforcement learning is able to perform even under weaker assumptions on players' knowledge about the environment and other players' strategies. We propose a reinforcement algorithm that uses a double-aggregation scheme in order to deepen players' insight about the environment and constant learning step-size which achieves a higher convergence rate. Numerical experiments are conducted to verify each algorithm's robustness and performance.",0
"This paper focuses on improving two game-theoretic learning algorithms: log-linear learning and reinforcement learning. The traditional analysis of log-linear learning requires a highly structured environment with strict implementation assumptions. However, we introduce a modified version of log-linear learning that maintains asymptotic guarantees while relaxing these structural assumptions, such as synchronous updates and limited player information. Conversely, reinforcement learning can succeed under weaker assumptions about player knowledge and strategy. Our proposed reinforcement algorithm incorporates a double-aggregation scheme and constant learning step-size to enhance players' understanding and achieve faster convergence. We conduct numerical experiments to test the robustness and performance of each algorithm.",1
"Many reinforcement-learning researchers treat the reward function as a part of the environment, meaning that the agent can only know the reward of a state if it encounters that state in a trial run. However, we argue that this is an unnecessary limitation and instead, the reward function should be provided to the learning algorithm. The advantage is that the algorithm can then use the reward function to check the reward for states that the agent hasn't even encountered yet. In addition, the algorithm can simultaneously learn policies for multiple reward functions. For each state, the algorithm would calculate the reward using each of the reward functions and add the rewards to its experience replay dataset. The Hindsight Experience Replay algorithm developed by Andrychowicz et al. (2017) does just this, and learns to generalize across a distribution of sparse, goal-based rewards. We extend this algorithm to linearly-weighted, multi-objective rewards and learn a single policy that can generalize across all linear combinations of the multi-objective reward. Whereas other multi-objective algorithms teach the Q-function to generalize across the reward weights, our algorithm enables the policy to generalize, and can thus be used with continuous actions.",0
"There is a debate among reinforcement-learning researchers about treating the reward function as a part of the environment. This means that the agent can only learn the reward of a state by encountering it in a trial run. However, we believe that this is an unnecessary limitation and that the reward function should be provided to the learning algorithm. This allows the algorithm to check the reward for states that the agent hasn't encountered yet and learn policies for multiple reward functions simultaneously. Our algorithm calculates the reward for each state using each of the reward functions and adds the rewards to its experience replay dataset. This approach is similar to the Hindsight Experience Replay algorithm developed by Andrychowicz et al. (2017), which generalizes across a distribution of sparse, goal-based rewards. However, we extend this algorithm to linearly-weighted, multi-objective rewards and learn a single policy that can generalize across all linear combinations of the multi-objective reward. Unlike other multi-objective algorithms, our algorithm enables the policy to generalize and can be used with continuous actions.",1
"Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task---rather than being productive---can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.",0
"Using deep reinforcement learning to teach robots tasks or controllers has been found to be effective in simulations. Learning in simulations has various advantages, such as having complete control over the simulated environment, including the ability to pause movements while performing computations. It also reduces the amount of time the robot spends learning a task, allowing for more productivity, by transferring the learned task to the real robot. However, fine-tuning the task on the real robot can take a significant amount of time, particularly for tasks involving complex dynamics. To minimize the need for fine-tuning, we suggest learning robustified controllers in simulation. Robustified controllers are learned by changing simulation parameters for successive training episodes, allowing for more efficient learning. This approach also eases the precise determination of physics parameters for the simulator, which can be difficult. Our approach was tested on a real setup, where a robot was programmed to solve a maze game with complex dynamics. We found that using a robustified controller reduced the amount of fine-tuning needed for transfer learning compared to a non-robustified controller.",1
"Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The ""actor"" in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to the lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.",0
"Large-scale electronic health records (EHRs) have paved the way for dynamic treatment recommendation systems that can significantly enhance practical clinical outcomes. Prior research suggests that supervised learning (such as matching doctor prescriptions) or reinforcement learning (such as maximizing survival rates) are viable options for treatment recommendations. However, none of these studies have explored the potential benefits of combining the two approaches. In this study, we propose a novel approach called Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which integrates supervised and reinforcement learning into a synergistic framework. Our model utilizes an off-policy actor-critic method to handle complex relationships between medications, diseases, and individual characteristics. The ""actor"" in the framework is optimized using both indicator and evaluation signals to ensure effective prescription and low mortality. To address the Partially-Observed Markov Decision Process (POMDP) challenges in real-world settings, we employ RNN. Our experiments using the publicly available MIMIC-3 dataset demonstrate that our model can reduce estimated mortality while accurately matching doctors' prescriptions.",1
"Deep reinforcement learning has become popular over recent years, showing superiority on different visual-input tasks such as playing Atari games and robot navigation. Although objects are important image elements, few work considers enhancing deep reinforcement learning with object characteristics. In this paper, we propose a novel method that can incorporate object recognition processing to deep reinforcement learning models. This approach can be adapted to any existing deep reinforcement learning frameworks. State-of-the-art results are shown in experiments on Atari games. We also propose a new approach called ""object saliency maps"" to visually explain the actions made by deep reinforcement learning agents.",0
"Over the past few years, deep reinforcement learning has gained popularity due to its exceptional performance in various visual-input tasks like robot navigation and playing Atari games. However, despite the significance of objects in image elements, only a few studies have explored the possibility of enhancing deep reinforcement learning with object characteristics. This paper suggests a new approach that can integrate object recognition processing into existing deep reinforcement learning frameworks. The proposed method shows state-of-the-art results in experiments conducted on Atari games. Additionally, this paper introduces a novel technique called ""object saliency maps"" that visually explains the actions taken by deep reinforcement learning agents.",1
"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.",0
"In the near future, self-governing AI systems will join human society to provide services and collaborate with people. For these systems to gain acceptance and trust, they must be transparent, and users should be able to comprehend their decision-making process. Transparency allows humans to create logical explanations of the system's actions and decisions, which is essential not only for user trust but also for software debugging and certification. Despite the significant advancements of Deep Neural Networks in various domains, they are not transparent. This paper addresses transparency in Deep Reinforcement Learning Networks (DRLN), which have shown exceptional success in controlling actions in image input domains such as Atari games. The paper proposes a new and comprehensive method that (a) combines explicit object recognition processing into deep reinforcement learning models, (b) creates ""object saliency maps"" that visualize internal states of DRLNs, allowing for the development of explanations, and (c) can be included in any existing deep reinforcement learning framework. The paper also presents computational results and human experiments to evaluate the effectiveness of this approach.",1
"Current imitation learning techniques are too restrictive because they require the agent and expert to share the same action space. However, oftentimes agents that act differently from the expert can solve the task just as good. For example, a person lifting a box can be imitated by a ceiling mounted robot or a desktop-based robotic-arm. In both cases, the end goal of lifting the box is achieved, perhaps using different strategies. We denote this setup as \textit{Inspiration Learning} - knowledge transfer between agents that operate in different action spaces. Since state-action expert demonstrations can no longer be used, Inspiration learning requires novel methods to guide the agent towards the end goal. In this work, we rely on ideas of Preferential based Reinforcement Learning (PbRL) to design Advantage Actor-Critic algorithms for solving inspiration learning tasks. Unlike classic actor-critic architectures, the critic we use consists of two parts: a) a state-value estimation as in common actor-critic algorithms and b) a single step reward function derived from an expert/agent classifier. We show that our method is capable of extending the current imitation framework to new horizons. This includes continuous-to-discrete action imitation, as well as primitive-to-macro action imitation.",0
"The current methods of imitation learning are too limiting as they require the agent and expert to have the same action space. However, agents that use different actions from the expert can still achieve the same task. For instance, a robot mounted on the ceiling or a desktop-based robotic arm can lift a box just like a person. This approach is known as Inspiration Learning, which involves knowledge transfer between agents operating in different action spaces. Since state-action expert demonstrations cannot be used, novel methods are required to guide agents to the intended goal. In this research, we employ Preferential based Reinforcement Learning (PbRL) concepts to develop Advantage Actor-Critic algorithms that solve Inspiration learning tasks. Our approach differs from traditional actor-critic frameworks as our critic has two components: a state-value estimation and a single-step reward function from an expert/agent classifier. Our results demonstrate that our method can expand the current imitation framework by enabling continuous-to-discrete action imitation and primitive-to-macro action imitation.",1
"Complex autonomous control systems are subjected to sensor failures, cyber-attacks, sensor noise, communication channel failures, etc. that introduce errors in the measurements. The corrupted information, if used for making decisions, can lead to degraded performance. We develop a framework for using adversarial deep reinforcement learning to design observer strategies that are robust to adversarial errors in information channels. We further show through simulation studies that the learned observation strategies perform remarkably well when the adversary's injected errors are bounded in some sense. We use neural network as function approximator in our studies with the understanding that any other suitable function approximating class can be used within our framework.",0
"Autonomous control systems of a complex nature are vulnerable to various factors such as sensor malfunctions, cyber-attacks, sensor noise, and communication channel failures. These factors can produce inaccuracies in the measurements, which when utilized for decision-making purposes can result in a decline in performance. To tackle this issue, we have developed a framework that leverages adversarial deep reinforcement learning to create observer strategies that can withstand adversarial errors in information channels. Our simulation studies demonstrate that the learned observation strategies perform exceptionally well when the errors introduced by the adversary are limited in some way. We have utilized neural networks as function approximators in our research, but our framework can adapt to other suitable function approximating classes as well.",1
"A probability density function (pdf) encodes the entire stochastic knowledge about data distribution, where data may represent stochastic observations in robotics, transition state pairs in reinforcement learning or any other empirically acquired modality. Inferring data pdf is of prime importance, allowing to analyze various model hypotheses and perform smart decision making. However, most density estimation techniques are limited in their representation expressiveness to specific kernel type or predetermined distribution family, and have other restrictions. For example, kernel density estimation (KDE) methods require meticulous parameter search and are extremely slow at querying new points. In this paper we present a novel non-parametric density estimation approach, DeepPDF, that uses a neural network to approximate a target pdf given samples from thereof. Such a representation provides high inference accuracy for a wide range of target pdfs using a relatively simple network structure, making our method highly statistically robust. This is done via a new stochastic optimization algorithm, \emph{Probabilistic Surface Optimization} (PSO), that turns to advantage the stochastic nature of sample points in order to force network output to be identical to the output of a target pdf. Once trained, query point evaluation can be efficiently done in DeepPDF by a simple network forward pass, with linear complexity in the number of query points. Moreover, the PSO algorithm is capable of inferring the frequency of data samples and may also be used in other statistical tasks such as conditional estimation and distribution transformation. We compare the derived approach with KDE methods showing its superior performance and accuracy.",0
"The probability density function (pdf) contains all the stochastic information about the distribution of data, which can represent various empirical modalities such as stochastic observations in robotics or transition state pairs in reinforcement learning. It is crucial to infer the data pdf to analyze different model hypotheses and make intelligent decisions. However, most density estimation techniques have limitations in terms of their expressiveness in representing specific kernel types or predetermined distribution families, as well as other restrictions. For instance, kernel density estimation (KDE) methods require a meticulous search for parameters and are sluggish when querying new points. This paper proposes a novel non-parametric density estimation approach called DeepPDF that employs a neural network to approximate a target pdf using samples. This method has a high inference accuracy for a broad range of target pdfs with a relatively straightforward network structure, making it statistically robust. The proposed approach uses a new stochastic optimization algorithm called Probabilistic Surface Optimization (PSO) that leverages the stochastic nature of sample points to ensure that the network output is identical to the target pdf output. Once trained, the DeepPDF method can efficiently evaluate query points using a simple network forward pass with linear complexity in the number of query points. Furthermore, the PSO algorithm can infer the frequency of data samples and can be used for other statistical tasks such as conditional estimation and distribution transformation. The proposed approach outperforms KDE methods in terms of performance and accuracy.",1
"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffers from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider this as a problem of transferring knowledge within a family of similar Markov decision processes.   For this purpose we assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",0
"Although Reinforcement Learning (RL) methods are powerful problem solvers, they often produce suboptimal policies in slightly different environments. This is particularly true in robotics, where diverse training and deployment conditions can be costly and retraining is not ideal. Simulation training offers a practical solution, but it is not entirely reliable in real-world scenarios due to the reality-gap. Therefore, this highlights the need for adaptive policies that can function efficiently in new environments. We propose solving this issue by transferring knowledge within similar Markov decision processes. Our approach utilizes a low-dimensional latent variable to generate Q-functions, which allows us to identify a master policy that can adapt to different values of the latent variable. Our method involves learning the generative mapping and an approximate posterior of the latent variables, enabling us to identify policies for new tasks by searching solely in the latent space. The low-dimensional space and master policy generated by our method enable fast adaptation to new environments. We validate our approach on both a pendulum swing-up task in simulation and a pushing task for simulation-to-real transfer.",1
"Early detection of cyber-attacks is crucial for a safe and reliable operation of the smart grid. In the literature, outlier detection schemes making sample-by-sample decisions and online detection schemes requiring perfect attack models have been proposed. In this paper, we formulate the online attack/anomaly detection problem as a partially observable Markov decision process (POMDP) problem and propose a universal robust online detection algorithm using the framework of model-free reinforcement learning (RL) for POMDPs. Numerical studies illustrate the effectiveness of the proposed RL-based algorithm in timely and accurate detection of cyber-attacks targeting the smart grid.",0
"Detecting cyber-attacks early is essential to ensure the smart grid operates safely and reliably. Current literature suggests outlier detection schemes that make decisions based on individual samples, and online detection schemes that require perfect attack models. In this study, we present a solution to the online attack/anomaly detection problem using a partially observable Markov decision process (POMDP) framework. We propose a universal robust online detection algorithm using model-free reinforcement learning (RL) for POMDPs. Our numerical studies demonstrate the effectiveness of the RL-based algorithm in detecting cyber-attacks targeting the smart grid accurately and promptly.",1
"Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.",0
"While model-based reinforcement learning approaches have the potential to be efficient with data, they often struggle to achieve the same performance as model-free methods due to difficulties in accurately learning dynamics models that match the real world. To address this, we introduce Model-Based Meta-Policy-Optimization (MB-MPO), which does not rely heavily on accurate learned dynamics models. Instead, MB-MPO uses an ensemble of learned dynamic models to meta-learn a policy that can adapt quickly to any model in the ensemble, requiring only one policy gradient step. This approach helps the meta-policy learn consistent dynamics predictions among the ensemble while shifting the responsibility for optimal behavior in the face of model discrepancies to the adaptation step. Our experiments demonstrate that MB-MPO is more resilient to model imperfections than previous model-based methods and can match the asymptotic performance of model-free methods while using significantly less experience.",1
"Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.",0
"Advanced performance in image captioning has been demonstrated by Reinforcement Learning (RL) approaches through direct optimization of testing metrics. However, this approach introduces learning biases that decrease the readability of generated text, and the large sample space leads to unstable and slow training. To solve these issues, we propose a coherent solution that constrains the action space using an n-gram language prior. Our evaluations on benchmarks show that RL with this add-on module performs favorably in terms of readability and convergence speed. Human evaluations also indicate that our model produces more human-readable and graceful captions. The implementation will be publicly available upon paper acceptance.",1
"Achieving machine intelligence requires a smooth integration of perception and reasoning, yet models developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where the goal is to carry out simple arithmetical algorithms on digits presented under natural conditions (e.g. hand-written, placed randomly). We propose a two-tiered architecture for tackling this problem. The lower tier consists of a heterogeneous collection of information processing modules, which can include pre-trained deep neural networks for locating and extracting characters from the image, as well as modules performing symbolic transformations on the representations extracted by perception. The higher tier consists of a controller, trained using reinforcement learning, which coordinates the modules in order to solve the high-level task. For instance, the controller may learn in what contexts to execute the perceptual networks and what symbolic transformations to apply to their outputs. The resulting model is able to solve a variety of tasks in the visual arithmetic domain, and has several advantages over standard, architecturally homogeneous feedforward networks including improved sample efficiency.",0
"To achieve machine intelligence, the integration of perception and reasoning is crucial. However, current models tend to focus on one over the other, making it difficult to manipulate symbols from complex perceptual spaces. For example, performing arithmetic algorithms on handwritten digits presented under natural conditions presents a challenge. To address this issue, we propose a two-tiered architecture. The lower tier comprises various information processing modules, including pre-trained deep neural networks for character recognition and modules for symbolic transformations of extracted representations. The higher tier is a controller trained using reinforcement learning that coordinates the modules to solve the task. The resulting model can perform various visual arithmetic tasks efficiently, surpassing standard feedforward networks in several ways.",1
"Recently it has shown that the policy-gradient methods for reinforcement learning have been utilized to train deep end-to-end systems on natural language processing tasks. What's more, with the complexity of understanding image content and diverse ways of describing image content in natural language, image captioning has been a challenging problem to deal with. To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN). However, in this paper, we propose a novel architecture for image captioning with deep reinforcement learning to optimize image captioning tasks. We utilize two networks called ""policy network"" and ""value network"" to collaboratively generate the captions of images. The experiments are conducted on Microsoft COCO dataset, and the experimental results have verified the effectiveness of the proposed method.",0
"Recent developments have seen policy-gradient methods for reinforcement learning being applied to train deep end-to-end systems for natural language processing tasks. This has posed a challenge in image captioning due to the intricate nature of comprehending image content and the various ways of describing it in natural language. Most state-of-the-art techniques have followed a sequential model such as recurrent neural networks (RNN). However, this paper introduces a novel approach using deep reinforcement learning to optimize image captioning tasks. The proposed method employs two networks, the ""policy network"" and the ""value network,"" to cooperatively generate captions for images. The effectiveness of the method is tested on the Microsoft COCO dataset, and the experimental results validate its efficacy.",1
"Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].",0
"Over the last few years, deep learning has achieved significant success across various application domains. This emerging field of machine learning has rapidly grown and is now being applied in numerous areas, leading to new opportunities. Different learning approaches have been proposed, including supervised, semi-supervised, and un-supervised learning. Empirical results demonstrate that deep learning outperforms traditional machine learning techniques in various fields such as Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and more. This report provides a concise overview of the development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we cover recent advanced DL techniques based on these approaches, as well as their evaluation in different application domains. We also include recently developed frameworks, SDKs, and benchmark datasets used to implement and evaluate deep learning approaches. While some surveys have been published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234], our paper provides a comprehensive overview of advanced techniques used to train large-scale deep learning models and the recently developed method of generative models [1].",1
"The reinforcement learning community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequential-decision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent's updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.",0
"The reinforcement learning community has developed algorithms that can outperform humans in specific tasks. However, these algorithms are typically trained for one task at a time, meaning that each agent can only solve the task it was trained on. This poses a challenge when trying to master multiple sequential-decision tasks simultaneously. Multi-task learning requires a balance between the needs of each task, as some tasks can be more salient to the learning process due to their rewards. To address this, we propose adapting the contribution of each task to the agent's updates so that all tasks have a similar impact on the learning dynamics. This approach resulted in state-of-the-art performance, with a single agent surpassing human-level performance in a set of 57 Atari games and 30 tasks in the DeepMind Lab.",1
"We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.",0
"In this research, we showcase the initial use of deep reinforcement learning for self-driving purposes. Our model starts from random parameters and can quickly learn a lane-following policy with just one monocular image as input. We adopt an accessible and universal reward metric: the vehicle's distance traveled without the need for human intervention. Our approach employs a model-free, continuous deep reinforcement learning algorithm that carries out all exploration and optimization while driving. This marks a new direction for autonomous driving, as it deviates from conventional reliance on fixed logical rules, mapping, and direct supervision. We also address the challenges and possibilities of extending this method to a broader spectrum of autonomous driving tasks.",1
"Real-time advertising allows advertisers to bid for each impression for a visiting user. To optimize specific goals such as maximizing revenue and return on investment (ROI) led by ad placements, advertisers not only need to estimate the relevance between the ads and user's interests, but most importantly require a strategic response with respect to other advertisers bidding in the market. In this paper, we formulate bidding optimization with multi-agent reinforcement learning. To deal with a large number of advertisers, we propose a clustering method and assign each cluster with a strategic bidding agent. A practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed and implemented to balance the tradeoff between the competition and cooperation among advertisers. The empirical study on our industry-scaled real-world data has demonstrated the effectiveness of our methods. Our results show cluster-based bidding would largely outperform single-agent and bandit approaches, and the coordinated bidding achieves better overall objectives than purely self-interested bidding agents.",0
"Real-time advertising enables advertisers to bid on every impression a user makes. To achieve specific goals like maximizing revenue and return on investment (ROI) from ad placements, advertisers must gauge the relevance of their ads to a user's interests and act strategically in response to competing bidders. This paper proposes optimizing bidding through multi-agent reinforcement learning and cluster-based bidding. The Distributed Coordinated Multi-Agent Bidding (DCMAB) approach balances competition and cooperation among advertisers and outperforms single-agent and bandit approaches. Empirical studies on real-world data demonstrate the effectiveness of our methods, showing that coordinated bidding achieves better overall objectives than purely self-interested bidding agents.",1
"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.",0
"Traffic decisions can be difficult to make due to personal preferences and environmental factors, making it challenging to model using only expert knowledge. To address this, we utilized Deep Reinforcement Learning to teach decision-making based on a concise semantic state representation, ensuring consistent modeling across various scenarios and adaptation capabilities for behavior changes without retraining. Our neural network input imitates Radar or Lidar sensors with a semantic scene description. We added a behavior adaptation function and parameterization to the state and reward. Our agent demonstrated an ability to follow traffic rules and drive safely in diverse situations with minimal expert knowledge and mid-level actions.",1
"Flatland is a simple, lightweight environment for fast prototyping and testing of reinforcement learning agents. It is of lower complexity compared to similar 3D platforms (e.g. DeepMind Lab or VizDoom), but emulates physical properties of the real world, such as continuity, multi-modal partially-observable states with first-person view and coherent physics. We propose to use it as an intermediary benchmark for problems related to Lifelong Learning. Flatland is highly customizable and offers a wide range of task difficulty to extensively evaluate the properties of artificial agents. We experiment with three reinforcement learning baseline agents and show that they can rapidly solve a navigation task in Flatland. A video of an agent acting in Flatland is available here: https://youtu.be/I5y6Y2ZypdA.",0
"Flatland is a lightweight and uncomplicated environment that allows for rapid prototyping and testing of reinforcement learning agents. While it is not as intricate as other 3D platforms like DeepMind Lab or VizDoom, it still replicates essential physical properties of the real world, such as coherent physics, first-person view, and multi-modal partially-observable states with continuity. We recommend using Flatland as an intermediary benchmark for Lifelong Learning problems since it is customizable and offers many different task difficulties to thoroughly assess artificial agents' capabilities. To demonstrate its effectiveness, we conducted experiments with three baseline reinforcement learning agents and found that they could quickly complete a navigation task in Flatland. Additionally, a video showcasing an agent in Flatland is available for viewing at https://youtu.be/I5y6Y2ZypdA.",1
"Autonomous vehicles (AVs) are on the road. To safely and efficiently interact with other road participants, AVs have to accurately predict the behavior of surrounding vehicles and plan accordingly. Such prediction should be probabilistic, to address the uncertainties in human behavior. Such prediction should also be interactive, since the distribution over all possible trajectories of the predicted vehicle depends not only on historical information, but also on future plans of other vehicles that interact with it. To achieve such interaction-aware predictions, we propose a probabilistic prediction approach based on hierarchical inverse reinforcement learning (IRL). First, we explicitly consider the hierarchical trajectory-generation process of human drivers involving both discrete and continuous driving decisions. Based on this, the distribution over all future trajectories of the predicted vehicle is formulated as a mixture of distributions partitioned by the discrete decisions. Then we apply IRL hierarchically to learn the distributions from real human demonstrations. A case study for the ramp-merging driving scenario is provided. The quantitative results show that the proposed approach can accurately predict both the discrete driving decisions such as yield or pass as well as the continuous trajectories.",0
"AVs require accurate prediction of surrounding vehicles' behavior to ensure safe and efficient road interactions. This prediction must address uncertainties in human behavior, be probabilistic, and interactive, as it depends on future plans of interacting vehicles. To achieve this, we propose a probabilistic prediction approach using hierarchical inverse reinforcement learning (IRL). Our approach considers the trajectory-generation process of human drivers and formulates the distribution over all future trajectories of the predicted vehicle as a mixture of distributions, partitioned by discrete decisions. We apply IRL hierarchically to learn distributions from human demonstrations, and provide a case study for the ramp-merging scenario. Our results show that our approach accurately predicts both discrete decisions and continuous trajectories.",1
"Rogue is a famous dungeon-crawling video-game of the 80ies, the ancestor of its gender. Rogue-like games are known for the necessity to explore partially observable and always different randomly-generated labyrinths, preventing any form of level replay. As such, they serve as a very natural and challenging task for reinforcement learning, requiring the acquisition of complex, non-reactive behaviors involving memory and planning. In this article we show how, exploiting a version of A3C partitioned on different situations, the agent is able to reach the stairs and descend to the next level in 98% of cases.",0
"The 80s video-game called Rogue is a renowned dungeon-crawling game and is considered the precursor of its genre. These kinds of games are referred to as Rogue-like games, and they require players to explore randomly generated labyrinths that are partially observable and always different. This means that there is no possibility of replaying levels. As a result, they offer a natural and challenging task for reinforcement learning, which involves acquiring complex, non-reactive behaviors that require memory and planning. Our article demonstrates how an agent can use a partitioned version of A3C to reach the stairs and descend to the next level in 98% of cases.",1
"Deep reinforcement learning has obtained significant breakthroughs in recent years. Most methods in deep-RL achieve good results via the maximization of the reward signal provided by the environment, typically in the form of discounted cumulative returns. Such reward signals represent the immediate feedback of a particular action performed by an agent. However, tasks with sparse reward signals are still challenging to on-policy methods. In this paper, we introduce an effective characterization of past reward statistics (which can be seen as long-term feedback signals) to supplement this immediate reward feedback. In particular, value functions are learned with multi-critics supervision, enabling complex value functions to be more easily approximated in on-policy learning, even when the reward signals are sparse. We also introduce a novel exploration mechanism called ""hot-wiring"" that can give a boost to seemingly trapped agents. We demonstrate the effectiveness of our advantage actor multi-critic (A2MC) method across the discrete domains in Atari games as well as continuous domains in the MuJoCo environments. A video demo is provided at https://youtu.be/zBmpf3Yz8tc.",0
"In recent years, deep reinforcement learning has achieved significant breakthroughs. The majority of deep-RL methods attain favorable results by maximizing the reward signal, typically in the form of discounted cumulative returns, provided by the environment. This signal represents immediate feedback about an agent's actions. However, on-policy methods still find it challenging to handle tasks with sparse reward signals. In this article, we propose an approach that supplements immediate reward feedback with a characterization of past reward statistics, which can be viewed as long-term feedback signals. We accomplish this by training value functions with multi-critics supervision, making it easier to approximate complex value functions in on-policy learning, even in the presence of sparse reward signals. Furthermore, we introduce a new exploration mechanism called ""hot-wiring"" that can help agents that appear to be stuck. We demonstrate the effectiveness of our advantage actor multi-critic (A2MC) method in both discrete domains, such as Atari games, and continuous domains, such as MuJoCo environments. A video demonstration is available at https://youtu.be/zBmpf3Yz8tc.",1
"Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.",0
"To combat the issue of sample-inefficiency in deep reinforcement learning (RL), experience replay is a valuable technique, but its efficacy is hindered by the scarcity of successful experiences in the replay buffer when dealing with binary and sparse rewards. Hindsight experience replay (HER) was introduced to address this problem by modifying unsuccessful transitions, but this approach results in a significant bias in the replay buffer experiences, resulting in suboptimal sample-efficiency improvements. This study examines the source of bias in HER and proposes an effective method called ARCHER, which employs counter-factual reasoning and increases the rewards calculated for hindsight experiences to counter the bias. The algorithm was tested on two continuous control environments, Reacher and Finger, with various reward functions, task complexities, and goal sampling strategies, demonstrating that using more aggressive hindsight rewards enhances sample efficiency and establishing ARCHER's superiority in RL applications with limited computing resources.",1
"We apply neural nets with ReLU gates in online reinforcement learning. Our goal is to train these networks in an incremental manner, without the computationally expensive experience replay. By studying how individual neural nodes behave in online training, we recognize that the global nature of ReLU gates can cause undesirable learning interference in each node's learning behavior. We propose reducing such interferences with two efficient input transformation methods that are geometric in nature and match well the geometric property of ReLU gates. The first one is tile coding, a classic binary encoding scheme originally designed for local generalization based on the topological structure of the input space. The second one (EmECS) is a new method we introduce; it is based on geometric properties of convex sets and topological embedding of the input space into the boundary of a convex set. We discuss the behavior of the network when it operates on the transformed inputs. We also compare it experimentally with some neural nets that do not use the same input transformations, and with the classic algorithm of tile coding plus a linear function approximator, and on several online reinforcement learning tasks, we show that the neural net with tile coding or EmECS can achieve not only faster learning but also more accurate approximations. Our results strongly suggest that geometric input transformation of this type can be effective for interference reduction and takes us a step closer to fully incremental reinforcement learning with neural nets.",0
"In online reinforcement learning, we utilize neural nets with ReLU gates and aim to train these networks incrementally, avoiding the costly experience replay. However, we have observed that the global nature of ReLU gates can cause unwanted learning interference among individual neural nodes during online training. To mitigate this interference, we propose two geometry-based input transformation methods: tile coding and EmECS. Tile coding, originally designed for local generalization, employs a binary encoding scheme based on the input space's topological structure. EmECS, a novel approach we introduce, is based on geometric properties of convex sets and topological embedding of the input space into a convex set's boundary. We evaluate the network's performance on transformed inputs and compare it with other neural nets that do not use similar input transformations, as well as the classic tile coding algorithm combined with a linear function approximator. Our experiments on various online reinforcement learning tasks demonstrate that neural nets using tile coding or EmECS achieve faster learning and more precise approximations. Our findings suggest that geometric input transformation can effectively reduce interference and bring us closer to fully incremental reinforcement learning with neural nets.",1
"Research in deep reinforcement learning (RL) has coalesced around improving performance on benchmarks like the Arcade Learning Environment. However, these benchmarks conspicuously miss important characteristics like abrupt context-dependent shifts in strategy and temporal sensitivity that are often present in real-world domains. As a result, RL research has not focused on these challenges, resulting in algorithms which do not understand critical changes in context, and have little notion of real world time. To tackle this issue, this paper introduces the game of Space Fortress as a RL benchmark which incorporates these characteristics. We show that existing state-of-the-art RL algorithms are unable to learn to play the Space Fortress game. We then confirm that this poor performance is due to the RL algorithms' context insensitivity and reward sparsity. We also identify independent axes along which to vary context and temporal sensitivity, allowing Space Fortress to be used as a testbed for understanding both characteristics in combination and also in isolation. We release Space Fortress as an open-source Gym environment.",0
"The focus of deep reinforcement learning (RL) research has been to improve performance on benchmarks such as the Arcade Learning Environment. However, these benchmarks lack important features such as sudden shifts in strategy and temporal sensitivity that are common in real-world situations. Therefore, RL research has not addressed these challenges, resulting in algorithms that cannot recognize significant changes in context or real-world time. In order to address this issue, this study proposes Space Fortress as a RL benchmark that incorporates these features. It is demonstrated that existing state-of-the-art RL algorithms cannot learn to play Space Fortress due to their context insensitivity and sparse rewards. The study also identifies independent axes for varying context and temporal sensitivity, making Space Fortress a useful testbed for investigating these features both in combination and isolation. The open-source Gym environment for Space Fortress is also released.",1
"We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: ""Are there any apples in the fridge?"" The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive objects (code and dataset available at https://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98",0
"The task of Interactive Question Answering (IQA) involves answering questions that require an autonomous agent to interact with a dynamic visual environment. For example, a question like ""Are there any apples in the fridge?"" presented to the agent requires the agent to navigate the scene, understand the visual elements, interact with objects, and plan a series of actions based on the question. However, popular reinforcement learning approaches with a single controller are not effective for IQA due to the large and diverse state space. To address this, we propose the Hierarchical Interactive Memory Network (HIMN) with a factorized set of controllers that allows the system to operate at multiple levels of temporal abstraction. To evaluate HIMN's performance, we introduce IQUAD V1, a new dataset built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 comprises 75,000 questions, each paired with a unique scene configuration. Our experiments show that HIMN outperforms popular single controller-based approaches on IQUAD V1. For more information, please refer to our video: https://youtu.be/pXd3C-1jr98.",1
"Considering its advantages in dealing with high-dimensional visual input and learning control policies in discrete domain, Deep Q Network (DQN) could be an alternative method of traditional auto-focus means in the future. In this paper, based on Deep Reinforcement Learning, we propose an end-to-end approach that can learn auto-focus policies from visual input and finish at a clear spot automatically. We demonstrate that our method - discretizing the action space with coarse to fine steps and applying DQN is not only a solution to auto-focus but also a general approach towards vision-based control problems. Separate phases of training in virtual and real environments are applied to obtain an effective model. Virtual experiments, which are carried out after the virtual training phase, indicates that our method could achieve 100% accuracy on a certain view with different focus range. Further training on real robots could eliminate the deviation between the simulator and real scenario, leading to reliable performances in real applications.",0
"The potential of Deep Q Network (DQN) as an alternative to traditional auto-focus methods in the future is evident due to its ability to handle high-dimensional visual input and learn control policies in a discrete domain. To achieve this, our paper proposes an end-to-end approach based on Deep Reinforcement Learning, which can learn auto-focus policies from visual input and automatically focus on a clear spot. By discretizing the action space with coarse to fine steps and applying DQN, our method not only provides a solution to auto-focus but also offers a general approach to vision-based control problems. To obtain an effective model, we use separate phases of training in virtual and real environments. Our virtual experiments after the virtual training phase show that our method achieves 100% accuracy on a certain view with different focus ranges. Further training on real robots can eliminate the deviation between the simulator and real scenarios, ensuring reliable performance in real applications.",1
"Recently deep reinforcement learning (DRL) has achieved outstanding success on solving many difficult and large-scale RL problems. However the high sample cost required for effective learning often makes DRL unaffordable in resource-limited applications. With the aim of improving sample efficiency and learning performance, we will develop a new DRL algorithm in this paper that seamless integrates entropy-induced and bootstrap-induced techniques for efficient and deep exploration of the learning environment. Specifically, a general form of Tsallis entropy regularizer will be utilized to drive entropy-induced exploration based on efficient approximation of optimal action-selection policies. Different from many existing works that rely on action dithering strategies for exploration, our algorithm is efficient in exploring actions with clear exploration value. Meanwhile, by employing an ensemble of Q-networks under varied Tsallis entropy regularization, the diversity of the ensemble can be further enhanced to enable effective bootstrap-induced exploration. Experiments on Atari game playing tasks clearly demonstrate that our new algorithm can achieve more efficient and effective exploration for DRL, in comparison to recently proposed exploration methods including Bootstrapped Deep Q-Network and UCB Q-Ensemble.",0
"In recent times, deep reinforcement learning (DRL) has proven to be a remarkable solution for tackling numerous challenging and extensive RL problems. However, the cost of obtaining sufficient samples for effective learning often renders DRL impractical for resource-limited applications. To address this challenge and enhance sample efficiency and learning performance, this paper will introduce a new DRL algorithm that seamlessly integrates entropy-induced and bootstrap-induced techniques for deep and efficient exploration of the learning environment. The paper will employ a general form of Tsallis entropy regularizer to motivate entropy-induced exploration based on optimal action-selection policies' efficient approximation. Unlike many existing works that rely on action dithering strategies for exploration, the proposed algorithm efficiently explores actions with clear exploration value. Additionally, the algorithm will utilize an ensemble of Q-networks under varied Tsallis entropy regularization to further enhance the ensemble's diversity and enable effective bootstrap-induced exploration. Results from experiments on Atari game playing tasks show that the new algorithm achieves more efficient and effective exploration for DRL compared to other recently proposed exploration methods, including Bootstrapped Deep Q-Network and UCB Q-Ensemble.",1
"A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io",0
"By utilizing compressed spatio-temporal representations, a generative recurrent neural network is trained in an unsupervised manner to simulate widely used reinforcement learning environments. The extracted features of the world model are then utilized to train simple and condensed policies using evolution, which results in excellent outcomes in a range of environments. Furthermore, we train our agent exclusively within a self-generated environment produced by its internal world model, before transferring this policy back to the real environment. For an interactive version of the paper, please visit https://worldmodels.github.io.",1
"In the past few years, deep reinforcement learning has been proven to solve problems which have complex states like video games or board games. The next step of intelligent agents would be able to generalize between tasks, and using prior experience to pick up new skills more quickly. However, most reinforcement learning algorithms for now are often suffering from catastrophic forgetting even when facing a very similar target task. Our approach enables the agents to generalize knowledge from a single source task, and boost the learning progress with a semisupervised learning method when facing a new task. We evaluate this approach on Atari games, which is a popular reinforcement learning benchmark, and show that it outperforms common baselines based on pre-training and fine-tuning.",0
"Recently, deep reinforcement learning has demonstrated its ability to tackle complex state problems such as video and board games. However, the next step for intelligent agents is to generalize between tasks and effectively apply prior experiences to expedite the acquisition of new skills. Despite this goal, current reinforcement learning algorithms frequently experience catastrophic forgetting, even when presented with similar target tasks. Our approach aims to address this issue by enabling agents to apply knowledge from a single source task and enhance learning through a semisupervised learning method when presented with a new task. We tested this approach on Atari games, a popular reinforcement learning benchmark, and found that it surpassed common baselines that rely on pre-training and fine-tuning.",1
"Most existing video summarisation methods are based on either supervised or unsupervised learning. In this paper, we propose a reinforcement learning-based weakly supervised method that exploits easy-to-obtain, video-level category labels and encourages summaries to contain category-related information and maintain category recognisability. Specifically, We formulate video summarisation as a sequential decision-making process and train a summarisation network with deep Q-learning (DQSN). A companion classification network is also trained to provide rewards for training the DQSN. With the classification network, we develop a global recognisability reward based on the classification result. Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning. Extensive experiments on two benchmark datasets show that the proposed approach achieves state-of-the-art performance.",0
"The majority of current approaches to video summarisation rely on supervised or unsupervised learning. Our research introduces a novel method that employs reinforcement learning and weak supervision, which utilises video-level category labels to facilitate inclusion of category-related information in summaries while maintaining category recognisability. Our approach involves formulating video summarisation as a sequential decision-making process and training a summarisation network using deep Q-learning (DQSN). Additionally, we train a companion classification network to provide rewards for training the DQSN. Our method incorporates a global recognisability reward based on the classification result, as well as a novel dense ranking-based reward to account for temporally delayed and sparse reward problems during long sequence reinforcement learning. Through extensive experiments conducted on two benchmark datasets, we demonstrate that our approach achieves state-of-the-art performance.",1
"We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",0
"Our focus is on developing deep neural network policies for reinforcement learning that have a hierarchical structure. Unlike other methods that limit the lower layers of a hierarchy to use higher-level modulating signals, we train each layer in our framework to directly solve the task. However, we incorporate a range of diverse strategies into each layer through a maximum entropy reinforcement learning objective and latent random variables, which are sampled during training. This objective results in the latent variables being included in the layer's policy, allowing the higher level layer to control the lower layer's behavior through this latent space. We retain full expressivity by ensuring that the mapping from latent variables to actions is invertible, thereby avoiding constraints on either layer's behavior. Our experiments show that our method outperforms single-layer policies on standard benchmark tasks by adding more layers and can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",1
"This paper explores the use of deep reinforcement learning agents to transfer knowledge from one environment to another. More specifically, the method takes advantage of asynchronous advantage actor critic (A3C) architecture to generalize a target game using an agent trained on a source game in Atari. Instead of fine-tuning a pre-trained model for the target game, we propose a learning approach to update the model using multiple agents trained in parallel with different representations of the target game. Visual mapping between video sequences of transfer pairs is used to derive new representations of the target game; training on these visual representations of the target game improves model updates in terms of performance, data efficiency and stability. In order to demonstrate the functionality of the architecture, Atari games Pong-v0 and Breakout-v0 are being used from the OpenAI gym environment; as the source and target environment.",0
"The aim of this study is to investigate the use of deep reinforcement learning agents for transferring knowledge between different environments. Specifically, the study utilizes the asynchronous advantage actor critic (A3C) architecture to generalize a target game by training an agent on a source game in Atari. Rather than fine-tuning a pre-trained model for the target game, the authors propose a learning method that involves updating the model using multiple agents trained in parallel with various representations of the target game. To achieve this, visual mapping is utilized between video sequences of transfer pairs to generate new representations of the target game. Training on these visual representations enhances model updates concerning performance, data efficiency, and stability. To demonstrate the effectiveness of the architecture, the authors utilize Atari games Pong-v0 and Breakout-v0 as the source and target environments from the OpenAI gym environment.",1
"Recent success in deep reinforcement learning is having an agent learn how to play Go and beat the world champion without any prior knowledge of the game. In that task, the agent has to make a decision on what action to take based on the positions of the pieces. Person Search is recently explored using natural language based text description of images for video surveillance applications (S.Li et.al). We see (Fu.et al) provides an end to end approach for object-based retrieval using deep reinforcement learning without constraints placed on which objects are being detected. However, we believe for real-world applications such as person search defining specific constraints which identify a person as opposed to starting with a general object detection will have benefits in terms of performance and computational resources required. In our task, Deep reinforcement learning would localize the person in an image by reshaping the sizes of the bounding boxes. Deep Reinforcement learning with appropriate constraints would look only for the relevant person in the image as opposed to an unconstrained approach where each individual objects in the image are ranked. For person search, the agent is trying to form a tight bounding box around the person in the image who matches the description. The bounding box is initialized to the full image and at each time step, the agent makes a decision on how to change the current bounding box so that it has a tighter bound around the person based on the description of the person and the pixel values of the current bounding box. After the agent takes an action, it will be given a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box. Once the agent believes that the bounding box is covering the person, it will indicate that the person is found.",0
"The latest achievement in deep reinforcement learning involves an agent mastering the game of Go and beating the world champion without any prior knowledge of the game. Meanwhile, a recent study (S.Li et.al) has been conducted to explore Person Search using natural language-based text description of images for video surveillance applications. Although Fu.et al provides an end-to-end approach for object-based retrieval using deep reinforcement learning, it is believed that defining specific constraints for identifying a person in an image would be more beneficial in real-world applications, such as person search, as it would enhance performance and minimize computational resources required. In this task, deep reinforcement learning techniques are utilized to localize the person in an image by reshaping the sizes of the bounding boxes. By applying appropriate constraints, the agent can focus solely on the relevant person in the image instead of ranking each individual object in an unconstrained approach. The goal of the agent in person search is to create a tight bounding box around the person in the image, whose description matches the given criteria. The bounding box is initially set to the full image, and at each time step, the agent makes a decision on how to modify the current bounding box to create a tighter bound around the person based on the description and pixel values. The agent is rewarded based on the Intersection over Union (IoU) of the current bounding box and the ground truth box, and it will indicate that the person has been found once it believes that the bounding box covers the person.",1
"We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.",0
"Our study utilizes the latest reinforcement learning algorithm to achieve end-to-end driving, without the need for any mediated perception such as object recognition or scene understanding. Our newly proposed reward and learning strategies work in tandem to enhance convergence and increase the robustness of driving, using only RGB images from a forward-facing camera. We employ an Asynchronous Actor Critic (A3C) framework to teach our car control in a physically and graphically realistic rally game. Our agents evolve simultaneously on various tracks featuring a range of road structures, graphics, and physics. We conduct a comprehensive evaluation and prove the generalization of our approach on unseen tracks and within legal speed limits. Our method also demonstrates some domain adaption capability during open loop tests on real image sequences.",1
"Assisted by neural networks, reinforcement learning agents have been able to solve increasingly complex tasks over the last years. The simulation environment in which the agents interact is an essential component in any reinforcement learning problem. The environment simulates the dynamics of the agents' world and hence provides feedback to their actions in terms of state observations and external rewards. To ease the design and simulation of such environments this work introduces $\texttt{APES}$, a highly customizable and open source package in Python to create 2D grid-world environments for reinforcement learning problems. $\texttt{APES}$ equips agents with algorithms to simulate any field of vision, it allows the creation and positioning of items and rewards according to user-defined rules, and supports the interaction of multiple agents.",0
"Neural networks have assisted reinforcement learning agents in solving increasingly complex tasks in recent years. The simulation environment plays a crucial role in any reinforcement learning problem as it simulates the agents' world dynamics and provides feedback to their actions through state observations and external rewards. To simplify the creation and simulation of such environments, this study introduces $\texttt{APES}$, a customizable and open source Python package that creates 2D grid-world environments for reinforcement learning problems. $\texttt{APES}$ comes equipped with algorithms that enable agents to simulate any field of vision, create and position items and rewards according to user-defined rules, and allow multiple agents to interact.",1
"We study an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. We propose a model-free RL method that modifies Delayed Q-learning and utilizes the long-term exploration bonus with provable efficiency. We show that our proposed method finds a near-optimal policy in polynomial time (PAC-MDP), and also provide experimental evidence that our proposed algorithm is an efficient exploration method.",0
"Our focus is on exploring a model-free RL technique that extends the counter-based exploration bonus methods by considering the long-term exploratory value of actions instead of a single step look-ahead. We present a modified version of Delayed Q-learning that integrates the long-term exploration bonus, which can be proven to be efficient. Our method is demonstrated to achieve close to optimal results in polynomial time (PAC-MDP), and we provide empirical data to support that it is an effective exploration approach.",1
"We introduce a new virtual environment for simulating a card game known as ""Big 2"". This is a four-player game of imperfect information with a relatively complicated action space (being allowed to play 1,2,3,4 or 5 card combinations from an initial starting hand of 13 cards). As such it poses a challenge for many current reinforcement learning methods. We then use the recently proposed ""Proximal Policy Optimization"" algorithm to train a deep neural network to play the game, purely learning via self-play, and find that it is able to reach a level which outperforms amateur human players after only a relatively short amount of training time and without needing to search a tree of future game states.",0
"A new virtual environment has been developed to simulate the card game ""Big 2"". This game involves four players and has an action space that allows players to play 1, 2, 3, 4 or 5 card combinations from an initial hand of 13 cards, making it complex and challenging for reinforcement learning methods. To address this challenge, a deep neural network was trained using the ""Proximal Policy Optimization"" algorithm through self-play. The network achieved a higher level of performance than amateur human players in a relatively short period of training time, without requiring a search of future game states.",1
"Estimating the value function for a fixed policy is a fundamental problem in reinforcement learning. Policy evaluation algorithms---to estimate value functions---continue to be developed, to improve convergence rates, improve stability and handle variability, particularly for off-policy learning. To understand the properties of these algorithms, the experimenter needs high-confidence estimates of the accuracy of the learned value functions. For environments with small, finite state-spaces, like chains, the true value function can be easily computed, to compute accuracy. For large, or continuous state-spaces, however, this is no longer feasible. In this paper, we address the largely open problem of how to obtain these high-confidence estimates, for general state-spaces. We provide a high-confidence bound on an empirical estimate of the value error to the true value error. We use this bound to design an offline sampling algorithm, which stores the required quantities to repeatedly compute value error estimates for any learned value function. We provide experiments investigating the number of samples required by this offline algorithm in simple benchmark reinforcement learning domains, and highlight that there are still many open questions to be solved for this important problem.",0
"Reinforcement learning involves the fundamental issue of estimating the value function for a fixed policy. While policy evaluation algorithms have been developed to estimate value functions and improve convergence rates, stability, and variability handling for off-policy learning, accurate evaluation of learned value functions is crucial to understanding these algorithms' properties. Small, finite state-space environments like chains can easily compute accuracy, while large or continuous state-spaces cannot. This paper addresses the challenge of obtaining high-confidence estimates of accuracy for general state-spaces, providing a high-confidence bound on empirical estimates of value error to true value error. An offline sampling algorithm is designed, storing the necessary data to repeatedly compute value error estimates for any learned value function. Simple benchmark reinforcement learning domains' experiments assess the number of samples required by this offline algorithm, highlighting the need to address unanswered questions regarding this critical issue.",1
"Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",0
"According to recent research, reinforcement learning (RL) is a successful technique for enhancing the performance of neural machine translation (NMT) systems. However, RL training can be challenging due to instability, especially in real-world systems that utilize deep models and large datasets. This study aims to improve NMT models using reinforcement learning by conducting a systematic evaluation of various factors, such as baseline reward and reward shaping, on several large-scale translation tasks. Additionally, we introduce a new approach that uses RL to further enhance the performance of NMT systems trained with monolingual data. By combining our findings, we achieve competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, including a state-of-the-art performance on the WMT17 Chinese-English translation task.",1
"We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.",0
"The focus of our investigation is the challenge of acquiring knowledge and creating strategies for extensive, sparsely rewarded domains with high dimensions and long horizons. While recent methodologies have proven effective in various Atari 2600 domains, such as Montezuma's Revenge and Venture, which require long-range planning and have sparse rewards, remain problematic for current techniques. However, previous investigations have found that methods utilizing abstraction, such as those proposed by Dietterich in 2000 and Sutton, Precup, and Singh in 1999, can be advantageous when tackling long-range challenges. By combining the latest deep reinforcement learning methods with established model-based approaches and incorporating an expert-provided state abstraction, we have developed a new algorithm. We have also created sample domains that illustrate the obstacles of long horizons, sparse rewards, and high-dimensional inputs. The results demonstrate that our abstraction-based approach surpasses previous methods on these domains, including Montezuma's Revenge and Venture, and exhibits backtracking behavior absent in previous methods.",1
"Indoor navigation aims at performing navigation within buildings. In scenes like home and factory, most intelligent mobile devices require an functionality of routing to guide itself precisely through indoor scenes to complete various tasks in order to serve human. In most scenarios, we expected an intelligent device capable of navigating itself in unseen environment. Although several solutions have been proposed to deal with this issue, they usually require pre-installed beacons or a map pre-built with SLAM, which means that they are not capable of working in novel environments. To address this, we proposed NavigationNet, a computer vision dataset and benchmark to allow the utilization of deep reinforcement learning on scene-understanding-based indoor navigation. We also proposed and formalized several typical indoor routing problems that are suitable for deep reinforcement learning.",0
"The goal of indoor navigation is to guide mobile devices through buildings such as homes and factories to perform various tasks for humans. However, in order to navigate through new and unfamiliar environments, these devices require advanced routing functionality. While some solutions have been proposed, such as pre-installed beacons or pre-built maps using SLAM technology, these methods can only be used in specific environments. To address this issue, we have introduced NavigationNet, which is a computer vision dataset and benchmark for deep reinforcement learning in indoor navigation. We have also defined several typical indoor routing problems that are well-suited for this approach.",1
"We study offline data poisoning attacks in contextual bandits, a class of reinforcement learning problems with important applications in online recommendation and adaptive medical treatment, among others. We provide a general attack framework based on convex optimization and show that by slightly manipulating rewards in the data, an attacker can force the bandit algorithm to pull a target arm for a target contextual vector. The target arm and target contextual vector are both chosen by the attacker. That is, the attacker can hijack the behavior of a contextual bandit. We also investigate the feasibility and the side effects of such attacks, and identify future directions for defense. Experiments on both synthetic and real-world data demonstrate the efficiency of the attack algorithm.",0
"Our focus is on offline data poisoning attacks in the area of contextual bandits, which are a type of reinforcement learning problems with significant applications in various fields, including online recommendations and personalized medical treatments. We present a comprehensive attack framework that employs convex optimization and proves that a slight modification of rewards in the data can result in the bandit algorithm selecting a specific arm for a particular contextual vector as dictated by the attacker. The attacker has the freedom to pick both the target arm and the target contextual vector, effectively taking over the decision-making process of the contextual bandit. Our study also explores the practicality and side effects of such attacks and suggests possible defense measures. Experiments carried out on synthetic and real-world data reveal the effectiveness of our attack algorithm.",1
"The large volume of video content and high viewing frequency demand automatic video summarization algorithms, of which a key property is the capability of modeling diversity. If videos are lengthy like hours-long egocentric videos, it is necessary to track the temporal structures of the videos and enforce local diversity. The local diversity refers to that the shots selected from a short time duration are diverse but visually similar shots are allowed to co-exist in the summary if they appear far apart in the video. In this paper, we propose a novel probabilistic model, built upon SeqDPP, to dynamically control the time span of a video segment upon which the local diversity is imposed. In particular, we enable SeqDPP to learn to automatically infer how local the local diversity is supposed to be from the input video. The resulting model is extremely involved to train by the hallmark maximum likelihood estimation (MLE), which further suffers from the exposure bias and non-differentiable evaluation metrics. To tackle these problems, we instead devise a reinforcement learning algorithm for training the proposed model. Extensive experiments verify the advantages of our model and the new learning algorithm over MLE-based methods.",0
"Due to the vast amount of video content available and the high frequency of viewing, there is a need for algorithms that can automatically summarize videos. A crucial feature of such algorithms is the ability to model diversity, particularly for lengthy egocentric videos where it is necessary to track the temporal structures and enforce local diversity. Local diversity pertains to selecting visually diverse shots within a short time duration, but visually similar shots can co-exist if they are far apart in the video. In this paper, we introduce a novel probabilistic model based on SeqDPP, which dynamically controls the time span of a video segment to impose local diversity. The model learns to infer the degree of local diversity required from the input video. Since the model is complex and difficult to train using maximum likelihood estimation (MLE), which is also susceptible to exposure bias and non-differentiable evaluation metrics, we propose a reinforcement learning algorithm for training instead. Our experiments demonstrate the superiority of our model and learning algorithm over MLE-based methods.",1
"Reinforcement learning approaches have long appealed to the data management community due to their ability to learn to control dynamic behavior from raw system performance. Recent successes in combining deep neural networks with reinforcement learning have sparked significant new interest in this domain. However, practical solutions remain elusive due to large training data requirements, algorithmic instability, and lack of standard tools. In this work, we introduce LIFT, an end-to-end software stack for applying deep reinforcement learning to data management tasks. While prior work has frequently explored applications in simulations, LIFT centers on utilizing human expertise to learn from demonstrations, thus lowering online training times. We further introduce TensorForce, a TensorFlow library for applied deep reinforcement learning exposing a unified declarative interface to common RL algorithms, thus providing a backend to LIFT. We demonstrate the utility of LIFT in two case studies in database compound indexing and resource management in stream processing. Results show LIFT controllers initialized from demonstrations can outperform human baselines and heuristics across latency metrics and space usage by up to 70%.",0
"The data management community has been interested in reinforcement learning techniques for a while due to their ability to learn how to control dynamic behavior from raw system performance. Recent advances in combining deep neural networks with reinforcement learning have led to renewed interest in this area. However, there are still challenges to finding practical solutions, such as the need for a large amount of training data, algorithmic instability, and a lack of standard tools. In this study, we present LIFT, a complete software stack that uses deep reinforcement learning for data management tasks. While previous research has focused on simulations, LIFT emphasizes the use of human expertise to learn from demonstrations, which reduces online training times. We also introduce TensorForce, a TensorFlow library that provides a unified declarative interface to common RL algorithms, which serves as a backend for LIFT. We demonstrate the effectiveness of LIFT in two case studies involving database compound indexing and resource management in stream processing, where the controllers initialized from demonstrations outperformed human baselines and heuristics in terms of latency metrics and space usage by up to 70%.",1
"Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the ""exposure bias"" during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (e.g., visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (e.g., ""man riding horse"") and comparisons (e.g., ""smaller cat""). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP",0
"The problem of sequence prediction for natural language output can encompass many vision-language tasks. Deep reinforcement learning (RL) has been used in recent advancements in image captioning to address ""exposure bias"" during training, where the ground-truth subsequence is exposed in every step prediction, leading to bias during testing when only the predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy and not the visual policy, resulting in a failure to capture visual context crucial for compositional reasoning, such as visual relationships and comparisons. To address this gap, we introduce the Context-Aware Visual Policy network (CAVP), which leverages previous visual attention as context at every time step to make optimal decisions for current word generation. CAVP can attend to complex visual compositions over time, as opposed to traditional visual attention that only fixes a single image region at every step. The entire image captioning model, including CAVP and its subsequent language policy network, can be efficiently optimized end-to-end using an actor-critic policy gradient method with respect to any caption evaluation metric. Our method outperforms state-of-the-art performances on MS-COCO offline split and online server using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP.",1
"In this paper we discuss policy iteration methods for approximate solution of a finite-state discounted Markov decision problem, with a focus on feature-based aggregation methods and their connection with deep reinforcement learning schemes. We introduce features of the states of the original problem, and we formulate a smaller ""aggregate"" Markov decision problem, whose states relate to the features. We discuss properties and possible implementations of this type of aggregation, including a new approach to approximate policy iteration. In this approach the policy improvement operation combines feature-based aggregation with feature construction using deep neural networks or other calculations. We argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation, than by the linear function of the features provided by neural network-based reinforcement learning, thereby potentially leading to more effective policy improvement.",0
The focus of this paper is policy iteration methods for solving a finite-state discounted Markov decision problem. The paper explores feature-based aggregation methods and their relation to deep reinforcement learning schemes. The authors introduce features of the states in the original problem and use them to formulate a smaller Markov decision problem. This new approach to approximate policy iteration involves combining feature-based aggregation with feature construction through deep neural networks or other calculations. The paper argues that the nonlinear function of the features provided by aggregation can lead to more effective policy improvement compared to the linear function of the features provided by neural network-based reinforcement learning.,1
This paper describes some of the possibilities of artificial neural networks that open up after solving the problem of catastrophic forgetting. A simple model and reinforcement learning applications of existing methods are also proposed.,0
"In this article, the potential of artificial neural networks is explored in light of overcoming catastrophic forgetting. Additionally, a basic model and reinforcement learning techniques using current methods are suggested.",1
"Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as ""importance mixing"" can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.",0
"Recently, deep neuroevolution, which is based on deep neural networks, has emerged as a competitor to deep reinforcement learning algorithms due to its superior parallelization capabilities. However, its sample efficiency still remains unsatisfactory. This paper aims to explore whether ""importance mixing"" can enhance its sample efficiency. The authors provide a clear explanation of importance mixing and its extension to reuse more samples. Furthermore, they conduct an empirical comparison using a basic benchmark and demonstrate that, although importance mixing does improve sample efficiency, it still lags behind deep reinforcement learning in this regard. Nonetheless, it is more stable.",1
"One of the major challenges of model-free visual tracking problem has been the difficulty originating from the unpredictable and drastic changes in the appearance of objects we target to track. Existing methods tackle this problem by updating the appearance model on-line in order to adapt to the changes in the appearance. Despite the success of these methods however, inaccurate and erroneous updates of the appearance model result in a tracker drift. In this paper, we introduce a novel real-time visual tracking algorithm based on a template selection strategy constructed by deep reinforcement learning methods. The tracking algorithm utilizes this strategy to choose the appropriate template for tracking a given frame. The template selection strategy is self-learned by utilizing a simple policy gradient method on numerous training episodes randomly generated from a tracking benchmark dataset. Our proposed reinforcement learning framework is generally applicable to other confidence map based tracking algorithms. The experiment shows that our tracking algorithm runs in real-time speed of 43 fps and the proposed policy network effectively decides the appropriate template for successful visual tracking.",0
"The model-free visual tracking problem presents a significant challenge due to the unpredictable and dramatic changes in the appearance of tracked objects. To address this issue, existing methods update the appearance model in real-time to adapt to these changes. However, inaccurate updates can cause the tracker to drift. This paper presents a new visual tracking algorithm that employs a template selection strategy developed through deep reinforcement learning methods. This strategy allows the algorithm to choose the most appropriate template for each frame, and it is learned through a policy gradient method using a tracking benchmark dataset. Our reinforcement learning framework is applicable to other confidence map-based tracking algorithms. Our experiments showed that our algorithm operates in real-time at 43 fps and successfully selects the optimal template for effective visual tracking.",1
"Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",0
"Although reinforcement learning is a useful and comprehensive approach to decision making and control, its implementation is often impeded by the requirement for extensive feature and reward engineering. Although deep reinforcement learning techniques can eliminate the necessity for explicit policy or value feature engineering, they still necessitate a manually specified reward function. Inverse reinforcement learning offers the potential for automated reward acquisition, but it has proven to be highly challenging to employ in large, high-dimensional problems with unknown dynamics. This study proposes adversarial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm using an adversarial reward learning formulation. AIRL can retrieve reward functions that are resistant to changes in dynamics, allowing us to learn policies even in instances of significant variation in the training environment. Our experiments show that AIRL surpasses previous methods in these transfer settings.",1
"Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/",0
"Reinforcement learning algorithms require environment rewards that are designed outside of the agent. However, manually annotating each environment with dense rewards is not practical for scaling up, leading to the development of intrinsic reward functions for agents. Curiosity is an intrinsic reward function that uses prediction error as a reward signal. This paper presents (a) the first large-scale study of curiosity-driven learning without any extrinsic rewards in 54 standard benchmark environments, including Atari games, showing surprisingly good performance and alignment with hand-designed extrinsic rewards in many game environments; (b) an investigation into the impact of using different feature spaces to compute prediction error, indicating that random features are sufficient for many popular RL game benchmarks, while learned features have better generalization to novel game levels in Super Mario Bros.; and (c) the limitations of prediction-based rewards in stochastic setups. Code and game-play videos are available at https://pathak22.github.io/large-scale-curiosity/.",1
"We present an approach for reconfiguration of dynamic visual sensor networks with deep reinforcement learning (RL). Our RL agent uses a modified asynchronous advantage actor-critic framework and the recently proposed Relational Network module at the foundation of its network architecture. To address the issue of sample inefficiency in current approaches to model-free reinforcement learning, we train our system in an abstract simulation environment that represents inputs from a dynamic scene. Our system is validated using inputs from a real-world scenario and preexisting object detection and tracking algorithms.",0
"Our study proposes a method for the modification of dynamic visual sensor networks using deep reinforcement learning (RL). Our RL agent employs a modified asynchronous advantage actor-critic framework and incorporates the Relational Network module into its network architecture. To combat the problem of sample inefficiency in current model-free reinforcement learning approaches, we train our system in an abstract simulation environment that simulates inputs from a dynamic scene. We validate our system using inputs from a real-world scenario and preexisting object detection and tracking algorithms.",1
"Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.",0
"Numerous Reinforcement Learning agents presently operate in an environment where they interact with humans, including colleagues, users, or clients. To ensure that these agents are effective, it is crucial that they adapt to individuals' preferences, learn quickly with their assistance, and behave safely around them. However, the majority of current approaches for learning from human feedback are hazardous since rewarding or punishing an agent after the fact does not immediately prevent it from engaging in undesirable behavior. In this article, we introduce Directed Policy Gradient (DPG), which extends Policy Gradient and makes it impervious to external directives that could otherwise undermine its on-policy nature. DPG allows a teacher or backup policy to override the agent before it behaves inappropriately, while also enabling the agent to benefit from human advice or directives to learn more quickly. Our experiments demonstrate that DPG enables the agent to learn significantly faster than reward-based methods while requiring substantially less advice.",1
"360$^{\circ}$ panoramas are a rich medium, yet notoriously difficult to visualize in the 2D image plane. We explore how intelligent rotations of a spherical image may enable content-aware projection with fewer perceptible distortions. Whereas existing approaches assume the viewpoint is fixed, intuitively some viewing angles within the sphere preserve high-level objects better than others. To discover the relationship between these optimal snap angles and the spherical panorama's content, we develop a reinforcement learning approach for the cubemap projection model. Implemented as a deep recurrent neural network, our method selects a sequence of rotation actions and receives reward for avoiding cube boundaries that overlap with important foreground objects. We show our approach creates more visually pleasing panoramas while using 5x less computation than the baseline.",0
"Although 360$^{\circ}$ panoramas are a valuable tool, they can be challenging to visualize in two dimensions. However, we investigate the potential of intelligent rotations of a spherical image to produce content-aware projection with fewer visible distortions. Unlike existing methods that assume a fixed viewpoint, some viewing angles within the sphere may preserve high-level objects more effectively. To identify the optimal snap angles and their relationship with the content of the spherical panorama, we use a reinforcement learning approach for the cubemap projection model. Our method, which is implemented as a deep recurrent neural network, selects a sequence of rotation actions and receives rewards for avoiding cube boundaries that overlap with important foreground objects. Our findings demonstrate that our approach can produce more aesthetically pleasing panoramas while consuming 5 times less computation than the baseline.",1
"Policy optimization is a core component of reinforcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its surrogate. Though often achieving encouraging empirical success, its underlying mathematical principle on {\em policy-distribution} optimization is unclear. We place policy optimization into the space of probability measures, and interpret it as Wasserstein gradient flows. On the probability-measure space, under specified circumstances, policy optimization becomes a convex problem in terms of distribution optimization. To make optimization feasible, we develop efficient algorithms by numerically solving the corresponding discrete gradient flows. Our technique is applicable to several RL settings, and is related to many state-of-the-art policy-optimization algorithms. Empirical results verify the effectiveness of our framework, often obtaining better performance compared to related algorithms.",0
"Reinforcement learning (RL) relies heavily on policy optimization, where existing methods optimize the parameters of a policy to maximize the expected total reward or its surrogate. Although these methods have shown promising results, the underlying mathematical principle of policy-distribution optimization remains unclear. In this study, we introduce a new approach that places policy optimization in the probability-measure space and interprets it as Wasserstein gradient flows. By doing so, policy optimization becomes a convex problem in terms of distribution optimization under certain circumstances. To enable optimization, we have developed efficient algorithms that solve the corresponding discrete gradient flows. Our technique is versatile and can be applied to various RL settings, and is related to many state-of-the-art policy-optimization algorithms. Our empirical results demonstrate the effectiveness of our framework, which often outperforms related algorithms.",1
"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",0
"A variety of challenging decision making and control tasks have been tackled successfully using model-free deep reinforcement learning (RL) algorithms. However, these algorithms are typically plagued by two major issues: high sample complexity and brittle convergence properties, which require careful hyperparameter tuning. As a result, their applicability to complex, real-world domains is severely limited. This paper introduces the soft actor-critic, an off-policy actor-critic deep RL algorithm built on the maximum entropy reinforcement learning framework. In this framework, the actor seeks to maximize both the expected reward and entropy, aiming to perform the task while acting randomly. Unlike previous deep RL methods that relied on Q-learning methods, our approach combines off-policy updates with a stable stochastic actor-critic formulation, achieving state-of-the-art performance on various continuous control benchmark tasks, surpassing on-policy and off-policy methods. Furthermore, we demonstrate that our approach is remarkably stable, producing similar outcomes across different random seeds, unlike other off-policy methods.",1
"Reinforcement learning studies how to balance exploration and exploitation in real-world systems, optimizing interactions with the world while simultaneously learning how the world operates. One general class of algorithms for such learning is the multi-armed bandit setting. Randomized probability matching, based upon the Thompson sampling approach introduced in the 1930s, has recently been shown to perform well and to enjoy provable optimality properties. It permits generative, interpretable modeling in a Bayesian setting, where prior knowledge is incorporated, and the computed posteriors naturally capture the full state of knowledge. In this work, we harness the information contained in the Bayesian posterior and estimate its sufficient statistics via sampling. In several application domains, for example in health and medicine, each interaction with the world can be expensive and invasive, whereas drawing samples from the model is relatively inexpensive. Exploiting this viewpoint, we develop a double sampling technique driven by the uncertainty in the learning process: it favors exploitation when certain about the properties of each arm, exploring otherwise. The proposed algorithm does not make any distributional assumption and it is applicable to complex reward distributions, as long as Bayesian posterior updates are computable. Utilizing the estimated posterior sufficient statistics, double sampling autonomously balances the exploration-exploitation tradeoff to make better informed decisions. We empirically show its reduced cumulative regret when compared to state-of-the-art alternatives in representative bandit settings.",0
"Reinforcement learning involves finding a balance between exploration and exploitation in real-world systems. This is achieved by optimizing interactions with the world while simultaneously learning how the world operates. One way to approach this is through multi-armed bandit algorithms, which have been shown to be effective and have provable optimality properties. These algorithms are based on the Thompson sampling approach, which is a randomized probability matching method introduced in the 1930s. This approach allows for generative, interpretable modeling in a Bayesian setting that incorporates prior knowledge, with computed posteriors capturing the full state of knowledge. In this study, we utilize the Bayesian posterior and estimate its sufficient statistics through sampling. This is particularly useful in domains where each interaction with the world is expensive and invasive, while drawing samples from the model is relatively inexpensive. By exploiting this viewpoint, we develop a double sampling technique that balances the exploration-exploitation tradeoff by favoring exploitation when certain about each arm's properties and exploring otherwise. The algorithm is not dependent on any distributional assumptions and is applicable to complex reward distributions, as long as Bayesian posterior updates are computable. The double sampling technique autonomously balances the exploration-exploitation tradeoff to make better informed decisions. Our empirical results show reduced cumulative regret when compared to state-of-the-art alternatives in representative bandit settings.",1
"The recently proposed distributional approach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a deep generative model of the value distribution, driven by the discrepancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to propose a GAN-based approach to DiRL, which leverages the strengths of GANs in learning distributions of high-dimensional data. In particular, we show that our GAN approach can be used for DiRL with multivariate rewards, an important setting which cannot be tackled with prior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exploit this idea to devise a novel exploration method that is driven by the discrepancy in estimating both values and states.",0
"The latest technique in reinforcement learning, Distributional approach to reinforcement learning (DiRL), focuses on acquiring knowledge about the value distribution, which is the distribution of the reward-to-go. Our research demonstrates that the distributional Bellman equation, which governs DiRL methods, is tantamount to a generative adversarial network (GAN) model. This means that DiRL is essentially a deep generative model of the value distribution, driven by the difference between the current value distribution and the distribution of the sum of the current reward and the next value. We utilize this understanding to propose a GAN-based approach to DiRL, which benefits from GANs' ability to learn high-dimensional data distributions. Our GAN model is well-suited for DiRL with multivariate rewards, which was not possible with previous methods. Besides, we introduce a novel exploration approach that relies on estimating both values and states' discrepancies, which is possible in the multivariate setting.",1
"We propose a reinforcement learning approach for real-time exposure control of a mobile camera that is personalizable. Our approach is based on Markov Decision Process (MDP). In the camera viewfinder or live preview mode, given the current frame, our system predicts the change in exposure so as to optimize the trade-off among image quality, fast convergence, and minimal temporal oscillation. We model the exposure prediction function as a fully convolutional neural network that can be trained through Gaussian policy gradient in an end-to-end fashion. As a result, our system can associate scene semantics with exposure values; it can also be extended to personalize the exposure adjustments for a user and device. We improve the learning performance by incorporating an adaptive metering module that links semantics with exposure. This adaptive metering module generalizes the conventional spot or matrix metering techniques. We validate our system using the MIT FiveK and our own datasets captured using iPhone 7 and Google Pixel. Experimental results show that our system exhibits stable real-time behavior while improving visual quality compared to what is achieved through native camera control.",0
"A personalized reinforcement learning method for real-time exposure control of mobile cameras is proposed using Markov Decision Process. Our system predicts the change in exposure based on the current frame in the camera viewfinder or live preview mode to optimize image quality, fast convergence, and minimal temporal oscillation. The exposure prediction function is modeled as a fully convolutional neural network trained through Gaussian policy gradient. Our system associates scene semantics with exposure values and can be customized for a user and device. The learning performance is enhanced by an adaptive metering module that links semantics with exposure, which generalizes conventional spot or matrix metering techniques. The system is validated using MIT FiveK and our own datasets captured using iPhone 7 and Google Pixel. Experimental results demonstrate stable real-time behavior and improved visual quality compared to native camera control.",1
"Within the context of autonomous driving a model-based reinforcement learning algorithm is proposed for the design of neural network-parameterized controllers. Classical model-based control methods, which include sampling- and lattice-based algorithms and model predictive control, suffer from the trade-off between model complexity and computational burden required for the online solution of expensive optimization or search problems at every short sampling time. To circumvent this trade-off, a 2-step procedure is motivated: first learning of a controller during offline training based on an arbitrarily complicated mathematical system model, before online fast feedforward evaluation of the trained controller. The contribution of this paper is the proposition of a simple gradient-free and model-based algorithm for deep reinforcement learning using task separation with hill climbing (TSHC). In particular, (i) simultaneous training on separate deterministic tasks with the purpose of encoding many motion primitives in a neural network, and (ii) the employment of maximally sparse rewards in combination with virtual velocity constraints (VVCs) in setpoint proximity are advocated.",0
"A reinforcement learning algorithm based on models is suggested for developing neural network-controlled autonomous driving. The traditional model-based control methods, like lattice-based algorithms, model predictive control, and sampling-based algorithms, have a drawback of balancing between the complexity of the model and computational burden required for solving expensive optimization or search problems at every short sampling time. To overcome this limitation, a two-step process is introduced: first, the controller is learned during offline training on a highly complex mathematical system model, followed by the fast feedforward evaluation of the trained controller online. This paper proposes a straightforward and model-based algorithm for deep reinforcement learning, which uses task separation with hill climbing (TSHC). It advocates for simultaneous training on separate deterministic tasks to encode many motion primitives in a neural network and the utilization of maximally sparse rewards in combination with virtual velocity constraints (VVCs) for proximity with setpoints.",1
"Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.",0
"The computation of uncertainty is crucial for developing dependable and strong deep learning systems. Although Variational Inference (VI) is a promising technique for such computations, compared to maximum-likelihood methods, it demands more implementation and execution efforts. The objective of this paper is to introduce new natural-gradient algorithms, which will decrease the exertion needed for Gaussian mean-field VI. Our algorithms can be merged within the Adam optimizer, and uncertainty estimates can be obtained at a low cost by utilizing the learning rate-adapting vector, obtained through perturbing the network weights during gradient evaluations. This method requires less memory, computation, and implementation effort than the existing VI methods while providing similar quality uncertainty estimates. Our research findings confirm this and indicate that the weight-perturbation in our algorithm could be beneficial for stochastic optimization and exploration in reinforcement learning.",1
"One of the challenges in model-based control of stochastic dynamical systems is that the state transition dynamics are involved, and it is not easy or efficient to make good-quality predictions of the states. Moreover, there are not many representational models for the majority of autonomous systems, as it is not easy to build a compact model that captures the entire dynamical subtleties and uncertainties. In this work, we present a hierarchical Bayesian linear regression model with local features to learn the dynamics of a micro-robotic system as well as two simpler examples, consisting of a stochastic mass-spring damper and a stochastic double inverted pendulum on a cart. The model is hierarchical since we assume non-stationary priors for the model parameters. These non-stationary priors make the model more flexible by imposing priors on the priors of the model. To solve the maximum likelihood (ML) problem for this hierarchical model, we use the variational expectation maximization (EM) algorithm, and enhance the procedure by introducing hidden target variables. The algorithm yields parsimonious model structures, and consistently provides fast and accurate predictions for all our examples involving large training and test sets. This demonstrates the effectiveness of the method in learning stochastic dynamics, which makes it suitable for future use in a paradigm, such as model-based reinforcement learning, to compute optimal control policies in real time.",0
"The modeling of stochastic dynamical systems presents a challenge due to the involvement of state transition dynamics, which can make it difficult and inefficient to generate high-quality predictions of the states. Additionally, there is a lack of representational models for autonomous systems, as creating a compact model that captures the full range of dynamical intricacies and uncertainties is challenging. To address these issues, the authors propose a hierarchical Bayesian linear regression model that utilizes local features to learn the dynamics of a micro-robotic system, as well as two simpler examples. The model is hierarchical, with non-stationary priors for the model parameters that increase flexibility by imposing priors on the priors. To solve the maximum likelihood problem, the authors employ the variational expectation maximization algorithm, which is enhanced by the incorporation of hidden target variables. The resulting model structures are parsimonious and consistently provide accurate predictions for large training and test sets. This approach holds promise for future applications in model-based reinforcement learning and real-time control policy computation.",1
"Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection---even choosing the number of nodes---remains an open question. Recent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. In this work, we propose several modeling and inference advances that consistently improve the compactness of the model learned while maintaining predictive performance, especially in smaller-sample settings including reinforcement learning.",0
"The capability of Bayesian Neural Networks (BNNs) to provide accurately calibrated posterior uncertainties has garnered significant interest lately. Nevertheless, there is still no clear solution for model selection, including the determination of the number of nodes. A new approach has suggested utilizing a horseshoe prior on node pre-activations in a BNN to disable nodes that do not contribute to data explanation. This study presents various modeling and inference developments that consistently enhance the efficiency of the model learned while retaining predictive performance, particularly in smaller-sample scenarios like reinforcement learning.",1
"Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage ""motion prior"" in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.",0
"While there have been significant advancements in deep-learning based semantic segmentation, real-world performance often suffers due to distribution mismatch. To address this issue, some domain adaptation and active learning methods have been proposed, but they have not explored the potential of leveraging information from videos, which are commonly captured by cameras. This study suggests utilizing the ""motion prior"" in videos to enhance human segmentation in a weakly-supervised active learning environment. By using optical flow to extract motion information, candidate foreground motion segments can be extracted and used to finetune the model. A memory-network-based policy model is learned to select strong candidate segments via reinforcement learning. This method improves human segmentation performance across multiple scenes and modalities, and can be combined with domain adaptation approaches to achieve additional performance gains.",1
"Self-supervised learning of convolutional neural networks can harness large amounts of cheap unlabeled data to train powerful feature representations. As surrogate task, we jointly address ordering of visual data in the spatial and temporal domain. The permutations of training samples, which are at the core of self-supervision by ordering, have so far been sampled randomly from a fixed preselected set. Based on deep reinforcement learning we propose a sampling policy that adapts to the state of the network, which is being trained. Therefore, new permutations are sampled according to their expected utility for updating the convolutional feature representation. Experimental evaluation on unsupervised and transfer learning tasks demonstrates competitive performance on standard benchmarks for image and video classification and nearest neighbor retrieval.",0
"Convolutional neural networks can use self-supervised learning to create powerful feature representations by utilizing abundant, low-cost unlabeled data. One surrogate task involves organizing visual data in both temporal and spatial domains. However, the current method of randomly selecting permutations from a predetermined set is limited. To address this, we propose a sampling policy that adapts to the current state of the network being trained. Using deep reinforcement learning, we can sample new permutations based on their expected utility in updating the feature representation. Our experimental evaluation for unsupervised and transfer learning tasks proves competitive with standard image and video classification and nearest neighbor retrieval benchmarks.",1
We examine the impact of learning Lipschitz continuous models in the context of model-based reinforcement learning. We provide a novel bound on multi-step prediction error of Lipschitz models where we quantify the error using the Wasserstein metric. We go on to prove an error bound for the value-function estimate arising from Lipschitz models and show that the estimated value function is itself Lipschitz. We conclude with empirical results that show the benefits of controlling the Lipschitz constant of neural-network models.,0
"The focus of our study is the influence of acquiring knowledge on Lipschitz continuous models in the model-based reinforcement learning domain. We introduce a fresh constraint on the multi-step prediction error of Lipschitz models, which is determined by utilizing the Wasserstein metric. Additionally, we demonstrate the existence of an error limit for the value-function estimate produced by Lipschitz models, and we verify that the estimated value function is Lipschitz. Finally, we present empirical evidence that highlights the advantages of managing the Lipschitz constant of neural network models.",1
"Today, the optimal performance of existing noise-suppression algorithms, both data-driven and those based on classic statistical methods, is range bound to specific levels of instantaneous input signal-to-noise ratios. In this paper, we present a new approach to improve the adaptivity of such algorithms enabling them to perform robustly across a wide range of input signal and noise types. Our methodology is based on the dynamic control of algorithmic parameters via reinforcement learning. Specifically, we model the noise-suppression module as a black box, requiring no knowledge of the algorithmic mechanics except a simple feedback from the output. We utilize this feedback as the reward signal for a reinforcement-learning agent that learns a policy to adapt the algorithmic parameters for every incoming audio frame (16 ms of data). Our preliminary results show that such a control mechanism can substantially increase the overall performance of the underlying noise-suppression algorithm; 42% and 16% improvements in output SNR and MSE, respectively, when compared to no adaptivity.",0
"Currently, noise-suppression algorithms have limited performance capabilities and are only effective within certain signal-to-noise ratio ranges. This paper introduces a new methodology that enhances the adaptivity of these algorithms, allowing them to perform reliably across a wider range of signal and noise types. The approach involves using reinforcement learning to dynamically control algorithmic parameters. By modeling the noise-suppression module as a black box, with feedback from the output as the reward signal, the reinforcement-learning agent can learn to adapt the algorithmic parameters for every incoming audio frame. Our preliminary results show that this approach can significantly improve the overall performance of the noise-suppression algorithm, with 42% and 16% increases in output SNR and MSE, respectively, compared to no adaptivity.",1
"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",0
"Our new method for learning the structure of convolutional neural networks (CNNs) is more effective than the most recent state-of-the-art methods, which rely on reinforcement learning and evolutionary algorithms. Our approach involves using a sequential model-based optimization (SMBO) strategy, where we search for structures in ascending order of complexity, while simultaneously creating a surrogate model to aid the search through structure space. When compared under the same search space, our method is up to five times more efficient than Zoph et al.'s (2018) RL method in terms of the number of model evaluations, and eight times quicker in terms of total compute. The structures we discover using this method achieve top-tier classification accuracies on CIFAR-10 and ImageNet.",1
"Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments.",0
"Previous investigations into the connection between vision and language for robot navigation have focused on enhancing model-free deep reinforcement learning (DRL) models in artificial settings. However, these models disregard the dynamics of actual environments and frequently struggle to generalize to new scenarios. This study takes a groundbreaking approach to bridging the gap between artificial research and real-world applications. We introduce a new hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to tackle a vision-language navigation task in the real world. Our look-ahead module tightly fuses a policy model with an environment model, which predicts the next state and reward. The experimental results demonstrate that our novel method surpasses the baseline models and performs the best on the Room-to-Room dataset in the real world. Furthermore, our adaptable approach is more transferable when applied to unfamiliar environments.",1
"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",0
"Algorithms that utilize imitation learning have the ability to learn a policy from expert demonstrations without requiring a reward signal. Nevertheless, most current techniques are not suitable for multi-agent settings because of the presence of multiple (Nash) equilibria and non-stationary environments. To address this, we have developed a new framework for multi-agent imitation learning in general Markov games, which is based on a generalized concept of inverse reinforcement learning. Additionally, we have introduced a practical multi-agent actor-critic algorithm that exhibits impressive empirical performance. With our approach, it is possible to imitate intricate behaviors in high-dimensional environments that involve multiple cooperative or competing agents.",1
"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.",0
"Q-values, which are state-action value functions, are commonly used in reinforcement learning (RL) and are the basis for popular algorithms like SARSA and Q-learning. Our proposal introduces a new idea of action value, which involves a Gaussian smoothed version of the expected Q-value. We demonstrate that these smoothed Q-values still satisfy the Bellman equation and can be learned from experience obtained from an environment. Furthermore, we can recover the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy from the gradient and Hessian of the smoothed Q-value function. Using these connections, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. This approach can also easily accommodate proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We observe that the ability to learn both the mean and covariance during training leads to significantly better results on standard continuous control benchmarks.",1
"We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure (referred to as ontology in this paper) that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers. OONP supports a rich family of operations (both symbolic and differentiable) for composing the ontology, and a big variety of forms (both symbolic and differentiable) for representing the state and the document. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL) , reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.",0
"The proposed framework, Object-oriented Neural Programming (OONP), is designed to semantically parse documents within specific domains. Essentially, OONP reads through a document and uses a pre-designed object-oriented data structure (known as ontology) to reflect the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process by utilizing a neural net-based Reader, which sequentially reads through the document and updates an intermediate ontology to summarize its partial understanding of the text. OONP offers a variety of different operations for composing the ontology, as well as various forms for representing the state and document. OONP can be trained with different forms and strengths of supervision, including supervised learning, reinforcement learning, and hybrid methods. Our experiments on both synthetic and real-world document parsing tasks have demonstrated that OONP can effectively handle complex ontologies with modest training data.",1
"While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30-90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.",0
"Although deeper convolutional networks are necessary for achieving maximum accuracy in visual perception tasks, shallower networks are adequate for many inputs. To take advantage of this observation, we developed SkipNet, a modified residual network that uses a gating network to selectively skip convolutional blocks based on the previous layer's activations. We approached the dynamic skipping issue in the context of sequential decision-making and proposed a hybrid learning algorithm combining supervised and reinforcement learning to overcome non-differentiable skipping decisions. On four benchmark datasets, SkipNet reduces computation by 30-90% while maintaining the accuracy of the original model and surpassing state-of-the-art dynamic networks and static compression techniques. We also conducted a qualitative evaluation of the gating policy, revealing a connection between image scale, saliency, and the number of skipped layers.",1
"In this article, we sketch an algorithm that extends the Q-learning algorithms to the continuous action space domain. Our method is based on the discretization of the action space. Despite the commonly used discretization methods, our method does not increase the discretized problem dimensionality exponentially. We will show that our proposed method is linear in complexity when the discretization is employed. The variant of the Q-learning algorithm presented in this work, labeled as Finite Step Q-Learning (FSQ), can be deployed to both shallow and deep neural network architectures.",0
"Our article presents an algorithm that broadens the Q-learning algorithm to include the continuous action space domain by discretizing the action space. Unlike other discretization methods, our approach does not exponentially increase the dimensionality of the problem. Our method has a linear complexity when discretization is used, as we demonstrate. We introduce the Finite Step Q-Learning (FSQ) algorithm, which can be used in shallow and deep neural network architectures.",1
"Machine Learning models become increasingly proficient in complex tasks. However, even for experts in the field, it can be difficult to understand what the model learned. This hampers trust and acceptance, and it obstructs the possibility to correct the model. There is therefore a need for transparency of machine learning models. The development of transparent classification models has received much attention, but there are few developments for achieving transparent Reinforcement Learning (RL) models. In this study we propose a method that enables a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we define a translation of states and actions to a description that is easier to understand for human users. Second, we developed a procedure that enables the agent to obtain the consequences of a single action, as well as its entire policy. The method calculates contrasts between the consequences of a policy derived from a user query, and of the learned policy of the agent. Third, a format for generating explanations was constructed. A pilot survey study was conducted to explore preferences of users for different explanation properties. Results indicate that human users tend to favor explanations about policy rather than about single actions.",0
"As Machine Learning models become more advanced, it can be challenging for experts to comprehend what the models have learned. This lack of understanding can impede trust and acceptance, and prevent correcting the model. Thus, there is a need for transparency in machine learning models. Although there has been much attention given to developing transparent classification models, there have been few advances in achieving transparency in Reinforcement Learning (RL) models. To address this issue, we propose a method that allows a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we translate states and actions into a language more easily understood by humans. Second, we developed a procedure that allows the agent to obtain the consequences of a single action or its entire policy. The method then calculates the differences between the consequences of the policy derived from a user query and the learned policy of the agent. Lastly, we created a format for generating explanations. We conducted a pilot survey study to explore user preferences for different explanations, which indicated that users preferred explanations about the policy rather than single actions.",1
"Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.",0
"Although reinforcement learning is a potent technique for training an agent to carry out a particular task, it has limitations. Specifically, an agent trained using this method is only proficient in accomplishing the single task assigned to it via its reward function. This approach is not ideal for situations that require an agent to perform diverse tasks, like navigating to different positions in a room or moving objects to various spots. To address this challenge, we introduce a novel approach that enables an agent to identify the range of tasks it can perform automatically. We leverage a generator network that recommends tasks for the agent to attempt, which are expressed as goal states. We optimize the generator network using adversarial training to create tasks that are suitably challenging for the agent continuously. Thus, our method generates a curriculum of tasks that the agent can master. Our approach demonstrates that an agent can learn to carry out an extensive range of tasks effectively and automatically, without any prior knowledge of its environment. Furthermore, our method can master tasks with sparse rewards, a difficulty that has traditionally been a significant obstacle.",1
"Activities in reinforcement learning (RL) revolve around learning the Markov decision process (MDP) model, in particular, the following parameters: state values, V; state-action values, Q; and policy, pi. These parameters are commonly implemented as an array. Scaling up the problem means scaling up the size of the array and this will quickly lead to a computational bottleneck. To get around this, the RL problem is commonly formulated to learn a specific task using hand-crafted input features to curb the size of the array. In this report, we discuss an alternative end-to-end Deep Reinforcement Learning (DRL) approach where the DRL attempts to learn general task representations which in our context refers to learning to play the Pong game from a sequence of screen snapshots without game-specific hand-crafted features. We apply artificial neural networks (ANN) to approximate a policy of the RL model. The policy network, via Policy Gradients (PG) method, learns to play the Pong game from a sequence of frames without any extra semantics apart from the pixel information and the score. In contrast to the traditional tabular RL approach where the contents in the array have clear interpretations such as V or Q, the interpretation of knowledge content from the weights of the policy network is more illusive. In this work, we experiment with various Deep ANN architectures i.e., Feed forward ANN (FFNN), Convolution ANN (CNN) and Asynchronous Advantage Actor-Critic (A3C). We also examine the activation of hidden nodes and the weights between the input and the hidden layers, before and after the DRL has successfully learnt to play the Pong game. Insights into the internal learning mechanisms and future research directions are then discussed.",0
"The main focus of reinforcement learning (RL) involves understanding the Markov decision process (MDP) model, specifically the state values (V), state-action values (Q), and policy (pi), which are usually comprised of an array. However, when scaling up the problem, the array size can become computationally limiting. To combat this issue, RL is often formulated to learn a particular task using pre-determined input features. This report explores an alternative approach using Deep Reinforcement Learning (DRL) to learn generalized task representations, specifically playing the Pong game from screen snapshots without hand-crafted features. Artificial neural networks (ANN) are used to approximate the RL model's policy, with the Policy Gradients (PG) method used to teach the network to play Pong using only pixel data and the score. Unlike traditional tabular RL, where content in the array has clear interpretations, the knowledge content from the policy network's weights is more difficult to interpret. The report experiments with various Deep ANN architectures (FFNN, CNN, and A3C) and examines the hidden node activation and input-to-hidden weight changes before and after successful learning of the Pong game. The study provides insights into internal learning mechanisms and potential research directions.",1
"We present NAVREN-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning RL. A suitable reward function is designed keeping in mind the cost and weight constraints for micro drone with minimum number of sensing modalities. Collection of small number of expert data and knowledge based data aggregation is integrated into the RL process to aid convergence. Experimentation is carried out on a Parrot AR drone in different indoor arenas and the results are compared with other baseline technologies. We demonstrate how the drone successfully avoids obstacles and navigates across different arenas.",0
"Introducing NAVREN-RL, a technique that employs end-to-end reinforcement learning (RL) to guide an unmanned aerial vehicle through an indoor Real Environment. The reward function is designed with the limitations in mind, such as the cost and weight of micro drones, and the minimum number of sensing modalities. A small amount of professional and knowledge-based data aggregation is combined with the RL process to assist with convergence. The Parrot AR drone is tested in various indoor arenas, and the findings are compared to other baseline technologies. We show how the drone effectively avoids obstacles and travels through different areas.",1
"Distributional reinforcement learning (distributional RL) has seen empirical success in complex Markov Decision Processes (MDPs) in the setting of nonlinear function approximation. However, there are many different ways in which one can leverage the distributional approach to reinforcement learning. In this paper, we propose GAN Q-learning, a novel distributional RL method based on generative adversarial networks (GANs) and analyze its performance in simple tabular environments, as well as OpenAI Gym. We empirically show that our algorithm leverages the flexibility and blackbox approach of deep learning models while providing a viable alternative to traditional methods.",0
"Empirical success has been observed in complex Markov Decision Processes (MDPs) through distributional reinforcement learning (distributional RL) when nonlinear function approximation is applied. However, there are several ways to utilize the distributional approach to reinforcement learning. In this study, we introduce GAN Q-learning, a novel distributional RL technique that utilizes generative adversarial networks (GANs), and assess its effectiveness in both simple tabular environments and OpenAI Gym. Our empirical findings demonstrate that our algorithm employs the adaptability and blackbox approach of deep learning models while presenting a feasible alternative to conventional methods.",1
"Deep neural network architectures have traditionally been designed and explored with human expertise in a long-lasting trial-and-error process. This process requires huge amount of time, expertise, and resources. To address this tedious problem, we propose a novel algorithm to optimally find hyperparameters of a deep network architecture automatically. We specifically focus on designing neural architectures for medical image segmentation task. Our proposed method is based on a policy gradient reinforcement learning for which the reward function is assigned a segmentation evaluation utility (i.e., dice index). We show the efficacy of the proposed method with its low computational cost in comparison with the state-of-the-art medical image segmentation networks. We also present a new architecture design, a densely connected encoder-decoder CNN, as a strong baseline architecture to apply the proposed hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As an application, we train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from a baseline segmentation architecture, the resulting network architecture obtains the state-of-the-art results in accuracy without performing any trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.",0
"Traditionally, deep neural network architectures have been developed and tested through a lengthy process of trial-and-error, requiring significant amounts of time, expertise, and resources. To address this issue, we propose a new algorithm that can automatically optimize the hyperparameters of a deep network architecture. Our focus is on the design of neural architectures for medical image segmentation tasks. Our method is based on a policy gradient reinforcement learning approach, with the reward function assigned a segmentation evaluation utility (i.e., dice index). We demonstrate the effectiveness of our method, which has low computational cost in comparison to state-of-the-art medical image segmentation networks. In addition, we present a new architecture design, consisting of a densely connected encoder-decoder CNN, as a strong baseline architecture to apply our hyperparameter search algorithm. We apply our algorithm to each layer of the baseline architectures and train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. As a result, the resulting network architecture achieves state-of-the-art accuracy without requiring trial-and-error based architecture design approaches or close supervision of hyperparameter changes.",1
"The idea of reusing information from previously learned tasks (source tasks) for the learning of new tasks (target tasks) has the potential to significantly improve the sample efficiency reinforcement learning agents. In this work, we describe an approach to concisely store and represent learned task knowledge, and reuse it by allowing it to guide the exploration of an agent while it learns new tasks. In order to do so, we use a measure of similarity that is defined directly in the space of parameterized representations of the value functions. This similarity measure is also used as a basis for a variant of the growing self-organizing map algorithm, which is simultaneously used to enable the storage of previously acquired task knowledge in an adaptive and scalable manner.We empirically validate our approach in a simulated navigation environment and discuss possible extensions to this approach along with potential applications where it could be particularly useful.",0
"The concept of taking information from previously learned tasks (known as source tasks) and applying it to new tasks (known as target tasks) has the potential to greatly enhance the efficiency of reinforcement learning agents. Our study outlines a method for compactly storing and representing learned task knowledge, which can be utilized to guide an agent's exploration while learning new tasks. To accomplish this, we use a similarity measure that is defined in the parameterized representations of the value functions. This similarity measure is also the basis for a version of the growing self-organizing map algorithm, which enables the storage of previously acquired task knowledge in a flexible and adaptable manner. We have tested our method in a simulated navigation environment and discuss potential extensions and applications where it could prove especially valuable.",1
"In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent's representations during training or via use as part of an explicit planning mechanism. However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment. Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.",0
"The use of generative and temporal models in model-based reinforcement learning can enhance an agent's performance through representation tuning during training or explicit planning. However, these models have only been applied in simple environments due to their difficulty in training for larger, potentially partially-observed and 3D environments. To address this issue, we introduce a novel action-conditioned generative model that includes a non-parametric spatial memory system storing learned, disentangled representations of the environment. The model uses a state-space model for low-dimensional spatial updates and a Variational Auto-Encoder for high-dimensional visual observations. This scalable architecture can make coherent predictions over hundreds of time steps in a variety of partially observed 2D and 3D environments.",1
"The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.",0
"The use of deep reinforcement learning (DRL) in cyber-physical systems is becoming more common, but there are concerns about the safety and reliability of autonomous agents. Researchers have discovered that it is possible for malicious actors to trick DRL policies into behaving improperly by generating adversarial attacks. While some studies have addressed certain types of attacks, most only focus on optimizing data space offline. This paper introduces the Meta-Learned Advantage Hierarchy (MLAH) framework, which is attack model-agnostic and better suited for reinforcement learning. The MLAH framework handles attacks in the decision space, directly reducing the learned bias introduced by the adversary. The algorithm involves learning separate sub-policies (nominal and adversarial) in an online manner, guided by a supervisory master agent that detects the presence of the adversary using the advantage function for the sub-policies. The proposed algorithm reduces bias in policy learning even in the presence of heavy state information attacks. The paper presents algorithm analysis and simulation results using popular OpenAI Gym environments.",1
"Deep Reinforcement Learning (DRL) has achieved impressive success in many applications. A key component of many DRL models is a neural network representing a Q function, to estimate the expected cumulative reward following a state-action pair. The Q function neural network contains a lot of implicit knowledge about the RL problems, but often remains unexamined and uninterpreted. To our knowledge, this work develops the first mimic learning framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than five baseline methods. The transparent tree structure of an LMUT facilitates understanding the network's learned knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.",0
"In numerous applications, Deep Reinforcement Learning (DRL) has demonstrated remarkable achievements. One of the vital elements of numerous DRL models is a neural network that represents a Q function, which approximates the expected cumulative reward after a state-action pair. The neural network that comprises the Q function includes a great deal of implicit knowledge about RL problems; however, it often remains unexplored and unexplained. As far as we know, this study presents the first mimic learning framework for Q functions in DRL. We propose Linear Model U-trees (LMUTs) to approximate the neural network's predictions. We teach an LMUT using a novel online algorithm that is well-matched for an active play environment, where the mimic learner monitors an ongoing interaction between the neural network and the environment. Empirical evidence reveals that an LMUT outperforms five benchmark methods in mimicking a Q function. The LMUT's clear tree structure enables us to understand the network's acquired knowledge by inspecting feature influence, extracting rules, and highlighting the super-pixels in image inputs.",1
"A variety of machine learning models have been proposed to assess the performance of players in professional sports. However, they have only a limited ability to model how player performance depends on the game context. This paper proposes a new approach to capturing game context: we apply Deep Reinforcement Learning (DRL) to learn an action-value Q function from 3M play-by-play events in the National Hockey League (NHL). The neural network representation integrates both continuous context signals and game history, using a possession-based LSTM. The learned Q-function is used to value players' actions under different game contexts. To assess a player's overall performance, we introduce a novel Game Impact Metric (GIM) that aggregates the values of the player's actions. Empirical Evaluation shows GIM is consistent throughout a play season, and correlates highly with standard success measures and future salary.",0
"Several machine learning models have been proposed to evaluate the performance of professional sports players. However, they have limitations in modeling how player performance relates to the context of the game. This paper suggests a new approach to incorporate game context by utilizing Deep Reinforcement Learning (DRL) to learn an action-value Q function from 3M play-by-play events in the National Hockey League (NHL). The neural network representation integrates continuous context signals and game history through a possession-based LSTM. The learned Q-function is utilized to determine players' actions' worth in different game contexts. To gauge a player's overall performance, a unique Game Impact Metric (GIM) is introduced, which sums up the values of the player's actions. Empirical analysis demonstrates that GIM is consistent throughout the season and highly correlates with standard success measures and future salary.",1
"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",0
"The problem of Multi-task Inverse Reinforcement Learning (IRL) involves inferring multiple reward functions from expert demonstrations. However, prior research that relied on Bayesian IRL has computational limitations, which restricts its scalability for complex environments. This paper introduces a new approach to multi-task IRL using the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Our experiments show that our approach can achieve one-shot imitation learning in a gridworld environment, while single-task IRL algorithms require hundreds of demonstrations to solve the same problem. We also discuss our preliminary work on extending our method to the function approximator setting of modern MCE IRL algorithms using meta-learning. Additionally, we evaluate our method on multi-task variants of common simulated robotics benchmarks, which reveal serious limitations of existing IRL algorithms. Finally, we provide suggestions for future research.",1
"Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.",0
"Although deep reinforcement learning has demonstrated impressive successes recently, its lack of data efficiency has hindered its application to real-world problems. To address this challenge, we propose the Bottleneck Simulator, a model-based reinforcement learning method that combines a factorized transition model of the environment with rollout simulations to learn an effective policy using few examples. The Bottleneck Simulator employs an abstract, discrete state to increase sample efficiency by reducing the number of model parameters and exploiting the environment's structural properties. Our mathematical analysis of the Bottleneck Simulator reveals that its performance is influenced by four sources of error: abstract space structure, transition model estimation variance, transition model estimation bias, and transition model class bias. To evaluate the Bottleneck Simulator, we applied it to two natural language processing tasks: a text adventure game and a complex dialogue response selection task. The Bottleneck Simulator outperformed competing approaches on both tasks, delivering excellent results.",1
"The present work extends the randomized shortest-paths framework (RSP), interpolating between shortest-path and random-walk routing in a network, in three directions. First, it shows how to deal with equality constraints on a subset of transition probabilities and develops a generic algorithm for solving this constrained RSP problem using Lagrangian duality. Second, it derives a surprisingly simple iterative procedure to compute the optimal, randomized, routing policy generalizing the previously developed ""soft"" Bellman-Ford algorithm. The resulting algorithm allows balancing exploitation and exploration in an optimal way by interpolating between a pure random behavior and the deterministic, optimal, policy (least-cost paths) while satisfying the constraints. Finally, the two algorithms are applied to Markov decision problems by considering the process as a constrained RSP on a bipartite state-action graph. In this context, the derived ""soft"" value iteration algorithm appears to be closely related to dynamic policy programming as well as Kullback-Leibler and path integral control, and similar to a recently introduced reinforcement learning exploration strategy. This shows that this strategy is optimal in the RSP sense - it minimizes expected path cost subject to relative entropy constraint. Simulation results on illustrative examples show that the model behaves as expected.",0
"This work expands upon the randomized shortest-paths framework (RSP) by exploring three different directions. Firstly, it addresses how to handle equality constraints on a subset of transition probabilities, presenting a general algorithm to solve this problem using Lagrangian duality. Secondly, it introduces an iterative process to compute the optimal randomized routing policy, which balances exploitation and exploration while adhering to the constraints. This algorithm is an extension of the previously developed ""soft"" Bellman-Ford algorithm. Finally, the two algorithms are applied to Markov decision problems by treating the process as a constrained RSP on a bipartite state-action graph. The ""soft"" value iteration algorithm derived from this approach is shown to be closely related to dynamic policy programming, Kullback-Leibler and path integral control, and a recently introduced reinforcement learning exploration strategy. Simulation results demonstrate that the model performs as expected.",1
"An important property for lifelong-learning agents is the ability to combine existing skills to solve unseen tasks. In general, however, it is unclear how to compose skills in a principled way. We provide a ""recipe"" for optimal value function composition in entropy-regularised reinforcement learning (RL) and then extend this to the standard RL setting. Composition is demonstrated in a video game environment, where an agent with an existing library of policies is able to solve new tasks without the need for further learning.",0
"The capacity to merge existing skills to tackle novel challenges is a crucial feature that lifelong-learning agents must possess. Nonetheless, the systematic approach to combining skills remains uncertain. We offer a ""formula"" for optimal value function combination in entropy-regularised reinforcement learning (RL) and subsequently expand it to the conventional RL context. We demonstrate this composition in a video game setting, where an agent equipped with a pre-existing set of policies can address fresh tasks without requiring additional learning.",1
"We introduce a deep generative model for functions. Our model provides a joint distribution p(f, z) over functions f and latent variables z which lets us efficiently sample from the marginal p(f) and maximize a variational lower bound on the entropy H(f). We can thus maximize objectives of the form E_{f~p(f)}[R(f)] + c*H(f), where R(f) denotes, e.g., a data log-likelihood term or an expected reward. Such objectives encompass Bayesian deep learning in function space, rather than parameter space, and Bayesian deep RL with representations of uncertainty that offer benefits over bootstrapping and parameter noise. In this short paper we describe our model, situate it in the context of prior work, and present proof-of-concept experiments for regression and RL.",0
"Our research presents a deep generative model that focuses on functions. By providing a joint distribution p(f, z) over functions f and latent variables z, we can efficiently sample from the marginal p(f) and maximize the entropy H(f) through a variational lower bound. This enables us to maximize objectives of the form E_{f~p(f)}[R(f)] + c*H(f), where R(f) represents a data log-likelihood term or an expected reward. Such objectives offer advantages over bootstrapping and parameter noise, and encompass Bayesian deep learning in function space instead of parameter space, as well as Bayesian deep RL with representations of uncertainty. In this paper, we detail our model, contextualize it among prior research, and provide proof-of-concept experiments for regression and RL.",1
"Autonomous urban driving navigation with complex multi-agent dynamics is under-explored due to the difficulty of learning an optimal driving policy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-fidelity car simulator. To alleviate the low exploration efficiency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the first successful case of the learned driving policy through reinforcement learning in the high-fidelity simulator, which performs better-than supervised imitation learning.",0
"The difficulty of learning an optimal driving policy has hindered the exploration of autonomous urban driving navigation with complex multi-agent dynamics. The traditional modular pipeline relies heavily on hand-designed rules and pre-processing perception systems, while supervised learning-based models are limited by the availability of extensive human experience. We introduce a Controllable Imitative Reinforcement Learning (CIRL) approach that enables a driving agent to achieve higher success rates using only vision inputs in a high-fidelity car simulator. To address the low exploration efficiency for large continuous action space, which often makes the use of classical RL challenging, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Additionally, we propose specialized adaptive policies and steering-angle reward designs for different control signals, based on shared representations, to enhance the model's ability to handle diverse cases. CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on various goal-directed driving tasks in the CARLA driving benchmark, and it exhibits superior generalization capability in unseen environments. This is the first successful case of the learned driving policy through reinforcement learning in a high-fidelity simulator, surpassing supervised imitation learning.",1
"Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The theoretical question of ""whether model-free algorithms can be made sample efficient"" is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions.   We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tilde{O}(\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single $\sqrt{H}$ factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes $\sqrt{T}$ regret without requiring access to a ""simulator.""",0
"Reinforcement learning (RL) algorithms that are model-free, such as Q-learning, update value functions or policies directly without explicitly modeling the environment. These algorithms are simpler and more flexible, making them more widespread in modern deep RL than model-based approaches. However, studies have suggested that model-free algorithms may require more samples to learn. The fundamental question of whether model-free algorithms can be made sample efficient remains unsolved, even in scenarios with finitely many states and actions. In this study, we prove that in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tilde{O}(\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single $\sqrt{H}$ factor. This is the first analysis in the model-free setting that establishes $\sqrt{T}$ regret without requiring access to a simulator.",1
"Model-based compression is an effective, facilitating, and expanded model of neural network models with limited computing and low power. However, conventional models of compression techniques utilize crafted features [2,3,12] and explore specialized areas for exploration and design of large spaces in terms of size, speed, and accuracy, which usually have returns Less and time is up. This paper will effectively analyze deep auto compression (ADC) and reinforcement learning strength in an effective sample and space design, and improve the compression quality of the model. The results of compression of the advanced model are obtained without any human effort and in a completely automated way. With a 4- fold reduction in FLOP, the accuracy of 2.8% is higher than the manual compression model for VGG-16 in ImageNet.",0
"Model-based compression presents an expanded neural network model that is effective and facilitates limited computing and low power. However, conventional compression techniques rely on crafted features and specialized areas for exploration and design, which often yield minimal returns and require significant time investment. This study examines the effectiveness of deep auto compression and reinforcement learning in sample and space design to enhance model compression quality. The results demonstrate that the advanced model achieves a 4-fold reduction in FLOP and 2.8% higher accuracy than manual compression methods for VGG-16 in ImageNet, without requiring any human effort and through a completely automated process.",1
"Learning a generative model is a key component of model-based reinforcement learning. Though learning a good model in the tabular setting is a simple task, learning a useful model in the approximate setting is challenging. In this context, an important question is the loss function used for model learning as varying the loss function can have a remarkable impact on effectiveness of planning. Recently Farahmand et al. (2017) proposed a value-aware model learning (VAML) objective that captures the structure of value function during model learning. Using tools from Asadi et al. (2018), we show that minimizing the VAML objective is in fact equivalent to minimizing the Wasserstein metric. This equivalence improves our understanding of value-aware models, and also creates a theoretical foundation for applications of Wasserstein in model-based reinforcement~learning.",0
"In model-based reinforcement learning, acquiring a generative model is crucial. While it's relatively easy to learn a model in the tabular setting, doing so in the approximate setting poses a challenge. One key aspect to consider is the loss function employed during model learning, as this can greatly impact planning effectiveness. Recently, Farahmand et al. (2017) introduced the value-aware model learning (VAML) objective, which incorporates the structure of the value function. By utilizing techniques from Asadi et al. (2018), we demonstrate that minimizing the VAML objective is equivalent to minimizing the Wasserstein metric. This finding advances our comprehension of value-aware models and establishes a theoretical basis for employing Wasserstein in model-based reinforcement learning applications.",1
"Existing person re-identification (re-id) methods assume the provision of accurately cropped person bounding boxes with minimum background noise, mostly by manually cropping. This is significantly breached in practice when person bounding boxes must be detected automatically given a very large number of images and/or videos processed. Compared to carefully cropped manually, auto-detected bounding boxes are far less accurate with random amount of background clutter which can degrade notably person re-id matching accuracy. In this work, we develop a joint learning deep model that optimises person re-id attention selection within any auto-detected person bounding boxes by reinforcement learning of background clutter minimisation subject to re-id label pairwise constraints. Specifically, we formulate a novel unified re-id architecture called Identity DiscriminativE Attention reinforcement Learning (IDEAL) to accurately select re-id attention in auto-detected bounding boxes for optimising re-id performance. Our model can improve re-id accuracy comparable to that from exhaustive human manual cropping of bounding boxes with additional advantages from identity discriminative attention selection that specially benefits re-id tasks beyond human knowledge. Extensive comparative evaluations demonstrate the re-id advantages of the proposed IDEAL model over a wide range of state-of-the-art re-id methods on two auto-detected re-id benchmarks CUHK03 and Market-1501.",0
"The current methods for person re-identification (re-id) assume that there are accurately cropped person bounding boxes with minimal background noise, which are usually achieved through manual cropping. However, this is not always feasible in practice when a large number of images and/or videos must be processed and person bounding boxes must be automatically detected. The accuracy of automatically detected bounding boxes is significantly lower than that of manually cropped ones, and the random amount of background clutter can have a negative impact on person re-id matching accuracy. To address this issue, we propose a joint learning deep model that optimizes person re-id attention selection within auto-detected person bounding boxes by minimizing background clutter through reinforcement learning. Our model, called Identity DiscriminativE Attention reinforcement Learning (IDEAL), accurately selects re-id attention in auto-detected bounding boxes to optimize re-id performance. The IDEAL model improves re-id accuracy comparable to that achieved through human manual cropping of bounding boxes, with additional benefits from identity discriminative attention selection that benefit re-id tasks beyond human knowledge. Extensive comparative evaluations demonstrate the advantages of the proposed IDEAL model over a wide range of state-of-the-art re-id methods on two auto-detected re-id benchmarks: CUHK03 and Market-1501.",1
"Learning from small data sets is critical in many practical applications where data collection is time consuming or expensive, e.g., robotics, animal experiments or drug design. Meta learning is one way to increase the data efficiency of learning algorithms by generalizing learned concepts from a set of training tasks to unseen, but related, tasks. Often, this relationship between tasks is hard coded or relies in some other way on human expertise. In this paper, we frame meta learning as a hierarchical latent variable model and infer the relationship between tasks automatically from data. We apply our framework in a model-based reinforcement learning setting and show that our meta-learning model effectively generalizes to novel tasks by identifying how new tasks relate to prior ones from minimal data. This results in up to a 60% reduction in the average interaction time needed to solve tasks compared to strong baselines.",0
"In many practical applications, such as robotics, animal experiments, or drug design, learning from small data sets is crucial due to the high cost or time-consuming nature of data collection. Meta learning is a technique that can improve the data efficiency of learning algorithms by expanding learned concepts from a set of training tasks to similar, unseen tasks. Typically, this connection between tasks involves some degree of human expertise or predetermined coding. However, in this study, we present meta learning as a hierarchical latent variable model, allowing for an automatic inference of the relationship between tasks from the data. Our approach is applied to a model-based reinforcement learning environment, demonstrating that our meta-learning model can effectively generalize to new tasks by identifying how they relate to prior ones based on minimal data. This leads to a 60% decrease in the average interaction time required to solve tasks compared to strong baselines.",1
"We introduce SCAL, an algorithm designed to perform efficient exploration-exploitation in any unknown weakly-communicating Markov decision process (MDP) for which an upper bound $c$ on the span of the optimal bias function is known. For an MDP with $S$ states, $A$ actions and $\Gamma \leq S$ possible next states, we prove a regret bound of $\widetilde{O}(c\sqrt{\Gamma SAT})$, which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the optimal bias span is finite and often much smaller than $D$ (e.g., $D=\infty$ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.",0
"SCAL is an algorithm that efficiently explores and exploits unknown weakly-communicating Markov decision processes (MDPs) with a known upper bound $c$ on the optimal bias function span. Our regret bound for an MDP with $S$ states, $A$ actions, and $\Gamma \leq S$ possible next states is $\widetilde{O}(c\sqrt{\Gamma SAT})$, which is a significant improvement over existing algorithms like UCRL and PSRL, whose regret scales linearly with the MDP diameter $D$. The optimal bias span is often much smaller than $D$, and our algorithm provides the first computationally efficient solution to the optimization problem at the core of REGAL.C, which was originally derived by Bartlett and Tewari (2009) but lacks a tractable algorithm. We present numerical simulations that demonstrate SCAL's superior performance over UCRL in MDPs with a large diameter and a small span, supporting our theoretical analysis.",1
"Reinforcement learning (RL) has advanced greatly in the past few years with the employment of effective deep neural networks (DNNs) on the policy networks. With the great effectiveness came serious vulnerability issues with DNNs that small adversarial perturbations on the input can change the output of the network. Several works have pointed out that learned agents with a DNN policy network can be manipulated against achieving the original task through a sequence of small perturbations on the input states. In this paper, we demonstrate furthermore that it is also possible to impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks. Our method involves the latest adversarial attack technique, Adversarial Transformer Network (ATN), that learns to generate the attack and is easy to integrate into the policy network. As a result of our attack, the victim agent is misguided to optimise for the adversarial reward over time. Our results expose serious security threats for RL applications in safety-critical systems including drones, medical analysis, and self-driving cars.",0
"The use of effective deep neural networks (DNNs) on policy networks has greatly advanced reinforcement learning (RL) in recent years. However, these advancements have also resulted in serious vulnerabilities as small adversarial perturbations on the input can alter the network's output. Previous research has shown that learned agents with DNN policy networks can be manipulated to prevent them from achieving their original task through a sequence of small input state perturbations. In this study, we take it a step further and demonstrate that we can also impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks using the Adversarial Transformer Network (ATN) technique. This method is easy to integrate into the policy network and results in the victim agent optimizing for the adversarial reward over time, posing a significant threat to RL applications in safety-critical systems, such as drones, medical analysis, and self-driving cars.",1
"Exploration is a difficult challenge in reinforcement learning and even recent state-of-the art curiosity-based methods rely on the simple epsilon-greedy strategy to generate novelty. We argue that pure random walks do not succeed to properly expand the exploration area in most environments and propose to replace single random action choices by random goals selection followed by several steps in their direction. This approach is compatible with any curiosity-based exploration and off-policy reinforcement learning agents and generates longer and safer trajectories than individual random actions. To illustrate this, we present a task-independent agent that learns to reach coordinates in screen frames and demonstrate its ability to explore with the game Super Mario Bros. improving significantly the score of a baseline DQN agent.",0
"Reinforcement learning poses a tough challenge when it comes to exploration, and even the most recent curiosity-based methods depend on the basic epsilon-greedy strategy to generate new experiences. However, we believe that pure random walks are not effective in expanding the exploration area in most environments. To address this issue, we suggest replacing single random actions with random goal selection followed by several steps in that direction. This approach can work with any curiosity-based exploration and off-policy reinforcement learning agents, and it produces longer and safer trajectories than individual random actions. We present an agent that can learn to reach coordinates on screen frames, which is not task-specific, and demonstrate its efficacy in exploring the game Super Mario Bros. Our agent significantly enhances the score of a baseline DQN agent.",1
"In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.",0
"Reinforcement learning typically involves allowing an agent to interact with its environment for a set amount of time, resetting it, and repeating the process in multiple episodes. The agent's objective can be either to maximize its performance over a fixed period or an indefinite period with time limits used only during training to diversify experience. This paper presents a formal account of how time limits can be handled effectively in each case. Not doing so can lead to suboptimal policies, training instability, state-aliasing, and invalidation of experience replay. For case (i), we argue that the remaining time should be included as part of the agent's input to avoid violating the Markov property. For case (ii), we suggest bootstrapping from the value of the state at the end of each episode. We demonstrate empirically the importance of our proposals in improving the performance and stability of existing reinforcement learning algorithms, achieving state-of-the-art results on several control tasks.",1
"In a voice-controlled smart-home, a controller must respond not only to user's requests but also according to the interaction context. This paper describes Arcades, a system which uses deep reinforcement learning to extract context from a graphical representation of home automation system and to update continuously its behavior to the user's one. This system is robust to changes in the environment (sensor breakdown or addition) through its graphical representation (scale well) and the reinforcement mechanism (adapt well). The experiments on realistic data demonstrate that this method promises to reach long life context-aware control of smart-home.",0
"Arcades is a system that employs deep reinforcement learning to extract context from a graphical representation of a home automation system and adjust its behavior to the user's requests. The controller in a voice-controlled smart-home must consider the interaction context, and Arcades is designed to be robust to changes in the environment through its graphical representation and reinforcement mechanism. Realistic data experiments demonstrate that this method is capable of achieving long-term context-aware control of a smart-home.",1
"Recent developments in deep reinforcement learning have enabled the creation of agents for solving a large variety of games given a visual input. These methods have been proven successful for 2D games, like the Atari games, or for simple tasks, like navigating in mazes. It is still an open question, how to address more complex environments, in which the reward is sparse and the state space is huge. In this paper we propose a divide and conquer deep reinforcement learning solution and we test our agent in the first person shooter (FPS) game of Doom. Our work is based on previous works in deep reinforcement learning and in Doom agents. We also present how our agent is able to perform better in unknown environments compared to a state of the art reinforcement learning algorithm.",0
"Advanced progress in deep reinforcement learning has made it possible to develop agents that can tackle a wide range of games, based on visual input. While these techniques have shown success in 2D games like Atari games, and simple tasks such as mazes, it remains unclear how to approach more complex environments that have sparse rewards and an enormous state space. To address this challenge, we propose a divide and conquer deep reinforcement learning approach and test our agent in the first-person shooter game Doom. Our work builds upon prior research in deep reinforcement learning and Doom agents. Additionally, we demonstrate how our agent outperforms a state-of-the-art reinforcement learning algorithm in unfamiliar environments.",1
"In recent years, reinforcement learning (RL) methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go, and Poker. However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding different players' diverse behaviors. In this paper, we present a novel method called Multi-Motivation Behavior Modeling (MMBM) that takes the multifaceted human motivations into consideration and models the underlying value structure of the players using inverse RL. Our approach does not require the access to the dynamic of the system, making it feasible to model complex interactive environments such as massively multiplayer online games. MMBM is tested on the World of Warcraft Avatar History dataset, which recorded over 70,000 users' gameplay spanning three years period. Our model reveals the significant difference of value structures among different player groups. Using the results of motivation modeling, we also predict and explain their diverse gameplay behaviors and provide a quantitative assessment of how the redesign of the game environment impacts players' behaviors.",0
"Reinforcement learning (RL) methods have been effectively employed to simulate gameplay, resulting in super-human performance in various games including Atari, Go, and Poker. However, these studies have primarily focused on winning the game, overlooking the intricate and diverse human motivations that are crucial in understanding players' behaviors. This paper introduces a new approach, Multi-Motivation Behavior Modeling (MMBM), which incorporates multifaceted human motivations and models players' underlying value structure using inverse RL. Our method does not necessitate access to the system's dynamics, making it possible to simulate complex interactive environments like massively multiplayer online games. We tested MMBM on the World of Warcraft Avatar History dataset, which documents over 70,000 users' gameplay over three years. Our model identifies significant disparities in value structures among different player groups, and we use the results of motivation modeling to predict and explain their varied gameplay behaviors. We also provide a quantitative evaluation of how game environment redesign affects players' behaviors.",1
"Consistently checking the statistical significance of experimental results is one of the mandatory methodological steps to address the so-called ""reproducibility crisis"" in deep reinforcement learning. In this tutorial paper, we explain how the number of random seeds relates to the probabilities of statistical errors. For both the t-test and the bootstrap confidence interval test, we recall theoretical guidelines to determine the number of random seeds one should use to provide a statistically significant comparison of the performance of two algorithms. Finally, we discuss the influence of deviations from the assumptions usually made by statistical tests. We show that they can lead to inaccurate evaluations of statistical errors and provide guidelines to counter these negative effects. We make our code available to perform the tests.",0
"To address the ""reproducibility crisis"" in deep reinforcement learning, it is crucial to consistently check the statistical significance of experimental results. This tutorial paper provides a comprehensive explanation of how the number of random seeds impacts the probabilities of statistical errors. The t-test and bootstrap confidence interval test are both discussed, and we provide theoretical guidelines for determining the appropriate number of random seeds required to achieve a statistically significant comparison of two algorithms. Additionally, we examine how deviations from standard statistical test assumptions can result in inaccurate evaluations of statistical errors and suggest guidelines to mitigate these issues. Our code is available to perform these tests.",1
"Multi-step temporal difference (TD) learning is an important approach in reinforcement learning, as it unifies one-step TD learning with Monte Carlo methods in a way where intermediate algorithms can outperform either extreme. They address a bias-variance trade off between reliance on current estimates, which could be poor, and incorporating longer sampled reward sequences into the updates. Especially in the off-policy setting, where the agent aims to learn about a policy different from the one generating its behaviour, the variance in the updates can cause learning to diverge as the number of sampled rewards used in the estimates increases. In this paper, we introduce per-decision control variates for multi-step TD algorithms, and compare them to existing methods. Our results show that including the control variates can greatly improve performance on both on and off-policy multi-step temporal difference learning tasks.",0
"Temporal difference (TD) learning using multiple steps is a significant method in reinforcement learning since it combines one-step TD learning and Monte Carlo approaches, resulting in intermediate algorithms that can surpass both extremes. These algorithms tackle the trade-off between bias and variance by balancing the reliance on current estimates, which may not be accurate, and including longer sequences of sampled rewards in the updates. When learning about a policy different from the one that generates its behavior, the variance in the updates can cause learning to diverge as the number of sampled rewards used in the estimates increases, particularly in the off-policy setting. In this study, we present per-decision control variates for multi-step TD algorithms and compare them to existing approaches. Our findings demonstrate that incorporating control variates can significantly enhance the performance of on and off-policy multi-step temporal difference learning tasks.",1
"Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.",0
"Due to their lengthy training times, modern deep learning methods pose a challenge for vanilla Bayesian hyperparameter optimization, which becomes computationally infeasible. Meanwhile, bandit-based configuration evaluation approaches based on random search lack direction and fail to converge to the best configurations in a timely manner. To overcome these shortcomings, we propose a hybrid approach that combines the best of both worlds: the strengths of Bayesian optimization and bandit-based methods. Our new state-of-the-art hyperparameter optimization method delivers strong anytime performance and achieves fast convergence to optimal configurations. It consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our approach is versatile and robust, yet conceptually simple and easy to implement.",1
"A key question in Reinforcement Learning is which representation an agent can learn to efficiently reuse knowledge between different tasks. Recently the Successor Representation was shown to have empirical benefits for transferring knowledge between tasks with shared transition dynamics. This paper presents Model Features: a feature representation that clusters behaviourally equivalent states and that is equivalent to a Model-Reduction. Further, we present a Successor Feature model which shows that learning Successor Features is equivalent to learning a Model-Reduction. A novel optimization objective is developed and we provide bounds showing that minimizing this objective results in an increasingly improved approximation of a Model-Reduction. Further, we provide transfer experiments on randomly generated MDPs which vary in their transition and reward functions but approximately preserve behavioural equivalence between states. These results demonstrate that Model Features are suitable for transfer between tasks with varying transition and reward functions.",0
"In Reinforcement Learning, a critical inquiry is determining which representation an agent can adopt to effectively reuse knowledge across different tasks. Recently, the Successor Representation has been proven to have practical advantages in transferring knowledge between tasks that share transition dynamics. This study introduces Model Features, a characteristic representation that groups states with similar behaviors and is comparable to a Model-Reduction. Additionally, a Successor Feature model is presented, demonstrating that learning Successor Features is equivalent to learning a Model-Reduction. A new optimization objective is developed, and we provide evidence that minimizing this objective leads to an increasingly better approximation of a Model-Reduction. Transfer experiments on randomly generated MDPs with varying transition and reward functions are also provided, demonstrating that Model Features are appropriate for transferring knowledge between tasks with different transition and reward functions while maintaining behavioral equivalence between states.",1
"Deep reinforcement learning has led to several recent breakthroughs, though the learned policies are often based on black-box neural networks. This makes them difficult to interpret and to impose desired specification constraints during learning. We present an iterative framework, MORL, for improving the learned policies using program synthesis. Concretely, we propose to use synthesis techniques to obtain a symbolic representation of the learned policy, which can then be debugged manually or automatically using program repair. After the repair step, we use behavior cloning to obtain the policy corresponding to the repaired program, which is then further improved using gradient descent. This process continues until the learned policy satisfies desired constraints. We instantiate MORL for the simple CartPole problem and show that the programmatic representation allows for high-level modifications that in turn lead to improved learning of the policies.",0
"Recent advances in deep reinforcement learning have resulted in significant breakthroughs. However, the policies derived from black-box neural networks can be challenging to comprehend and enforce specific constraints during the learning process. In this study, we propose an iterative framework, MORL, that utilizes program synthesis to enhance the learned policies. We achieve this by obtaining a symbolic representation of the learned policy using synthesis techniques, which can be debugged manually or automatically using program repair. After the repair stage, we use behavior cloning to obtain the policy corresponding to the fixed program. We then further enhance the policy using gradient descent until it satisfies the desired constraints. We apply MORL to the CartPole problem and demonstrate that the programmatic representation allows for high-level modifications, resulting in improved policy learning.",1
"This paper studies the potential of the return distribution for exploration in deterministic reinforcement learning (RL) environments. We study network losses and propagation mechanisms for Gaussian, Categorical and Gaussian mixture distributions. Combined with exploration policies that leverage this return distribution, we solve, for example, a randomized Chain task of length 100, which has not been reported before when learning with neural networks.",0
"The focus of this research is to investigate the possibility of utilizing the return distribution to facilitate exploration in deterministic reinforcement learning (RL) environments. The study delves into the propagation mechanisms and network losses of Gaussian, Categorical, and Gaussian mixture distributions. By incorporating exploration policies that capitalize on this return distribution, we were able to successfully solve a randomized Chain task of length 100, which has not been previously achieved with neural networks.",1
"We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.",0
"Our framework for continual learning domains involves a straightforward and scalable approach, allowing for sequential task learning. Our approach maintains a consistent number of parameters and aims to sustain performance on previously learned tasks while expediting learning on future ones. This is accomplished by training a two-component network, consisting of a knowledge base that can solve past problems and an active column that efficiently learns the current task. Once a new task is learned, the active column is compressed into the knowledge base, ensuring that previous skills are not compromised. This process involves no growth in architecture, no access to prior data, and no task-specific parameters. We showcase the efficacy of this progression and compression method on handwritten alphabet classification, as well as two reinforcement learning domains: Atari games and 3D maze navigation.",1
"The use of ensembles of neural networks (NNs) for the quantification of predictive uncertainty is widespread. However, the current justification is intuitive rather than analytical. This work proposes one minor modification to the normal ensembling methodology, which we prove allows the ensemble to perform Bayesian inference, hence converging to the corresponding Gaussian Process as both the total number of NNs, and the size of each, tend to infinity. This working paper provides early-stage results in a reinforcement learning setting, analysing the practicality of the technique for an ensemble of small, finite number. Using the uncertainty estimates produced by anchored ensembles to govern the exploration-exploitation process results in steadier, more stable learning.",0
"Ensemble of neural networks (NNs) are widely used to measure predictive uncertainty, but the justification for their use is currently based on intuition rather than analysis. This study suggests a minor modification to the traditional ensembling approach, which has been proven to enable Bayesian inference and converge to the Gaussian Process as the number and size of NNs increase. The paper presents initial findings in a reinforcement learning scenario, examining the feasibility of the method for a small and finite ensemble. Utilizing the uncertainty estimates generated by anchored ensembles to guide the exploration-exploitation process leads to more consistent and stable learning.",1
"In a large E-commerce platform, all the participants compete for impressions under the allocation mechanism of the platform. Existing methods mainly focus on the short-term return based on the current observations instead of the long-term return. In this paper, we formally establish the lifecycle model for products, by defining the introduction, growth, maturity and decline stages and their transitions throughout the whole life period. Based on such model, we further propose a reinforcement learning based mechanism design framework for impression allocation, which incorporates the first principal component based permutation and the novel experiences generation method, to maximize short-term as well as long-term return of the platform. With the power of trial-and-error, it is possible to optimize impression allocation strategies globally which is contribute to the healthy development of participants and the platform itself. We evaluate our algorithm on a simulated environment built based on one of the largest E-commerce platforms, and a significant improvement has been achieved in comparison with the baseline solutions.",0
"In the context of a vast E-commerce platform, all the players strive to obtain impressions through the platform's allocation mechanism. However, current methods prioritize short-term gain based on present observations instead of long-term results. This study introduces a lifecycle model for products that outlines the various stages of introduction, growth, maturity, and decline, along with their transitions throughout the product's lifespan. Building on this model, a reinforcement learning-based mechanism design framework is proposed for impression allocation. The framework includes a first principal component-based permutation and a unique experiences generation method, which aims to maximize both short-term and long-term returns for the platform. The trial-and-error approach enables the optimization of impression allocation strategies globally, ultimately contributing to the healthy development of the platform and its participants. The algorithm is evaluated using a simulated environment based on one of the largest E-commerce platforms, and a significant improvement was recorded compared to the baseline solutions.",1
"Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.",0
"Effective machine learning systems require reliable methods for reasoning under uncertainty. Although Bayesian methods offer a general framework for quantifying uncertainty, the accuracy of their uncertainty estimates is often compromised by model misspecification and approximate inference. This means that a 90% credible interval may not accurately contain the true outcome 90% of the time. To overcome this limitation, we propose a straightforward approach for calibrating any regression algorithm, which is able to produce calibrated uncertainty estimates when applied to Bayesian and probabilistic models with sufficient data. Our approach is based on Platt scaling and builds on previous work on classification. We demonstrate the effectiveness of this approach across Bayesian linear regression, feedforward, and recurrent neural networks, consistently producing well-calibrated credible intervals that enhance performance on time series forecasting and model-based reinforcement learning tasks.",1
"We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.",0
"Our proposed method for deep reinforcement learning (RL) surpasses traditional approaches in terms of effectiveness, generalization capacity, and interpretability, thanks to the use of structured perception and relational reasoning. By employing self-attention, our approach iteratively examines relations between entities in a scene and guides a model-free policy. Our experiments demonstrate that our agent solves a novel navigation and planning task called Box-World with interpretable solutions that outperform baselines in terms of sample complexity, ability to generalize to more complex scenes, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games, outperforming human grandmasters on four. With a focus on architectural inductive biases, our work advances new avenues for overcoming complex challenges in deep RL.",1
"One of the key challenges in applying reinforcement learning to real-life problems is that the amount of train-and-error required to learn a good policy increases drastically as the task becomes complex. One potential solution to this problem is to combine reinforcement learning with automated symbol planning and utilize prior knowledge on the domain. However, existing methods have limitations in their applicability and expressiveness. In this paper we propose a hierarchical reinforcement learning method based on abductive symbolic planning. The planner can deal with user-defined evaluation functions and is not based on the Herbrand theorem. Therefore it can utilize prior knowledge of the rewards and can work in a domain where the state space is unknown. We demonstrate empirically that our architecture significantly improves learning efficiency with respect to the amount of training examples on the evaluation domain, in which the state space is unknown and there exist multiple goals.",0
"Reinforcement learning poses a major challenge in solving real-world problems as more trial-and-error is required to develop a good policy in complex tasks. To overcome this issue, reinforcement learning can be combined with automated symbol planning and prior domain knowledge. However, current methods have limitations in application and expression. This paper proposes a hierarchical reinforcement learning approach that utilizes abductive symbolic planning. The planner can handle user-defined evaluation functions and does not rely on the Herbrand theorem, allowing it to use prior knowledge of rewards and work in unknown state spaces. Empirical results show significant improvement in learning efficiency in an evaluation domain with multiple goals and unknown state space.",1
"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",0
"Our objective is to use a solitary reinforcement learning agent with a single set of parameters to solve a vast array of tasks. However, the primary challenge we face is handling the large amount of data and prolonged training time. To address this challenge, we have developed IMPALA (Importance Weighted Actor-Learner Architecture), a new distributed agent that optimizes resource utilization during single-machine training and scales up to thousands of machines without compromising data efficiency. We ensure stable learning with high throughput by utilizing decoupled acting and learning alongside V-trace, an innovative off-policy correction method. We demonstrate the effectiveness of IMPALA on DMLab-30 (30 tasks from the DeepMind Lab environment) and Atari-57 (all available Atari games in the Arcade Learning Environment). Our results display that IMPALA outperforms previous agents with less data and exhibits positive transfer between tasks due to its multi-task approach.",1
"Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.",0
"Although dextrous multi-fingered hands are versatile, they are difficult to control due to their high dimensionality and numerous potential contacts. While deep reinforcement learning (DRL) provides a model-agnostic method to control complex systems, it has not yet been applied to high-dimensional dextrous manipulation and is challenging to deploy on physical systems due to sample inefficiency. Consequently, DRL has only been successful with simpler manipulators and tasks. However, in this study, we demonstrate that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand and solve them from scratch in simulated experiments. Moreover, using a small number of human demonstrations can significantly reduce sample complexity, allowing learning with sample sizes equivalent to a few hours of robot experience. Policies learned with demonstrations exhibit very natural movements and are surprisingly more robust.",1
"The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners.",0
"The dangers of overfitting in machine learning are widely recognized, but most of the solutions and methods for identifying the issue were created for supervised learning. This study seeks to provide fresh insights into the identification and prevention of overfitting in deep Reinforcement Learning (RL) techniques, with an emphasis on continuous domains. The research investigates various aspects, including the definition and diagnosis of overfitting in MDPs, and reducing risks by introducing diverse training. The study complements recent discoveries on the fragility of deep RL methods and provides practical observations for RL practitioners and researchers.",1
"In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.",0
"Over the past few years, deep learning has become increasingly popular due to its remarkable outcomes in various fields, including pattern recognition, speech and image recognition, and natural language processing. Moreover, recent studies have demonstrated that the combination of deep learning and reinforcement learning can be utilized to acquire valuable representations for problems involving high-dimensional raw data input. This chapter provides an overview of the latest advancements in deep reinforcement learning, particularly in relation to the most commonly employed deep architectures such as autoencoders, convolutional neural networks, and recurrent neural networks, which have been successfully integrated with the reinforcement learning framework.",1
"All-goals updating exploits the off-policy nature of Q-learning to update all possible goals an agent could have from each transition in the world, and was introduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work this was mostly explored in small-state RL problems that allowed tabular representations and where all possible goals could be explicitly enumerated and learned separately. In this paper we empirically explore 3 different extensions of the idea of updating many (instead of all) goals in the context of RL with deep neural networks (or DeepRL for short). First, in a direct adaptation of Kaelbling's approach we explore if many-goals updating can be used to achieve mastery in non-tabular visual-observation domains. Second, we explore whether many-goals updating can be used to pre-train a network to subsequently learn faster and better on a single main task of interest. Third, we explore whether many-goals updating can be used to provide auxiliary task updates in training a network to learn faster and better on a single main task of interest. We provide comparisons to baselines for each of the 3 extensions.",0
"Kaelbling (1993) introduced all-goals updating in Reinforcement Learning (RL), which takes advantage of the off-policy nature of Q-learning to update all possible goals an agent could have from each transition in the world. However, this approach has mainly been explored in small-state RL problems with tabular representations where all possible goals could be explicitly enumerated and learned separately. In this study, we investigate the feasibility of updating many goals instead of all goals in RL with deep neural networks (DeepRL). We explore three different extensions of this idea, including using many-goals updating to achieve mastery in non-tabular visual-observation domains, pre-training a network to subsequently learn faster and better on a single main task of interest, and providing auxiliary task updates in training a network to learn faster and better on a single main task of interest. We compare our results to baselines for each of the three extensions.",1
We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually unifies multiple previous methods in exploration. We also derive a practical algorithm that achieves efficient exploration on challenging control tasks.,0
"Our proposed framework combines distributional reinforcement learning with recent efforts to merge Bayesian parameter updates and deep reinforcement learning. Through this, we unify various exploration methods and present a practical algorithm that effectively handles difficult control tasks.",1
"Recognition of surgical gesture is crucial for surgical skill assessment and efficient surgery training. Prior works on this task are based on either variant graphical models such as HMMs and CRFs, or deep learning models such as Recurrent Neural Networks and Temporal Convolutional Networks. Most of the current approaches usually suffer from over-segmentation and therefore low segment-level edit scores. In contrast, we present an essentially different methodology by modeling the task as a sequential decision-making process. An intelligent agent is trained using reinforcement learning with hierarchical features from a deep model. Temporal consistency is integrated into our action design and reward mechanism to reduce over-segmentation errors. Experiments on JIGSAWS dataset demonstrate that the proposed method performs better than state-of-the-art methods in terms of the edit score and on par in frame-wise accuracy. Our code will be released later.",0
"Surgical skill assessment and efficient surgery training require the recognition of surgical gestures. Previous studies have used graphical models, such as HMMs and CRFs, or deep learning models, such as Recurrent Neural Networks and Temporal Convolutional Networks, to accomplish this task. However, these methods often suffer from over-segmentation, which leads to low segment-level edit scores. In contrast, our approach models the task as a sequential decision-making process and trains an intelligent agent using reinforcement learning with hierarchical features from a deep model. We integrate temporal consistency into our action design and reward mechanism to reduce over-segmentation errors. Our experiments on the JIGSAWS dataset demonstrate that our method outperforms state-of-the-art methods in terms of the edit score and performs on par in frame-wise accuracy. We will release our code at a later time.",1
"Neural networks allow Q-learning reinforcement learning agents such as deep Q-networks (DQN) to approximate complex mappings from state spaces to value functions. However, this also brings drawbacks when compared to other function approximators such as tile coding or their generalisations, radial basis functions (RBF) because they introduce instability due to the side effect of globalised updates present in neural networks. This instability does not even vanish in neural networks that do not have any hidden layers. In this paper, we show that simple modifications to the structure of the neural network can improve stability of DQN learning when a multi-layer perceptron is used for function approximation.",0
"The utilization of neural networks enables reinforcement learning agents like deep Q-networks (DQN) to estimate intricate mappings from state spaces to value functions. Nonetheless, this approach also has its downsides when compared to other function approximators such as tile coding or radial basis functions (RBF), as it introduces instability caused by the globalized updates inherent in neural networks. This instability still persists even in neural networks without hidden layers. This paper presents modifications to the neural network structure that can enhance the stability of DQN learning, particularly when utilizing a multi-layer perceptron for function approximation.",1
"Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, object positions and types, and other factors. We term this kind of imitation learning ""imitation-from-observation,"" and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations in the same environment configuration, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show the effectiveness of our approach in learning a wide range of real-world robotic tasks modeled after common household chores from videos of a human demonstrator, including sweeping, ladling almonds, pushing objects as well as a number of tasks in simulation.",0
"When an explicit reward function is not available, autonomous systems can use imitation learning to acquire control policies by imitating an expert's demonstrations, typically a human operator. However, current imitation learning methods assume that the agent receives observation-action tuples, which is different from how humans and animals learn through imitation. Humans observe behaviors and then determine which actions will achieve the desired results, adjusting for changes in viewpoint, surroundings, object positions, and other factors. This approach is called ""imitation-from-observation,"" and we propose a new method that combines video prediction with context translation and deep reinforcement learning. This method removes the assumption that demonstrations must take place in the same environment configuration and allows for learning of skills that involve tool use simply by observing videos of human tool use. Our experiments show that this approach is effective in learning real-world robotic tasks, including sweeping, ladling almonds, pushing objects, and several simulation tasks based on everyday household chores.",1
"Inverse reinforcement learning (IRL) aims to explain observed strategic behavior by fitting reinforcement learning models to behavioral data. However, traditional IRL methods are only applicable when the observations are in the form of state-action paths. This assumption may not hold in many real-world modeling settings, where only partial or summarized observations are available. In general, we may assume that there is a summarizing function $\sigma$, which acts as a filter between us and the true state-action paths that constitute the demonstration. Some initial approaches to extending IRL to such situations have been presented, but with very specific assumptions about the structure of $\sigma$, such as that only certain state observations are missing. This paper instead focuses on the most general case of the problem, where no assumptions are made about the summarizing function, except that it can be evaluated. We demonstrate that inference is still possible. The paper presents exact and approximate inference algorithms that allow full posterior inference, which is particularly important for assessing parameter uncertainty in this challenging inference situation. Empirical scalability is demonstrated to reasonably sized problems, and practical applicability is demonstrated by estimating the posterior for a cognitive science RL model based on an observed user's task completion time only.",0
"The purpose of Inverse reinforcement learning (IRL) is to explain strategic behavior by fitting reinforcement learning models to observed behavioral data. However, traditional IRL methods are limited to cases where the observations are in the form of state-action paths, which is not always the case in real-world modeling settings where only partial or summarized observations are available. It is assumed that there is a summarizing function $\sigma$ that acts as a filter between the true state-action paths that constitute the demonstration and us. Some initial approaches have been proposed to extend IRL to such situations, but they make specific assumptions about the structure of $\sigma$, such as only certain state observations being missing. This paper focuses on the most general problem of the situation, where the summarizing function is evaluated without any assumptions made about its structure. The paper presents exact and approximate inference algorithms that enable full posterior inference, which is crucial for assessing parameter uncertainty in this challenging inference scenario. The empirical scalability of the proposed algorithms is demonstrated for reasonably sized problems, and their practical applicability is shown by estimating the posterior for a cognitive science RL model based on an observed user's task completion time.",1
"Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.",0
"Probabilistic models, such as scalable and adaptable Bayesian neural networks with latent variables, are capable of accommodating uncertainty in the estimation of network weights and can capture intricate noise patterns in data using latent variables. Our research demonstrates a technique for separating and analyzing uncertainty into epistemic and aleatoric components to aid in decision-making. This approach enables us to effectively identify informative points for functions with heteroscedastic and bimodal noise through active learning. With the decomposition, we also introduce a new risk-sensitive criterion for reinforcement learning that identifies policies that balance expected cost, model-bias and noise aversion.",1
"In this paper, we provide two new stable online algorithms for the problem of prediction in reinforcement learning, \emph{i.e.}, estimating the value function of a model-free Markov reward process using the linear function approximation architecture and with memory and computation costs scaling quadratically in the size of the feature set. The algorithms employ the multi-timescale stochastic approximation variant of the very popular cross entropy (CE) optimization method which is a model based search method to find the global optimum of a real-valued function. A proof of convergence of the algorithms using the ODE method is provided. We supplement our theoretical results with experimental comparisons. The algorithms achieve good performance fairly consistently on many RL benchmark problems with regards to computational efficiency, accuracy and stability.",0
"This paper presents two novel and reliable online algorithms for reinforcement learning prediction. These algorithms aim to estimate the value function of a model-free Markov reward process using the linear function approximation architecture. They incur memory and computation costs that increase quadratically with the feature set size. The algorithms utilize the multi-timescale stochastic approximation variant of the cross entropy optimization method, which is a model-based search technique that finds the global optimum of a real-valued function. We provide proof of convergence of the algorithms using the ODE method and supplement our theoretical findings with experimental comparisons. The algorithms exhibit consistent and efficient performance on multiple RL benchmark problems in terms of accuracy, stability, and computational efficiency.",1
"In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.",0
"Our work expands on recent developments in distributional reinforcement learning to create a versatile, adaptable, and cutting-edge DQN variant. Our approach utilizes quantile regression to estimate the complete quantile function for the state-action return distribution. This technique reconfigures a distribution over the sample space, resulting in an implicitly defined return distribution and a broad range of risk-aware policies. Our experiments on the ALE's 57 Atari 2600 games demonstrate superior results, and we employ our algorithm's implicitly defined distributions to investigate the impacts of risk-conscious policies in Atari games.",1
"We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.",0
"A novel reinforcement learning algorithm, named as Maximum aposteriori Policy Optimisation (MPO), is introduced in this study. The algorithm is based on the relative entropy objective and employs coordinate ascent for its implementation. Our derivation establishes a direct relationship between our approach and several existing methods. We have formulated two off-policy algorithms, which have been shown to be equally competitive with the current state-of-the-art in deep reinforcement learning. Specifically, in the case of continuous control, our method outperforms other methods in terms of sample efficiency, premature convergence, and robustness to hyperparameter settings, while achieving comparable or superior final performance.",1
"We explore the use of deep learning and deep reinforcement learning for optimization problems in transportation. Many transportation system analysis tasks are formulated as an optimization problem - such as optimal control problems in intelligent transportation systems and long term urban planning. Often transportation models used to represent dynamics of a transportation system involve large data sets with complex input-output interactions and are difficult to use in the context of optimization. Use of deep learning metamodels can produce a lower dimensional representation of those relations and allow to implement optimization and reinforcement learning algorithms in an efficient manner. In particular, we develop deep learning models for calibrating transportation simulators and for reinforcement learning to solve the problem of optimal scheduling of travelers on the network.",0
"The application of deep learning and deep reinforcement learning is investigated for optimization issues in transportation. Various transportation system analysis tasks are structured as optimization problems, including intelligent transportation system optimal control problems and long-term urban planning. These transportation models contain intricate input-output interactions and voluminous datasets, making them challenging to utilize in optimization scenarios. However, the use of deep learning metamodels can provide a reduced dimensional portrayal of these relationships, making it feasible to execute optimization and reinforcement learning algorithms effectively. Specifically, we construct deep learning models to calibrate transportation simulators and employ reinforcement learning to address the optimal scheduling of travelers on the network.",1
"Recent work has shown that reinforcement learning (RL) is a promising approach to control dynamical systems described by partial differential equations (PDE). This paper shows how to use RL to tackle more general PDE control problems that have continuous high-dimensional action spaces with spatial relationship among action dimensions. In particular, we propose the concept of action descriptors, which encode regularities among spatially-extended action dimensions and enable the agent to control high-dimensional action PDEs. We provide theoretical evidence suggesting that this approach can be more sample efficient compared to a conventional approach that treats each action dimension separately and does not explicitly exploit the spatial regularity of the action space. The action descriptor approach is then used within the deep deterministic policy gradient algorithm. Experiments on two PDE control problems, with up to 256-dimensional continuous actions, show the advantage of the proposed approach over the conventional one.",0
"Recent research has indicated that reinforcement learning (RL) is a promising method for regulating dynamical systems represented by partial differential equations (PDE). This article demonstrates how RL can be applied to address more generalized PDE control problems that have continuous high-dimensional action spaces with spatial relationships among action dimensions. The authors suggest the idea of action descriptors, which incorporate patterns among spatially-extended action dimensions and enable the agent to manage high-dimensional action PDEs. Theoretical proof suggests that this approach is more efficient in terms of sample usage when compared to a conventional approach that handles each action dimension independently and does not explicitly employ the spatial consistency of the action space. The action descriptor approach is integrated into the deep deterministic policy gradient algorithm. Experiments on two PDE control problems, which involve up to 256-dimensional continuous actions, demonstrate the superiority of the proposed approach over the conventional one.",1
"Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalise across diverse problems.",0
"The goal of Active Learning (AL) is to develop high-performance classifiers with minimal annotation costs by identifying the most advantageous subset of unlabeled instances to label. The significance of AL has led to numerous studies that have proposed various AL algorithms with different theoretical and intuitive foundations. Instead of relying on these traditional methods, we suggest that designing active learning algorithms should be approached as a meta-learning problem, where the best criterion is learned from data. We have developed a deep neural network that functions as an active learning algorithm, taking in the base learner state and unlabeled point set as input and predicting the next best point to annotate. By training this active query policy network using reinforcement learning, we can produce the best non-myopic policy for a given dataset. However, the main challenge lies in achieving learner generalization across heterogeneous datasets. To address this, we propose a multi-task dataset-embedding approach that enables the training of dataset-agnostic active learners. Our results demonstrate that AL algorithms developed using this approach can generalize effectively across a variety of problems.",1
"There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels---such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.",0
"It is becoming increasingly clear that converting supervised learning targets into soft targets can lead to substantial improvements in performance. Previous studies have mainly focused on classification tasks, where hard zero-one values are transformed into soft labels by adding label noise, incorporating ambiguity, or using distillation. Concurrently, there is some indication from reinforcement learning experiments in the regression setting that learning distributions can also enhance performance. Our research delves into the underlying reasons for this improvement in a regression setting. We propose a new method of distributional regression loss, which we find significantly enhances prediction accuracy. While there are several theories that suggest that reducing overfitting and improving representations could be responsible for this improvement, our results suggest an alternative hypothesis. We find that this loss is simpler to optimize, with gradients that behave better, leading to improved generalization. We provide theoretical support for this idea by characterizing the norm of the gradients of this loss.",1
"Modern reinforcement learning algorithms reach super-human performance on many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce Variational State Tabulation (VaST), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular model. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.",0
"Despite achieving super-human performance on numerous board and video games, modern reinforcement learning algorithms are sample inefficient. This means they require significantly more playing experience than humans to reach an equal level of performance. To address this issue, agents can build a model of the environment and use planning methods to update their policy. In this article, we present the Variational State Tabulation (VaST) approach, which maps high-dimensional state space environments, such as those with visual inputs, to an abstract tabular model. By utilizing the highly efficient Prioritized Sweeping with small backups planning method, VaST can be used to update state-action values. We demonstrate how VaST can quickly learn how to maximize rewards in tasks like 3D navigation and adapt efficiently to sudden changes in rewards or transition probabilities.",1
"In traditional reinforcement learning, an agent maximizes the reward collected during its interaction with the environment by approximating the optimal policy through the estimation of value functions. Typically, given a state s and action a, the corresponding value is the expected discounted sum of rewards. The optimal action is then chosen to be the action a with the largest value estimated by value function. However, recent developments have shown both theoretical and experimental evidence of superior performance when value function is replaced with value distribution in context of deep Q learning [1]. In this paper, we develop a new algorithm that combines advantage actor-critic with value distribution estimated by quantile regression. We evaluated this new algorithm, termed Distributional Advantage Actor-Critic (DA2C or QR-A2C) on a variety of tasks, and observed it to achieve at least as good as baseline algorithms, and outperforming baseline in some tasks with smaller variance and increased stability.",0
"The conventional approach to reinforcement learning involves an agent maximizing its reward by estimating the optimal policy through value function approximation. The value of a state-action pair is determined by the expected discounted sum of rewards and the optimal action is chosen based on the largest estimated value. However, recent research has shown that using value distribution instead of value function can lead to superior performance. In this study, we present a new algorithm called Distributional Advantage Actor-Critic (DA2C or QR-A2C), which combines advantage actor-critic with value distribution estimated by quantile regression. Our evaluation on various tasks revealed that DA2C performs at least as well as baseline algorithms, and outperforms them in some tasks with increased stability and smaller variance.",1
"We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma's Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.",0
"Our research investigates the effective use of expert feedback in learning policies for sequential decision-making. Our focus is on addressing the challenges posed by sparse rewards and long time horizons in reinforcement learning. To tackle these challenges, we introduce a new algorithmic framework, called hierarchical guidance, which exploits the hierarchical structure of the problem to facilitate different modes of expert interaction. Our framework can integrate imitation learning (IL) and reinforcement learning (RL) at different levels, achieving significant reductions in both expert effort and exploration costs. We use long-horizon benchmarks, such as Montezuma's Revenge, to demonstrate that our approach outperforms hierarchical RL in terms of learning speed and standard IL in terms of label-efficiency. Additionally, we provide a theoretical analysis of the labeling cost for certain instantiations of our framework.",1
"In model-based reinforcement learning it is typical to decouple the problems of learning the dynamics model and learning the reward function. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. It is not clear a priori what value the reward function should assign to such states. This paper presents a novel error bound that accounts for the reward model's behavior in states sampled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned. Empirically, this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed.",0
"Typically, in model-based reinforcement learning, the learning of the dynamics model and the reward function are separate issues. However, if the dynamics model is incorrect, it may produce states that would not normally occur in the real environment. It is unclear how to assign a value to such states for the reward function. This paper introduces a new error bound that considers the behavior of the reward model in states that are sampled from the model. This bound is applied to enhance the existing Hallucinated DAgger-MC algorithm, which provides theoretical performance assurances in deterministic MDPs and does not presume that a perfect model can be learned. Empirically, this method of reward learning can greatly enhance control performance in cases where the dynamics model is inaccurate.",1
"We propose a fully automatic method to find standardized view planes in 3D image acquisitions. Standard view images are important in clinical practice as they provide a means to perform biometric measurements from similar anatomical regions. These views are often constrained to the native orientation of a 3D image acquisition. Navigating through target anatomy to find the required view plane is tedious and operator-dependent. For this task, we employ a multi-scale reinforcement learning (RL) agent framework and extensively evaluate several Deep Q-Network (DQN) based strategies. RL enables a natural learning paradigm by interaction with the environment, which can be used to mimic experienced operators. We evaluate our results using the distance between the anatomical landmarks and detected planes, and the angles between their normal vector and target. The proposed algorithm is assessed on the mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and 4.84mm, respectively.",0
"A method is proposed to automatically identify standardized view planes in 3D image acquisitions. Standard view images are crucial in clinical practice for performing biometric measurements from similar anatomical regions, but are often restricted to the native orientation of a 3D image acquisition, making it tiresome and dependent on the operator to navigate through target anatomy to find the required view plane. To address this issue, a multi-scale reinforcement learning (RL) agent framework is employed, with Deep Q-Network (DQN) based strategies extensively evaluated. The RL approach mimics experienced operators by learning through interaction with the environment. The accuracy of the proposed algorithm is evaluated using the distance between anatomical landmarks and detected planes, as well as the angles between their normal vector and target. The mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI are assessed, with accuracy achieved being 1.53mm, 1.98mm, and 4.84mm, respectively.",1
"The balance between exploration and exploitation is a key problem for reinforcement learning methods, especially for Q-learning. In this paper, a fidelity-based probabilistic Q-learning (FPQL) approach is presented to naturally solve this problem and applied for learning control of quantum systems. In this approach, fidelity is adopted to help direct the learning process and the probability of each action to be selected at a certain state is updated iteratively along with the learning process, which leads to a natural exploration strategy instead of a pointed one with configured parameters. A probabilistic Q-learning (PQL) algorithm is first presented to demonstrate the basic idea of probabilistic action selection. Then the FPQL algorithm is presented for learning control of quantum systems. Two examples (a spin- 1/2 system and a lamda-type atomic system) are demonstrated to test the performance of the FPQL algorithm. The results show that FPQL algorithms attain a better balance between exploration and exploitation, and can also avoid local optimal policies and accelerate the learning process.",0
"Reinforcement learning methods, particularly Q-learning, face a crucial challenge of maintaining a balance between exploration and exploitation. To address this issue, this paper introduces an approach called fidelity-based probabilistic Q-learning (FPQL), which is utilized for learning control of quantum systems. The FPQL approach utilizes fidelity to guide the learning process, and iteratively updates the probability of selecting each action at a given state, resulting in a natural exploration strategy rather than a targeted one with preset parameters. Initially, a probabilistic Q-learning (PQL) algorithm is presented to illustrate the fundamental concept of probabilistic action selection. Subsequently, the FPQL algorithm is introduced for learning control of quantum systems, and two examples (a spin-1/2 system and a lambda-type atomic system) are provided to evaluate the performance of the FPQL algorithm. The outcomes show that the FPQL algorithm achieves a superior balance between exploration and exploitation, prevents local optimal policies, and enhances the learning process.",1
"An appealing property of the natural gradient is that it is invariant to arbitrary differentiable reparameterizations of the model. However, this invariance property requires infinitesimal steps and is lost in practical implementations with small but finite step sizes. In this paper, we study invariance properties from a combined perspective of Riemannian geometry and numerical differential equation solving. We define the order of invariance of a numerical method to be its convergence order to an invariant solution. We propose to use higher-order integrators and geodesic corrections to obtain more invariant optimization trajectories. We prove the numerical convergence properties of geodesic corrected updates and show that they can be as computationally efficient as plain natural gradient. Experimentally, we demonstrate that invariance leads to faster optimization and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.",0
"The natural gradient has an attractive feature of being unaffected by arbitrary differentiable reparameterizations of the model, but this property requires infinitesimal steps and is not practical with small but finite step sizes. This study explores the invariance of numerical methods using a combined perspective of Riemannian geometry and numerical differential equation solving. The order of invariance of a numerical method is defined as its convergence order to an invariant solution. To achieve more invariant optimization trajectories, higher-order integrators and geodesic corrections are proposed. The numerical convergence properties of geodesic corrected updates are proven, and they are shown to be as computationally efficient as plain natural gradient. Experimental results demonstrate that invariance leads to faster optimization, and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.",1
"In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",0
"In this study, we approach hierarchical reinforcement learning from the perspective of representation learning. Our goal is to solve the problem of learning lower layers in a hierarchy by transforming it into the problem of learning trajectory-level generative models. To achieve this, we have developed a model called SeCTAR, which is inspired by variational autoencoders and learns continuous latent representations of trajectories. SeCTAR consists of a latent-conditioned policy and a latent-conditioned model, which work together to generate trajectories that match each other given the same latent input. This allows the model to predict the outcome of closed loop policy behavior and provides a built-in prediction mechanism. To perform hierarchical RL with SeCTAR, we have proposed a novel algorithm that combines model-based planning in the learned latent space with an unsupervised exploration objective. Our experiments show that SeCTAR is highly effective at reasoning over long horizons with sparse rewards, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",1
"We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.",0
"We have developed a novel transformation technique for efficient neural architecture search that preserves function. This transformation enables the reuse of previously trained networks and successful architectures, resulting in better sample efficiency. Our aim is to overcome the limitations of current network transformation operations that only modify layer-level architecture, such as adding or pruning filters, which fail to change connection path topology. Our proposed path-level transformation operations allow the meta-controller to modify the path topology of the network, maintaining the benefits of weight reuse while efficiently designing complex structures, such as Inception models. We also propose a bidirectional tree-structured reinforcement learning meta-controller that explores a versatile tree-structured architecture space, similar to multi-branch architectures. Our experiments on limited computational resources with image classification datasets show improved parameter efficiency and better test results, demonstrating the transferability and effectiveness of our designed architectures (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting).",1
"The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search.   In this paper, we describe how we interface GVGAI to the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of GVGAI games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions.",0
"The GVGAI competition and its framework are utilized to evaluate AI algorithms on various games coded in a specific language. Although the competition has garnered much attention, it has concentrated on online planning, which permits the usage of algorithms like Monte Carlo Tree Search. This study details the process of connecting GVGAI to the OpenAI Gym environment, a prevalent method of linking agents to reinforcement learning problems. Through this interface, we assess how prevalent implementations of various deep reinforcement learning algorithms perform on multiple GVGAI games. Additionally, we analyze the outcomes to provide an initial indication of the games' comparative difficulty and their similarity to those in the Arcade Learning Environment under comparable circumstances.",1
"Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.",0
"There is a high demand for reinforcement learning methods that can handle real-world sequential decision making problems that are partially observable and have an unknown environment model. This is due to the fact that only incomplete and noisy observations are available. This paper proposes a solution called deep variational reinforcement learning (DVRL), which involves an inductive bias that enables an agent to learn a generative model of the environment and make effective inferences in that model. An n-step approximation to the evidence lower bound (ELBO) is developed to train the model jointly with the policy, resulting in a suitable latent state representation for the control task. The proposed method is compared to previous approaches that rely on recurrent neural networks to encode the past in experiments on Mountain Hike and flickering Atari, and it is demonstrated that DVRL outperforms them.",1
"Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.",0
"Various applications have yielded exciting results through deep learning on graph structures. Despite this, there has been little attention given to the robustness of these models, in contrast to the extensive research on image or text adversarial attack and defense. In this paper, we concentrate on adversarial attacks that deceive the model by modifying the combinatorial structure of the data. Our research presents a reinforcement learning based attack method that learns a generalizable attack policy using only prediction labels from the target classifier. Additionally, we provide variants of genetic algorithms and gradient methods in scenarios where prediction confidence or gradients are available. By using both synthetic and real-world data, we demonstrate that a family of Graph Neural Network models are vulnerable to these attacks in both graph-level and node-level classification tasks. Furthermore, we show that these attacks can be used to diagnose the learned classifiers.",1
"In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.",0
"In various situations, a group of agents must coordinate their actions in a decentralised manner. However, in simulated or laboratory settings, agents can be trained centrally with access to global state information and no communication constraints. To take advantage of centralised learning, joint action-values conditioned on extra state information can be learned, but it is unclear how to extract decentralised policies. Our solution is QMIX, a novel method that can train decentralised policies in a centralised end-to-end fashion. QMIX estimates joint action-values using a network that combines per-agent values that only condition on local observations. We ensure that the joint-action value is monotonic in the per-agent values, allowing for easy maximisation of the joint action-value in off-policy learning and consistency between centralised and decentralised policies. We evaluate QMIX on challenging StarCraft II micromanagement tasks and show that it outperforms existing value-based multi-agent reinforcement learning methods.",1
"Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",0
"Deep reinforcement learning (RL) faces a significant challenge in domain adaptation. Obtaining data in many scenarios is difficult, leading to the need for agents to learn a source policy in an environment where data is easily available. The hope is that this policy will generalize well to the target domain. To address this problem, we present DARLA, a multi-stage RL agent that prioritizes learning to see before acting. DARLA's vision is based on acquiring a disentangled representation of the observed environment. Once DARLA can see, it can acquire source policies that are robust to domain shifts, even without access to the target domain. DARLA outperforms traditional baselines in zero-shot domain adaptation scenarios in a range of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C, and EC).",1
"We propose to learn a curriculum or a syllabus for supervised learning and deep reinforcement learning with deep neural networks by an attachable deep neural network, called ScreenerNet. Specifically, we learn a weight for each sample by jointly training the ScreenerNet and the main network in an end-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor requires to remember the past learning history. We show the networks augmented with the ScreenerNet achieve early convergence with better accuracy than the state-of-the-art curricular learning methods in extensive experiments using three popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a Cart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend other curriculum learning methods such as Prioritized Experience Replay (PER) for further accuracy improvement.",0
"Our proposal involves utilizing an attachable deep neural network, known as ScreenerNet, to acquire a curriculum or syllabus for deep reinforcement learning and supervised learning with deep neural networks. The ScreenerNet assists in training the main network by determining an appropriate weight for each sample through an end-to-end self-paced approach. In contrast to other methods, the ScreenerNet is not affected by sampling bias and does not require recollection of past learning experiences. Our experiments using popular vision datasets, including MNIST, CIFAR10, and Pascal VOC2012, as well as a Cart-pole task utilizing Deep Q-learning, demonstrate that the ScreenerNet augmented networks achieve early convergence and better accuracy than the current state-of-the-art curricular learning techniques. Furthermore, the ScreenerNet has the potential to enhance other curriculum learning methods, such as Prioritized Experience Replay (PER), to further improve accuracy.",1
"This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be not optimal. Compared to previous works that decouple agents in the game by assuming optimality in expert strategies, we introduce a new objective function that directly pits experts against Nash Equilibrium strategies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. In our setting the model and algorithm do not decouple by agent. In order to find Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In our numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to previous approaches using tabular representations. Moreover, with sub-optimal expert demonstrations our algorithms recover both reward functions and strategies with good quality.",0
"This article examines the issue of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are not optimal. The study introduces a new objective function that directly pits experts against Nash Equilibrium strategies, as opposed to prior research which assumed optimality in expert strategies and decoupled agents in the game. The article also presents an algorithm to solve for the reward function in the context of inverse reinforcement learning, utilizing deep neural networks as model approximations. The model and algorithm are not decoupled by agent in this setting. To address Nash Equilibrium in large-scale games, an adversarial training algorithm for zero-sum stochastic games is proposed, with the theoretical appeal of non-existence of local optima in its objective function. Numerical experiments show that the Nash Equilibrium and inverse reinforcement learning algorithms are effective in addressing games that were not previously possible with tabular representations. Additionally, with sub-optimal expert demonstrations, the algorithms recover both reward functions and strategies with good quality.",1
"Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework {\em PEORL} that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision-making in a dynamic environment with uncertainties.   Symbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL.",0
"Intelligent autonomous agents have been constructed using both reinforcement learning and symbolic planning. However, reinforcement learning necessitates a vast amount of experience gained from real-world interactions, making it impractical. On the other hand, symbolic planning relies on manually crafted symbolic knowledge that may not be adaptable to changes in the domain. The paper proposes a unified framework named PEORL, which integrates hierarchical reinforcement learning with symbolic planning to handle decision-making in a dynamic environment with uncertainties. The agent's task execution and learning are directed by symbolic plans, and the gained experience is used to enhance planning. This approach accelerates policy search and strengthens symbolic plans in intricate domains. The benchmark domains of HRL have been used to test the framework.",1
"When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov's smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm's sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.",0
"For decades, the challenge of solving the Bellman optimality equation with stability guarantees using function approximation has remained unresolved in reinforcement learning. This is due to the fact that the Bellman operator can potentially result in expansion and cause oscillating and divergent behavior of commonly used algorithms like Q-learning. In this study, we address this issue by reformulating the Bellman equation into a novel primal-dual optimization problem with Nesterov's smoothing technique and the Legendre-Fenchel transformation. Our proposed algorithm, Smoothed Bellman Error Embedding, can be applied to any differentiable function class and provides the first convergence guarantee for general nonlinear function approximation. We also analyze the sample complexity of the algorithm and compare it favorably to state-of-the-art baselines in various benchmark control problems.",1
"We introduce Mix&Match (M&M) - a training framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents. In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally. We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods. (2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.",0
"Introducing Mix&Match (M&M) - a framework for training RL agents that are difficult or slow to train. M&M utilizes a unique process to automatically create a curriculum for agents, allowing for the gradual training of more complex agents by building on the solutions found by simpler ones. Unlike traditional curriculum learning approaches, M&M does not modify tasks or environments presented, but instead alters how the policy is represented internally. Our method has been proven successful in three experimental setups: (1) training an agent to control over 700 actions in a challenging 3D first-person task, resulting in faster training and better final performance than traditional methods. (2) progressing through a curriculum of architectural variants defining an agent's internal state. (3) improving agent performance in a multitask setting.",1
Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.,0
"The presence of exogenous state variables and rewards may impede reinforcement learning by introducing uncontrolled fluctuations into the reward signal. We provide a formal definition for exogenous state variables and rewards and establish the circumstances under which an MDP with exogenous state can be separated into an exogenous Markov Reward Process containing only the exogenous state+reward and an endogenous Markov Decision Process based only on endogenous rewards. Moreover, we determine a variance-covariance requirement for Monte Carlo policy evaluation on the endogenous MDP to be accelerated as compared to utilizing the full MDP. This acceleration is likely to apply to all RL algorithms. We create two algorithms to identify the exogenous variables and test them on various MDPs. Our findings demonstrate that the algorithms are feasible and can substantially hasten reinforcement learning.",1
"Recent developments have established the vulnerability of deep reinforcement learning to policy manipulation attacks via intentionally perturbed inputs, known as adversarial examples. In this work, we propose a technique for mitigation of such attacks based on addition of noise to the parameter space of deep reinforcement learners during training. We experimentally verify the effect of parameter-space noise in reducing the transferability of adversarial examples, and demonstrate the promising performance of this technique in mitigating the impact of whitebox and blackbox attacks at both test and training times.",0
"Deep reinforcement learning has been found to be susceptible to policy manipulation attacks through the use of deliberately distorted inputs, also known as adversarial examples. To counteract this, we present a method that involves adding noise to the parameter space of deep reinforcement learners during their training, which helps to reduce the transferability of adversarial examples. Our experiments confirm that this approach is effective in mitigating the effects of whitebox and blackbox attacks during both training and testing, and we believe that it holds great promise for improving the overall performance of deep reinforcement learning systems.",1
"While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a ""relational inductive bias"": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object- and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more naive approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.",0
"Although deep learning systems are highly proficient in tasks such as language processing, object classification, and gameplay, they struggle with constructing or modifying complex systems like a tower of blocks. The authors propose that these systems lack a ""relational inductive bias,"" which is the ability to reason about inter-object relationships and make decisions using a structured description of a scene. To test this theory, the authors developed a task that involves gluing pairs of blocks together to stabilize a tower and measured human performance. They then introduced a deep reinforcement learning agent that used object- and relation-centric scene and policy representations to complete the task. The results showed that the structured representations allowed the agent to outperform both humans and less sophisticated approaches, indicating that relational inductive bias is a crucial element in solving structured reasoning problems and building intelligent, adaptable machines.",1
"Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.",0
"Decades ago, theoretical and empirical results using tabular representations and linear function approximators shaped our understanding of reinforcement learning (RL). These results suggested that temporal differencing (TD) RL methods were better than direct Monte Carlo estimation (MC). However, in modern deep RL, which involves complex environments and deep nonlinear models, we need to re-evaluate TD's role. To do this, we've created environments that control for specific factors that affect performance. Our findings show that, when comparing TD with infinite-horizon MC, we can replicate classic results in modern settings. However, finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. Therefore, deep RL can use MC as a viable alternative to TD.",1
"Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-task in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.",0
"The task of determining the minimum or maximum mean from a finite set of distributions is crucial in fields such as game tree search, planning, and reinforcement learning. To formalize this task, we propose sequentially testing the comparison of the minimum mean from the set of distributions to a given threshold. We have developed improved non-asymptotic lower bounds that demonstrate how different sampling behavior is necessary for low and high true minimums to achieve optimality. Existing policies such as Thompson Sampling and Lower Confidence Bounds policy are only effective for one of these cases. To address this, we introduce a new approach called Murphy Sampling that is optimal for both low and high true minimums. Additionally, we have designed advanced self-normalized deviation inequalities that enable more aggressive stopping rules. Our theoretical guarantees are supported by experiments that demonstrate the superior performance of Murphy Sampling in practical applications.",1
"The research on deep reinforcement learning which estimates Q-value by deep learning has been attracted the interest of researchers recently. In deep reinforcement learning, it is important to efficiently learn the experiences that an agent has collected by exploring environment. We propose NEC2DQN that improves learning speed of a poor sample efficiency algorithm such as DQN by using good one such as NEC at the beginning of learning. We show it is able to learn faster than Double DQN or N-step DQN in the experiments of Pong.",0
"Recently, researchers have been focusing on the study of deep reinforcement learning, where Q-value is estimated through deep learning. Learning the experiences that an agent has gathered by exploring the environment efficiently is crucial in this area. Our proposal, NEC2DQN, enhances the learning speed of a low sample efficiency algorithm like DQN by implementing a superior one such as NEC at the start of the learning process. Through our experiments with Pong, we demonstrate that NEC2DQN can learn at a faster rate than Double DQN or N-step DQN.",1
"The deep reinforcement learning method usually requires a large number of training images and executing actions to obtain sufficient results. When it is extended a real-task in the real environment with an actual robot, the method will be required more training images due to complexities or noises of the input images, and executing a lot of actions on the real robot also becomes a serious problem. Therefore, we propose an extended deep reinforcement learning method that is applied a generative model to initialize the network for reducing the number of training trials. In this paper, we used a deep q-network method as the deep reinforcement learning method and a deep auto-encoder as the generative model. We conducted experiments on three different tasks: a cart-pole game, an atari game, and a real-game with an actual robot. The proposed method trained efficiently on all tasks than the previous method, especially 2.5 times faster on a task with real environment images.",0
"To obtain satisfactory results, the deep reinforcement learning approach typically necessitates an abundance of training images and actions executed. In the event of extending this method to real-world tasks with an actual robot, obtaining adequate outcomes becomes more challenging due to the input images' complexities or noises, and performing numerous actions on the real robot poses a significant issue. To address this problem, we propose an advanced deep reinforcement learning method that employs a generative model to initialize the network and diminish the number of training attempts. Our approach employs a deep q-network method for deep reinforcement learning and a deep auto-encoder for the generative model. We conducted experiments on three distinct tasks: a cart-pole game, an atari game, and a real-game with an actual robot. Our proposed method exhibited more efficient training across all tasks than the previous method, particularly 2.5 times faster on a task involving real environment images.",1
"In recent years, deep reinforcement learning has been shown to be adept at solving sequential decision processes with high-dimensional state spaces such as in the Atari games. Many reinforcement learning problems, however, involve high-dimensional discrete action spaces as well as high-dimensional state spaces. This paper considers entropy bonus, which is used to encourage exploration in policy gradient. In the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. We apply these estimators to several models for the parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we test our algorithms on two environments: a multi-hunter multi-rabbit grid game and a multi-agent multi-arm bandit problem. The results show that our entropy estimators substantially improve performance with marginal additional computational cost.",0
"Recently, deep reinforcement learning has demonstrated its proficiency in tackling sequential decision-making processes with high-dimensional state spaces, such as those found in Atari games. Nevertheless, many reinforcement learning issues involve high-dimensional discrete action spaces, along with high-dimensional state spaces. This study explores the use of an entropy bonus to encourage policy gradient exploration. In instances where action spaces are high-dimensional, computing entropy and its gradient necessitates enumerating all actions in the action space and performing forward and backpropagation for each action, which may not be practical. We present several novel, unbiased estimators for the entropy bonus and its gradient. These estimators are applied to various models for parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we evaluate our algorithms on two environments: a multi-hunter multi-rabbit grid game and a multi-agent multi-arm bandit problem. The outcomes indicate that our entropy estimators can significantly improve performance with only a marginal increase in computational cost.",1
"Episodic memory is a psychology term which refers to the ability to recall specific events from the past. We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful. Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states. The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on. Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory. Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.",0
"The term ""episodic memory"" in psychology refers to the ability to recollect specific past events. One benefit of this memory type is its ability to assign credit to a particular state when recalled information is deemed useful. Taking inspiration from this, and the growing trend of external memory mechanisms in deep learning systems to handle long-term dependencies, we propose a new algorithm that employs a reservoir sampling technique to maintain an external memory of a fixed number of past states. This algorithm enables a deep reinforcement learning agent to learn online and selectively remember states that prove to be valuable for future recollection. Importantly, this method allows for efficient online computation of gradient estimates concerning the external memory's write process. Thus, unlike previous external memory mechanisms, it is practical to use in an online reinforcement learning environment.",1
"We study active object tracking, where a tracker takes as input the visual observation (i.e., frame sequence) and produces the camera control signal (e.g., move forward, turn left, etc.). Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many expensive trial-and-errors in realworld. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-toaction prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.",0
"The focus of our research is on active object tracking, whereby the tracker utilizes visual observations (i.e. frame sequences) to determine the camera control signal (such as moving forward, turning left, etc.). Typically, traditional methods tackle tracking and camera control separately, which poses a challenge in tuning them jointly. Additionally, this method requires extensive human labeling efforts and expensive trial-and-error processes in the real world. To address these problems, we propose an end-to-end solution using deep reinforcement learning, wherein a ConvNet-LSTM function approximator is utilized for direct frame-to-action prediction. In this paper, we also introduce an environment augmentation technique and a customized reward function, both of which are crucial for successful training. Our tracker, trained in simulators such as ViZDoom and Unreal Engine, exhibits good generalization by tracking unseen objects, moving paths, appearances, backgrounds, and distracting objects. It can also restore tracking even when the target is lost occasionally. Furthermore, based on our experiments on the VOT dataset, we find that the tracking ability obtained solely from simulators has the potential to transfer to real-world scenarios.",1
"Scheduling the transmission of time-sensitive data to multiple users over error-prone communication channels is studied with the goal of minimizing the long-term average age of information (AoI) at the users under a constraint on the average number of transmissions at the source node. After each transmission, the source receives an instantaneous ACK/NACK feedback from the intended receiver and decides on what time and to which user to transmit the next update. The optimal scheduling policy is first studied under different feedback mechanisms when the channel statistics are known; in particular, the standard automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols are considered. Then a reinforcement learning (RL) approach is introduced, which does not assume any a priori information on the random processes governing the channel states. Different RL methods are verified and compared through numerical simulations.",0
"The aim of this study is to minimize the long-term average age of information (AoI) at multiple users when transmitting time-sensitive data over communication channels with errors. The source node is required to adhere to an average number of transmissions constraint. The source node receives feedback in the form of ACK/NACK from the intended receiver after each transmission and decides when and to whom to transmit the next update. The optimal scheduling policy is studied using various feedback mechanisms when the channel statistics are known, such as the automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols. Additionally, a reinforcement learning (RL) approach is introduced, which requires no a priori information on the random processes governing the channel states. Numerical simulations are performed to compare and verify different RL methods.",1
"In recent years an increasing number of researchers and practitioners have been suggesting algorithms for large-scale neural network architecture search: genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. None of them, however, demonstrated high-performance without training new experiments in the presence of unseen datasets. We propose a new deep neural network accuracy predictor, that estimates in fractions of a second classification performance for unseen input datasets, without training. In contrast to previously proposed approaches, our prediction is not only calibrated on the topological network information, but also on the characterization of the dataset-difficulty which allows us to re-tune the prediction without any training. Our predictor achieves a performance which exceeds 100 networks per second on a single GPU, thus creating the opportunity to perform large-scale architecture search within a few minutes. We present results of two searches performed in 400 seconds on a single GPU. Our best discovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for CIFAR-100, verified by training. These networks are performance competitive with other automatically discovered state-of-the-art networks however we only needed a small fraction of the time to solution and computational resources.",0
"Recently, an increasing number of researchers and practitioners have proposed algorithms for large-scale neural network architecture search, including genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. However, none of these approaches have shown high performance without the need for new experiments when presented with unseen datasets. To address this issue, we introduce a new deep neural network accuracy predictor that can estimate classification performance for unseen input datasets in fractions of a second, without requiring any training. Unlike previous approaches, our predictor is calibrated not only on the topological network information but also on the dataset difficulty characterization, allowing us to re-tune the prediction without any additional training. Our predictor can perform over 100 networks per second on a single GPU, making it possible to conduct large-scale architecture search within a few minutes. We demonstrate the effectiveness of our approach by performing two searches in 400 seconds on a single GPU. Our best discovered networks achieve high accuracy rates of 93.67% for CIFAR-10 and 81.01% for CIFAR-100, which are competitive with other state-of-the-art networks that were discovered automatically. However, our approach requires only a small fraction of the time and computational resources.",1
Self-play is an unsupervised training procedure which enables the reinforcement learning agents to explore the environment without requiring any external rewards. We augment the self-play setting by providing an external memory where the agent can store experience from the previous tasks. This enables the agent to come up with more diverse self-play tasks resulting in faster exploration of the environment. The agent pretrained in the memory augmented self-play setting easily outperforms the agent pretrained in no-memory self-play setting.,0
"The unsupervised training technique of self-play allows reinforcement learning agents to navigate the environment without relying on external rewards. To enhance this approach, we introduce an external memory that enables the agent to save past experiences and generate more varied self-play tasks, leading to quicker exploration of the environment. As a result, the agent trained with the memory-augmented self-play method performs significantly better than the agent trained without memory augmentation.",1
"Most artificial intelligence models have limiting ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.",0
"Artificial intelligence models often struggle to quickly solve new tasks without losing previously learned knowledge. However, continual learning has emerged as a potential solution to this issue by allowing models to learn tasks sequentially. This study proposes a new approach to continual learning called Reinforced Continual Learning, which uses advanced reinforcement learning strategies to find the optimal neural architecture for each new task. This method effectively prevents catastrophic forgetting and performs well on new tasks. Experiments on sequential classification tasks using MNIST and CIFAR-100 datasets demonstrate that this approach outperforms other existing continual learning methods for deep networks.",1
"Much attention has been devoted recently to the development of machine learning algorithms with the goal of improving treatment policies in healthcare. Reinforcement learning (RL) is a sub-field within machine learning that is concerned with learning how to make sequences of decisions so as to optimize long-term effects. Already, RL algorithms have been proposed to identify decision-making strategies for mechanical ventilation, sepsis management and treatment of schizophrenia. However, before implementing treatment policies learned by black-box algorithms in high-stakes clinical decision problems, special care must be taken in the evaluation of these policies.   In this document, our goal is to expose some of the subtleties associated with evaluating RL algorithms in healthcare. We aim to provide a conceptual starting point for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new ways of treating patients. In the following, we describe how choices about how to summarize a history, variance of statistical estimators, and confounders in more ad-hoc measures can result in unreliable, even misleading estimates of the quality of a treatment policy. We also provide suggestions for mitigating these effects---for while there is much promise for mining observational health data to uncover better treatment policies, evaluation must be performed thoughtfully.",0
"Recently, there has been a significant focus on improving treatment policies in healthcare by developing machine learning algorithms. Reinforcement learning (RL) is a subfield of machine learning that aims to optimize long-term effects by learning how to make a sequence of decisions. RL algorithms have been proposed for the treatment of mechanical ventilation, sepsis management, and schizophrenia. However, before implementing any policies learned by black-box algorithms in high-stakes clinical decision-making, it is crucial to evaluate these policies carefully. Our objective is to highlight the complexities associated with evaluating RL algorithms in healthcare and provide a conceptual foundation for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new patient treatment methods. We explain how choices regarding summarizing a history, variance of statistical estimators, and confounders can potentially result in unreliable and misleading estimates of treatment policy quality. We also advise on how to mitigate these effects, as while there is potential for mining observational health data to uncover better treatment policies, careful evaluation is essential.",1
"We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results.",0
"A new method for deriving updates for policy gradients in reinforcement learning is proposed using Fourier analysis. This approach transforms integrals associated with expected policy gradients into multiplications by recasting them as convolutions. Analytical solutions are obtained which capture the low variance benefits of EPG in numerous settings. Trigonometric and radial basis functions are utilized for the critic, both of which have the universal approximation property. Policy selection can be nearly arbitrary, encompassing hybrid continuous-discrete probability distributions and mixtures. Additionally, a general family of sample-based estimators for stochastic policy gradients is derived, unifying existing approximation outcomes. It is believed that this technique has the potential to shape the next generation of policy gradient techniques, bolstered by analytical findings.",1
"Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.",0
"New perspectives in chemical synthesis can be achieved through deep generative models for graph-structured data. By using differentiable models to generate molecular graphs, the need for costly search procedures in the extensive and discrete space of chemical structures can be avoided. MolGAN is introduced as an implicit, likelihood-free generative model for small molecular graphs. It operates on graph-structured data, eliminating the need for expensive graph matching procedures or node ordering heuristics. Furthermore, our approach combines generative adversarial networks with reinforcement learning to encourage the generation of molecules with specific desired chemical properties. Our experiments on the QM9 chemical database reveal that our model can generate nearly 100% valid compounds. MolGAN outperforms recent proposals that use string-based (SMILES) representations of molecules and a likelihood-based method that directly generates graphs, despite being susceptible to mode collapse.",1
"The question of how to explore, i.e., take actions with uncertain outcomes to learn about possible future rewards, is a key question in reinforcement learning (RL). Here, we show a surprising result: We show that Q-learning with nonlinear Q-function and no explicit exploration (i.e., a purely greedy policy) can learn several standard benchmark tasks, including mountain car, equally well as, or better than, the most commonly-used $\epsilon$-greedy exploration. We carefully examine this result and show that both the depth of the Q-network and the type of nonlinearity are important to induce such deterministic exploration.",0
"A fundamental inquiry in reinforcement learning (RL) pertains to how to conduct actions with uncertain outcomes to gain insight into prospective rewards. Our study reveals a remarkable finding: employing Q-learning with a nonlinear Q-function and a solely greedy policy, without explicit exploration, can perform just as effectively, or even better, than the widely utilized $\epsilon$-greedy exploration, in various standard benchmark tasks, such as mountain car. We meticulously scrutinize this outcome and demonstrate that both the depth of the Q-network and the nonlinearity type play a crucial role in inducing such deterministic exploration.",1
"Despite significant advances in the field of deep Reinforcement Learning (RL), today's algorithms still fail to learn human-level policies consistently over a set of diverse tasks such as Atari 2600 games. We identify three key challenges that any algorithm needs to master in order to perform well on all games: processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently. In this paper, we propose an algorithm that addresses each of these challenges and is able to learn human-level policies on nearly all Atari games. A new transformed Bellman operator allows our algorithm to process rewards of varying densities and scales; an auxiliary temporal consistency loss allows us to train stably using a discount factor of $\gamma = 0.999$ (instead of $\gamma = 0.99$) extending the effective planning horizon by an order of magnitude; and we ease the exploration problem by using human demonstrations that guide the agent towards rewarding states. When tested on a set of 42 Atari games, our algorithm exceeds the performance of an average human on 40 games using a common set of hyper parameters. Furthermore, it is the first deep RL algorithm to solve the first level of Montezuma's Revenge.",0
"Despite significant advancements, today's deep Reinforcement Learning (RL) algorithms still struggle to consistently learn human-level policies across a diverse set of tasks, including Atari 2600 games. We have identified three key challenges that any algorithm must overcome to perform well on all games: processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently. Our proposed algorithm addresses each of these challenges and has the ability to learn human-level policies on almost all Atari games. By using a new transformed Bellman operator, our algorithm can process rewards of varying densities and scales. Additionally, an auxiliary temporal consistency loss allows for stable training using a discount factor of $\gamma = 0.999$, increasing the effective planning horizon by an order of magnitude. We also ease the exploration problem by utilizing human demonstrations that direct the agent towards rewarding states. When tested on 42 Atari games, our algorithm surpasses the performance of an average human on 40 games using a common set of hyperparameters. It is also the first deep RL algorithm to solve the first level of Montezuma's Revenge.",1
"We show that when a third party, the adversary, steps into the two-party setting (agent and operator) of safely interruptible reinforcement learning, a trade-off has to be made between the probability of following the optimal policy in the limit, and the probability of escaping a dangerous situation created by the adversary. So far, the work on safely interruptible agents has assumed a perfect perception of the agent about its environment (no adversary), and therefore implicitly set the second probability to zero, by explicitly seeking a value of one for the first probability. We show that (1) agents can be made both interruptible and adversary-resilient, and (2) the interruptibility can be made safe in the sense that the agent itself will not seek to avoid it. We also solve the problem that arises when the agent does not go completely greedy, i.e. issues with safe exploration in the limit. Resilience to perturbed perception, safe exploration in the limit, and safe interruptibility are the three pillars of what we call \emph{virtuously safe reinforcement learning}.",0
"In safely interruptible reinforcement learning, the presence of an adversary creates a trade-off between the likelihood of following the optimal policy and the likelihood of avoiding danger. Previous research on safely interruptible agents has assumed no adversary and focused solely on achieving a probability of one for the first likelihood. However, we demonstrate that interruptibility can be made safe and agents can be made both interruptible and adversary-resilient. Additionally, we address the issue of safe exploration in the limit. Our approach, which we call ""virtuously safe reinforcement learning,"" is built on the three pillars of resilience to perturbed perception, safe exploration in the limit, and safe interruptibility.",1
"In this paper, we propose to combine imitation and reinforcement learning via the idea of reward shaping using an oracle. We study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner's planning horizon as function of its accuracy: a globally optimal oracle can shorten the planning horizon to one, leading to a one-step greedy Markov Decision Process which is much easier to optimize, while an oracle that is far away from the optimality requires planning over a longer horizon to achieve near-optimal performance. Hence our new insight bridges the gap and interpolates between imitation learning and reinforcement learning. Motivated by the above mentioned insights, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximize the total reshaped reward over a finite planning horizon when the oracle is sub-optimal. We experimentally demonstrate that a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.",0
"This paper introduces a new approach that combines imitation and reinforcement learning through the use of an oracle to shape rewards. The study evaluates the effectiveness of the cost-to-go oracle on the planning horizon and shows that a globally optimal oracle can shorten the planning horizon to one step, resulting in a simpler Markov Decision Process. However, an oracle that is not optimal requires planning over a longer horizon to achieve near-optimal performance. This insight bridges the gap between imitation and reinforcement learning. Based on these findings, the paper proposes Truncated Horizon Policy Search (THOR), a method that searches for policies maximizing reshaped reward over a finite planning horizon when the oracle is sub-optimal. Results show that THOR outperforms both RL and IL baselines, even with a sub-optimal oracle.",1
"We consider the transfer of experience samples (i.e., tuples < s, a, s', r >) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.",0
"Our focus is on transferring experience samples from a set of source tasks to improve the learning process in a target task in reinforcement learning (RL). However, most existing approaches solely concentrate on selecting relevant source samples without considering model discrepancies. In contrast, we present a model-based technique that automatically evaluates the importance weight of each source sample and uses them in a batch RL algorithm to solve the target task. The importance weight determines the contribution of each sample to the learning process. We provide a finite-sample analysis of the proposed algorithm based on the results of importance weighting in supervised learning literature. Our empirical comparison with state-of-the-art approaches demonstrates that our algorithm achieves better learning performance and is highly resistant to negative transfer, even when some source tasks differ significantly from the target task.",1
"Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition. Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study.",0
"Although Deep reinforcement learning (DRL) has been successful in developing advanced video-game AI, most current DRL game agents rely on learning end-to-end from the video-output of the game, which is excessive for certain applications and can lead to additional issues. Moreover, working directly on pixel-based raw video data differs significantly from a human player's approach. To address these challenges, we propose a new approach that allows DRL agents to learn from object information. This is achieved using an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector to represent the current game-state and fulfills the DRL task simultaneously. We evaluate our OEN-based DRL agent against state-of-the-art methods on a range of games from the GVG-AI Competition. Our experimental results demonstrate that our object-based DRL agent performs comparably to the approaches used in our comparative study.",1
"Imitation learning (IL) consists of a set of tools that leverage expert demonstrations to quickly learn policies. However, if the expert is suboptimal, IL can yield policies with inferior performance compared to reinforcement learning (RL). In this paper, we aim to provide an algorithm that combines the best aspects of RL and IL. We accomplish this by formulating several popular RL and IL algorithms in a common mirror descent framework, showing that these algorithms can be viewed as a variation on a single approach. We then propose LOKI, a strategy for policy learning that first performs a small but random number of IL iterations before switching to a policy gradient RL method. We show that if the switching time is properly randomized, LOKI can learn to outperform a suboptimal expert and converge faster than running policy gradient from scratch. Finally, we evaluate the performance of LOKI experimentally in several simulated environments.",0
"The set of tools known as Imitation Learning (IL) allows for policies to be quickly learned by utilizing expert demonstrations. However, policies generated through IL may not perform as well as those created through Reinforcement Learning (RL) if the expert is not optimal. This paper aims to combine the best aspects of both RL and IL by formulating various popular algorithms from each in a common mirror descent framework. By doing so, the algorithms can be viewed as variations of a single approach. The proposed strategy, LOKI, first performs a small number of IL iterations before switching to a policy gradient RL method. Proper randomization of the switching time allows LOKI to outperform suboptimal experts and converge faster than running policy gradient from scratch. Lastly, the performance of LOKI is tested in various simulated environments.",1
"We consider reinforcement learning in changing Markov Decision Processes where both the state-transition probabilities and the reward functions may vary over time. For this problem setting, we propose an algorithm using a sliding window approach and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. We also characterize the optimal window size suitable for our algorithm. These results are complemented by a sample complexity bound on the number of sub-optimal steps taken by the algorithm. Finally, we present some experimental results to support our theoretical analysis.",0
"Our focus is on reinforcement learning in Markov Decision Processes that undergo changes, allowing for variations in both the reward functions and state-transition probabilities. To address this problem, we introduce an algorithm that uses a sliding window approach and offer assurances of its performance by comparing its regret to that of the optimal non-stationary policy. Additionally, we determine the optimal window size for our algorithm and provide a sample complexity bound to estimate the number of sub-optimal steps taken by the algorithm. Finally, we present experimental results that support our theoretical analysis.",1
"Policy evaluation with linear function approximation is an important problem in reinforcement learning. When facing high-dimensional feature spaces, such a problem becomes extremely hard considering the computation efficiency and quality of approximations. We propose a new algorithm, LSTD($\lambda$)-RP, which leverages random projection techniques and takes eligibility traces into consideration to tackle the above two challenges. We carry out theoretical analysis of LSTD($\lambda$)-RP, and provide meaningful upper bounds of the estimation error, approximation error and total generalization error. These results demonstrate that LSTD($\lambda$)-RP can benefit from random projection and eligibility traces strategies, and LSTD($\lambda$)-RP can achieve better performances than prior LSTD-RP and LSTD($\lambda$) algorithms.",0
"Reinforcement learning faces a significant challenge when evaluating policies using linear function approximation, especially when dealing with high-dimensional feature spaces. The computation efficiency and quality of approximations are major concerns. To address these challenges, we propose a new algorithm, LSTD($\lambda$)-RP, which combines random projection techniques and eligibility traces. We conducted theoretical analysis and found that LSTD($\lambda$)-RP can achieve better performance than previous algorithms, such as LSTD-RP and LSTD($\lambda$). Our results demonstrate that LSTD($\lambda$)-RP benefits from random projection and eligibility traces strategies, with upper bounds on the estimation error, approximation error, and total generalization error providing meaningful insights.",1
"We design a new myopic strategy for a wide class of sequential design of experiment (DOE) problems, where the goal is to collect data in order to to fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling (MPS), is inspired by the classical posterior (Thompson) sampling algorithm for multi-armed bandits and leverages the flexibility of probabilistic programming and approximate Bayesian inference to address a broad set of problems. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of DOE tasks, and more importantly, enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies.",0
"We have developed a new tactic, known as Myopic Posterior Sampling (MPS), to tackle a range of sequential design of experiment (DOE) problems. The aim of these problems is to gather information to meet a specific goal. Our MPS approach draws inspiration from the classical posterior (Thompson) sampling algorithm for multi-armed bandits, while also utilising probabilistic programming and approximate Bayesian inference to handle a broad range of problems. Our strategy is versatile and has proven competitive with more specialised methods across a variety of DOE tasks. It also enables the tackling of complex DOE targets that existing methods cannot address. Theoretical analysis has shown that MPS can achieve sublinear regret against natural benchmark policies, thanks to our use of adaptive submodularity and reinforcement learning concepts.",1
"The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a return. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance.",0
"Reinforcement learning algorithms aim to estimate and optimize the value function, but unlike supervised learning, there is no available teacher or oracle to provide the true value function. Instead, they estimate and optimize a proxy for the value function based on a sampled and bootstrapped approximation called a return. The choice of return significantly impacts the algorithm's nature, including the rate of future reward discount, bootstrapping timing and methods, and reward nature. These decisions are critical to the success of RL algorithms. This article discusses a gradient-based meta-learning algorithm that can adapt the return's nature while interacting and learning from the environment. The algorithm achieved a new state-of-the-art performance when applied to 57 games on the Atari 2600 environment over 200 million frames.",1
"Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.",0
"Deep Reinforcement Learning (RL) has emerged as a competitive approach for learning in sequential decision-making problems with observable environments, such as computer Go. However, there has been limited progress in using deep RL to handle partially observable environments. To address this, we propose a new architecture called the Action-specific Deep Recurrent Q-Network (ADRQN) that improves learning performance in partially observable domains. The ADRQN encodes actions using a fully connected layer and couples them with a convolutional observation to create an action-observation pair. An LSTM layer integrates the time series of action-observation pairs, learning latent states that a fully connected layer uses to compute Q-values, similar to conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of the ADRQN architecture in various partially observable domains, including flickering Atari games.",1
"A core novelty of Alpha Zero is the interleaving of tree search and deep learning, which has proven very successful in board games like Chess, Shogi and Go. These games have a discrete action space. However, many real-world reinforcement learning domains have continuous action spaces, for example in robotic control, navigation and self-driving cars. This paper presents the necessary theoretical extensions of Alpha Zero to deal with continuous action space. We also provide some preliminary experiments on the Pendulum swing-up task, empirically showing the feasibility of our approach. Thereby, this work provides a first step towards the application of iterated search and learning in domains with a continuous action space.",0
"Alpha Zero's unique feature is its combination of tree search and deep learning, which has been successful in board games such as Chess, Shogi, and Go. However, real-world reinforcement learning domains often involve continuous action spaces, such as robotic control, navigation, and self-driving cars. This study presents theoretical extensions of Alpha Zero to address continuous action spaces and provides preliminary experiments on the Pendulum swing-up task to demonstrate the feasibility of the approach. This work is a crucial initial step towards integrating iterated search and learning in continuous action space domains.",1
"Dyna-style reinforcement learning is a powerful approach for problems where not much real data is available. The main idea is to supplement real trajectories, or sequences of sampled states over time, with simulated ones sampled from a learned model of the environment. However, in large state spaces, the problem of learning a good generative model of the environment has been open so far. We propose to use deep belief networks to learn an environment model for use in Dyna. We present our approach and validate it empirically on problems where the state observations consist of images. Our results demonstrate that using deep belief networks, which are full generative models, significantly outperforms the use of linear expectation models, proposed in Sutton et al. (2008)",0
"When real data is scarce, Dyna-style reinforcement learning can be a valuable approach. This involves supplementing real trajectories with simulated ones generated from a learned model of the environment. However, the challenge of developing a robust generative model of the environment remains unsolved in large state spaces. To address this issue, we propose using deep belief networks in Dyna. Our methodology is tested with problems that involve image state observations, and our results indicate that deep belief networks, which are complete generative models, outperform the linear expectation models proposed in Sutton et al. (2008).",1
"We introduce reinforcement learning for heterogeneous teams in which rewards for an agent are additively factored into local costs, stimuli unique to each agent, and global rewards, those shared by all agents in the domain. Motivating domains include coordination of varied robotic platforms, which incur different costs for the same action, but share an overall goal. We present two templates for learning in this setting with factored rewards: a generalization of Perkins' Monte Carlo exploring starts for POMDPs to canonical MPOMDPs, with a single policy mapping joint observations of all agents to joint actions (MCES-MP); and another with each agent individually mapping joint observations to their own action (MCES-FMP). We use probably approximately local optimal (PALO) bounds to analyze sample complexity, instantiating these templates to PALO learning. We promote sample efficiency by including a policy space pruning technique, and evaluate the approaches on three domains of heterogeneous agents demonstrating that MCES-FMP yields improved policies in less samples compared to MCES-MP and a previous benchmark.",0
"Reinforcement learning is implemented for diverse groups comprising agents whose rewards are factored into local costs, individual stimuli, and global rewards. The objective is to coordinate dissimilar robotic platforms that incur varying costs for the same action while sharing an overall goal. The study presents two learning templates with factored rewards: a modified version of Perkins' Monte Carlo exploring starts for POMDPs to canonical MPOMDPs, with a single policy mapping joint observations of all agents to joint actions (MCES-MP); and another with each agent individually mapping joint observations to their own action (MCES-FMP). Sample complexity is analyzed using probably approximately local optimal (PALO) bounds. The templates are instantiated to PALO learning to promote sample efficiency by including a policy space pruning technique. The study evaluates the approaches on three domains of heterogeneous agents and demonstrates that MCES-FMP yields improved policies in less time compared to MCES-MP and a previous benchmark.",1
"In e-commerce platforms such as Amazon and TaoBao, ranking items in a search session is a typical multi-step decision-making problem. Learning to rank (LTR) methods have been widely applied to ranking problems. However, such methods often consider different ranking steps in a session to be independent, which conversely may be highly correlated to each other. For better utilizing the correlation between different ranking steps, in this paper, we propose to use reinforcement learning (RL) to learn an optimal ranking policy which maximizes the expected accumulative rewards in a search session. Firstly, we formally define the concept of search session Markov decision process (SSMDP) to formulate the multi-step ranking problem. Secondly, we analyze the property of SSMDP and theoretically prove the necessity of maximizing accumulative rewards. Lastly, we propose a novel policy gradient algorithm for learning an optimal ranking policy, which is able to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments are conducted in simulation and TaoBao search engine. The results demonstrate that our algorithm performs much better than online LTR methods, with more than 40% and 30% growth of total transaction amount in the simulation and the real application, respectively.",0
"Ranking items in search sessions on e-commerce platforms like Amazon and TaoBao involves a multi-step decision-making process. To address this issue, learning to rank (LTR) methods have been commonly used. However, these methods often treat each ranking step independently, although they may be highly correlated. To better utilize the correlation between different ranking steps, this paper proposes using reinforcement learning (RL) to learn an optimal ranking policy that maximizes the expected accumulative rewards during a search session. The concept of search session Markov decision process (SSMDP) is formally defined to formulate the multi-step ranking problem, and the necessity of maximizing accumulative rewards is theoretically proven. Lastly, a novel policy gradient algorithm is proposed to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments conducted in both simulation and TaoBao search engine show that our algorithm outperforms online LTR methods, with over 40% and 30% growth of total transaction amount in the simulation and the real application, respectively.",1
"Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.",0
"When agents trained in simulation are deployed into the real world, they may make errors due to differences between their training and execution environments. These mistakes can be dangerous and difficult to detect as the agent is unable to predict them in advance. To mitigate the occurrence of costly errors in real-world applications, we suggest using oracle feedback to develop a predictive model of these blind spots. Our focus is on blind spots that arise in reinforcement learning due to limited state representation, where the agent is unable to differentiate between multiple states as it lacks the necessary features to accurately represent the true state of the world. We formalize the problem of identifying blind spots in RL as a supervised learning problem with class imbalance and noise. We use various forms of oracle feedback, including demonstrations and corrections, to train models to recognize blind spots in unexplored regions of the state space. Our approach combines techniques for label aggregation, calibration, and supervised learning. Through our evaluation on two domains, we demonstrate that our method outperforms baseline approaches and that the learned model can be employed to selectively seek oracle feedback at execution time to prevent errors. We also examine the biases of different types of feedback and their impact on discovering blind spots.",1
"In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user's policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user's actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user's input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) flying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user's private information through observations, but receives a reward signal and user input that both depend on the user's intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user's input. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",0
"Shared autonomy involves combining user input with semi-autonomous control to achieve a common goal, which may be unknown beforehand. However, current methods rely on assumptions about the environment dynamics, user policy, and possible user goals, limiting their applicability in real-world scenarios. To address this, we propose a deep reinforcement learning framework for model-free shared autonomy that uses human-in-the-loop reinforcement learning with neural network function approximation. Our approach learns an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only supervision. We balance the need to follow user commands closely with the need to deviate from suboptimal user actions by discarding low-value actions and selecting the remaining action closest to the user's input. Our algorithm is demonstrated to assist users in real-time control tasks without direct access to the user's private information, with controlled studies involving users and synthetic pilots playing a video game and a pilot study with users flying a real quadrotor. This paper illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",1
"We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action continuous-state reinforcement learning. MAC is a policy gradient algorithm that uses the agent's explicit representation of all action values to estimate the gradient of the policy, rather than using only the actions that were actually executed. We prove that this approach reduces variance in the policy gradient estimate relative to traditional actor-critic methods. We show empirical results on two control domains and on six Atari games, where MAC is competitive with state-of-the-art policy search algorithms.",0
"Our proposed algorithm, known as Mean Actor-Critic (MAC), is designed for reinforcement learning with continuous-state and discrete-action systems. Unlike traditional actor-critic methods, MAC employs an explicit representation of all action values to estimate the policy gradient, resulting in reduced variance. We have demonstrated the effectiveness of this approach through empirical testing on two control domains and six Atari games, where MAC has proven to be competitive with current state-of-the-art policy search algorithms.",1
"Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",0
"The task of generating a program that meets certain specifications automatically is known as program synthesis. Recently, various neural approaches have been proposed for program synthesis, many of which use a sequence generation paradigm similar to neural machine translation. In this approach, sequence-to-sequence models are trained to maximize the probability of known reference programs. Despite its impressive results, this strategy has two main drawbacks. Firstly, it overlooks the fact that many different programs may fulfill a given specification, particularly with incomplete specifications like a few input-output examples. By maximizing the probability of only one reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer's performance. Secondly, this approach ignores the fact that programs have a strict syntax that can be checked efficiently. To overcome these limitations, we apply reinforcement learning to a supervised model that explicitly maximizes the probability of generating semantically correct programs. Furthermore, we introduce a training method that directly maximizes the probability of generating syntactically correct programs that meet the specification. Our contributions lead to enhanced accuracy of the models, particularly when the training data is limited.",1
"We study the inverse optimal control problem in social sciences: we aim at learning a user's true cost function from the observed temporal behavior. In contrast to traditional phenomenological works that aim to learn a generative model to fit the behavioral data, we propose a novel variational principle and treat user as a reinforcement learning algorithm, which acts by optimizing his cost function. We first propose a unified KL framework that generalizes existing maximum entropy inverse optimal control methods. We further propose a two-step Wasserstein inverse optimal control framework. In the first step, we compute the optimal measure with a novel mass transport equation. In the second step, we formulate the learning problem as a generative adversarial network. In two real world experiments - recommender systems and social networks, we show that our framework obtains significant performance gains over both existing inverse optimal control methods and point process based generative models.",0
"Our focus in social sciences is on the inverse optimal control problem, which involves learning a user's true cost function from their observed behavior over time. Rather than using traditional phenomenological approaches to fit behavioral data with a generative model, we propose a novel variational principle that treats the user as a reinforcement learning algorithm that optimizes their cost function. Our approach includes a unified KL framework that builds on existing maximum entropy inverse optimal control methods, as well as a two-step Wasserstein inverse optimal control framework. In the first step, we leverage a novel mass transport equation to compute the optimal measure, while in the second step we use a generative adversarial network to formulate the learning problem. Our experiments on recommender systems and social networks demonstrate that our approach outperforms existing inverse optimal control methods and point process-based generative models.",1
"In this paper, we explore using deep reinforcement learning for problems with multiple agents. Most existing methods for deep multi-agent reinforcement learning consider only a small number of agents. When the number of agents increases, the dimensionality of the input and control spaces increase as well, and these methods do not scale well. To address this, we propose casting the multi-agent reinforcement learning problem as a distributed optimization problem. Our algorithm assumes that for multi-agent settings, policies of individual agents in a given population live close to each other in parameter space and can be approximated by a single policy. With this simple assumption, we show our algorithm to be extremely effective for reinforcement learning in multi-agent settings. We demonstrate its effectiveness against existing comparable approaches on co-operative and competitive tasks.",0
"The aim of this study is to investigate the use of deep reinforcement learning in managing problems with multiple agents. Although current deep multi-agent reinforcement learning methods only consider a limited number of agents, the number of agents and the input and control spaces' dimensionality increases as the number of agents increases, resulting in poor scalability. To overcome this challenge, we propose treating the multi-agent reinforcement learning issue as a distributed optimization problem. Our approach assumes that in multi-agent settings, individual agents' policies are located close to each other in parameter space and can be modeled by a single policy, resulting in a highly effective reinforcement learning algorithm for multi-agent settings. We demonstrate its efficacy in cooperative and competitive tasks, outperforming comparable existing methods.",1
"We introduce a method to disentangle controllable and uncontrollable factors of variation by interacting with the world. Disentanglement leads to good representations and is important when applying deep neural networks (DNNs) in fields where explanations are required. This study attempts to improve an existing reinforcement learning (RL) approach to disentangle controllable and uncontrollable factors of variation, because the method lacks a mechanism to represent uncontrollable obstacles. To address this problem, we train two DNNs simultaneously: one that represents the controllable object and another that represents uncontrollable obstacles. For stable training, we applied a pretraining approach using a model robust against uncontrollable obstacles. Simulation experiments demonstrate that the proposed model can disentangle independently controllable and uncontrollable factors without annotated data.",0
"In this study, we present a technique to separate factors of variation that can be controlled from those that cannot be controlled by interacting with the environment. This disentanglement is crucial for creating effective representations, particularly in fields where explanations are needed and deep neural networks (DNNs) are used. However, an existing reinforcement learning (RL) approach lacks the ability to represent uncontrollable obstacles, which limits its effectiveness. To address this issue, we simultaneously train two DNNs: one for controllable objects and one for uncontrollable obstacles. We use a pretraining method with a robust model to ensure stable training. The proposed model can disentangle controllable and uncontrollable factors without the need for annotated data, as demonstrated in simulation experiments.",1
"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",0
"The problem of Inverse Reinforcement Learning (IRL) involves learning an agent's preferences from its behavior on a task. Although this problem has been thoroughly researched, the problem of online IRL has received less attention. Online IRL is when observations are incrementally gathered, but rerunning an IRL method is not feasible due to the demands of the application. We have introduced a new framework for online IRL, called incremental IRL (I2RL), and a new method that improves maximum entropy IRL with hidden variables in this setting. Our formal analysis shows that the new method has an increasing performance with more demonstration data and a probabilistically bounded error, both under full and partial observability. In a simulated robotic application of penetrating a continuous patrol under occlusion, experiments show the new method's relatively improved performance and speed up, validating the usefulness of online IRL.",1
"The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.",0
"The mathematical formalization of intelligent decision making provided by the framework of reinforcement learning or optimal control is powerful and widely applicable. Although the reinforcement learning problem is effective in reasoning about uncertainty, the connection between reinforcement learning and probabilistic models' inference is not immediately apparent. However, this connection is valuable in algorithm design, as formalizing a problem as probabilistic inference allows for the use of a wide range of approximate inference tools, flexible and powerful model extension, and reasoning about partial observability and compositionality. This article explores how a generalization of the reinforcement learning or optimal control problem, known as maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics and variational inference in the case of stochastic dynamics. The article provides a detailed derivation of this framework, reviews previous work that has employed these ideas to propose new reinforcement learning and control algorithms, and discusses future research perspectives.",1
"We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information.",0
"A new technique for deep reinforcement learning is introduced in this study, which detects moving objects automatically and utilizes relevant information for action selection. The technique employs unsupervised detection of moving objects by utilizing structure from motion, rather than learning a policy directly from raw images. The agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the agent's policy on the moving objects. As the agent interacts with the environment, it identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, named Motion-Oriented Reinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Additionally, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. Practitioners can gain insight into the policy by examining the segmentation and motion of each object detected by the agent, enabling them to confirm if the policy is based on reasonable information.",1
"Meta-learning approaches have been proposed to tackle the few-shot learning problem.Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined.Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",0
"The issue of few-shot learning has been addressed through the use of meta-learning techniques. Typically, a meta-learner is trained on various tasks to increase its adaptability to new tasks. However, over-training during the meta-training phase may result in a meta-learner that is biased towards existing tasks and unable to adapt to new ones, particularly with limited examples for updating the model. To prevent this bias and enhance the meta-learner's generalizability, we propose a novel strategy called Task-Agnostic Meta-Learning (TAML) algorithms. Our entropy-based approach meta-learns an unbiased initial model with the highest uncertainty over output labels by preventing over-performance in classification tasks. Additionally, we introduce a more general TAML approach that minimizes inequality of initial losses beyond classification tasks, where applicable. Our experiments on benchmarked datasets demonstrate the superiority of our proposed methods over other meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",1
"In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance it is crucial to guarantee the safety of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel \emph{Lyapunov} method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.",0
"In reinforcement learning (RL) problems in the real world, agents must avoid violating constraints while optimizing their objective function. Ensuring agent safety is critical during both training and deployment, as actions could damage hardware. To incorporate safety into RL, we utilize constrained Markov decision problems (CMDPs), which extend standard MDPs with constraints on expected cumulative costs. Our approach uses a novel Lyapunov method, which constructs Lyapunov functions to guarantee global safety of behavior policy through local, linear constraints. We demonstrate how to transform DP and RL algorithms into their safe counterparts using the Lyapunov approach. We evaluate these algorithms in various CMDP tasks on a safety benchmark domain and prove their effectiveness in balancing constraint satisfaction and performance.",1
"Good temporal representations are crucial for video understanding, and the state-of-the-art video recognition framework is based on two-stream networks. In such framework, besides the regular ConvNets responsible for RGB frame inputs, a second network is introduced to handle the temporal representation, usually the optical flow (OF). However, OF or other task-oriented flow is computationally costly, and is thus typically pre-computed. Critically, this prevents the two-stream approach from being applied to reinforcement learning (RL) applications such as video game playing, where the next state depends on current state and action choices. Inspired by the early vision systems of mammals and insects, we propose a fast event-driven representation (EDR) that models several major properties of early retinal circuits: (1) logarithmic input response, (2) multi-timescale temporal smoothing to filter noise, and (3) bipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the directional information for fast speed (> 9000 fps), EDR en-ables fast real-time inference/learning in video applications that require interaction between an agent and the world such as game-playing, virtual robotics, and domain adaptation. In this vein, we use EDR to demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games, something that has not been possible with pre-computed OF. Moreover, with UCF-101 video action recognition experiments, we show that EDR performs near state-of-the-art in accuracy while achieving a 1,500x speedup in input representation processing, as compared to optical flow.",0
"The effectiveness of video understanding relies heavily on accurate temporal representations. Presently, the leading approach for video recognition involves two-stream networks, in which a second network is implemented to handle temporal representation, often using optical flow. However, optical flow is computationally expensive and is typically pre-computed, which makes it unsuitable for reinforcement learning (RL) applications such as video game playing. To address this issue, we propose a fast event-driven representation (EDR) that mimics early retinal circuits found in mammals and insects. EDR provides fast real-time inference and learning for video applications that require interaction between an agent and the world. By using EDR, we have achieved performance improvements in reinforcement learning algorithms for Atari games, which was not possible with pre-computed optical flow. Additionally, our UCF-101 video action recognition experiments demonstrate that EDR performs close to state-of-the-art in accuracy while providing a 1,500x speedup in input representation processing compared to optical flow.",1
"Reinforcement learning (RL) algorithms have made huge progress in recent years by leveraging the power of deep neural networks (DNN). Despite the success, deep RL algorithms are known to be sample inefficient, often requiring many rounds of interaction with the environments to obtain satisfactory performance. Recently, episodic memory based RL has attracted attention due to its ability to latch on good actions quickly. In this paper, we present a simple yet effective biologically inspired RL algorithm called Episodic Memory Deep Q-Networks (EMDQN), which leverages episodic memory to supervise an agent during training. Experiments show that our proposed method can lead to better sample efficiency and is more likely to find good policies. It only requires 1/5 of the interactions of DQN to achieve many state-of-the-art performances on Atari games, significantly outperforming regular DQN and other episodic memory based RL algorithms.",0
"The utilization of deep neural networks (DNN) has greatly improved reinforcement learning (RL) algorithms in recent times. However, DNN-based RL algorithms are often inefficient in their use of samples, and require numerous rounds of interaction with the environment to achieve satisfactory results. Episodic memory based RL has recently become a popular approach due to its ability to quickly identify good actions. Our paper introduces a new RL algorithm, called Episodic Memory Deep Q-Networks (EMDQN), which is inspired by biological processes and uses episodic memory to supervise the agent during training. Our experiments demonstrate that EMDQN offers superior sample efficiency and better policy outcomes than regular DQN and other episodic memory based RL algorithms. It achieves state-of-the-art performance on Atari games, using only 1/5 of the interactions required by DQN.",1
"Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and $N$-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg",0
"The ability for robots to navigate complex environments independently is crucial for their practical use. Traditional methods involve the robot creating a map of the environment and then using a localization and planning system to move through it. However, these methods can be problematic due to their assumptions, high computational requirements, and inability to learn from errors. In contrast, learning-based methods improve with experience, but their high sample complexity makes them difficult to use in real-world situations. To overcome this issue, we propose a generalized computation graph that integrates value-based model-free and model-based methods, with various instantiations between them. We apply this graph to create a navigation model that can learn from visual input and requires minimal samples. Our experiments using a simulated car demonstrate the effectiveness of our approach, outperforming other methods. Furthermore, our real-world experiments using an RC car show that our model can navigate complex indoor environments with only a few hours of autonomous, self-supervised training. Videos and code for the experiments are available at github.com/gkahn13/gcg.",1
"To improve the quality of computation experience for mobile devices, mobile-edge computing (MEC) is a promising paradigm by providing computing capabilities in close proximity within a sliced radio access network (RAN), which supports both traditional communication and MEC services. Nevertheless, the design of computation offloading policies for a virtual MEC system remains challenging. Specifically, whether to execute a computation task at the mobile device or to offload it for MEC server execution should adapt to the time-varying network dynamics. In this paper, we consider MEC for a representative mobile user in an ultra-dense sliced RAN, where multiple base stations (BSs) are available to be selected for computation offloading. The problem of solving an optimal computation offloading policy is modelled as a Markov decision process, where our objective is to maximize the long-term utility performance whereby an offloading decision is made based on the task queue state, the energy queue state as well as the channel qualities between MU and BSs. To break the curse of high dimensionality in state space, we first propose a double deep Q-network (DQN) based strategic computation offloading algorithm to learn the optimal policy without knowing a priori knowledge of network dynamics. Then motivated by the additive structure of the utility function, a Q-function decomposition technique is combined with the double DQN, which leads to novel learning algorithm for the solving of stochastic computation offloading. Numerical experiments show that our proposed learning algorithms achieve a significant improvement in computation offloading performance compared with the baseline policies.",0
"Mobile-edge computing (MEC) offers a promising solution to enhance the quality of computation experience for mobile devices. This paradigm provides computing capabilities in close proximity through a sliced radio access network (RAN), supporting both traditional communication and MEC services. However, designing computation offloading policies for virtual MEC systems is challenging. The decision to execute a task on the mobile device or offload it for MEC server execution must adapt to the time-varying network dynamics. This paper presents a Markov decision process model for solving an optimal computation offloading policy for a representative mobile user in an ultra-dense sliced RAN, where multiple base stations (BSs) are available for selection. Our objective is to maximize long-term utility performance by making offloading decisions based on task queue state, energy queue state, and channel qualities between mobile users and BSs. To address the high dimensionality in state space, we propose a double deep Q-network (DQN) based strategic computation offloading algorithm to learn the optimal policy without prior knowledge of network dynamics. Additionally, we combine a Q-function decomposition technique with the double DQN to solve stochastic computation offloading. Numerical experiments demonstrate that our proposed learning algorithms significantly improve computation offloading performance compared to baseline policies.",1
"We formulate the problem of sampling and recovering clustered graph signal as a multi-armed bandit (MAB) problem. This formulation lends naturally to learning sampling strategies using the well-known gradient MAB algorithm. In particular, the sampling strategy is represented as a probability distribution over the individual arms of the MAB and optimized using gradient ascent. Some illustrative numerical experiments indicate that the sampling strategies based on the gradient MAB algorithm outperform existing sampling methods.",0
The clustered graph signal sampling and recovery problem can be framed as a multi-armed bandit (MAB) problem. This approach allows for the development of sampling strategies through the gradient MAB algorithm. The sampling strategy is modeled as a probability distribution over the MAB arms and optimized using gradient ascent. Experimental results demonstrate that the gradient MAB algorithm-based sampling strategies perform better than current methods.,1
"A common problem in Machine Learning and statistics consists in detecting whether the current sample in a stream of data belongs to the same distribution as previous ones, is an isolated outlier or inaugurates a new distribution of data. We present a hierarchical Bayesian algorithm that aims at learning a time-specific approximate posterior distribution of the parameters describing the distribution of the data observed. We derive the update equations of the variational parameters of the approximate posterior at each time step for models from the exponential family, and show that these updates find interesting correspondents in Reinforcement Learning (RL). In this perspective, our model can be seen as a hierarchical RL algorithm that learns a posterior distribution according to a certain stability confidence that is, in turn, learned according to its own stability confidence. Finally, we show some applications of our generic model, first in a RL context, next with an adaptive Bayesian Autoregressive model, and finally in the context of Stochastic Gradient Descent optimization.",0
"Detecting whether a sample in a stream of data belongs to the same distribution as previous samples, is an isolated outlier, or inaugurates a new distribution is a common issue in Machine Learning and statistics. To address this, we introduce a hierarchical Bayesian algorithm that learns an approximate posterior distribution of the observed data's distribution parameters at each specific time. We demonstrate how to update the variational parameters of the approximate posterior for models from the exponential family, which also have correspondences in Reinforcement Learning. Our model can be viewed as a hierarchical RL algorithm that learns a posterior distribution based on a stability confidence, which is learned according to its own stability confidence. We also provide some examples of our generic model's application, including in a RL context, with an adaptive Bayesian Autoregressive model, and in the context of Stochastic Gradient Descent optimization.",1
"Reinforcement learning (RL) agents performing complex tasks must be able to remember observations and actions across sizable time intervals. This is especially true during the initial learning stages, when exploratory behaviour can increase the delay between specific actions and their effects. Many new or popular approaches for learning these distant correlations employ backpropagation through time (BPTT), but this technique requires storing observation traces long enough to span the interval between cause and effect. Besides memory demands, learning dynamics like vanishing gradients and slow convergence due to infrequent weight updates can reduce BPTT's practicality; meanwhile, although online recurrent network learning is a developing topic, most approaches are not efficient enough to use as replacements. We propose a simple, effective memory strategy that can extend the window over which BPTT can learn without requiring longer traces. We explore this approach empirically on a few tasks and discuss its implications.",0
"When performing complex tasks, reinforcement learning (RL) agents need to retain information about observations and actions over significant periods of time. This is particularly important during the initial stages of learning, when experimental behavior can lead to delays between actions and their consequences. Although backpropagation through time (BPTT) is a popular method for learning distant correlations, it necessitates storing observation traces for a long enough period to cover the gap between cause and effect. Furthermore, BPTT's practicality is hampered by issues such as memory requirements, vanishing gradients, and slow convergence due to infrequent weight updates. While online recurrent network learning is an area of active research, most approaches are not efficient enough to replace BPTT. Our proposal is a straightforward, effective memory technique that can extend the learning window of BPTT without necessitating longer traces. We experimentally investigate this approach on several tasks and consider its implications.",1
"Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function. Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimalist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We introduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss function (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning.",0
"Acquiring locomotion skills is a difficult task that typically involves the use of motion capture, finite state machines, or morphology-specific knowledge to guide motion generation algorithms in order to create realistic and smooth movements. Deep reinforcement learning (DRL) is a promising approach for creating automatic locomotion control, which has been applied to the creation of a running controller for biped characters. However, the resulting motions often look unrealistic. This paper proposes a minimalist approach to the locomotion problem that does not rely on motion examples, finite state machines, or morphology-specific knowledge. Instead, it introduces two modifications to the DRL approach that produce symmetric, low-energy, and more realistic movements. The first modification adds a new term to the loss function to encourage symmetric actions, while the second introduces a new curriculum learning method that provides physical assistance to help the character with balance and movement. The algorithm gradually reduces the assistance until the character can move without help. Since the method does not require motion capture data, it can be applied to various character morphologies. The paper provides examples of locomotion controllers for bipeds, full humanoids, quadrupeds, and hexapods, showing that the learned policies can produce symmetric, low-energy gaits and appropriate gait patterns without guidance from motion examples or contact planning.",1
"In recent years, attention has been focused on the relationship between black-box optimiza- tion problem and reinforcement learning problem. In this research, we propose the Mirror Descent Search (MDS) algorithm which is applicable both for black box optimization prob- lems and reinforcement learning problems. Our method is based on the mirror descent method, which is a general optimization algorithm. The contribution of this research is roughly twofold. We propose two essential algorithms, called MDS and Accelerated Mirror Descent Search (AMDS), and two more approximate algorithms: Gaussian Mirror Descent Search (G-MDS) and Gaussian Accelerated Mirror Descent Search (G-AMDS). This re- search shows that the advanced methods developed in the context of the mirror descent research can be applied to reinforcement learning problem. We also clarify the relationship between an existing reinforcement learning algorithm and our method. With two evaluation experiments, we show our proposed algorithms converge faster than some state-of-the-art methods.",0
"The focus of recent years has been on the connection between black-box optimization and reinforcement learning problems. The Mirror Descent Search (MDS) algorithm has been proposed in this study, capable of solving both types of problems. The algorithm is based on the mirror descent method, a general optimization algorithm. The research contributes in two ways: proposing two essential algorithms, MDS and Accelerated Mirror Descent Search (AMDS), and two approximate algorithms, Gaussian Mirror Descent Search (G-MDS) and Gaussian Accelerated Mirror Descent Search (G-AMDS). Additionally, the research demonstrates that the advanced methods developed in mirror descent research can be applied to reinforcement learning problems. The study also clarifies the relationship between the proposed method and an existing reinforcement learning algorithm. Two evaluation experiments indicate that the proposed algorithms converge faster compared to some state-of-the-art methods.",1
"We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.   An interactive version of this paper is available at https://worldmodels.github.io/",0
"The focus of our investigation is the construction of generative neural network models for widely used reinforcement learning environments. Our model of the world can undergo unsupervised training to rapidly acquire a condensed spatial and temporal understanding of the environment. By utilizing characteristics derived from the world model as inputs for an agent, we can develop a streamlined and straightforward policy that is capable of accomplishing the necessary task. Moreover, we can even train our agent exclusively within its own simulated dream world created by the world model and transfer this policy back to the real environment. For an interactive version of our study, please visit https://worldmodels.github.io/.",1
"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach'. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",0
"The act of teaching is crucial in our society as it spreads knowledge and educates future generations. A competent teacher will carefully choose appropriate teaching materials, employ effective methodologies, and design targeted examinations based on the learning behaviors of their students. However, the role of teaching in the field of artificial intelligence has not been fully explored, with more emphasis placed on machine learning. This paper argues that equal attention should be given to teaching and proposes an optimization framework, called ""learning to teach,"" to develop effective teaching strategies. This approach involves two intelligent agents - a student model and a teacher model - that interact with each other. The teacher model uses reinforcement learning to optimize its teaching strategies based on feedback from the student model, resulting in teacher-student co-evolution. We demonstrate the practical application of this approach by using it to train deep neural networks. Our results show that using learning to teach techniques can achieve similar accuracy with less training data and fewer iterations for various types of DNN models, such as multi-layer perceptron, convolutional neural networks, and recurrent neural networks, for different machine learning tasks, including image classification and text understanding.",1
"Multi-shot pedestrian re-identification problem is at the core of surveillance video analysis. It matches two tracks of pedestrians from different cameras. In contrary to existing works that aggregate single frames features by time series model such as recurrent neural network, in this paper, we propose an interpretable reinforcement learning based approach to this problem. Particularly, we train an agent to verify a pair of images at each time. The agent could choose to output the result (same or different) or request another pair of images to verify (unsure). By this way, our model implicitly learns the difficulty of image pairs, and postpone the decision when the model does not accumulate enough evidence. Moreover, by adjusting the reward for unsure action, we can easily trade off between speed and accuracy. In three open benchmarks, our method are competitive with the state-of-the-art methods while only using 3% to 6% images. These promising results demonstrate that our method is favorable in both efficiency and performance.",0
"At the heart of surveillance video analysis lies the multi-shot pedestrian re-identification problem, which involves matching two pedestrian tracks from different cameras. In contrast to existing approaches that use time series models such as recurrent neural network to aggregate single frames features, this paper proposes an interpretable reinforcement learning-based solution. Specifically, the approach trains an agent to verify pairs of images at each time, with the option to output a result (same or different) or request additional image pairs (unsure). This enables the model to learn the difficulty of image pairs and delay decisions until sufficient evidence is accumulated. By adjusting the reward for unsure actions, the model can balance speed and accuracy. In three open benchmarks, the proposed method is competitive with state-of-the-art approaches while using only 3% to 6% of images, indicating its effectiveness and efficiency.",1
"Current reinforcement learning (RL) methods can successfully learn single tasks but often generalize poorly to modest perturbations in task domain or training procedure. In this work, we present a decoupled learning strategy for RL that creates a shared representation space where knowledge can be robustly transferred. We separate learning the task representation, the forward dynamics, the inverse dynamics and the reward function of the domain, and show that this decoupling improves performance within the task, transfers well to changes in dynamics and reward, and can be effectively used for online planning. Empirical results show good performance in both continuous and discrete RL domains.",0
"Although current reinforcement learning (RL) techniques can effectively learn individual tasks, they often struggle to adapt to slight variations in the task domain or training procedure. This study introduces a decoupled learning approach for RL that establishes a shared representation space capable of robustly transferring knowledge. By separating the learning of the task representation, forward and inverse dynamics, and reward function of the domain, we demonstrate that this decoupling enhances performance within the task, transfers well to changes in dynamics and reward, and can be used for online planning. Our empirical findings exhibit favorable outcomes in both continuous and discrete RL domains.",1
"How would you search for a unique, fashionable shoe that a friend wore and you want to buy, but you didn't take a picture? Existing approaches propose interactive image search as a promising venue. However, they either entrust the user with taking the initiative to provide informative feedback, or give all control to the system which determines informative questions to ask. Instead, we propose a mixed-initiative framework where both the user and system can be active participants, depending on whose initiative will be more beneficial for obtaining high-quality search results. We develop a reinforcement learning approach which dynamically decides which of three interaction opportunities to give to the user: drawing a sketch, providing free-form attribute feedback, or answering attribute-based questions. By allowing these three options, our system optimizes both the informativeness and exploration capabilities allowing faster image retrieval. We outperform three baselines on three datasets and extensive experimental settings.",0
"If you want to find a unique and fashionable shoe that a friend wore, but you didn't take a picture, how would you go about it? One approach is interactive image search, but existing methods either rely on the user to provide feedback or leave everything up to the system. We suggest a mixed-initiative framework where both the user and system can participate in the search, depending on who can contribute more to obtaining high-quality results. Our reinforcement learning approach dynamically determines which of three interaction opportunities to offer the user: sketching, providing attribute feedback, or answering attribute-based questions. By allowing these options, our system optimizes both informativeness and exploration for faster image retrieval. We outperform three baselines on three datasets and in various experimental settings.",1
"Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.",0
"One of the most competitive approaches to learning in fully observable environments, such as computer Go, is Deep Reinforcement Learning (RL). However, there has been little research in using deep RL to handle partially observable environments. To address this, we propose the Action-specific Deep Recurrent Q-Network (ADRQN) architecture to improve learning performance in partially observable domains. A fully connected layer encodes actions, which are then combined with a convolutional observation to create an action-observation pair. An LSTM layer integrates the time series of action-observation pairs and learns latent states, from which a fully connected layer computes Q-values, similar to conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in various partially observable domains, including flickering Atari games.",1
"For many applications with limited computation, communication, storage and energy resources, there is an imperative need of computer vision methods that could select an informative subset of the input video for efficient processing at or near real time. In the literature, there are two relevant groups of approaches: generating a trailer for a video or fast-forwarding while watching/processing the video. The first group is supported by video summarization techniques, which require processing of the entire video to select an important subset for showing to users. In the second group, current fast-forwarding methods depend on either manual control or automatic adaptation of playback speed, which often do not present an accurate representation and may still require processing of every frame. In this paper, we introduce FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration from video summarization and does fast-forwarding differently. It is an online framework that automatically fast-forwards a video and presents a representative subset of frames to users on the fly. It does not require processing the entire video, but just the portion that is selected by the fast-forward agent, which makes the process very computationally efficient. The online nature of our proposed method also enables the users to begin fast-forwarding at any point of the video. Experiments on two real-world datasets demonstrate that our method can provide better representation of the input video with much less processing requirement.",0
"Computer vision methods that can efficiently process input video in real time are in high demand for applications with limited computation, communication, storage, and energy resources. There are two relevant approaches in the literature: generating a trailer for the video or fast-forwarding while watching/processing the video. Video summarization techniques are used in the first group, which involves processing the entire video to select an important subset for users. Current fast-forwarding methods in the second group often do not provide an accurate representation and may still require processing every frame. To address these limitations, we present FastForwardNet (FFNet), an online framework that automatically fast-forwards a video and presents a representative subset of frames to users in real time. Our method is computationally efficient as it only requires processing the portion of the video selected by the fast-forward agent. Additionally, users can begin fast-forwarding at any point in the video. Our experiments on two real-world datasets demonstrate that FFNet provides a better representation of the input video with significantly less processing requirement.",1
"Deep reinforcement learning has shown its success in game playing. However, 2.5D fighting games would be a challenging task to handle due to ambiguity in visual appearances like height or depth of the characters. Moreover, actions in such games typically involve particular sequential action orders, which also makes the network design very difficult. Based on the network of Asynchronous Advantage Actor-Critic (A3C), we create an OpenAI-gym-like gaming environment with the game of Little Fighter 2 (LF2), and present a novel A3C+ network for learning RL agents. The introduced model includes a Recurrent Info network, which utilizes game-related info features with recurrent layers to observe combo skills for fighting. In the experiments, we consider LF2 in different settings, which successfully demonstrates the use of our proposed model for learning 2.5D fighting games.",0
"Although deep reinforcement learning has proven to be effective in game playing, handling 2.5D fighting games is particularly challenging due to the visual ambiguity surrounding character height and depth. Additionally, the sequential nature of actions in these games makes designing a network even more difficult. To overcome these obstacles, we utilized the Asynchronous Advantage Actor-Critic (A3C) network to create a game environment similar to OpenAI-gym, using Little Fighter 2 (LF2). Our novel A3C+ network includes a Recurrent Info network that uses recurrent layers to observe combo skills for fighting while utilizing game-related info features. Through experiments with LF2 in various settings, we successfully demonstrate the effectiveness of our proposed model for learning to play 2.5D fighting games.",1
"In reinforcement learning, temporal difference (TD) is the most direct algorithm to learn the value function of a policy. For large or infinite state spaces, exact representations of the value function are usually not available, and it must be approximated by a function in some parametric family.   However, with \emph{nonlinear} parametric approximations (such as neural networks), TD is not guaranteed to converge to a good approximation of the true value function within the family, and is known to diverge even in relatively simple cases. TD lacks an interpretation as a stochastic gradient descent of an error between the true and approximate value functions, which would provide such guarantees.   We prove that approximate TD is a gradient descent provided the current policy is \emph{reversible}. This holds even with nonlinear approximations.   A policy with transition probabilities $P(s,s')$ between states is reversible if there exists a function $\mu$ over states such that $\frac{P(s,s')}{P(s',s)}=\frac{\mu(s')}{\mu(s)}$. In particular, every move can be undone with some probability. This condition is restrictive; it is satisfied, for instance, for a navigation problem in any unoriented graph.   In this case, approximate TD is exactly a gradient descent of the \emph{Dirichlet norm}, the norm of the difference of \emph{gradients} between the true and approximate value functions. The Dirichlet norm also controls the bias of approximate policy gradient. These results hold even with no decay factor ($\gamma=1$) and do not rely on contractivity of the Bellman operator, thus proving stability of TD even with $\gamma=1$ for reversible policies.",0
"Temporal difference (TD) is a popular algorithm used in reinforcement learning to learn the value function of a policy. However, when dealing with large or infinite state spaces, it becomes necessary to approximate the value function using a parametric function. Nonlinear parametric approximations such as neural networks can cause TD to diverge, making it difficult to obtain a good approximation of the true value function within the parametric family. This is because TD does not have an interpretation as a stochastic gradient descent of an error between the true and approximate value functions, which would provide guarantees. In this study, we demonstrate that approximate TD can be a gradient descent if the current policy is reversible, even with nonlinear approximations. A reversible policy is one in which every move can be undone with some probability. This condition is restrictive but is satisfied in some cases such as a navigation problem in any unoriented graph. In these instances, approximate TD is a gradient descent of the Dirichlet norm, which controls the bias of approximate policy gradient. These findings are significant as they prove the stability of TD even when the decay factor is equal to one and do not rely on the contractivity of the Bellman operator.",1
"Multimodal wearable sensor data classification plays an important role in ubiquitous computing and has a wide range of applications in scenarios from healthcare to entertainment. However, most existing work in this field employs domain-specific approaches and is thus ineffective in complex sit- uations where multi-modality sensor data are col- lected. Moreover, the wearable sensor data are less informative than the conventional data such as texts or images. In this paper, to improve the adapt- ability of such classification methods across differ- ent application domains, we turn this classification task into a game and apply a deep reinforcement learning scheme to deal with complex situations dynamically. Additionally, we introduce a selective attention mechanism into the reinforcement learn- ing scheme to focus on the crucial dimensions of the data. This mechanism helps to capture extra information from the signal and thus it is able to significantly improve the discriminative power of the classifier. We carry out several experiments on three wearable sensor datasets and demonstrate the competitive performance of the proposed approach compared to several state-of-the-art baselines.",0
"The classification of multimodal wearable sensor data is crucial in ubiquitous computing and has diverse applications ranging from healthcare to entertainment. However, the majority of existing research in this area employs domain-specific techniques and is ineffective in complex situations where multiple sensor data are collected. Additionally, wearable sensor data is less informative compared to conventional data like text or images. To enhance the adaptability of classification methods across various application domains, our paper presents a game-based approach using deep reinforcement learning to dynamically handle complex situations. Moreover, we introduce a selective attention mechanism into the reinforcement learning scheme to concentrate on critical data dimensions. This mechanism captures additional information from the signal and significantly enhances the classifier's discriminative power. We conducted several experiments on three wearable sensor datasets and demonstrated the competitive performance of our approach compared to several state-of-the-art baselines.",1
"Recently experience replay is widely used in various deep reinforcement learning (RL) algorithms, in this paper we rethink the utility of experience replay. It introduces a new hyper-parameter, the memory buffer size, which needs carefully tuning. However unfortunately the importance of this new hyper-parameter has been underestimated in the community for a long time. In this paper we did a systematic empirical study of experience replay under various function representations. We showcase that a large replay buffer can significantly hurt the performance. Moreover, we propose a simple O(1) method to remedy the negative influence of a large replay buffer. We showcase its utility in both simple grid world and challenging domains like Atari games.",0
"The use of experience replay has become widespread in deep reinforcement learning (RL) algorithms. However, this paper aims to reevaluate the significance of experience replay and its new hyper-parameter, the memory buffer size, which requires careful tuning. Unfortunately, the community has overlooked the importance of this hyper-parameter for a long time. This paper presents a systematic empirical study of experience replay using various function representations. Our findings show that a large replay buffer can significantly harm performance. To address this issue, we introduce a simple O(1) method to counteract the negative effects of a large replay buffer. We demonstrate the effectiveness of our proposed method in both simple grid world and challenging Atari games.",1
"We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.",0
"Our proposal involves a metalearning strategy to acquire gradient-based reinforcement learning (RL) techniques. The approach centers on the creation of a differentiable loss function that, when optimized by an agent, leads to high rewards. This loss function is constructed using temporal convolutions applied to the agent's experience, resulting in a highly adaptable approach that facilitates quick task learning. Our experimental findings indicate that our evolved policy gradient algorithm (EPG) outperforms an off-the-shelf policy gradient method in terms of faster learning in various randomized environments. Furthermore, we demonstrate that EPG's learned loss can generalize to new tasks outside of its training data, displaying different behavior from other well-known metalearning algorithms.",1
"Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead partitions the initial state space into ""slices"", and optimizes an ensemble of policies, each on a different slice. The ensemble is gradually unified into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed at http://bit.ly/dnc-rl",0
"The standard model-free deep reinforcement learning algorithms optimize policies that can perform well in unpredictable environments by sampling a new initial state for each trial. However, problems with significant initial state variation produce high-variance gradient estimates for model-free RL, which makes direct policy or value function optimization a challenge. In this paper, we propose a new algorithm called divide-and-conquer RL that partitions the initial state space into slices and optimizes an ensemble of policies, each on a different slice. This ensemble is gradually merged into a single policy that can perform well on the entire state space. Our approach is effective in solving complex tasks where traditional deep RL methods are not successful. We have observed that our divide-and-conquer RL significantly outperforms conventional policy gradient methods in grasping, manipulation, and locomotion tasks. Additionally, our algorithm's learned policies can be viewed in videos available at http://bit.ly/dnc-rl.",1
"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism should be able to encourage an agent to take actions that lead to less frequent states which may yield higher accumulative future return. However, both knowing about the future and evaluating the frequentness of states are non-trivial tasks, especially for deep RL domains, where a state is represented by high-dimensional image frames. In this paper, we propose a novel informed exploration framework for deep RL, where we build the capability for an RL agent to predict over the future transitions and evaluate the frequentness for the predicted future frames in a meaningful manner. To this end, we train a deep prediction model to predict future frames given a state-action pair, and a convolutional autoencoder model to hash over the seen frames. In addition, to utilize the counts derived from the seen frames to evaluate the frequentness for the predicted frames, we tackle the challenge of matching the predicted future frames and their corresponding seen frames at the latent feature level. In this way, we derive a reliable metric for evaluating the novelty of the future direction pointed by each action, and hence inform the agent to explore the least frequent one.",0
"Efficient exploration mechanisms in deep reinforcement learning (RL) tasks require agents to take actions that result in less frequent states, which can lead to higher accumulative future returns. However, this is challenging, particularly in deep RL domains where states are represented by high-dimensional image frames and predicting the future and evaluating state frequentness is difficult. In this study, we propose an informed exploration framework for deep RL, where RL agents can predict future transitions and evaluate frequentness for predicted future frames. Our approach trains a deep prediction model to predict future frames and a convolutional autoencoder model to hash seen frames. To evaluate frequentness for predicted frames, we match predicted and seen frames at the latent feature level, providing a reliable metric for determining the novelty of future directions and encouraging agents to explore the least frequent one.",1
"Approximate linear programming (ALP) represents one of the major algorithmic families to solve large-scale Markov decision processes (MDP). In this work, we study a primal-dual formulation of the ALP, and develop a scalable, model-free algorithm called bilinear $\pi$ learning for reinforcement learning when a sampling oracle is provided. This algorithm enjoys a number of advantages. First, it adopts (bi)linear models to represent the high-dimensional value function and state-action distributions, using given state and action features. Its run-time complexity depends on the number of features, not the size of the underlying MDPs. Second, it operates in a fully online fashion without having to store any sample, thus having minimal memory footprint. Third, we prove that it is sample-efficient, solving for the optimal policy to high precision with a sample complexity linear in the dimension of the parameter space.",0
"The Approximate Linear Programming (ALP) algorithm is a significant approach to solving large-scale Markov decision processes (MDP). This study examines a primal-dual formulation of ALP and introduces a model-free algorithm called bilinear $\pi$ learning for reinforcement learning with a sampling oracle. This algorithm has several benefits. It uses (bi)linear models to represent the high-dimensional value function and state-action distributions, which rely on the given state and action features. Its runtime complexity is dependent on the number of features rather than the MDP's size. Additionally, it operates entirely online and requires no sample storage, resulting in a minimal memory footprint. Finally, the algorithm is sample-efficient and can achieve optimal policy with high precision, with a sample complexity that is linear in the dimension of the parameter space.",1
Policy gradient methods are widely used in reinforcement learning algorithms to search for better policies in the parameterized policy space. They do gradient search in the policy space and are known to converge very slowly. Nesterov developed an accelerated gradient search algorithm for convex optimization problems. This has been recently extended for non-convex and also stochastic optimization. We use Nesterov's acceleration for policy gradient search in the well-known actor-critic algorithm and show the convergence using ODE method. We tested this algorithm on a scheduling problem. Here an incoming job is scheduled into one of the four queues based on the queue lengths. We see from experimental results that algorithm using Nesterov's acceleration has significantly better performance compared to algorithm which do not use acceleration. To the best of our knowledge this is the first time Nesterov's acceleration has been used with actor-critic algorithm.,0
"Reinforcement learning algorithms often utilize policy gradient methods to explore parameterized policy spaces in search of better policies. These methods involve gradient searches in the policy space, but they are notorious for their slow convergence rates. Nesterov's accelerated gradient search algorithm was initially developed for convex optimization problems, but it has since been extended to non-convex and stochastic optimization. We have implemented Nesterov's acceleration in the actor-critic algorithm for policy gradient search and demonstrated its convergence using the ODE method. Our algorithm was tested on a scheduling problem where jobs are assigned to one of four queues based on queue lengths. Our experimental results show that the algorithm using Nesterov's acceleration outperforms those without acceleration. To our knowledge, this is the first instance of Nesterov's acceleration being applied to the actor-critic algorithm.",1
"In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.",0
"Our report introduces a novel benchmark for reinforcement learning (RL) that utilizes the Sonic the Hedgehog (TM) video game series. The benchmark's primary goal is to assess the capabilities of transfer learning and few-shot learning algorithms in the RL domain. Additionally, we assess several baseline algorithms' performance on this new benchmark.",1
"Projective simulation (PS) is a model for intelligent agents with a deliberation capacity that is based on episodic memory. The model has been shown to provide a flexible framework for constructing reinforcement-learning agents, and it allows for quantum mechanical generalization, which leads to a speed-up in deliberation time. PS agents have been applied successfully in the context of complex skill learning in robotics, and in the design of state-of-the-art quantum experiments. In this paper, we study the performance of projective simulation in two benchmarking problems in navigation, namely the grid world and the mountain car problem. The performance of PS is compared to standard tabular reinforcement learning approaches, Q-learning and SARSA. Our comparison demonstrates that the performance of PS and standard learning approaches are qualitatively and quantitatively similar, while it is much easier to choose optimal model parameters in case of projective simulation, with a reduced computational effort of one to two orders of magnitude. Our results show that the projective simulation model stands out for its simplicity in terms of the number of model parameters, which makes it simple to set up the learning agent in unknown task environments.",0
"The model called Projective Simulation (PS) uses episodic memory to create intelligent agents with a deliberation capacity. This flexible framework is effective in constructing reinforcement-learning agents and allows for quantum mechanical generalization, leading to faster deliberation. PS has been successfully applied in complex skill learning for robotics and state-of-the-art quantum experiments. This study analyzes the performance of PS in two benchmarking problems in navigation - the grid world and the mountain car problem. PS is compared to standard tabular reinforcement learning approaches, Q-learning and SARSA, and the results show that PS and standard learning approaches perform similarly. However, choosing optimal model parameters is easier in PS, with a reduced computational effort of one to two orders of magnitude. The simplicity of the PS model in terms of the number of parameters makes it easy to set up the learning agent in unknown task environments.",1
"Deep Reinforcement Learning (deep RL) has made several breakthroughs in recent years in applications ranging from complex control tasks in unmanned vehicles to game playing. Despite their success, deep RL still lacks several important capacities of human intelligence, such as transfer learning, abstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL) seeks to incorporate such capacities to deep Q-networks (DQN) by learning a relevant symbolic representation prior to using Q-learning. In this paper, we propose a novel extension of DSRL, which we call Symbolic Reinforcement Learning with Common Sense (SRL+CS), offering a better balance between generalization and specialization, inspired by principles of common sense when assigning rewards and aggregating Q-values. Experiments reported in this paper show that SRL+CS learns consistently faster than Q-learning and DSRL, achieving also a higher accuracy. In the hardest case, where agents were trained in a deterministic environment and tested in a random environment, SRL+CS achieves nearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. To the best of our knowledge, this is the first case of near perfect zero-shot transfer learning using Reinforcement Learning.",0
"In recent years, Deep Reinforcement Learning (deep RL) has achieved significant breakthroughs in various fields, from unmanned vehicle control to game playing. Despite its success, deep RL still lacks some crucial human intelligence capabilities such as transfer learning, abstraction, and interpretability. Deep Symbolic Reinforcement Learning (DSRL) aims to address these shortcomings by learning a relevant symbolic representation before using Q-learning in deep Q-networks (DQN). This paper proposes an innovative extension of DSRL, called Symbolic Reinforcement Learning with Common Sense (SRL+CS), that balances generalization and specialization better and incorporates common sense principles when assigning rewards and aggregating Q-values. Experimental results indicate that SRL+CS outperforms Q-learning and DSRL by consistently learning faster and achieving higher accuracy. In the most challenging scenario, where agents were trained in a deterministic environment and tested in a random environment, SRL+CS achieved nearly 100% average accuracy, compared to DSRL's 70% and DQN's 50%. This is the first instance of near-perfect zero-shot transfer learning using Reinforcement Learning, to the best of our knowledge.",1
"A critical and challenging problem in reinforcement learning is how to learn the state-action value function from the experience replay buffer and simultaneously keep sample efficiency and faster convergence to a high quality solution. In prior works, transitions are uniformly sampled at random from the replay buffer or sampled based on their priority measured by temporal-difference (TD) error. However, these approaches do not fully take into consideration the intrinsic characteristics of transition distribution in the state space and could result in redundant and unnecessary TD updates, slowing down the convergence of the learning procedure. To overcome this problem, we propose a novel state distribution-aware sampling method to balance the replay times for transitions with skew distribution, which takes into account both the occurrence frequencies of transitions and the uncertainty of state-action values. Consequently, our approach could reduce the unnecessary TD updates and increase the TD updates for state-action value with more uncertainty, making the experience replay more effective and efficient. Extensive experiments are conducted on both classic control tasks and Atari 2600 games based on OpenAI gym platform and the experimental results demonstrate the effectiveness of our approach in comparison with the standard DQN approach.",0
"Reinforcement learning faces a complex and critical issue regarding how to learn the state-action value function while maintaining both sample efficiency and faster convergence to a high-quality solution from the experience replay buffer. Previous research has used random uniform sampling or prioritized sampling based on temporal-difference error to select transitions from the replay buffer, but this can result in unnecessary TD updates and slow down the learning process. To tackle this problem, we propose a novel state distribution-aware sampling method that considers the occurrence frequencies of transitions and the uncertainty of state-action values to balance replay times for transitions with skewed distribution. This approach reduces redundant TD updates and increases updates for state-action values with more uncertainty, making the experience replay more efficient. We test our approach on classic control tasks and Atari 2600 games using the OpenAI gym platform, and the results demonstrate its effectiveness in comparison to the standard DQN approach.",1
"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of $N$-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.",0
"The approach taken in this study involves utilizing the distributional perspective on reinforcement learning, which has been highly successful, and modifying it to fit the continuous control setting. This is accomplished by integrating it into a distributed framework for off-policy learning, creating the Distributed Distributional Deep Deterministic Policy Gradient algorithm (D4PG). Additional enhancements such as the implementation of $N$-step returns and prioritized experience replay were also incorporated. Through experimentation, the individual contributions of each technique were examined, as well as their combined impact. The results demonstrated that the D4PG algorithm outperformed other methods in a diverse range of control tasks, manipulation tasks, and obstacle-based locomotion tasks, establishing it as the leading approach.",1
"We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning. Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population.",0
"Our focus is on how to represent the behavior of large populations and predict the evolution of a population distribution over a discrete state space. To achieve this, we propose a discrete time mean field game (MFG) model that is based on game theory. This model enables us to understand how individual actions affect the population as a whole and predict how the distribution of the population will evolve over time. We combine MFG with Markov decision processes (MDP) to create a special MFG that is reducible to an MDP. This allows us to expand the reach of mean field game theory and use deep inverse reinforcement learning to infer MFG models of real-world systems. Our approach involves learning both the reward function and forward dynamics of an MFG from real data. We also present the first empirical test of a mean field game model of a real-world social media population.",1
"One of the most significant bottleneck in training large scale machine learning models on parameter server (PS) is the communication overhead, because it needs to frequently exchange the model gradients between the workers and servers during the training iterations. Gradient quantization has been proposed as an effective approach to reducing the communication volume. One key issue in gradient quantization is setting the number of bits for quantizing the gradients. Small number of bits can significantly reduce the communication overhead while hurts the gradient accuracies, and vise versa. An ideal quantization method would dynamically balance the communication overhead and model accuracy, through adjusting the number bits according to the knowledge learned from the immediate past training iterations. Existing methods, however, quantize the gradients either with fixed number of bits, or with predefined heuristic rules. In this paper we propose a novel adaptive quantization method within the framework of reinforcement learning. The method, referred to as MQGrad, formalizes the selection of quantization bits as actions in a Markov decision process (MDP) where the MDP states records the information collected from the past optimization iterations (e.g., the sequence of the loss function values). During the training iterations of a machine learning algorithm, MQGrad continuously updates the MDP state according to the changes of the loss function. Based on the information, MDP learns to select the optimal actions (number of bits) to quantize the gradients. Experimental results based on a benchmark dataset showed that MQGrad can accelerate the learning of a large scale deep neural network while keeping its prediction accuracies.",0
"The main challenge to training large scale machine learning models on parameter server (PS) is the communication overhead, which involves frequent exchange of model gradients between workers and servers during training iterations. To reduce communication volume, gradient quantization has been proposed as an effective approach. However, the number of bits used for quantizing the gradients is critical, as a small number of bits can significantly reduce communication overhead while compromising gradient accuracies, and vice versa. An ideal quantization method should dynamically balance communication overhead and model accuracy by adjusting the number of bits based on knowledge gained from past training iterations. Existing methods use fixed numbers of bits or predefined heuristic rules for gradient quantization. This paper proposes MQGrad, an adaptive quantization method based on reinforcement learning. MQGrad selects quantization bits as actions in a Markov decision process (MDP) using information collected from past optimization iterations (e.g., loss function values). During machine learning algorithm training iterations, MQGrad continuously updates the MDP state according to loss function changes, learning to select optimal actions (number of bits) for gradient quantization. Benchmark dataset experiments showed that MQGrad accelerates the learning of large scale deep neural networks while maintaining prediction accuracies.",1
"Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ""robustly"": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.",0
"Deep Reinforcement Learning has made significant advancements in recent years thanks to large scale neural networks, innovative training algorithms, and parallel computing devices. However, the increased training power comes with the potential risk of overfitting, which is a concern in critical fields like healthcare and finance. To address this, we conducted a systematic study of standard RL agents and found that they can overfit in various ways, and this overfitting can happen even when commonly used techniques that add stochasticity are utilized. We observed that the same agents and learning algorithms can have drastically different test performance, even when they achieve optimal rewards during training. Therefore, more careful evaluation protocols are necessary in RL. We conclude with a discussion on overfitting in RL and a study of generalization behaviors from the perspective of inductive bias.",1
"Image segmentation needs both local boundary position information and global object context information. The performance of the recent state-of-the-art method, fully convolutional networks, reaches a bottleneck due to the neural network limit after balancing between the two types of information simultaneously in an end-to-end training style. To overcome this problem, we divide the semantic image segmentation into temporal subtasks. First, we find a possible pixel position of some object boundary; then trace the boundary at steps within a limited length until the whole object is outlined. We present the first deep reinforcement learning approach to semantic image segmentation, called DeepOutline, which outperforms other algorithms in Coco detection leaderboard in the middle and large size person category in Coco val2017 dataset. Meanwhile, it provides an insight into a divide and conquer way by reinforcement learning on computer vision problems.",0
"Both local boundary position information and global object context information are required for image segmentation. However, the fully convolutional networks, which are the most advanced method available, have reached a bottleneck. This is due to the neural network limit when balancing both types of information simultaneously in an end-to-end training approach. To solve this issue, we have divided the semantic image segmentation into temporal subtasks. First, we locate a possible pixel position of an object boundary and then trace the boundary at steps within a limited length until the entire object is outlined. Our solution, DeepOutline, is the first deep reinforcement learning approach to semantic image segmentation. It outperforms other algorithms in the Coco detection leaderboard, particularly in the middle and large size person category in Coco val2017 dataset. Furthermore, our approach provides insight into a divide and conquer strategy using reinforcement learning for computer vision problems.",1
"In real world systems, the predictions of deployed Machine Learned models affect the training data available to build subsequent models. This introduces a bias in the training data that needs to be addressed. Existing solutions to this problem attempt to resolve the problem by either casting this in the reinforcement learning framework or by quantifying the bias and re-weighting the loss functions. In this work, we develop a novel Adversarial Neural Network (ANN) model, an alternative approach which creates a representation of the data that is invariant to the bias. We take the Paid Search auction as our working example and ad display position features as the confounding features for this setting. We show the success of this approach empirically on both synthetic data as well as real world paid search auction data from a major search engine.",0
"The deployment of Machine Learned models in real world systems can affect subsequent model training data, creating a bias that requires attention. To address this issue, existing solutions employ reinforcement learning frameworks or re-weighting loss functions to quantify the bias. However, we propose a new approach using an Adversarial Neural Network (ANN) model that generates a bias-invariant data representation. Our study focuses on the Paid Search auction and ad display position features as confounding factors. Empirical evidence from synthetic and real world paid search auction data from a major search engine supports the effectiveness of our method.",1
"Very recently proximal policy optimization (PPO) algorithms have been proposed as first-order optimization methods for effective reinforcement learning. While PPO is inspired by the same learning theory that justifies trust region policy optimization (TRPO), PPO substantially simplifies algorithm design and improves data efficiency by performing multiple epochs of \emph{clipped policy optimization} from sampled data. Although clipping in PPO stands for an important new mechanism for efficient and reliable policy update, it may fail to adaptively improve learning performance in accordance with the importance of each sampled state. To address this issue, a new surrogate learning objective featuring an adaptive clipping mechanism is proposed in this paper, enabling us to develop a new algorithm, known as PPO-$\lambda$. PPO-$\lambda$ optimizes policies repeatedly based on a theoretical target for adaptive policy improvement. Meanwhile, destructively large policy update can be effectively prevented through both clipping and adaptive control of a hyperparameter $\lambda$ in PPO-$\lambda$, ensuring high learning reliability. PPO-$\lambda$ enjoys the same simple and efficient design as PPO. Empirically on several Atari game playing tasks and benchmark control tasks, PPO-$\lambda$ also achieved clearly better performance than PPO.",0
"Recently, first-order optimization methods for effective reinforcement learning have been proposed in the form of proximal policy optimization (PPO) algorithms. Inspired by the same learning theory that justifies trust region policy optimization (TRPO), PPO simplifies algorithm design and improves data efficiency by performing multiple epochs of clipped policy optimization from sampled data. While PPO's clipping mechanism is an important new feature for efficient and reliable policy update, it may not adaptively improve learning performance in accordance with the importance of each sampled state. To address this issue, a new algorithm known as PPO-$\lambda$ is proposed in this paper, which optimizes policies repeatedly based on a theoretical target for adaptive policy improvement. PPO-$\lambda$ prevents destructively large policy updates through both clipping and adaptive control of a hyperparameter $\lambda$, ensuring high learning reliability. PPO-$\lambda$ shares the same simple and efficient design as PPO, and empirically achieved better performance than PPO on several Atari game playing tasks and benchmark control tasks.",1
"Determinantal point processes (DPPs) are an important concept in random matrix theory and combinatorics. They have also recently attracted interest in the study of numerical methods for machine learning, as they offer an elegant ""missing link"" between independent Monte Carlo sampling and deterministic evaluation on regular grids, applicable to a general set of spaces. This is helpful whenever an algorithm explores to reduce uncertainty, such as in active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. To draw samples from a DPP in practice, existing literature focuses on approximate schemes of low cost, or comparably inefficient exact algorithms like rejection sampling. We point out that, for many settings of relevance to machine learning, it is also possible to draw exact samples from DPPs on continuous domains. We start from an intuitive example on the real line, which is then generalized to multivariate real vector spaces. We also compare to previously studied approximations, showing that exact sampling, despite higher cost, can be preferable where precision is needed.",0
"Determinantal point processes (DPPs) are a significant concept in the fields of random matrix theory and combinatorics, as well as in the study of numerical methods for machine learning. They provide a valuable bridge between independent Monte Carlo sampling and deterministic evaluation on regular grids, making them useful in a variety of spaces. DPPs are particularly beneficial in algorithms that aim to reduce uncertainty, such as active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. While existing literature on DPPs focuses on low-cost approximate schemes or inefficient exact algorithms like rejection sampling, we argue that it is possible to draw exact samples from DPPs on continuous domains in many machine learning settings. We provide an intuitive example on the real line, which is then extended to multivariate real vector spaces. We also demonstrate that, despite being more costly, exact sampling can be preferable to previously studied approximations when precision is necessary.",1
"Motivated by recent advance of machine learning using Deep Reinforcement Learning this paper proposes a modified architecture that produces more robust agents and speeds up the training process. Our architecture is based on Asynchronous Advantage Actor-Critic (A3C) algorithm where the total input dimensionality is halved by dividing the input into two independent streams. We use ViZDoom, 3D world software that is based on the classical first person shooter video game, Doom, as a test case. The experiments show that in comparison to single input agents, the proposed architecture succeeds to have the same playing performance and shows more robust behavior, achieving significant reduction in the number of training parameters of almost 30%.",0
"This paper introduces a modified architecture that enhances the robustness of agents and accelerates the training process, inspired by recent progress in Deep Reinforcement Learning. Our proposed architecture is based on the Asynchronous Advantage Actor-Critic (A3C) algorithm and divides the input into two independent streams, halving the total input dimensionality. To test our architecture, we utilize ViZDoom, a 3D world software based on the popular first-person shooter game, Doom. Our experiments demonstrate that the proposed architecture, with its reduced number of training parameters by almost 30%, achieves comparable playing performance to single input agents while exhibiting more resilient behavior.",1
"Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a 'distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the 'distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at https://sites.google.com/view/distort-and-recover/",0
"Typically, learning-based color enhancement methods learn to transform input images into retouched images. However, most of these approaches either necessitate costly input-retouched image pairs or yield non-interpretable outcomes. This paper introduces a novel approach to color enhancement based on deep reinforcement learning (DRL), which models the incremental nature of human retouching. The color enhancement process is represented as a Markov Decision Process, where global color adjustment operations are classified as actions. Through training, our agent learns the optimal sequence of actions for global enhancement. Furthermore, we propose a 'distort-and-recover' training scheme that only requires high-quality reference images instead of input and retouched image pairs. By distorting the color distribution of reference images, we form distorted-reference image pairs for training. We demonstrate through extensive experiments that our approach yields satisfactory enhancement results and is more suitable for the 'distort-and-recover' training scheme than previous supervised methods. Supplementary material and code can be accessed at https://sites.google.com/view/distort-and-recover/.",1
"This paper presents an open-source enforcement learning toolkit named CytonRL (https://github.com/arthurxlw/cytonRL). The toolkit implements four recent advanced deep Q-learning algorithms from scratch using C++ and NVIDIA's GPU-accelerated libraries. The code is simple and elegant, owing to an open-source general-purpose neural network library named CytonLib. Benchmark shows that the toolkit achieves competitive performances on the popular Atari game of Breakout.",0
"A toolkit called CytonRL (https://github.com/arthurxlw/cytonRL), designed for enforcement learning and open-source, is presented in this paper. The toolkit has been developed using C++ and NVIDIA's GPU-accelerated libraries and includes four advanced deep Q-learning algorithms. The implementation is elegant and straightforward, thanks to the open-source neural network library CytonLib. The benchmark results indicate that the toolkit performs competitively on the popular Atari game Breakout.",1
"The recent popularity of deep neural networks (DNNs) has generated a lot of research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference -- i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation.   Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark for DNN training, called TBD (TBD is short for Training Benchmark for DNNs), that uses a representative set of DNN models that cover a wide range of machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) by performing an extensive performance analysis of training these different applications on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). TBD currently covers six major application domains and eight different state-of-the-art models.   We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of new and existing metrics and methodologies to analyze the results, and utilization of domain specific characteristics of DNN training. We also build a new set of tools for memory profiling in all three major frameworks; much needed tools that can finally shed some light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. By using our tools and methodologies, we make several important observations and recommendations on where the future research and optimization of DNN training should be focused.",0
"The recent popularity of deep neural networks (DNNs) has sparked research interest in efficient computation for DNN-related tasks. However, the research focus has been narrow, mainly on inference and image classification networks. In this work, we aim to broaden the perspective by proposing a new benchmark for DNN training, called TBD, which includes a representative set of DNN models covering various machine learning applications. We conduct an extensive performance analysis of training these models on three major deep learning frameworks across different hardware configurations. TBD currently covers six major application domains and eight state-of-the-art models. We introduce a new toolchain for performance analysis that utilizes existing and new metrics and methodologies to analyze results, as well as domain-specific characteristics of DNN training. We also present a new set of memory profiling tools for all three major frameworks. Our observations and recommendations highlight areas for future research and optimization of DNN training.",1
"We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates across the action when estimating the gradient, instead of relying only on the action in the sampled trajectory. We establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. We also prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and, for the Gaussian case, with no computational overhead. Finally, we show that it is optimal in a certain sense to explore with a Gaussian policy such that the covariance is proportional to the exponential of the scaled Hessian of the critic with respect to the actions. We present empirical results confirming that this new form of exploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic in four challenging MuJoCo domains.",0
"Our proposed method, Expected Policy Gradients (EPG), combines Stochastic Policy Gradients (SPG) and Deterministic Policy Gradients (DPG) for reinforcement learning. EPG is inspired by Expected SARSA and estimates the gradient by integrating across actions, rather than only relying on actions in the sampled trajectory. We introduce a new policy gradient theorem that encompasses both stochastic and deterministic policy gradient theorems as special cases. Furthermore, we demonstrate that EPG reduces the variance of gradient estimates, without necessitating deterministic policies and with no additional computational cost in the Gaussian case. We also prove that exploring with a Gaussian policy proportional to the exponential of the scaled Hessian of the critic with respect to actions is optimal in a certain sense. Finally, we present empirical evidence that our proposed exploration method outperforms DPG with the Ornstein-Uhlenbeck heuristic in four challenging MuJoCo domains.",1
In this work we introduce the application of black-box quantum control as an interesting rein- forcement learning problem to the machine learning community. We analyze the structure of the reinforcement learning problems arising in quantum physics and argue that agents parameterized by long short-term memory (LSTM) networks trained via stochastic policy gradients yield a general method to solving them. In this context we introduce a variant of the proximal policy optimization (PPO) algorithm called the memory proximal policy optimization (MPPO) which is based on this analysis. We then show how it can be applied to specific learning tasks and present results of nu- merical experiments showing that our method achieves state-of-the-art results for several learning tasks in quantum control with discrete and continouous control parameters.,0
"This work presents the use of black-box quantum control as a reinforcement learning problem for the machine learning community. The reinforcement learning problems in quantum physics are examined, and it is suggested that agents trained with stochastic policy gradients and parameterized by long short-term memory (LSTM) networks provide a general solution. To this end, a variant of the proximal policy optimization (PPO) algorithm, called memory proximal policy optimization (MPPO), is introduced. The MPPO algorithm is applied to specific learning tasks, and the results of numerical experiments demonstrate that our method achieves state-of-the-art results for several learning tasks in quantum control with both discrete and continuous control parameters.",1
"Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step.",0
"The study of human free-hand sketches has been conducted in different areas such as sketch recognition, synthesis, and fine-grained sketch-based image retrieval. A major obstacle in sketch analysis is the wide range of drawing styles used by humans, particularly in terms of abstraction. This research introduces a stroke-level sketch abstraction model that considers sketch abstraction as a trade-off between sketch recognizability and the number of strokes used. Specifically, a model is trained to generate abstract sketches using reinforcement learning of a stroke removal policy that predicts which strokes can be safely removed without affecting recognizability. This abstraction model has various applications, including modeling stroke saliency, synthesizing sketches of different abstraction levels for a specific category, and training a FG-SBIR model without the need for photo-sketch pairs.",1
"Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-values improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to efficiently learn continuous MDPs. We demonstrate this by showing that our approach surpasses state of the art performance in the Freeway Atari 2600 game.",0
"Reinforcement Learning relies heavily on exploration, which is often accomplished through stochastic action-selection. However, exploration can be made more effective by focusing on acquiring new knowledge about the environment. Visit-counters are a proven method for achieving directed exploration, but they are limited in their effectiveness due to their locality. Although there are some model-based solutions to this issue, a model-free approach has not been developed yet. Our proposed solution is $E$-values, which are a more generalized version of counters. These values can be used to evaluate the exploratory value as it propagates through state-action trajectories. We have compared our approach to traditional RL techniques and have shown that using $E$-values leads to improved learning and performance compared to using traditional counters. We have also demonstrated how our method can be applied to function approximation to efficiently learn continuous MDPs. Our results show that our approach outperforms the state-of-the-art in the Freeway Atari 2600 game.",1
"We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.",0
"Our study introduces a unique method of image restoration through reinforcement learning. In contrast to previous research that mainly focuses on training a single large network for a specific task, we create a toolbox containing various small-scale convolutional networks with different complexities and specializations. Using our approach, RL-Restore, we teach the system to select suitable tools from the toolbox to progressively improve the quality of a damaged image. To accomplish this, we develop a step-wise reward function that gauges the image's restoration progress and use it to train the action policy. Furthermore, we implement a joint learning mechanism that enables the agent and tools to manage uncertainty more effectively. Unlike traditional human-designed networks, RL-Restore utilizes a dynamically constructed toolchain to repair images that have complicated and unknown distortions, resulting in more efficient parameters.",1
"Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Toubin et. al.) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.",0
"Teaching an agent to navigate through an unfamiliar 3D environment is a tough challenge, even in simulated environments. For the agent to adapt to unseen environments, it must be resistant to low-level and high-level variations like color, texture, object, and layout changes. Therefore, to improve overall adaptability, all types of variations in the environment have to be considered at different levels of data augmentation. To achieve this, we introduce House3D, a comprehensive and efficient environment containing 45,622 visually realistic 3D scenes of houses, from single-room studios to multi-storied houses, equipped with diverse, fully labeled 3D objects, textures, and scene layouts. The House3D's diversity allows for scene-level augmentation, while its label-rich nature enables pixel and task-level augmentations like domain randomization and multi-task training. Using a subset of House3D, we demonstrate that reinforcement learning agents trained with a variety of augmentations outperform our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is available for public use at http://github.com/facebookresearch/House3D.",1
"Hierarchical Modular Reinforcement Learning (HMRL), consists of 2 layered learning where Profit Sharing works to plan a prey position in the higher layer and Q-learning method trains the state-actions to the target in the lower layer. In this paper, we expanded HMRL to multi-target problem to take the distance between targets to the consideration. The function, called `AT field', can estimate the interests for an agent according to the distance between 2 agents and the advantage/disadvantage of the other agent. Moreover, the knowledge related to state-action rules is extracted by C4.5. The action under the situation is decided by using the acquired knowledge. To verify the effectiveness of proposed method, some experimental results are reported.",0
"The concept of Hierarchical Modular Reinforcement Learning (HMRL) involves a two-layered learning approach, wherein Profit Sharing is utilized to strategize the position of the prey in the higher layer, while the Q-learning method trains the state-actions to the target in the lower layer. Our research aims to extend HMRL to address multi-target problems by considering the distance between targets. To achieve this, we introduced a new function, referred to as the `AT field,' that estimates the interests of an agent based on the distance between two agents and the relative advantage/disadvantage of the other agent. Additionally, we leveraged the C4.5 algorithm to extract knowledge related to state-action rules, which is then used to make decisions regarding actions in a given situation. We present experimental results to demonstrate the effectiveness of our proposed method.",1
"Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid the significant effort needed to hand-craft the required dialogue flow, the Dialogue Management (DM) module can be cast as a continuous Markov Decision Process (MDP) and trained through Reinforcement Learning (RL). Several RL models have been investigated over recent years. However, the lack of a common benchmarking framework makes it difficult to perform a fair comparison between different models and their capability to generalise to different environments. Therefore, this paper proposes a set of challenging simulated environments for dialogue model development and evaluation. To provide some baselines, we investigate a number of representative parametric algorithms, namely deep reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and compare them to a non-parametric model, GP-SARSA. Both the environments and policy models are implemented using the publicly available PyDial toolkit and released on-line, in order to establish a testbed framework for further experiments and to facilitate experimental reproducibility.",0
"Dialogue assistants are quickly becoming a vital tool for daily use. However, creating the necessary dialogue flow can be a significant effort. To alleviate this, the Dialogue Management (DM) module can be transformed into a continuous Markov Decision Process (MDP) and trained using Reinforcement Learning (RL). Over the years, various RL models have been explored, but the lack of a standard benchmarking framework makes it challenging to compare different models and their ability to adapt to different environments. Therefore, this study suggests a series of challenging simulated environments to develop and assess dialogue models. To provide a baseline, representative parametric algorithms, such as DQN, A2C, and Natural Actor-Critic, and a non-parametric model, GP-SARSA, were investigated. The on-line release of both the environments and policy models using the publicly accessible PyDial toolkit establishes a testbed framework for further experimentation and facilitates experimental reproducibility.",1
"A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.",0
"For decades, people have dreamed of a robot that could understand and carry out instructions given in natural language. This idea was popularized in shows like The Jetsons, where robots assisted humans with daily tasks. Unfortunately, this dream has yet to become a reality. However, recent advancements in language and vision technologies have made significant strides in related areas. These advancements are particularly important for tasks like natural-language navigation, which require robots to interpret instructions based on what they see. This process is similar to Visual Question Answering, as both tasks involve visually-grounded sequence-to-sequence translation problems. To encourage the application of these methods to natural-language navigation, the Matterport3D Simulator has been developed. This reinforcement learning environment uses real imagery and can support a variety of embodied vision and language tasks. The Room-to-Room (R2R) dataset is the first benchmark dataset for visually-grounded natural language navigation in real buildings and was created using this simulator.",1
"In 2015, Google's DeepMind announced an advancement in creating an autonomous agent based on deep reinforcement learning (DRL) that could beat a professional player in a series of 49 Atari games. However, the current manifestation of DRL is still immature, and has significant drawbacks. One of DRL's imperfections is its lack of ""exploration"" during the training process, especially when working with high-dimensional problems. In this paper, we propose a mixed strategy approach that mimics behaviors of human when interacting with environment, and create a ""thinking"" agent that allows for more efficient exploration in the DRL training process. The simulation results based on the Breakout game show that our scheme achieves a higher probability of obtaining a maximum score than does the baseline DRL algorithm, i.e., the asynchronous advantage actor-critic method. The proposed scheme therefore can be applied effectively to solving a complicated task in a real-world application.",0
"Google's DeepMind made an announcement in 2015 about developing an independent agent utilizing deep reinforcement learning (DRL) that could outperform a professional player in 49 Atari games. However, DRL's current form is still in its early stages and has some significant limitations, such as insufficient exploration during the training process, especially when working with high-dimensional problems. In this article, we suggest a mixed strategy approach that mimics human behavior when interacting with the environment, resulting in a ""thinking"" agent that can explore more efficiently during the DRL training process. Our simulation results, based on the Breakout game, demonstrate that our approach has a higher chance of achieving the maximum score than the baseline DRL algorithm, the asynchronous advantage actor-critic method. As a result, our proposed scheme can be effectively applied to solve complex real-world challenges.",1
"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",0
"The challenge of complex visuomotor control involves acquiring abstract representations that are useful for planning, goal specification, and generalization. To address this challenge, we present universal planning networks (UPN), which integrate differentiable planning into a goal-directed policy. This planning computation utilizes a latent space forward model to generate an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are trained end-to-end to optimize a supervised imitation learning objective. The learned representations are effective for gradient-based trajectory optimization in goal-directed visual imitation and can also serve as a metric for image-based goal specification. Moreover, the representations can be utilized to define distance-based rewards for model-free reinforcement learning, leading to more effective learning when tackling new tasks that involve image-based goals. We were able to successfully transfer visuomotor planning strategies across robots with different morphologies and actuation capabilities.",1
"To ensure stability of learning, state-of-the-art generalized policy iteration algorithms augment the policy improvement step with a trust region constraint bounding the information loss. The size of the trust region is commonly determined by the Kullback-Leibler (KL) divergence, which not only captures the notion of distance well but also yields closed-form solutions. In this paper, we consider a more general class of f-divergences and derive the corresponding policy update rules. The generic solution is expressed through the derivative of the convex conjugate function to f and includes the KL solution as a special case. Within the class of f-divergences, we further focus on a one-parameter family of $\alpha$-divergences to study effects of the choice of divergence on policy improvement. Previously known as well as new policy updates emerge for different values of $\alpha$. We show that every type of policy update comes with a compatible policy evaluation resulting from the chosen f-divergence. Interestingly, the mean-squared Bellman error minimization is closely related to policy evaluation with the Pearson $\chi^2$-divergence penalty, while the KL divergence results in the soft-max policy update and a log-sum-exp critic. We carry out asymptotic analysis of the solutions for different values of $\alpha$ and demonstrate the effects of using different divergence functions on a multi-armed bandit problem and on common standard reinforcement learning problems.",0
"Cutting-edge generalized policy iteration algorithms utilize trust region constraints to limit information loss during the policy improvement step, ensuring stable learning. The Kullback-Leibler (KL) divergence is commonly used to determine the size of the trust region due to its ability to measure distance and provide closed-form solutions. However, this paper explores a broader range of f-divergences and their corresponding policy update rules, which can be expressed through the derivative of the convex conjugate function to f and include the KL solution as a special case. The focus is on a one-parameter family of $\alpha$-divergences, which generate different policy updates and evaluations. The Pearson $\chi^2$-divergence penalty is closely related to mean-squared Bellman error minimization, while the KL divergence results in a soft-max policy update and a log-sum-exp critic. The effects of using different divergence functions are analyzed via asymptotic analysis and demonstrated in various reinforcement learning problems.",1
"All reinforcement learning algorithms must handle the trade-off between exploration and exploitation. Many state-of-the-art deep reinforcement learning methods use noise in the action selection, such as Gaussian noise in policy gradient methods or $\epsilon$-greedy in Q-learning. While these methods are appealing due to their simplicity, they do not explore the state space in a methodical manner. We present an approach that uses a model to derive reward bonuses as a means of intrinsic motivation to improve model-free reinforcement learning. A key insight of our approach is that this dynamics model can be learned in the latent feature space of a value function, representing the dynamics of the agent and the environment. This method is both theoretically grounded and computationally advantageous, permitting the efficient use of Bayesian information-theoretic methods in high-dimensional state spaces. We evaluate our method on several continuous control tasks, focusing on improving exploration.",0
"Reinforcement learning algorithms all face the challenge of balancing exploration and exploitation. While some cutting-edge deep reinforcement learning techniques rely on action selection noise like Gaussian noise in policy gradient methods or $\epsilon$-greedy in Q-learning, these methods are not systematic in exploring the state space. Our proposal offers an alternative that employs a model to generate reward bonuses serving as intrinsic motivation for enhancing model-free reinforcement learning. The crux of our approach is that the dynamics model can be acquired in the latent feature space of a value function that illustrates the interaction between the agent and the environment. This method is both theoretically sound and computationally advantageous, allowing for the efficient use of Bayesian information-theoretic methods in high-dimensional state spaces. We demonstrate the effectiveness of our method in improving exploration by evaluating it on several continuous control tasks.",1
"Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets.",0
"Recent years have seen significant advancements in deep generative networks. However, these models can sometimes focus too much on minor details in datasets due to their decoders lacking strong inductive biases. Graphics engines could be useful in solving this problem by representing images as high-level programs that abstract away low-level details. Unfortunately, current methods that combine deep learning and renderers have limitations, such as requiring large amounts of supervision or facing difficulties in scaling their inference algorithms. To address these issues, we have developed SPIRAL, an adversarially trained agent that generates a program executed by a graphics engine to interpret and sample images. The agent's objective is to deceive a discriminator network that distinguishes between real and rendered data, trained using a distributed reinforcement learning setup without any supervision. Interestingly, using the discriminator's output as a reward signal was found to be crucial for the agent to make meaningful progress in rendering output. Our work is the first demonstration of an end-to-end, unsupervised, and adversarial inverse graphics agent on challenging real-world and synthetic datasets including MNIST, Omniglot, CelebA, and 3D images.",1
"In this paper, we present an online reinforcement learning algorithm, called Renewal Monte Carlo (RMC), for infinite horizon Markov decision processes with a designated start state. RMC is a Monte Carlo algorithm and retains the advantages of Monte Carlo methods including low bias, simplicity, and ease of implementation while, at the same time, circumvents their key drawbacks of high variance and delayed (end of episode) updates. The key ideas behind RMC are as follows. First, under any reasonable policy, the reward process is ergodic. So, by renewal theory, the performance of a policy is equal to the ratio of expected discounted reward to the expected discounted time over a regenerative cycle. Second, by carefully examining the expression for performance gradient, we propose a stochastic approximation algorithm that only requires estimates of the expected discounted reward and discounted time over a regenerative cycle and their gradients. We propose two unbiased estimators for evaluating performance gradients---a likelihood ratio based estimator and a simultaneous perturbation based estimator---and show that for both estimators, RMC converges to a locally optimal policy. We generalize the RMC algorithm to post-decision state models and also present a variant that converges faster to an approximately optimal policy. We conclude by presenting numerical experiments on a randomly generated MDP, event-triggered communication, and inventory management.",0
"The article introduces Renewal Monte Carlo (RMC), an online reinforcement learning algorithm that deals with infinite horizon Markov decision processes that have a specific start state. RMC is a Monte Carlo algorithm that has low bias, simplicity, and ease of implementation, while overcoming the high variance and delayed updates drawbacks of Monte Carlo methods. RMC's approach is based on two key ideas. Firstly, the reward process is ergodic for any policy, and a policy's performance is the ratio of the expected discounted reward to the expected discounted time over a regenerative cycle. Secondly, the performance gradient expression is examined, and a stochastic approximation algorithm is proposed that only requires estimates of the expected discounted reward and discounted time over a regenerative cycle and their gradients. Two unbiased estimators are proposed for performance gradients, and it is shown that RMC converges to a locally optimal policy for both estimators. The article also presents variations of the RMC algorithm and concludes with numerical experiments on different scenarios.",1
"Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services.",0
"Stochastic neural net weights have diverse applications, such as regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. However, the vast number of weights in a mini-batch constrains the variance reduction effect of large mini-batches, as all examples in a mini-batch typically share the same weight perturbation. To address this issue, we propose flipout, an efficient method that implicitly samples pseudo-independent weight perturbations for each example, thus decorrelating the gradients within a mini-batch. Empirical evidence indicates that flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. Moreover, it speeds up training neural networks with multiplicative Gaussian perturbations, and is effective at regularizing LSTMs, surpassing previous approaches. Additionally, flipout facilitates the vectorization of evolution strategies, enabling a single GPU with flipout to handle the same throughput as at least 40 CPU cores using existing methods, resulting in a four-fold cost reduction on Amazon Web Services.",1
"The problem of object localization and recognition on autonomous mobile robots is still an active topic. In this context, we tackle the problem of learning a model of visual saliency directly on a robot. This model, learned and improved on-the-fly during the robot's exploration provides an efficient tool for localizing relevant objects within their environment. The proposed approach includes two intertwined components. On the one hand, we describe a method for learning and incrementally updating a model of visual saliency from a depth-based object detector. This model of saliency can also be exploited to produce bounding box proposals around objects of interest. On the other hand, we investigate an autonomous exploration technique to efficiently learn such a saliency model. The proposed exploration, called Reinforcement Learning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot's exploration so that samples selected by the robot are likely to improve the current model of saliency. We then demonstrate that such a saliency model learned directly on a robot outperforms several state-of-the-art saliency techniques, and that RL-IAC can drastically decrease the required time for learning a reliable saliency model.",0
"Object localization and recognition on autonomous mobile robots is an ongoing issue. Our aim is to address this by developing a model of visual saliency on a robot. This model is learned and enhanced during the robot's exploration, and it efficiently localizes important objects within its environment. Our proposed method consists of two interconnected components. Firstly, we present a technique to learn and update a model of visual saliency using a depth-based object detector. This model can also be used to generate bounding box suggestions for objects of interest. Secondly, we explore an autonomous exploration strategy, RL-IAC, which can efficiently learn the saliency model. This approach drives the robot's exploration so that selected samples improve the current saliency model. Our results show that the saliency model learned directly on a robot outperforms several state-of-the-art saliency techniques, and RL-IAC significantly reduces the time required to learn a reliable saliency model.",1
"Learning probability distributions on the weights of neural networks (NNs) has recently proven beneficial in many applications. Bayesian methods, such as Stein variational gradient descent (SVGD), offer an elegant framework to reason about NN model uncertainty. However, by assuming independent Gaussian priors for the individual NN weights (as often applied), SVGD does not impose prior knowledge that there is often structural information (dependence) among weights. We propose efficient posterior learning of structural weight uncertainty, within an SVGD framework, by employing matrix variate Gaussian priors on NN parameters. We further investigate the learned structural uncertainty in sequential decision-making problems, including contextual bandits and reinforcement learning. Experiments on several synthetic and real datasets indicate the superiority of our model, compared with state-of-the-art methods.",0
"In recent times, the acquisition of knowledge on probability distributions related to neural network (NN) weights has been advantageous in numerous applications. Bayesian techniques, like Stein variational gradient descent (SVGD), provide an effective basis for reasoning about NN model uncertainty. Nonetheless, relying on independent Gaussian priors for individual NN weights (as is often done), SVGD does not take into account the existence of structural information (dependence) among weights, which is commonly present. We suggest an efficient manner of acquiring posterior knowledge on structural weight uncertainty within an SVGD framework, by making use of matrix variate Gaussian priors on NN parameters. Additionally, we analyze the acquired structural uncertainty in contexts that require sequential decision-making, such as contextual bandits and reinforcement learning. Our experiments on several synthetic and actual datasets demonstrate that our model outperforms state-of-the-art methods.",1
"In the NIPS 2017 Learning to Run challenge, participants were tasked with building a controller for a musculoskeletal model to make it run as fast as possible through an obstacle course. Top participants were invited to describe their algorithms. In this work, we present eight solutions that used deep reinforcement learning approaches, based on algorithms such as Deep Deterministic Policy Gradient, Proximal Policy Optimization, and Trust Region Policy Optimization. Many solutions use similar relaxations and heuristics, such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending. However, each of the eight teams implemented different modifications of the known algorithms.",0
"The NIPS 2017 Learning to Run challenge required participants to develop a controller for a musculoskeletal model that could traverse an obstacle course as quickly as possible. The best performers were given the opportunity to explain their methods. This study showcases eight solutions that leverage deep reinforcement learning techniques, including Deep Deterministic Policy Gradient, Proximal Policy Optimization, and Trust Region Policy Optimization. Although several solutions utilize comparable relaxations and heuristics such as reward shaping, frame skipping, and policy blending, each team introduced their own unique adjustments to the established algorithms.",1
"Recent advances in policy gradient methods and deep learning have demonstrated their applicability for complex reinforcement learning problems. However, the variance of the performance gradient estimates obtained from the simulation is often excessive, leading to poor sample efficiency. In this paper, we apply the stochastic variance reduced gradient descent (SVRG) to model-free policy gradient to significantly improve the sample-efficiency. The SVRG estimation is incorporated into a trust-region Newton conjugate gradient framework for the policy optimization. On several Mujoco tasks, our method achieves significantly better performance compared to the state-of-the-art model-free policy gradient methods in robotic continuous control such as trust region policy optimization (TRPO)",0
"Policy gradient methods and deep learning have made substantial progress lately, proving their effectiveness for intricate reinforcement learning problems. Nevertheless, the performance gradient estimates derived from simulation frequently exhibit excessive variance, leading to suboptimal sample efficiency. This study introduces the stochastic variance reduced gradient descent (SVRG) technique to enhance the sample efficiency of model-free policy gradient. The SVRG estimation is integrated into a trust-region Newton conjugate gradient framework to optimize policy. Our approach surpasses state-of-the-art model-free policy gradient methods like trust region policy optimization (TRPO) in robotic continuous control, as evidenced by superior performance on several Mujoco tasks.",1
"Existing inefficient traffic light control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. In terms of how to dynamically adjust traffic signals' duration, existing works either split the traffic signal into equal duration or extract limited traffic information from the real data. In this paper, we study how to decide the traffic signals' duration based on the collected data from different sensors and vehicular networks. We propose a deep reinforcement learning model to control the traffic light. In the model, we quantify the complex traffic scenario as states by collecting data and dividing the whole intersection into small grids. The timing changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map the states to rewards. The proposed model is composed of several components to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the simulation results show the efficiency of our model in controlling traffic lights.",0
"Numerous problems arise due to the existing inefficient control of traffic lights, including long delays and energy wastage. To enhance efficiency, it is necessary to use real-time traffic information as input and dynamically adjust the duration of traffic lights accordingly. Previous studies either split the traffic signal into equal duration or extract limited traffic information from real data to dynamically adjust the duration of traffic signals. This paper explores how to determine the duration of traffic signals based on data collected from various sensors and vehicular networks. We propose a deep reinforcement learning model to control traffic lights, which quantifies the complex traffic scenario as states by collecting data and dividing the intersection into small grids. The timing changes of a traffic light are modeled as a high-dimension Markov decision process, with the reward being the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is used to map the states to rewards. Our proposed model includes several components, such as dueling network, target network, double Q-learning network, and prioritized experience replay, to improve its performance. We evaluate our model through simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the results demonstrate the efficiency of our model in controlling traffic lights.",1
"Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.",0
"The task of generating a written explanation of the actions in a video is known as video captioning. While previous attempts, such as the sequence-to-sequence model, have shown potential in providing a general description of a brief video, it remains difficult to caption a video that includes multiple detailed actions. This article proposes a new hierarchical reinforcement learning structure for video captioning, where a Manager module at a high level learns to create sub-goals and a Worker module at a low level recognizes the basic actions necessary to achieve the sub-goal. By reinforcing video captioning at different levels with this compositional structure, our method outperforms all baseline methods on a recently introduced large-scale dataset for detailed video captioning. Additionally, our single model has already achieved the best results on the widely used MSR-VTT dataset.",1
"Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called ""partial observability"". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.",0
"Despite their limited sensory range, animals are able to carry out goal-directed behaviors by exploring their environment and storing memories of important information not immediately available. While recent progress has been made in developing artificial intelligence agents that can perform tasks at a human level by merging reinforcement learning algorithms with deep neural networks, these agents still struggle with partially observable tasks. To solve this problem, access to extensive memory is not enough; the right information must also be stored in the right format. The Memory, RL, and Inference Network (MERLIN) is a model that uses predictive modeling to guide memory formation and can solve tasks in 3D virtual reality environments with severe partial observability. This model provides a single learning agent architecture that can solve behavioral tasks in psychology and neurobiology without any assumptions about sensory input dimensionality or experience duration.",1
"We introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images. Detection progresses in a coarse-to-fine manner, first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy. Built upon reinforcement learning, our approach consists of a model (R-net) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model (Q-net) that sequentially selects regions to zoom in. Experiments on the Caltech Pedestrians dataset show that our approach reduces the number of processed pixels by over 50% without a drop in detection accuracy. The merits of our approach become more significant on a high resolution test set collected from YFCC100M dataset, where our approach maintains high detection performance while reducing the number of processed pixels by about 70% and the detection time by over 50%.",0
"To address the challenge of detecting objects of varied sizes in high resolution images, we have developed a versatile framework that maintains accuracy while minimizing computational costs. Our approach involves a stepwise detection process, starting with a low-resolution image and gradually analyzing higher resolution regions that are likely to enhance detection accuracy. Our approach is underpinned by reinforcement learning, with two models (R-net and Q-net) that predict accuracy gain and sequentially identify regions for analysis. Our experiments on the Caltech Pedestrians dataset demonstrate that our approach reduces processed pixels by over 50% without sacrificing detection accuracy. Our framework is particularly effective on a high resolution test set from the YFCC100M dataset, where we achieve high detection performance, reduce processed pixels by about 70%, and decrease detection time by over 50%.",1
"Unfair pricing policies have been shown to be one of the most negative perceptions customers can have concerning pricing, and may result in long-term losses for a company. Despite the fact that dynamic pricing models help companies maximize revenue, fairness and equality should be taken into account in order to avoid unfair price differences between groups of customers. This paper shows how to solve dynamic pricing by using Reinforcement Learning (RL) techniques so that prices are maximized while keeping a balance between revenue and fairness. We demonstrate that RL provides two main features to support fairness in dynamic pricing: on the one hand, RL is able to learn from recent experience, adapting the pricing policy to complex market environments; on the other hand, it provides a trade-off between short and long-term objectives, hence integrating fairness into the model's core. Considering these two features, we propose the application of RL for revenue optimization, with the additional integration of fairness as part of the learning procedure by using Jain's index as a metric. Results in a simulated environment show a significant improvement in fairness while at the same time maintaining optimisation of revenue.",0
"Customers often perceive unfair pricing policies as a negative aspect of pricing, which can result in long-term losses for a company. Although dynamic pricing models can help maximize revenue, companies should consider fairness and equality to avoid creating unfair price differences among customer groups. This paper proposes using Reinforcement Learning (RL) techniques to solve dynamic pricing issues by balancing revenue and fairness. RL offers two main features for supporting fairness in dynamic pricing: learning from recent experience to adapt to complex market environments and providing a trade-off between short and long-term objectives. The proposed solution integrates fairness into the learning procedure using Jain's index as a metric. Results from a simulated environment show a significant improvement in fairness while maintaining revenue optimization.",1
"Goals for reinforcement learning problems are typically defined through hand-specified rewards. To design such problems, developers of learning algorithms must inherently be aware of what the task goals are, yet we often require agents to discover them on their own without any supervision beyond these sparse rewards. While much of the power of reinforcement learning derives from the concept that agents can learn with little guidance, this requirement greatly burdens the training process. If we relax this one restriction and endow the agent with knowledge of the reward function, and in particular of the goal, we can leverage backwards induction to accelerate training. To achieve this, we propose training a model to learn to take imagined reversal steps from known goal states. Rather than training an agent exclusively to determine how to reach a goal while moving forwards in time, our approach travels backwards to jointly predict how we got there. We evaluate our work in Gridworld and Towers of Hanoi and empirically demonstrate that it yields better performance than standard DDQN.",0
"Typically, reinforcement learning problems involve defining goals through manually specified rewards. Developers of learning algorithms must have knowledge of the task goals to design these problems. However, agents are often required to discover the goals on their own with sparse rewards, which increases the burden of the training process. Although reinforcement learning allows agents to learn with minimal guidance, we can accelerate training by providing the agent with knowledge of the reward function and goal. Our proposal involves training a model to learn to take imagined reversal steps from known goal states, rather than training an agent to reach a goal while moving forwards in time. We apply our approach to Gridworld and Towers of Hanoi and show that it performs better than standard DDQN through empirical evaluation.",1
"Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.",0
"The manual labeling of datasets with object masks is a time-consuming process. In this study, we adopt the concept of Polygon-RNN to generate polygonal annotations of objects interactively with human involvement. We have made several crucial enhancements to the model, including the development of a novel CNN encoder architecture, effective training with Reinforcement Learning, and the use of a Graph Neural Network to significantly increase output resolution and accurately annotate high-resolution objects in images. Our model, referred to as Polygon-RNN++, has been extensively evaluated on the Cityscapes dataset and has shown significant improvement over the original model, both in automatic and interactive modes. It has also demonstrated strong generalization capabilities when trained on one dataset and used on others from different domains. By using simple online fine-tuning, we have achieved a significant reduction in annotation time for new datasets, bringing us closer to an interactive annotation tool for practical use.",1
"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.",0
"Active Reinforcement Learning (ARL) is a modified version of RL in which the agent only receives reward information if it incurs a cost, making exploration more difficult. RL techniques such as optimism, Thompson sampling, and random exploration are not applicable to ARL. We connect ARL in tabular environments to Bayes-Adaptive MDPs and present an ARL algorithm that uses Monte-Carlo Tree Search and is asymptotically Bayes optimal. Our algorithm performs nearly optimally on small Bandit problems and MDPs and outperforms a Q-learner with specialized heuristics for ARL on larger MDPs. By examining exploration behavior in depth, we identify challenges to scaling up simulation-based algorithms for ARL.",1
"The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore \emph{local} regions close to what the actor policy dictates. In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a \emph{global exploration} that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning tasks.",0
"The success of deep Q-learning and deep deterministic policy gradient (DDPG) in off-policy learning largely relies on the selection of the exploration policy. Most current exploration techniques involve introducing noise to the existing actor policy, which restricts exploration to nearby areas. Our research proposes a straightforward meta-policy gradient algorithm that can adaptively train the exploration policy in DDPG. Our approach enables us to develop versatile exploration behaviors that are not influenced by the actor policy, resulting in a more extensive exploration range that accelerates the learning process. Through a comprehensive analysis, we demonstrate that our method considerably enhances the sample-efficiency of DDPG in various reinforcement learning tasks.",1
"Inspired by the seminal work on Stein Variational Inference and Stein Variational Policy Gradient, we derived a method to generate samples from the posterior variational parameter distribution by \textit{explicitly} minimizing the KL divergence to match the target distribution in an amortize fashion. Consequently, we applied this varational inference technique into vanilla policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations for reinforcement learning problems.",0
"We developed a technique for producing samples from the posterior variational parameter distribution that draws influence from the influential research on Stein Variational Inference and Stein Variational Policy Gradient. Our approach involves minimizing the KL divergence to ameliorate the matching of the target distribution in an amortized manner. As a result, we integrated this variational inference method into reinforcement learning problems utilizing Bayesian Neural Network parameterizations for vanilla policy gradient, TRPO, and PPO.",1
"Here we propose using the successor representation (SR) to accelerate learning in a constructive knowledge system based on general value functions (GVFs). In real-world settings like robotics for unstructured and dynamic environments, it is infeasible to model all meaningful aspects of a system and its environment by hand due to both complexity and size. Instead, robots must be capable of learning and adapting to changes in their environment and task, incrementally constructing models from their own experience. GVFs, taken from the field of reinforcement learning (RL), are a way of modeling the world as predictive questions. One approach to such models proposes a massive network of interconnected and interdependent GVFs, which are incrementally added over time. It is reasonable to expect that new, incrementally added predictions can be learned more swiftly if the learning process leverages knowledge gained from past experience. The SR provides such a means of separating the dynamics of the world from the prediction targets and thus capturing regularities that can be reused across multiple GVFs. As a primary contribution of this work, we show that using SR-based predictions can improve sample efficiency and learning speed in a continual learning setting where new predictions are incrementally added and learned over time. We analyze our approach in a grid-world and then demonstrate its potential on data from a physical robot arm.",0
"Our proposal suggests utilizing the successor representation (SR) to expedite learning in a constructive knowledge system that is based on general value functions (GVFs). In dynamic and unstructured environments such as robotics, it is impractical to manually model all essential aspects of the system and its surroundings due to their complexity and size. Therefore, robots must possess the ability to learn and adapt to changes in their environment and task, constructing models incrementally from their own experiences. GVFs, which stem from reinforcement learning (RL), are a strategy for modeling the world as predictive questions. One method involves a network of interconnected and interdependent GVFs that grow over time. It is reasonable to assume that the learning process can be accelerated by leveraging past knowledge to swiftly learn new predictions. The SR allows for the separation of world dynamics from prediction targets, capturing regularities that can be reused across multiple GVFs. Our primary contribution demonstrates that the utilization of SR-based predictions can enhance sample efficiency and learning speed in a continual learning setting, where new predictions are added and learned over time. We evaluate our approach in a grid-world and then showcase its potential with data from a physical robot arm.",1
"RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.",0
"RANSAC is a significant algorithm in robust optimization, which is widely used in computer vision applications. However, with the emergence of deep learning pipelines that can be trained end-to-end, RANSAC's non-differentiable hypothesis selection process has prevented it from being integrated into such pipelines. In this study, we present two methods to address this issue. The more promising solution is based on reinforcement learning, where we replace deterministic hypothesis selection with probabilistic selection, allowing us to derive the expected loss concerning the learnable parameters. We call this approach DSAC, the differentiable version of RANSAC. We apply DSAC to the camera localization problem, where deep learning has not yet proven more effective than traditional methods. We demonstrate that by minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we can improve accuracy. In the future, DSAC can be implemented as a robust optimization component in any deep learning pipeline.",1
"Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.",0
"Although policy gradient methods have been successful in deep reinforcement learning, they are hindered by the high variance of gradient estimates, which is especially prevalent in problems with long horizons or high-dimensional action spaces. To address this, we have developed a bias-free action-dependent baseline for variance reduction, which utilizes the structural form of the stochastic policy and does not rely on any additional MDP assumptions. Our approach has been validated through theoretical analysis and numerical results, including a comparison with the suboptimal state-dependent baseline. This has resulted in a computationally efficient policy gradient algorithm that can handle high-dimensional control problems, as shown in a synthetic 2000-dimensional target matching task. Our experiments demonstrate that the use of action-dependent baselines leads to faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. We also extend the concept of incorporating additional information into baselines for improved variance reduction to partially observed and multi-agent tasks.",1
"We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate",0
"Our proposed approach involves using self-supervised learning to acquire representations and robotic behaviors solely from unlabeled videos recorded from various angles. We examine how this representation can be utilized in two robotic imitation scenarios: imitating human object interactions from video footage, and imitating human poses. A viewpoint-invariant representation that captures the relationship between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose is required for human behavior imitation. We train our representation using a metric learning loss, where the embedding space attracts multiple simultaneous viewpoints of the same observation, but repels temporal neighbors that are visually similar but functionally different. Our model is trained to recognize commonalities between dissimilar images and differences between similar ones. This approach enables the discovery of attributes that remain constant across viewpoints but vary over time, while ignoring nuisance variables such as occlusions, motion blur, lighting, and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without requiring an explicit correspondence, and that it can function as a reward function in a reinforcement learning algorithm. Although representations are learned from an unlabeled set of task-related videos, robot behaviors such as pouring are learned from a single third-person demonstration by a human. The reward functions obtained by following the human demonstrations under the learned representation facilitate efficient reinforcement learning that is practical for real-world robotic systems. Our video results, open-source code, and dataset are available at https://sermanet.github.io/imitate.",1
"Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots. In this work, we develop a learning task with a UR5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard. Our study suggests some mitigating steps to help future experimenters avoid difficulties and pitfalls. We show that highly reliable and repeatable experiments can be performed in our setup, indicating the possibility of reinforcement learning research extensively based on real-world robots.",0
"Reinforcement learning is a promising means of creating adaptable solutions for complex and diverse robotic tasks that are difficult to engineer. However, using real-world robots for learning is often unreliable and challenging, leading to low adoption in reinforcement learning research. This problem is compounded by the absence of guidelines for setting up learning tasks involving robots. In this research, we establish a learning task utilizing a UR5 robotic arm to identify critical elements of task setup and assess their impact on the challenges involved with robots. Our findings demonstrate that learning performance is significantly influenced by the setup, and hence, overlooking or omitting setup details can impede effective learning, reproducibility, and fair comparisons. Our study recommends some mitigating measures to help future experimenters avoid difficulties and pitfalls. We demonstrate that our setup enables highly reliable and repeatable experiments, indicating that reinforcement learning research extensively based on real-world robots is feasible.",1
"A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.",0
"In model-free reinforcement learning, it is commonly believed that exploring the space of actions leads to better sample efficiency than random search in the parameter space of policies. However, we prove this belief wrong by introducing a random search method that trains static, linear policies for continuous control problems. Our method matches the state-of-the-art sample efficiency on the MuJoCo locomotion tasks and finds a near-optimal controller for a difficult instance of the Linear Quadratic Regulator problem, even when the dynamics are not known. Our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. Additionally, we evaluate the performance of our method over hundreds of random seeds and various hyperparameter configurations for each benchmark task. Our simulations suggest that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms, as there is a high variability in performance in these benchmark tasks.",1
"An online reinforcement learning algorithm is anytime if it does not need to know in advance the horizon T of the experiment. A well-known technique to obtain an anytime algorithm from any non-anytime algorithm is the ""Doubling Trick"". In the context of adversarial or stochastic multi-armed bandits, the performance of an algorithm is measured by its regret, and we study two families of sequences of growing horizons (geometric and exponential) to generalize previously known results that certain doubling tricks can be used to conserve certain regret bounds. In a broad setting, we prove that a geometric doubling trick can be used to conserve (minimax) bounds in $R\_T = O(\sqrt{T})$ but cannot conserve (distribution-dependent) bounds in $R\_T = O(\log T)$. We give insights as to why exponential doubling tricks may be better, as they conserve bounds in $R\_T = O(\log T)$, and are close to conserving bounds in $R\_T = O(\sqrt{T})$.",0
"If an online reinforcement learning algorithm can function without prior knowledge of the experiment's horizon T, it is considered an anytime algorithm. The ""Doubling Trick"" is a well-known technique for converting non-anytime algorithms into anytime algorithms. In the context of adversarial or stochastic multi-armed bandits, regret is used to measure an algorithm's performance, and we examine two sequences of growing horizons (geometric and exponential) to extend existing knowledge that certain doubling tricks can maintain specific regret bounds. Our findings demonstrate that a geometric doubling trick can preserve (minimax) bounds in $R\_T = O(\sqrt{T})$ but not maintain (distribution-dependent) bounds in $R\_T = O(\log T)$ in a broad setting. Exponential doubling tricks may be superior since they can maintain bounds in $R\_T = O(\log T)$ and are close to preserving bounds in $R\_T = O(\sqrt{T})$.",1
"Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.",0
"The effectiveness of model-free deep reinforcement learning has been demonstrated in various domains, including video games and simulated robotic manipulation and locomotion. However, these methods are not ideal for real-world robotic tasks with limited interaction time. To address this issue, we investigated the use of maximum entropy policies trained via soft Q-learning for real-world robotic manipulation. Two key features of soft Q-learning make it suitable for such applications: the ability to learn multimodal exploration strategies through expressive energy-based models and the capability to compose policies to create new ones while bounding the optimality based on the divergence between the policies. This compositionality is particularly useful in real-world manipulation, where it can lead to significant efficiency gains. Our experiments indicate that soft Q-learning is more sample efficient than previous model-free deep reinforcement learning methods, and that compositionality can be applied to both simulated and real-world tasks.",1
"In recent years, visual question answering (VQA) has become topical. The premise of VQA's significance as a benchmark in AI, is that both the image and textual question need to be well understood and mutually grounded in order to infer the correct answer. However, current VQA models perhaps `understand' less than initially hoped, and instead master the easier task of exploiting cues given away in the question and biases in the answer distribution. In this paper we propose the inverse problem of VQA (iVQA). The iVQA task is to generate a question that corresponds to a given image and answer pair. We propose a variational iVQA model that can generate diverse, grammatically correct and content correlated questions that match the given answer. Based on this model, we show that iVQA is an interesting benchmark for visuo-linguistic understanding, and a more challenging alternative to VQA because an iVQA model needs to understand the image better to be successful. As a second contribution, we show how to use iVQA in a novel reinforcement learning framework to diagnose any existing VQA model by way of exposing its belief set: the set of question-answer pairs that the VQA model would predict true for a given image. This provides a completely new window into what VQA models `believe' about images. We show that existing VQA models have more erroneous beliefs than previously thought, revealing their intrinsic weaknesses. Suggestions are then made on how to address these weaknesses going forward.",0
"Visual question answering (VQA) has become a popular topic in recent years. The significance of VQA lies in its ability to serve as a benchmark in AI. Both the image and the textual question must be well understood and grounded for the correct answer to be inferred. However, current VQA models may not understand as much as initially expected. Instead, they may rely on exploiting cues in the question and biases in the answer distribution. This paper introduces the inverse problem of VQA (iVQA), which requires generating a question corresponding to a given image and answer pair. We propose a variational iVQA model that can generate diverse, grammatically correct, and content-correlated questions matching the answer. We demonstrate that iVQA is an interesting benchmark for visuo-linguistic understanding and a more challenging alternative to VQA because an iVQA model must understand the image better to be successful. The paper also introduces a novel reinforcement learning framework that uses iVQA to diagnose existing VQA models by exposing their belief set. This approach provides new insights into what VQA models ""believe"" about images. We show that existing VQA models have more erroneous beliefs than previously thought, revealing their intrinsic weaknesses. Finally, we offer suggestions on how to address these weaknesses going forward.",1
"The existing image captioning approaches typically train a one-stage sentence decoder, which is difficult to generate rich fine-grained descriptions. On the other hand, multi-stage image caption model is hard to train due to the vanishing gradient problem. In this paper, we propose a coarse-to-fine multi-stage prediction framework for image captioning, composed of multiple decoders each of which operates on the output of the previous stage, producing increasingly refined image descriptions. Our proposed learning approach addresses the difficulty of vanishing gradients during training by providing a learning objective function that enforces intermediate supervisions. Particularly, we optimize our model with a reinforcement learning approach which utilizes the output of each intermediate decoder's test-time inference algorithm as well as the output of its preceding decoder to normalize the rewards, which simultaneously solves the well-known exposure bias problem and the loss-evaluation mismatch problem. We extensively evaluate the proposed approach on MSCOCO and show that our approach can achieve the state-of-the-art performance.",0
"The typical method for image captioning involves training a one-stage sentence decoder, which struggles to produce detailed descriptions. Conversely, a multi-stage image caption model is difficult to train due to the vanishing gradient problem. To address these issues, we propose a coarse-to-fine multi-stage prediction framework for image captioning. This framework consists of multiple decoders, each building on the output of the previous stage to generate increasingly refined image descriptions. To overcome the vanishing gradient challenge during training, we introduce an objective function that enforces intermediate supervisions. In addition, we optimize our model using a reinforcement learning approach that utilizes the output of each intermediate decoder's test-time inference algorithm and the output of its preceding decoder to normalize the rewards. This approach simultaneously solves the exposure bias problem and the loss-evaluation mismatch problem. We conduct a thorough evaluation of our approach on MSCOCO and demonstrate that it achieves state-of-the-art performance.",1
"Many practical environments contain catastrophic states that an optimal agent would visit infrequently or never. Even on toy problems, Deep Reinforcement Learning (DRL) agents tend to periodically revisit these states upon forgetting their existence under a new policy. We introduce intrinsic fear (IF), a learned reward shaping that guards DRL agents against periodic catastrophes. IF agents possess a fear model trained to predict the probability of imminent catastrophe. This score is then used to penalize the Q-learning objective. Our theoretical analysis bounds the reduction in average return due to learning on the perturbed objective. We also prove robustness to classification errors. As a bonus, IF models tend to learn faster, owing to reward shaping. Experiments demonstrate that intrinsic-fear DQNs solve otherwise pathological environments and improve on several Atari games.",0
"In practical situations, there are states that an optimal agent would avoid or rarely visit. However, Deep Reinforcement Learning (DRL) agents tend to forget and revisit these states periodically, even on simpler problems. To address this issue, we propose intrinsic fear (IF), a reward shaping technique that trains DRL agents to recognize and avoid catastrophic states. IF agents use a fear model to predict the likelihood of such states and penalize the Q-learning objective accordingly. Our theoretical analysis shows that this approach can reduce the average return, but we also prove that it is resilient to classification errors. Additionally, IF models learn faster thanks to reward shaping. Experimentally, we show that IF DQNs can successfully navigate otherwise problematic environments and outperform regular DQNs on several Atari games.",1
"In this work, we provide theoretical guarantees for reward decomposition in deterministic MDPs. Reward decomposition is a special case of Hierarchical Reinforcement Learning, that allows one to learn many policies in parallel and combine them into a composite solution. Our approach builds on mapping this problem into a Reward Discounted Traveling Salesman Problem, and then deriving approximate solutions for it. In particular, we focus on approximate solutions that are local, i.e., solutions that only observe information about the current state. Local policies are easy to implement and do not require substantial computational resources as they do not perform planning. While local deterministic policies, like Nearest Neighbor, are being used in practice for hierarchical reinforcement learning, we propose three stochastic policies that guarantee better performance than any deterministic policy.",0
"The objective of our research is to provide a theoretical basis for reward decomposition in deterministic MDPs. This technique is a form of Hierarchical Reinforcement Learning that enables the learning of multiple policies simultaneously and their integration into a unified solution. To achieve this, we have formulated a solution to the Reward Discounted Traveling Salesman Problem, which allows for an approximate solution to this complex problem. Our study has focused on local solutions that offer several advantages, including ease of implementation and minimal computational resources. While deterministic policies such as Nearest Neighbor are currently in use, we introduce three new stochastic policies that provide superior performance.",1
"The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.",0
"As machine learning is increasingly relied upon to make important decisions, the ability to interpret its models has become crucial. Our proposed method, called model extraction, aims to interpret complex blackbox models by approximating them with more easily interpretable models. If the approximation is of high quality, statistical properties of the complex model can be reflected in the interpretable model. We demonstrate the effectiveness of model extraction in understanding and debugging random forests and neural nets trained on various datasets from the UCI Machine Learning Repository, as well as controlling policies learned for classical reinforcement learning problems.",1
"Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.",0
"The practical optimization problems have widely used genetic algorithms. The operators such as mutation, crossover, and selection, inspired by natural selection, have proved to be efficient heuristics for search and black-box optimization. However, these algorithms have not been successful in deep reinforcement learning due to the negative consequences resulting from parameter crossovers of neural networks. In this study, we introduce a new genetic algorithm called Genetic Policy Optimization (GPO) that utilizes imitation learning for policy crossover in the state space and policy gradient methods for mutation. Through experiments on MuJoCo tasks, we have demonstrated that GPO outperforms state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.",1
"Image cropping aims at improving the aesthetic quality of images by adjusting their composition. Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism. The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size. Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming. Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem. Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping. Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The agent is evaluated on several popular unseen cropping datasets. Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods.",0
"The purpose of image cropping is to enhance the visual appeal of images by adjusting their composition. Most methods for weakly supervised cropping, which do not involve bounding box supervision, use the sliding window mechanism. However, this method has limitations, such as fixed aspect ratios and arbitrary cropping regions, and can create a large number of windows that are time-consuming to process. To address these challenges, we propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework that formulates the image cropping process as a sequential decision-making process. Our approach develops an aesthetics-aware reward function that is optimized for image cropping and uses a comprehensive state representation that includes current observation and historical experience. We train the agent using the actor-critic architecture in an end-to-end manner and evaluate it on several popular datasets. Our method achieves state-of-the-art performance with fewer candidate windows and less time than previous weakly supervised methods.",1
"We introduce a new class of reinforcement learning methods referred to as {\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em episodes}, each composed of several {\em steps}, in which it chooses an action and observes a feedback signal. Moreover, in each step, it can take a special action, called the $stop$ action, that ends the current episode. After the $stop$ action is taken, the learner collects a terminal reward, and observes the costs and terminal rewards associated with each step of the episode. The goal of the learner is to maximize its cumulative gain (i.e., the terminal reward minus costs) over all episodes by learning to choose the best sequence of actions based on the feedback. First, we define an {\em oracle} benchmark, which sequentially selects the actions that maximize the expected immediate gain. Then, we propose our online learning algorithm, named {\em FeedBack Adaptive Learning} (FeedBAL), and prove that its regret with respect to the benchmark is bounded with high probability and increases logarithmically in expectation. Moreover, the regret only has polynomial dependence on the number of steps, actions and states. eMAB can be used to model applications that involve humans in the loop, ranging from personalized medical screening to personalized web-based education, where sequences of actions are taken in each episode, and optimal behavior requires adapting the chosen actions based on the feedback.",0
"We present a novel category of reinforcement learning strategies called ""episodic multi-armed bandits"" (eMAB). The eMAB approach involves the learner operating in ""episodes,"" which consist of multiple ""steps."" During each step, the learner selects an action and then receives feedback. Additionally, the learner can select a ""stop"" action that concludes the episode. When the ""stop"" action is taken, the learner receives a terminal reward and observes the costs and terminal rewards associated with each step of the episode. The objective of the learner is to optimize its cumulative gain by choosing the best sequence of actions based on the feedback. We begin by defining an ""oracle"" benchmark that chooses actions that maximize the anticipated immediate gain. We then introduce our online learning algorithm, named ""FeedBack Adaptive Learning"" (FeedBAL), and demonstrate that its regret with respect to the benchmark is limited with a high probability and increases logarithmically in expectation. Furthermore, the regret is only polynomially dependent on the number of steps, actions, and states. The eMAB approach can be used to simulate scenarios in which humans are actively involved, ranging from personalized medical screening to personalized web-based education, where optimal behavior necessitates adjusting the chosen actions based on feedback.",1
"Deep Q-learning is investigated as an end-to-end solution to estimate the optimal strategies for acting on time series input. Experiments are conducted on two idealized trading games. 1) Univariate: the only input is a wave-like price time series, and 2) Bivariate: the input includes a random stepwise price time series and a noisy signal time series, which is positively correlated with future price changes. The Univariate game tests whether the agent can capture the underlying dynamics, and the Bivariate game tests whether the agent can utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are used to model Q values. For both games, all agents successfully find a profitable strategy. The GRU-based agents show best overall performance in the Univariate game, while the MLP-based agents outperform others in the Bivariate game.",0
"The study examines Deep Q-learning as a comprehensive approach to determine the most effective tactics to act on time series input. The study involves conducting experiments on two trading games: 1) Univariate, where the input is solely a wave-like price time series, and 2) Bivariate, where the input consists of a random stepwise price time series and a noisy signal time series that is positively associated with future price changes. The purpose of the Univariate game is to test whether the agent can grasp the fundamental dynamics, while the Bivariate game assesses whether the agent can utilize the hidden relationship between the inputs. To model Q values, Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are employed. All agents in both games successfully identify a profitable strategy. The GRU-based agents show superior overall performance in the Univariate game, while the MLP-based agents outperform the others in the Bivariate game.",1
"The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick & place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input.   The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.",0
"The technical report serves two purposes. Firstly, it introduces a collection of demanding continuous control tasks which are integrated with OpenAI Gym and use current robotics hardware. These tasks include manipulating objects with a Fetch robotic arm, such as pushing, sliding, and pick & place, as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework, in which an agent is given extra input on what to do. Secondly, the report presents several specific research ideas for enhancing RL algorithms, with a focus on Multi-Goal RL and Hindsight Experience Replay.",1
"We present a method for using previously-trained 'teacher' agents to kickstart the training of a new 'student' agent. To this end, we leverage ideas from policy distillation and population based training. Our method places no constraints on the architecture of the teacher or student agents, and it regulates itself to allow the students to surpass their teachers in performance. We show that, on a challenging and computationally-intensive multi-task benchmark (DMLab-30), kickstarted training improves the data efficiency of new agents, making it significantly easier to iterate on their design. We also show that the same kickstarting pipeline can allow a single student agent to leverage multiple 'expert' teachers which specialize on individual tasks. In this setting kickstarting yields surprisingly large gains, with the kickstarted agent matching the performance of an agent trained from scratch in almost 10x fewer steps, and surpassing its final performance by 42 percent. Kickstarting is conceptually simple and can easily be incorporated into reinforcement learning experiments.",0
"We have developed a technique that utilizes 'teacher' agents that have been previously trained to initiate the training of a new 'student' agent. Our method draws inspiration from policy distillation and population based training, and it does not impose any limitations on the structure of the teacher or student agents. Moreover, the method allows the student agents to exceed the performance of their teachers. Our research demonstrates that kickstarted training significantly enhances the efficiency of new agent data, which simplifies the process of iterating on their design. Additionally, we have revealed that a single student agent can benefit from multiple 'expert' teachers that specialize in specific tasks. This approach yields remarkable gains, with the kickstarted agent achieving performance comparable to an agent trained from scratch in roughly ten times fewer steps and surpassing its final performance by 42 percent. Kickstarting is an uncomplicated concept that can be readily integrated into reinforcement learning experiments.",1
"During the 2017 NBA playoffs, Celtics coach Brad Stevens was faced with a difficult decision when defending against the Cavaliers: ""Do you double and risk giving up easy shots, or stay at home and do the best you can?"" It's a tough call, but finding a good defensive strategy that effectively incorporates doubling can make all the difference in the NBA. In this paper, we analyze double teaming in the NBA, quantifying the trade-off between risk and reward. Using player trajectory data pertaining to over 643,000 possessions, we identified when the ball handler was double teamed. Given these data and the corresponding outcome (i.e., was the defense successful), we used deep reinforcement learning to estimate the quality of the defensive actions. We present qualitative and quantitative results summarizing our learned defensive strategy for defending. We show that our policy value estimates are predictive of points per possession and win percentage. Overall, the proposed framework represents a step toward a more comprehensive understanding of defensive strategies in the NBA.",0
"In the 2017 NBA playoffs, Celtics coach Brad Stevens faced a challenging dilemma while defending against the Cavaliers: should he double-team and risk conceding easy shots or remain defensive and do his best? While it's a tough decision, doubling can significantly impact defensive strategies in the NBA. This paper examines double teaming in the NBA and evaluates the trade-off between risk and reward. We used player trajectory data from over 643,000 possessions to determine when the ball handler was double-teamed. We utilized deep reinforcement learning to estimate the quality of defensive actions and present qualitative and quantitative results summarizing our learned defensive strategy. Our policy value estimates are predictive of points per possession and win percentage. In conclusion, our proposed framework represents a significant step towards comprehensively understanding defensive strategies in the NBA.",1
"In recent years, deep learning techniques have been developed to improve the performance of program synthesis from input-output examples. Albeit its significant progress, the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity. In this work, we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. We show that this class of tasks are much more challenging than previously studied tasks, and the test accuracy of existing approaches is almost 0%.   We tackle the challenges by developing three novel techniques inspired by three novel observations, which reveal the key ingredients of using deep learning to synthesize a complex program. First, the use of a non-differentiable machine is the key to effectively restrict the search space. Thus our proposed approach learns a neural program operating a domain-specific non-differentiable machine. Second, recursion is the key to achieve generalizability. Thus, we bake-in the notion of recursion in the design of our non-differentiable machine. Third, reinforcement learning is the key to learn how to operate the non-differentiable machine, but it is also hard to train the model effectively with existing reinforcement learning algorithms from a cold boot. We develop a novel two-phase reinforcement learning-based search algorithm to overcome this issue. In our evaluation, we show that using our novel approach, neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500x longer than the training samples.",0
"Recent years have seen the development of deep learning techniques to enhance program synthesis from input-output examples. Despite notable progress, current approaches can only synthesize simple programs. This study proposes a new class of challenging tasks in program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. These tasks are more challenging than previous ones, with existing approaches achieving almost 0% accuracy. To address these challenges, the study develops three novel techniques based on key observations. Firstly, a non-differentiable machine effectively restricts the search space. Thus, the proposed approach learns a neural program operating a domain-specific non-differentiable machine. Secondly, recursion achieves generalizability, which is baked into the design of the non-differentiable machine. Thirdly, reinforcement learning is necessary to learn how to operate the machine, but training the model is difficult with existing algorithms. Thus, a novel two-phase reinforcement learning-based search algorithm is developed. Results demonstrate that the proposed approach achieves 100% test accuracy on inputs 500 times longer than training samples.",1
"Reinforcement Learning and the Evolutionary Strategy are two major approaches in addressing complicated control problems. Both are strong contenders and have their own devotee communities. Both groups have been very active in developing new advances in their own domain and devising, in recent years, leading-edge techniques to address complex continuous control tasks. Here, in the context of Deep Reinforcement Learning, we formulate a parallelized version of the Proximal Policy Optimization method and a Deep Deterministic Policy Gradient method. Moreover, we conduct a thorough comparison between the state-of-the-art techniques in both camps fro continuous control; evolutionary methods and Deep Reinforcement Learning methods. The results show there is no consistent winner.",0
"Two prominent approaches for tackling intricate control problems are Reinforcement Learning and Evolutionary Strategy. Each has its own enthusiastic following and has made significant strides in their respective fields. In recent years, both groups have developed innovative techniques to address demanding continuous control tasks. For this study, we present a parallelized version of the Proximal Policy Optimization method and a Deep Deterministic Policy Gradient method within the context of Deep Reinforcement Learning. Additionally, we conduct a comprehensive comparison of the latest techniques from both Evolutionary Strategy and Deep Reinforcement Learning for continuous control tasks. The outcomes indicate that there is no clear victor.",1
"We introduce Recurrent Predictive State Policy (RPSP) networks, a recurrent architecture that brings insights from predictive state representations to reinforcement learning in partially observable environments. Predictive state policy networks consist of a recursive filter, which keeps track of a belief about the state of the environment, and a reactive policy that directly maps beliefs to actions, to maximize the cumulative reward. The recursive filter leverages predictive state representations (PSRs) (Rosencrantz and Gordon, 2004; Sun et al., 2016) by modeling predictive state-- a prediction of the distribution of future observations conditioned on history and future actions. This representation gives rise to a rich class of statistically consistent algorithms (Hefny et al., 2018) to initialize the recursive filter. Predictive state serves as an equivalent representation of a belief state. Therefore, the policy component of the RPSP-network can be purely reactive, simplifying training while still allowing optimal behaviour. Moreover, we use the PSR interpretation during training as well, by incorporating prediction error in the loss function. The entire network (recursive filter and reactive policy) is still differentiable and can be trained using gradient based methods. We optimize our policy using a combination of policy gradient based on rewards (Williams, 1992) and gradient descent based on prediction error. We show the efficacy of RPSP-networks under partial observability on a set of robotic control tasks from OpenAI Gym. We empirically show that RPSP-networks perform well compared with memory-preserving networks such as GRUs, as well as finite memory models, being the overall best performing method.",0
"We present Recurrent Predictive State Policy (RPSP) networks, a type of recurrent architecture designed for reinforcement learning in partially observable environments. These networks utilize predictive state representations (PSRs) to keep track of the environment's state and map beliefs to actions that maximize the cumulative reward. The recursive filter of the RPSP network models predictive state, which predicts the distribution of future observations based on history and future actions, and is used to initialize the filter. The policy component of RPSP can be purely reactive due to the predictive state representation, simplifying training while still allowing optimal behavior. We incorporate prediction error into the loss function during training and optimize using a combination of policy gradient and gradient descent. We demonstrate the effectiveness of RPSP networks on robotic control tasks from OpenAI Gym, outperforming memory-preserving networks such as GRUs and finite memory models.",1
"We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward deep neural network that allows selective execution. Given an input, only a subset of D2NN neurons are executed, and the particular subset is determined by the D2NN itself. By pruning unnecessary computation depending on input, D2NNs provide a way to improve computational efficiency. To achieve dynamic selective execution, a D2NN augments a feed-forward deep neural network (directed acyclic graph of differentiable modules) with controller modules. Each controller module is a sub-network whose output is a decision that controls whether other modules can execute. A D2NN is trained end to end. Both regular and controller modules in a D2NN are learnable and are jointly trained to optimize both accuracy and efficiency. Such training is achieved by integrating backpropagation with reinforcement learning. With extensive experiments of various D2NN architectures on image classification tasks, we demonstrate that D2NNs are general and flexible, and can effectively optimize accuracy-efficiency trade-offs.",0
"We have developed Dynamic Deep Neural Networks (D2NNs), a novel form of feed-forward deep neural network that can selectively execute. Instead of activating all D2NN neurons for a given input, only a specific subset is determined by the network itself. D2NNs are designed to boost computational efficiency by removing unnecessary computations based on input. To enable dynamic selective execution, a D2NN adds controller modules to a feed-forward deep neural network (a directed acyclic graph of differentiable modules). Each controller module is a sub-network that produces a decision dictating whether other modules can execute. D2NNs are trained end-to-end, with both regular and controller modules being learnable and jointly optimized for accuracy and efficiency. Backpropagation is combined with reinforcement learning for training. Through extensive experiments on image classification tasks, we demonstrate that D2NNs are versatile and adaptable, allowing for effective optimization of the accuracy-efficiency trade-off.",1
"In video captioning task, the best practice has been achieved by attention-based models which associate salient visual components with sentences in the video. However, existing study follows a common procedure which includes a frame-level appearance modeling and motion modeling on equal interval frame sampling, which may bring about redundant visual information, sensitivity to content noise and unnecessary computation cost.   We propose a plug-and-play PickNet to perform informative frame picking in video captioning. Based on a standard Encoder-Decoder framework, we develop a reinforcement-learning-based procedure to train the network sequentially, where the reward of each frame picking action is designed by maximizing visual diversity and minimizing textual discrepancy. If the candidate is rewarded, it will be selected and the corresponding latent representation of Encoder-Decoder will be updated for future trials. This procedure goes on until the end of the video sequence. Consequently, a compact frame subset can be selected to represent the visual information and perform video captioning without performance degradation. Experiment results shows that our model can use 6-8 frames to achieve competitive performance across popular benchmarks.",0
"Attention-based models have been successful in video captioning by linking important visual elements to sentences in the video. However, traditional methods involve appearance and motion modeling on every frame, resulting in excessive visual information, sensitivity to content noise, and unnecessary computation costs. Our proposed solution is the PickNet, a plug-and-play tool that selects informative frames for video captioning. Using a reinforcement-learning-based procedure, the network is trained sequentially to maximize visual diversity and minimize textual discrepancy for each frame picking action. If a candidate frame is rewarded, it is selected and the Encoder-Decoder's latent representation is updated for future trials until the end of the video sequence. With our method, a compact frame subset can represent visual information and perform video captioning without performance degradation. Our experimental results show that our model can achieve competitive performance using 6-8 frames across popular benchmarks.",1
"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",0
"Our proposal is to utilize a distributed architecture for deep reinforcement learning on a large scale. This architecture allows agents to learn more effectively by processing a significantly larger amount of data than previously possible. Our algorithm separates the act of decision-making from the learning process. The actors select actions based on a shared neural network while interacting with their own instance of the environment, and store their experiences in a shared memory for later use. The learner then reviews samples from this memory and updates the neural network accordingly. Our architecture prioritizes the most important data generated by the actors, resulting in more efficient use of resources. Our approach significantly surpasses the current state-of-the-art in the Arcade Learning Environment, achieving superior performance in a fraction of the training time.",1
"Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.",0
"New model-free reinforcement learning techniques suggest integrating learned dynamics models to decrease sample complexity. These approaches aim to incorporate imagined data and model uncertainty to expedite learning continuous control tasks. However, they rely on restrictive heuristics that restrict the dynamics model's usage. Our proposal, model-based value expansion, manages the model's uncertainty by limiting imagination to a fixed depth. By broadening the learned dynamics models' usage within a model-free reinforcement learning algorithm, we enhance value estimation, resulting in reduced sample complexity for learning.",1
"We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.",0
"In the realm of Reinforcement Learning (RL), we present a new learning technique called Scheduled Auxiliary Control (SAC-X). SAC-X allows for the acquisition of intricate behaviors from scratch, even in the presence of multiple sparse reward signals. The agent is furnished with a collection of general auxiliary tasks that it endeavors to learn simultaneously through off-policy RL. Our approach centers around the concept that by actively scheduling and executing auxiliary policies, the agent can explore its environment efficiently and excel in sparse reward RL. In various difficult robotic manipulation scenarios, our experiments showcase the efficacy of our methodology.",1
"Most reinforcement learning algorithms are inefficient for learning multiple tasks in complex robotic systems, where different tasks share a set of actions. In such environments a compound policy may be learnt with shared neural network parameters, which performs multiple tasks concurrently. However such compound policy may get biased towards a task or the gradients from different tasks negate each other, making the learning unstable and sometimes less data efficient. In this paper, we propose a new approach for simultaneous training of multiple tasks sharing a set of common actions in continuous action spaces, which we call as DiGrad (Differential Policy Gradient). The proposed framework is based on differential policy gradients and can accommodate multi-task learning in a single actor-critic network. We also propose a simple heuristic in the differential policy gradient update to further improve the learning. The proposed architecture was tested on 8 link planar manipulator and 27 degrees of freedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2 end effectors respectively. We show that our approach supports efficient multi-task learning in complex robotic systems, outperforming related methods in continuous action spaces.",0
"Learning multiple tasks in complex robotic systems using reinforcement learning algorithms can be inefficient as different tasks share a set of actions. To overcome this, a compound policy can be learned with shared neural network parameters that can perform multiple tasks concurrently. However, this approach has limitations as the compound policy may become biased towards a specific task or the gradients from different tasks may cancel each other, leading to unstable learning and reduced data efficiency. Our paper introduces a new approach called DiGrad (Differential Policy Gradient) for simultaneous training of multiple tasks that share a set of common actions in continuous action spaces. This framework is based on differential policy gradients and can handle multi-task learning in a single actor-critic network. We also propose a simple heuristic in the differential policy gradient update to enhance the learning. We tested our approach on a planar manipulator with 8 links and a humanoid with 27 degrees of freedom for learning multi-goal reachability tasks for 3 and 2 end effectors respectively. Our results demonstrate that DiGrad supports efficient multi-task learning in complex robotic systems and outperforms related methods in continuous action spaces.",1
"We consider the problem of \emph{fully decentralized} multi-agent reinforcement learning (MARL), where the agents are located at the nodes of a time-varying communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors. To this end, we propose two decentralized actor-critic algorithms with function approximation, which are applicable to large-scale MARL problems where both the number of states and the number of agents are massively large. Under the decentralized structure, the actor step is performed individually by each agent with no need to infer the policies of others. For the critic step, we propose a consensus update via communication over the network. Our algorithms are fully incremental and can be implemented in an online fashion. Convergence analyses of the algorithms are provided when the value functions are approximated within the class of linear functions. Extensive simulation results with both linear and nonlinear function approximations are presented to validate the proposed algorithms. Our work appears to be the first study of fully decentralized MARL algorithms for networked agents with function approximation, with provable convergence guarantees.",0
"The focus of our study is on fully decentralized multi-agent reinforcement learning (MARL). Our agents are located at the nodes of a communication network that varies over time. We assume that each agent has a unique reward function that is only known to that agent. Additionally, agents make individual decisions based on local information and messages received from their neighbors. The collective goal of the agents is to maximize the globally averaged return over the network by exchanging information with their neighbors. We introduce two decentralized actor-critic algorithms with function approximation that can be applied to large-scale MARL problems with a massive number of states and agents. Our algorithms are fully incremental and can be implemented online. The actor step is performed independently by each agent, and the critic step is updated via communication over the network. We provide convergence analyses of the algorithms when the value functions are approximated within the class of linear functions. We validate the proposed algorithms with extensive simulation results using both linear and nonlinear function approximations. Our study is the first to introduce fully decentralized MARL algorithms with function approximation for networked agents, with provable convergence guarantees.",1
"Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.~fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.",0
"Variational Bayesian neural networks blend the flexible nature of deep learning with Bayesian uncertainty evaluation. However, the selection between inexpensive yet simple variational families (such as fully factorized) or costly and intricate inference procedures is a challenge. Our study indicates that natural gradient ascent with adaptive weight noise implicitly adapts a variational posterior to optimize the evidence lower bound (ELBO). This realization enables us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using different noisy versions of natural gradient, Adam, and K-FAC, enabling the scaling up of modern ConvNets. Our noisy K-FAC algorithm, when tested on standard regression benchmarks, outperforms existing methods by producing better predictions and matching Hamiltonian Monte Carlo's predictive variances. Its enhanced uncertainty estimates lead to more efficient exploration in active learning and intrinsic motivation for reinforcement learning.",1
"Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",0
"Great strides have been made in deep reinforcement learning recently, particularly in the areas of Go and Atari games. However, there remains a significant challenge in balancing exploration and exploitation in complex domains. To address this, Thompson Sampling and its reinforcement learning extension offer a promising approach to exploration that only requires access to posterior samples of the model. In addition, recent advances in approximate Bayesian methods have made it practical to use posterior approximation for flexible neural network models. This makes it appealing to consider using approximate Bayesian neural networks in a Thompson Sampling framework. To assess the impact of using an approximate posterior on Thompson Sampling, we tested established and newly developed methods for approximate posterior sampling combined with Thompson Sampling on a series of contextual bandit problems. Our findings indicated that many of the approaches that have been successful in supervised learning did not perform as well in sequential decision-making scenarios. The challenge of adapting slowly converging uncertainty estimates to the online setting was particularly notable.",1
"Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",0
"Reinforcement learning (RL) has faced the challenge of exploring environments with sparse rewards. While some tasks are naturally suited to a sparse reward system, manually shaping rewards can lead to suboptimal results. Unfortunately, finding a non-zero reward becomes increasingly difficult as task horizon and action dimensionality increase, limiting the practicality of RL in real-world scenarios. To address this issue, our study leverages demonstrations to overcome the exploration problem and successfully complete complex robotics tasks with continuous control, such as stacking blocks with a robot arm. Our approach, which builds on Deep Deterministic Policy Gradients and Hindsight Experience Replay, offers a significant speedup over RL when applied to simulated robotics tasks and requires only a small set of demonstrations. Additionally, our method can solve previously unsolvable tasks and often outperforms the demonstrator policy.",1
"Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our method uses gradients of a neural network trained jointly with model parameters or policies, and is applicable in both discrete and continuous settings. We demonstrate this framework for training discrete latent-variable models. We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.",0
"The basis of deep learning and reinforcement learning is gradient-based optimization. Despite the possibility of not knowing or having a non-differentiable mechanism to optimize, utilizing high-variance or biased gradient estimates is usually the optimal approach. To address this, we present a general framework that allows for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our approach involves utilizing the gradients of a neural network that is trained in conjunction with model parameters or policies, and it can be applied to both continuous and discrete settings. We exhibit the effectiveness of this framework by implementing it in the training of discrete latent-variable models. Additionally, we introduce an action-conditional extension of the advantage actor-critic reinforcement learning algorithm that is unbiased.",1
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",0
"Reinforcement learning offers the possibility for agents to break down a task into smaller subtasks, which can enhance the speed of learning and planning. Nonetheless, discovering useful options autonomously remains a significant challenge in the field. Our paper focuses on the concept of using representation learning methods to guide option discovery, specifically investigating eigenoptions. These options are obtained from representations that encode diffusive information flow in the environment. We have expanded existing algorithms for eigenoption discovery to include settings where stochastic transitions and handcrafted features are not available. Our proposed algorithm discovers eigenoptions while learning non-linear state representations from raw pixels, utilizing the successes of deep reinforcement learning and the equivalence between proto-value functions and the successor representation. We provide insight into our approach using traditional tabular domains and showcase its potential with Atari 2600 games.",1
"Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.   We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",0
"One of the primary difficulties in Reinforcement Learning (RL) is dealing with limited rewards. To address this issue, we introduce a new method called Hindsight Experience Replay. This technique enables efficient sample learning from sparse binary incentives, thus eliminating the need for intricate reward engineering. It can be integrated with any off-policy RL algorithm and functions as an implicit curriculum. We apply this approach to the manipulation of objects using a robotic arm and test it on three different tasks: pushing, sliding, and pick-and-place. Binary rewards indicating task completion are the only incentives used. Our ablation studies demonstrate that Hindsight Experience Replay is an essential component for successful training in these challenging environments. Furthermore, we prove that our policies, developed using a physics simulation, can be implemented on a physical robot and effectively execute the assigned task.",1
"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.",0
"The use of policy gradient methods has led to impressive accomplishments in tackling difficult reinforcement learning problems. Nevertheless, these methods frequently encounter the obstacle of high variance in policy gradient estimation, resulting in suboptimal training sample efficiency. This paper suggests a control variate approach to effectively mitigate the variance issue associated with policy gradient methods. Inspired by Stein's identity, our technique expands on previous control variate methods implemented in REINFORCE and advantage actor-critic by introducing more comprehensive action-dependent baseline functions. Empirical evidence indicates that our approach notably enhances the sample efficiency of the most advanced policy gradient methodologies.",1
"We study an important yet under-addressed problem of quickly and safely improving policies in online reinforcement learning domains. As its solution, we propose a novel exploration strategy - diverse exploration (DE), which learns and deploys a diverse set of safe policies to explore the environment. We provide DE theory explaining why diversity in behavior policies enables effective exploration without sacrificing exploitation. Our empirical study shows that an online policy improvement algorithm framework implementing the DE strategy can achieve both fast policy improvement and safe online performance.",0
"The problem of enhancing policies in online reinforcement learning domains in a rapid and secure manner has received insufficient attention. To tackle this issue, we suggest a unique exploration method called diverse exploration (DE), which trains and employs a diverse range of secure policies for environment exploration. We present the theory behind DE, which establishes that varied behavioral policies aid in fruitful exploration without compromising exploitation. Our experimental findings indicate that an online policy enhancement algorithm framework utilizing the DE approach can attain speedy policy improvement and safe online performance.",1
"In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.",0
"Over the past few years, Deep Reinforcement Learning has made significant progress in addressing various critical problems related to sequential decision-making. For many control applications, a generic multilayer perceptron (MLP) is used for non-vision parts of the policy network. This study introduces a new neural network architecture for the policy network representation that is straightforward yet highly effective. The proposed Structured Control Net (SCN) divides the generic MLP into two sub-modules. The nonlinear control module is responsible for forward-looking and global control, while the linear control module stabilizes the local dynamics around the residual of global control. The combination of linear and nonlinear policies enhances training sample efficiency, final episodic reward, and the generalization of learned policy. Additionally, this architecture requires a smaller network and is applicable to different training methods. The hypothesis was validated by achieving competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, using various ablation and generalization tests, and employing multiple black-box and policy gradient training methods. Furthermore, incorporating problem-specific priors into the architecture has the potential to enhance broader control tasks. As a case study, locomotion tasks were improved by emulating biological central pattern generators (CPGs) as the nonlinear part of the architecture.",1
"Distributional approaches to value-based reinforcement learning model the entire distribution of returns, rather than just their expected values, and have recently been shown to yield state-of-the-art empirical performance. This was demonstrated by the recently proposed C51 algorithm, based on categorical distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However, the theoretical properties of CDRL algorithms are not yet well understood. In this paper, we introduce a framework to analyse CDRL algorithms, establish the importance of the projected distributional Bellman operator in distributional RL, draw fundamental connections between CDRL and the Cram\'er distance, and give a proof of convergence for sample-based categorical distributional reinforcement learning algorithms.",0
"Value-based reinforcement learning using distributional approaches considers the whole range of return distributions, instead of just their expected values. This method has shown superior empirical performance, as demonstrated by the C51 algorithm based on categorical distributional reinforcement learning (CDRL) proposed by Bellemare et al. in 2017. However, the theoretical properties of CDRL algorithms are not yet fully comprehended. In this study, we present a framework for analyzing CDRL algorithms, emphasize the importance of the projected distributional Bellman operator in distributional RL, establish essential links between CDRL and the Cram\'er distance, and provide evidence of convergence for sample-based categorical distributional reinforcement learning algorithms.",1
"Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls.",0
"The reinforcement learning problems can be solved by actor-critic methods, which involve updating a parameterized policy known as an actor to increase an estimate of the expected return, called a critic. However, current actor-critic methods only utilize values or gradients of the critic to update the policy parameter. To overcome this limitation, our study proposes a new actor-critic method named the guide actor-critic (GAC). The GAC approach initially learns a guide actor that locally maximizes the critic, and then updates the policy parameter based on the guide actor through supervised learning. Our research makes two primary theoretical contributions. Firstly, we demonstrate that GAC updates the guide actor through second-order optimization in the action space, wherein the curvature matrix is based on the Hessians of the critic. Secondly, we reveal that deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Our empirical results indicate that the GAC method is a promising reinforcement learning technique for continuous controls.",1
"The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.",0
"Developing effective heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires specialized knowledge and trial-and-error. Is it possible to automate this laborious process and learn the algorithms instead? Frequently, the same optimization problem is tackled repeatedly in real-world scenarios, maintaining the problem structure but varying in data. This presents an opportunity for learning heuristic algorithms that utilize the structure of these recurring problems. This paper proposes a novel combination of reinforcement learning and graph embedding to overcome this challenge. The learned greedy policy acts as a meta-algorithm that constructs a solution incrementally, with the action determined by the output of a graph embedding network that captures the current state of the solution. Our framework can be applied to a variety of optimization problems over graphs and can learn effective algorithms for the Minimum Vertex Cover, Maximum Cut, and Traveling Salesman problems.",1
"Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",0
"Reinforcement learning (RL) faces a significant challenge in exploration, which many current deep RL methods address by utilizing task-agnostic objectives, such as information gain or bonuses based on state visitation. However, RL applications often involve learning multiple tasks, and past tasks can inform exploration strategies for new ones. This study investigates how prior tasks can guide effective exploration in new situations, introducing a new gradient-based fast adaptation algorithm called model agnostic exploration with structured noise (MAESN) that leverages prior experience. MAESN initializes a policy and acquires a latent exploration space that can insert structured stochasticity into the policy, leading to exploration strategies that are informed by past knowledge and more effective than random action-space noise. Compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods, MAESN proves more effective at learning exploration strategies. The method is evaluated on various simulated tasks, including locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",1
"Mild cognitive impairment (MCI) is a prodromal phase in the progression from normal aging to dementia, especially Alzheimers disease. Even though there is mild cognitive decline in MCI patients, they have normal overall cognition and thus is challenging to distinguish from normal aging. Using transcribed data obtained from recorded conversational interactions between participants and trained interviewers, and applying supervised learning models to these data, a recent clinical trial has shown a promising result in differentiating MCI from normal aging. However, the substantial amount of interactions with medical staff can still incur significant medical care expenses in practice. In this paper, we propose a novel reinforcement learning (RL) framework to train an efficient dialogue agent on existing transcripts from clinical trials. Specifically, the agent is trained to sketch disease-specific lexical probability distribution, and thus to converse in a way that maximizes the diagnosis accuracy and minimizes the number of conversation turns. We evaluate the performance of the proposed reinforcement learning framework on the MCI diagnosis from a real clinical trial. The results show that while using only a few turns of conversation, our framework can significantly outperform state-of-the-art supervised learning approaches.",0
"Mild cognitive impairment (MCI) is a preliminary stage in the progression towards dementia, particularly Alzheimer's disease. Although MCI patients experience mild cognitive decline, their overall cognition remains normal, making it difficult to distinguish from normal aging. A recent clinical trial employed supervised learning models on transcribed conversational data between participants and trained interviewers, exhibiting a promising outcome in identifying MCI patients. However, the frequent interactions with medical staff may result in significant medical care expenses. To address this, we propose a novel reinforcement learning (RL) framework to train an efficient dialogue agent on existing transcripts from clinical trials. The agent is trained to create a disease-specific lexical probability distribution, enabling it to converse in a manner that maximizes diagnosis accuracy while minimizing conversation turns. The proposed RL framework's performance is evaluated based on MCI diagnosis from a real clinical trial, indicating that it significantly outperforms state-of-the-art supervised learning methods while utilizing only a few conversation turns.",1
"While great advances are made in pattern recognition and machine learning, the successes of such fields remain restricted to narrow applications and seem to break down when training data is scarce, a shift in domain occurs, or when intelligent reasoning is required for rapid adaptation to new environments. In this work, we list several of the shortcomings of modern machine-learning solutions, specifically in the contexts of computer vision and in reinforcement learning and suggest directions to explore in order to try to ameliorate these weaknesses.",0
"Although pattern recognition and machine learning have made significant progress, their achievements are limited to specific applications and are not effective when there is a lack of training data, a change in domain, or the need for quick adaptation to new environments. In this study, we identify various inadequacies of current machine-learning approaches, particularly in the areas of computer vision and reinforcement learning, and propose potential avenues for addressing these limitations.",1
"We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.",0
"Our new approach to deep reinforcement learning, called Imagination-Augmented Agents (I2As), merges model-free and model-based aspects. Unlike traditional model-based reinforcement learning and planning techniques that dictate how a model should be utilized to create a policy, I2As learn to interpret forecasts from a trained environment model in various ways by integrating predictions as supplementary context in deep policy networks. I2As demonstrate better performance, data efficiency, and resilience to errors in model specification than several baseline methods.",1
"Video summarization aims to facilitate large-scale video browsing by producing short, concise summaries that are diverse and representative of original videos. In this paper, we formulate video summarization as a sequential decision-making process and develop a deep summarization network (DSN) to summarize videos. DSN predicts for each video frame a probability, which indicates how likely a frame is selected, and then takes actions based on the probability distributions to select frames, forming video summaries. To train our DSN, we propose an end-to-end, reinforcement learning-based framework, where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all. During training, the reward function judges how diverse and representative the generated summaries are, while DSN strives for earning higher rewards by learning to produce more diverse and more representative summaries. Since labels are not required, our method can be fully unsupervised. Extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches.",0
"The goal of video summarization is to make it easier to browse through large amounts of video footage by creating brief, but comprehensive summaries that accurately represent the original content. This study presents a deep summarization network (DSN) that uses a sequential decision-making process to select frames from a video based on their probability of being relevant. To train the DSN, the researchers developed a reinforcement learning-based framework that uses a novel reward function to evaluate the diversity and representativeness of the generated summaries. Unlike other methods, this approach does not rely on user interactions or labels, making it fully unsupervised. The researchers conducted extensive experiments on two benchmark datasets, which demonstrated that their unsupervised method performs better than other unsupervised approaches and is comparable to or better than most supervised methods.",1
"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems, including agents that can move with skill and agility through their environment. An open problem in this setting is that of developing good strategies for integrating or merging policies for multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. We extend policy distillation methods to the continuous action setting and leverage this technique to combine expert policies, as evaluated in the domain of simulated bipedal locomotion across different classes of terrain. We also introduce an input injection method for augmenting an existing policy network to exploit new input features. Lastly, our method uses transfer learning to assist in the efficient acquisition of new skills. The combination of these methods allows a policy to be incrementally augmented with new skills. We compare our progressive learning and integration via distillation (PLAID) method against three alternative baselines.",0
"The ability of deep reinforcement learning to handle continuous control problems has been steadily increasing. This includes the development of agents that can deftly and efficiently navigate through their environment. However, a challenge in this area is to effectively merge policies for multiple skills, each of which is specialized in its own skill and state distribution. To address this, we extend policy distillation methods to the continuous action setting and apply it to expert policies in the simulated bipedal locomotion domain across various terrain classes. We also introduce an input injection method to add new input features to an existing policy network. Additionally, our method employs transfer learning to help acquire new skills more efficiently. By combining these techniques, we enable the gradual augmentation of a policy with new skills. We evaluate our progressive learning and integration via distillation (PLAID) method against three alternative baselines.",1
"Sepsis is a life-threatening condition affecting one million people per year in the US in which dysregulation of the body's own immune system causes damage to its tissues, resulting in a 28 - 50% mortality rate. Clinical trials for sepsis treatment over the last 20 years have failed to produce a single currently FDA approved drug treatment. In this study, we attempt to discover an effective cytokine mediation treatment strategy for sepsis using a previously developed agent-based model that simulates the innate immune response to infection: the Innate Immune Response agent-based model (IIRABM). Previous attempts at reducing mortality with multi-cytokine mediation using the IIRABM have failed to reduce mortality across all patient parameterizations and motivated us to investigate whether adaptive, personalized multi-cytokine mediation can control the trajectory of sepsis and lower patient mortality. We used the IIRABM to compute a treatment policy in which systemic patient measurements are used in a feedback loop to inform future treatment. Using deep reinforcement learning, we identified a policy that achieves 0% mortality on the patient parameterization on which it was trained. More importantly, this policy also achieves 0.8% mortality over 500 randomly selected patient parameterizations with baseline mortalities ranging from 1 - 99% (with an average of 49%) spanning the entire clinically plausible parameter space of the IIRABM. These results suggest that adaptive, personalized multi-cytokine mediation therapy could be a promising approach for treating sepsis. We hope that this work motivates researchers to consider such an approach as part of future clinical trials. To the best of our knowledge, this work is the first to consider adaptive, personalized multi-cytokine mediation therapy for sepsis, and is the first to exploit deep reinforcement learning on a biological simulation.",0
"Sepsis is a severe medical condition that endangers the lives of one million people annually in the United States. It occurs when the body's immune system malfunctions and damages its tissues, resulting in a mortality rate of 28-50%. Despite numerous clinical trials carried out over the last two decades, no drug treatment has been approved by the FDA. This study aims to discover an effective treatment strategy for sepsis using the Innate Immune Response agent-based model (IIRABM), which simulates the immune response to infection. Previous attempts to reduce mortality using multi-cytokine mediation have failed to produce positive results and prompted us to explore the possibility of personalized multi-cytokine mediation treatment. We used the IIRABM to develop a treatment policy that employs systemic patient measurements to inform future treatment. Deep reinforcement learning was utilized to identify a policy that achieved 0% mortality on the patient parameterization on which it was trained. Importantly, this policy also achieved a mortality rate of 0.8% over 500 randomly selected patient parameterizations, with baseline mortalities ranging from 1-99%, spanning the entire clinically plausible parameter space of the IIRABM. These findings support the potential of adaptive, personalized multi-cytokine mediation therapy for the treatment of sepsis. This work is the first to consider this approach and to employ deep reinforcement learning on a biological simulation. We hope that our study encourages future clinical trials to explore adaptive, personalized multi-cytokine mediation therapy for the treatment of sepsis.",1
"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.",0
"One of the main obstacles in model-based reinforcement learning (RL) is developing environment models that are both efficient and precise. Our research demonstrates that state-space models, which operate on condensed state representations and are constructed through careful generative modeling, significantly decrease the computational resources needed to forecast action sequences. Our extensive experiments confirm that state-space models accurately capture the dynamics of Atari games from raw pixels. The improved computational efficiency of state-space models coupled with their high accuracy make them a viable option for RL applications. Our study showcases that agents utilizing these models for decision making outperform strong model-free baselines in the game MSPACMAN, indicating the potential for using learned environment models in planning.",1
"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction dramatically reduces the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, besides supervised training on execution traces, CNPI can be trained by policy gradient reinforcement learning with appropriately designed curricula.",0
"Our proposal is to enhance the universality and learnability of Neural Programmer-Interpreters (NPI) by introducing combinator abstraction into neural programming. This will involve a new NPI architecture called Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction simplifies the number and complexity of programs that the core controller of CNPI has to interpret, while still enabling the CNPI to interpret complex programs by collaborating with other components. We suggest a small set of four combinators to capture the most common programming patterns. The finiteness and simplicity of this set, along with the offloading of interpretation burden from the core, facilitates the construction of a CNPI that is universal with respect to the set of all combinatorizable programs. This is suitable for solving most algorithmic tasks. Furthermore, CNPI can be trained by policy gradient reinforcement learning with appropriately designed curricula, in addition to supervised training on execution traces.",1
"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples that are easily to be incorrectly retrieved, which can help to model the correlations. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality. While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Experiments on 3 widely-used datasets verify the effectiveness of our proposed approach.",0
"The objective of cross-modal hashing is to convert dissimilar multimedia data into a Hamming space that is uniform, making it possible to retrieve information quickly and flexibly across various modalities. Although supervised cross-modal hashing techniques have made notable advances by including semantic side data, they face two significant challenges: (1) they need extensive labeled data, which is arduous and challenging to obtain, and (2) they do not make use of the abundant unlabeled data found across different modalities, particularly the margin examples that could assist in modeling correlations. To tackle these issues, this study introduces a new approach called Semi-supervised Cross-Modal Hashing by Generative Adversarial Network (SCH-GAN). The study aims to leverage the GAN's capacity for modeling data distributions to enhance cross-modal hashing learning in an adversarial manner. The main contributions of this paper are two-fold: (1) it proposes a new generative adversarial network for cross-modal hashing, where the generative model chooses margin examples of one modality from the unlabeled data, while the discriminative model distinguishes between the chosen examples and the true positive examples of the query. The two models engage in a minimax game to enhance the hashing performance of the discriminative model. (2) It introduces a reinforcement learning-based algorithm to drive the training of SCH-GAN. The generative model uses the correlation score predicted by the discriminative model as a reward and selects examples close to the margin to enhance the discriminative model by maximizing the margin between positive and negative data. The study verifies the effectiveness of the proposed method through experiments on three widely-used datasets.",1
"In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.",0
"When generating data without supervision, it is often desirable to bias the process towards certain metrics. To achieve this, we propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL). By using RL, the data generation process can be biased towards specific metrics, while the GAN component of the reward function ensures that the model still retains information learned from previous observations. Our approach builds upon previous studies that utilized GANs and RL to generate sequence data. We test our model in several settings, including generating molecules encoded as text sequences (SMILES) and music generation. Our results demonstrate that our method can effectively bias the data generation process towards desired metrics.",1
"Reliable and effective multi-task learning is a prerequisite for the development of robotic agents that can quickly learn to accomplish related, everyday tasks. However, in the reinforcement learning domain, multi-task learning has not exhibited the same level of success as in other domains, such as computer vision. In addition, most reinforcement learning research on multi-task learning has been focused on discrete action spaces, which are not used for robotic control in the real-world. In this work, we apply multi-task learning methods to continuous action spaces and benchmark their performance on a series of simulated continuous control tasks. Most notably, we show that multi-task learning outperforms our baselines and alternative knowledge sharing methods.",0
"For the development of efficient robotic agents capable of learning related everyday tasks quickly, reliable and effective multi-task learning is crucial. However, the success of multi-task learning in the reinforcement learning domain has not been as significant as in other domains, such as computer vision. Moreover, the research on multi-task learning in reinforcement learning has mainly concentrated on discrete action spaces that are not practical for robotic control in the real world. Therefore, in this study, we have applied multi-task learning techniques to continuous action spaces and evaluated their performance on various simulated continuous control tasks. Our results demonstrate that multi-task learning surpasses our baselines and alternative knowledge sharing approaches.",1
"With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.",0
"The scientific community has taken an interest in multimodal sentiment analysis due to the growing popularity of video sharing platforms like YouTube and Facebook. Previous works in this field have focused on holistic information in speech segments, such as bag of words representations and average facial expression intensity. However, a new deep architecture for multimodal sentiment analysis has been developed, which performs modality fusion at the word level. The Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model is composed of two modules that address the challenges of fusion when there are noisy modalities. The Gated Multimodal Embedding module filters out these modalities, while the LSTM with Temporal Attention performs word-level fusion. Our model achieved state-of-the-art sentiment classification and regression results on the Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset. Our qualitative analysis highlighted the importance of the Temporal Attention Layer in sentiment prediction, particularly when additional acoustic and visual modalities are noisy. Our study contributes to the development of new models for multimodal fusion and opens up new areas of research in sentiment analysis in human communication.",1
"Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.",0
"Rather than injecting noise into the action space, deep reinforcement learning (RL) techniques can alternatively introduce noise directly into the agent's parameters. This approach can result in more consistent exploration and a wider range of behaviors. While methods like evolutionary strategies use parameter perturbations, they lose all temporal structure and require a greater number of samples. By combining parameter noise with traditional RL techniques, the benefits of both methods can be leveraged. Through experimentation with DQN, DDPG, and TRPO on high-dimensional discrete action environments and continuous control tasks, we show that both off-policy and on-policy methods benefit from this approach. Our findings indicate that RL with parameter noise is more efficient than traditional RL with action space noise and evolutionary strategies on their own.",1
"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize accurately and efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to localize accurately while minimizing the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.",0
"To estimate the location of an autonomous agent in an environment, the problem of localization arises. Traditional methods of localization, which involve filtering the belief based on observations, are not optimal due to the lack of control over the agent's actions. Our proposed solution called ""Active Neural Localizer"" is a fully differentiable neural network that learns to accurately and efficiently localize. The model combines structured belief with multiplicative interactions to propagate belief and a policy model to minimize the number of steps required for localization. Training is done end-to-end with reinforcement learning. The model's effectiveness is tested in various simulation environments such as 2D and 3D mazes and photo-realistic environments in the Doom and Unreal game engines. Results show that the learned policy is effective in idealistic settings and that the model can learn the policy and perceptual model jointly from raw-pixel based RGB observations. Additionally, the model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.",1
"The dramatic success of deep neural networks across multiple application areas often relies on experts painstakingly designing a network architecture specific to each task. To simplify this process and make it more accessible, an emerging research effort seeks to automate the design of neural network architectures, using e.g. evolutionary algorithms or reinforcement learning or simple search in a constrained space of neural modules.   Considering the typical size of the search space (e.g. $10^{10}$ candidates for a $10$-layer network) and the cost of evaluating a single candidate, current architecture search methods are very restricted. They either rely on static pre-built modules to be recombined for the task at hand, or they define a static hand-crafted framework within which they can generate new architectures from the simplest possible operations.   In this paper, we relax these restrictions, by capitalizing on the collective wisdom contained in the plethora of neural networks published in online code repositories. Concretely, we (a) extract and publish GitGraph, a corpus of neural architectures and their descriptions; (b) we create problem-specific neural architecture search spaces, implemented as a textual search mechanism over GitGraph; (c) we propose a method of identifying unique common subgraphs within the architectures solving each problem (e.g., image processing, reinforcement learning), that can then serve as modules in the newly created problem specific neural search space.",0
"Deep neural networks have achieved remarkable success in various fields, but require experts to design specific network architectures for each task. To simplify this process, researchers are exploring ways to automate the design of neural network architectures, such as using evolutionary algorithms, reinforcement learning, or simple search within a limited space of neural modules. However, current architecture search methods are constrained due to the vast search space and high cost of evaluating a single candidate. To address this, we propose a new approach that leverages the collective knowledge within the numerous neural networks published in online code repositories. Specifically, we (a) create GitGraph, a dataset of neural architectures and their descriptions; (b) develop a problem-specific neural architecture search mechanism using GitGraph; and (c) identify common subgraphs within architectures solving each problem, which can be used as modules in the new problem-specific neural search space.",1
"Recent work in deep reinforcement learning has allowed algorithms to learn complex tasks such as Atari 2600 games just from the reward provided by the game, but these algorithms presently require millions of training steps in order to learn, making them approximately five orders of magnitude slower than humans. One reason for this is that humans build robust shared representations that are applicable to collections of problems, making it much easier to assimilate new variants. This paper first introduces the idea of automatically-generated game sets to aid in transfer learning research, and then demonstrates the utility of shared representations by showing that models can substantially benefit from the incorporation of relevant architectural priors. This technique affords a remarkable 50x positive transfer on a toy problem-set.",0
"The advancement of deep reinforcement learning has enabled machines to learn complicated tasks like Atari 2600 games solely based on the game's reward. However, these algorithms currently require millions of training steps, which makes them almost five orders of magnitude slower than humans. One of the reasons for this is that humans have the ability to construct strong shared representations that can be applied to different problems, making it simpler to learn new variations. This study introduces the concept of automatically generated game sets to assist in transfer learning research. It then demonstrates the effectiveness of shared representations by revealing that incorporating relevant architectural priors can significantly benefit models. Implementing this technique resulted in a remarkable 50x positive transfer on a toy problem-set.",1
"Rapid advances of hardware-based technologies during the past decades have opened up new possibilities for Life scientists to gather multimodal data in various application domains (e.g., Omics, Bioimaging, Medical Imaging, and [Brain/Body]-Machine Interfaces), thus generating novel opportunities for development of dedicated data intensive machine learning techniques. Overall, recent research in Deep learning (DL), Reinforcement learning (RL), and their combination (Deep RL) promise to revolutionize Artificial Intelligence. The growth in computational power accompanied by faster and increased data storage and declining computing costs have already allowed scientists in various fields to apply these techniques on datasets that were previously intractable for their size and complexity. This review article provides a comprehensive survey on the application of DL, RL, and Deep RL techniques in mining Biological data. In addition, we compare performances of DL techniques when applied to different datasets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.",0
"The advancement of hardware-based technology in recent decades has created new opportunities for Life scientists to gather multimodal data in various application domains such as Omics, Bioimaging, Medical Imaging, and [Brain/Body]-Machine Interfaces. This has led to the development of dedicated data intensive machine learning techniques. The emergence of Deep learning (DL), Reinforcement learning (RL), and their combination (Deep RL) holds great promise for revolutionizing Artificial Intelligence. The increased computational power, faster and larger data storage, and declining computing costs have enabled scientists in various fields to apply these techniques on previously intractable datasets due to their size and complexity. This review article provides a comprehensive survey of the application of DL, RL, and Deep RL techniques in mining Biological data. Additionally, the performances of DL techniques on different datasets across various application domains are compared. Finally, open issues in this challenging research area are outlined and future development perspectives are discussed.",1
"Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.",0
"In healthcare and other high-stakes applications, it is crucial to have statistical performance bounds for reinforcement learning (RL) algorithms. A new framework, called Uniform-PAC, has been introduced in this paper to measure the performance of such algorithms theoretically. The Uniform-PAC framework is a stronger version of the traditional Probably Approximately Correct (PAC) framework. Unlike the PAC framework, the uniform version provides high probability regret guarantees and serves as a connection between the two setups that were previously missing in the literature. A new algorithm has been developed that is Uniform-PAC and achieves optimal regret and PAC guarantees except for a horizon factor. The benefits of the new framework have been demonstrated for finite-state episodic MDPs.",1
"Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by incorporating deep neural networks in learning representations from the input to RL. However, the conventional deep neural network architecture is limited in learning representations for multi-task RL (MT-RL), as multiple tasks can refer to different kinds of representations. In this paper, we thus propose a novel deep neural network architecture, namely generalization tower network (GTN), which can achieve MT-RL within a single learned model. Specifically, the architecture of GTN is composed of both horizontal and vertical streams. In our GTN architecture, horizontal streams are used to learn representation shared in similar tasks. In contrast, the vertical streams are introduced to be more suitable for handling diverse tasks, which encodes hierarchical shared knowledge of these tasks. The effectiveness of the introduced vertical stream is validated by experimental results. Experimental results further verify that our GTN architecture is able to advance the state-of-the-art MT-RL, via being tested on 51 Atari games.",0
"The use of deep neural networks in reinforcement learning has resulted in significant progress, known as deep learning. However, traditional deep neural network structures have limitations in multi-task reinforcement learning, as different tasks may require different representations. To address this issue, we propose a new deep neural network architecture called the generalization tower network (GTN) that can accomplish multi-task reinforcement learning within one model. The GTN architecture comprises horizontal and vertical streams, where horizontal streams learn shared representations for similar tasks, and vertical streams encode hierarchical shared knowledge for diverse tasks. Results from experiments demonstrate that the vertical stream is effective, and the GTN architecture advances multi-task reinforcement learning, as shown in tests on 51 Atari games.",1
"Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",0
"The use of Multi-task learning (MTL) with neural networks aims to enhance performance by leveraging commonalities in tasks. However, task interference often affects this approach, reducing the advantages of transfer. To combat this issue, we introduce a new neural network and training algorithm known as the routing network paradigm. This network is a self-organizing neural network that comprises a router and one or more function blocks, which can be any neural network, for example, a fully-connected or a convolutional layer. When given an input, the router chooses a function block to apply and returns the output recursively to the router until a fixed recursion depth is reached. By doing so, the routing network dynamically composes different function blocks for each input. The router and function blocks are jointly trained using a collaborative multi-agent reinforcement learning (MARL) approach. Our experiments show that our model outperforms cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. We achieve a significant improvement in accuracy, with sharper convergence. Furthermore, routing networks have nearly constant per-task training cost, while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks), we achieve cross-stitch performance levels with an 85% reduction in training time.",1
"Stochastic composition optimization draws much attention recently and has been successful in many emerging applications of machine learning, statistical analysis, and reinforcement learning. In this paper, we focus on the composition problem with nonsmooth regularization penalty. Previous works either have slow convergence rate or do not provide complete convergence analysis for the general problem. In this paper, we tackle these two issues by proposing a new stochastic composition optimization method for composition problem with nonsmooth regularization penalty. In our method, we apply variance reduction technique to accelerate the speed of convergence. To the best of our knowledge, our method admits the fastest convergence rate for stochastic composition optimization: for strongly convex composition problem, our algorithm is proved to admit linear convergence; for general composition problem, our algorithm significantly improves the state-of-the-art convergence rate from $O(T^{-1/2})$ to $O((n_1+n_2)^{{2}/{3}}T^{-1})$. Finally, we apply our proposed algorithm to portfolio management and policy evaluation in reinforcement learning. Experimental results verify our theoretical analysis.",0
"Recently, stochastic composition optimization has gained significant attention due to its success in various machine learning, statistical analysis, and reinforcement learning applications. This paper focuses on the composition problem with nonsmooth regularization penalty, a problem that previous methods have struggled with due to slow convergence rates and incomplete convergence analysis. To address these issues, we propose a new stochastic composition optimization method that utilizes variance reduction techniques to accelerate convergence. Our method achieves the fastest convergence rate for stochastic composition optimization, with linear convergence for strongly convex composition problems and significantly improved convergence rates for general problems. We demonstrate the effectiveness of our algorithm in portfolio management and policy evaluation in reinforcement learning, with experimental results supporting our theoretical analysis.",1
"Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \times 32$ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public",0
"High dimensional data, such as images or audio, requires density estimation, which is best achieved through autoregressive generative models. These models view density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution of the next element based on previous elements. The effectiveness of this approach is limited by the RNN's ability to model long-range dependencies, which can be improved by using causal convolutions instead of conventional RNNs. Our new generative model architecture combines causal convolutions with self-attention, inspired by recent work in meta reinforcement learning. Our implementation achieves state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \times 32$ ImageNet (3.80 bits per dim), and is available at https://github.com/neocxi/pixelsnail-public.",1
"Captioning models are typically trained using the cross-entropy loss. However, their performance is evaluated on other metrics designed to better correlate with human assessments. Recently, it has been shown that reinforcement learning (RL) can directly optimize these metrics in tasks such as captioning. However, this is computationally costly and requires specifying a baseline reward at each step to make training converge. We propose a fast approach to optimize one's objective of interest through the REINFORCE algorithm. First we show that, by replacing model samples with ground-truth sentences, RL training can be seen as a form of weighted cross-entropy loss, giving a fast, RL-based pre-training algorithm. Second, we propose to use the consensus among ground-truth captions of the same video as the baseline reward. This can be computed very efficiently. We call the complete proposal Consensus-based Sequence Training (CST). Applied to the MSRVTT video captioning benchmark, our proposals train significantly faster than comparable methods and establish a new state-of-the-art on the task, improving the CIDEr score from 47.3 to 54.2.",0
"Typically, captioning models are trained using the cross-entropy loss, but their performance is evaluated using other metrics that are better aligned with human assessments. Recently, it has been discovered that reinforcement learning (RL) can optimize these metrics more directly for tasks such as captioning. However, this approach is computationally expensive and requires a baseline reward at each step to ensure successful training. To address this issue, we present a new method based on the REINFORCE algorithm that quickly optimizes desired objectives. Firstly, we demonstrate that RL training can be viewed as a form of weighted cross-entropy loss by replacing model samples with ground-truth sentences, enabling fast RL-based pre-training. Secondly, we suggest using the consensus among ground-truth captions of the same video as the baseline reward, which is computationally efficient. Our complete approach, called Consensus-based Sequence Training (CST), trains much faster than comparable methods on the MSRVTT video captioning benchmark and achieves a new state-of-the-art by improving the CIDEr score from 47.3 to 54.2.",1
"Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.",0
"Recently, Q-learning and policy gradients, which are the two primary families of reinforcement learning algorithms, were found to be equivalent when a softmax relaxation is applied to one component and an entropic regularization to the other. The relationship between this outcome and the established convex duality of the softmax function and Shannon entropy, also known as the Donsker-Varadhan formula, offers a concise confirmation of the equivalence. Furthermore, we utilize the principles of convex analysis to demonstrate a new policy inequality with respect to soft Q-learning by interpreting this duality.",1
"Reinforcement learning (RL) has been successfully used to solve many continuous control tasks. Despite its impressive results however, fundamental questions regarding the sample complexity of RL on continuous problems remain open. We study the performance of RL in this setting by considering the behavior of the Least-Squares Temporal Difference (LSTD) estimator on the classic Linear Quadratic Regulator (LQR) problem from optimal control. We give the first finite-time analysis of the number of samples needed to estimate the value function for a fixed static state-feedback policy to within $\varepsilon$-relative error. In the process of deriving our result, we give a general characterization for when the minimum eigenvalue of the empirical covariance matrix formed along the sample path of a fast-mixing stochastic process concentrates above zero, extending a result by Koltchinskii and Mendelson in the independent covariates setting. Finally, we provide experimental evidence indicating that our analysis correctly captures the qualitative behavior of LSTD on several LQR instances.",0
"Although Reinforcement Learning (RL) has proven effective in addressing numerous continuous control tasks, questions regarding RL's sample complexity on continuous problems persist. We investigate RL's performance in this context by examining the Least-Squares Temporal Difference (LSTD) estimator's behavior in the Linear Quadratic Regulator (LQR) problem from optimal control. Our study presents the first finite-time analysis of the number of samples required to estimate the value function for a fixed static state-feedback policy to within $\varepsilon$-relative error. Along the way, we provide a comprehensive description of when the minimum eigenvalue of the empirical covariance matrix formed from a fast-mixing stochastic process concentrates above zero, which extends a prior result by Koltchinskii and Mendelson in the independent covariates setting. Finally, we offer experimental evidence demonstrating that our analysis accurately captures LSTD's qualitative performance on various LQR instances.",1
"It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge. We address the problem of learning to look around: if a visual agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training. As a result, 1) the learned ""look around"" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments. Completion episodes are shown at https://goo.gl/BgWX3W.",0
"Capturing good observations autonomously is a significant challenge, despite the common assumption of access to intelligently captured inputs. Our focus is on the problem of teaching a visual agent to look around and learn efficient exploratory behaviors to acquire informative observations. We propose a reinforcement learning solution where the agent is rewarded for actions that lessen its uncertainty about unobserved areas in its environment. Our approach involves a recurrent neural network-based method for active completion of panoramic natural scenes and 3D object shapes. Crucially, our learned policies are not tied to any recognition task or semantic content seen during training. This allows for the learned ""look around"" behavior to be relevant for new tasks and unseen environments, and training data acquisition requires no manual labeling. Our approach is demonstrated through tests in diverse settings, showing that our approach learns useful generic policies that transfer to new tasks and environments. Completion episodes can be viewed at https://goo.gl/BgWX3W.",1
"Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints and angles, even in the presence of optical distortions. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we study how viewpoint-invariant visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. The problem that must be solved by this controller is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing system must use its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to most visual servoing methods, which either assume known dynamics or require a calibration phase. We show how we can learn this recurrent controller using simulated data and a reinforcement learning objective. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: https://fsadeghi.github.io/Sim2RealViewInvariantServo",0
"The dexterity of humans in controlling their limbs and tools from various perspectives and angles, even in the presence of optical distortions, is impressive. This ability, known as visual servoing, is applied in robotics to move a tool or endpoint to a desired location with visual feedback. This study focuses on learning viewpoint-invariant visual servoing skills in a robotic manipulation scenario. The researchers trained a deep recurrent controller to determine the actions that move the robotic arm's endpoint to a target object. However, this problem is fundamentally ambiguous, especially under severe variation in viewpoint. The visual servoing system uses its memory of past movements to understand the actions' effects on the robot's motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This skill contrasts with most visual servoing methods, which require a calibration phase or assume known dynamics. The researchers used simulated data and reinforcement learning to train the recurrent controller, then adapted the resulting model to real-world robots by disentangling perception from control and only adjusting the visual layers. The model successfully servoed to new objects from different viewpoints on a Kuka IIWA robotic arm. Supplementary videos are available at https://fsadeghi.github.io/Sim2RealViewInvariantServo.",1
"Recognizing multiple labels of images is a fundamental but challenging task in computer vision, and remarkable progress has been attained by localizing semantic-aware image regions and predicting their labels with deep convolutional neural networks. The step of hypothesis regions (region proposals) localization in these existing multi-label image recognition pipelines, however, usually takes redundant computation cost, e.g., generating hundreds of meaningless proposals with non-discriminative information and extracting their features, and the spatial contextual dependency modeling among the localized regions are often ignored or over-simplified. To resolve these issues, this paper proposes a recurrent attention reinforcement learning framework to iteratively discover a sequence of attentional and informative regions that are related to different semantic objects and further predict label scores conditioned on these regions. Besides, our method explicitly models long-term dependencies among these attentional regions that help to capture semantic label co-occurrence and thus facilitate multi-label recognition. Extensive experiments and comparisons on two large-scale benchmarks (i.e., PASCAL VOC and MS-COCO) show that our model achieves superior performance over existing state-of-the-art methods in both performance and efficiency as well as explicitly identifying image-level semantic labels to specific object regions.",0
"Computer vision faces the challenge of recognizing multiple labels in images. Localizing semantic-aware regions and predicting their labels with deep convolutional neural networks has shown significant progress. However, the hypothesis regions localization, or region proposals, in these multi-label image recognition pipelines often result in redundant computation costs. This happens when hundreds of meaningless proposals with non-discriminative information are generated, and their features extracted. Moreover, the spatial contextual dependency modeling among the localized regions is often ignored or oversimplified. To address these issues, this paper introduces a recurrent attention reinforcement learning framework. This framework iteratively discovers a sequence of attentional and informative regions related to different semantic objects. It further predicts label scores conditioned on these regions. Additionally, it explicitly models long-term dependencies among these attentional regions, which facilitate multi-label recognition by capturing semantic label co-occurrence. The proposed model achieves superior performance over existing state-of-the-art methods in both performance and efficiency while identifying image-level semantic labels to specific object regions. Extensive experiments and comparisons on two large-scale benchmarks, namely PASCAL VOC and MS-COCO, support these findings.",1
"Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance $W_2$, policy dynamics are governed by the Fokker-Planck (heat) equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.",0
"When the change in policy is restricted to a small Kullback-Leibler divergence, policy gradient methods tend to perform better. Our approach involves limiting the change in policy to a small Wasserstein distance (or trust region), and we apply it to both the discrete and continuous multi-armed bandit scenarios with entropy regularization. As we approach small steps relative to the Wasserstein distance $W_2$, the Fokker-Planck (heat) equation governs the policy dynamics, in accordance with the Jordan-Kinderlehrer-Otto result. This means that policies experience diffusion and advection, concentrating near actions with high reward. Our findings shed light on the nature of convergence in probability matching setup, and also offer a rationale for common empirical practices such as Gaussian policy priors and additive gradient noise.",1
"Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but play a substantial role in determining the optimal policy. Experimental results across different domains show that ALOQ can learn more efficiently and robustly than existing methods.",0
"Various reinforcement learning problems have seen success with Bayesian optimisation, but the traditional simulator-based approach for learning optimal policies does not take advantage of the opportunity to improve learning by modifying certain environment variables. These variables are typically unobservable state features randomly determined by the environment in a physical setting but controllable in a simulator. This paper addresses the challenge of finding a strong policy while considering the impact of these environment variables. We introduce Alternating Optimisation and Quadrature (ALOQ), which combines Bayesian optimisation and Bayesian quadrature to handle such scenarios. ALOQ is capable of handling significant rare events that may not be observable through random sampling but still have a significant impact on the optimal policy. Experimental results across various domains demonstrate that ALOQ can learn more effectively and robustly than existing methods.",1
"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger `teacher' network as input and outputs a compressed `student' network derived from the `teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large `teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input `teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller `teacher' networks can be used to rapidly speed up training on larger `teacher' networks.",0
"Although larger and deeper neural networks have advanced computer vision tasks, their real-world adoption is hindered by hardware and speed limitations. Traditional model compression methods aim to solve this by modifying the architecture manually or using pre-defined heuristics. However, this is challenging since the space of reduced architectures is vast. This paper proposes a data-driven approach using reinforcement learning to learn compressed network architectures. The method takes a larger 'teacher' network as input and outputs a compressed 'student' network. The approach involves two stages: aggressively removing layers from the 'teacher' model using a recurrent policy network, and carefully reducing the size of each remaining layer using another recurrent policy network. The resulting network is evaluated based on accuracy and compression, and the reward signal is used with policy gradients to train the policies to find locally optimal student networks. The experiments show that models such as ResNet-34 can be compressed by more than 10x while maintaining similar performance to the input 'teacher' network. Furthermore, pre-trained policies on smaller 'teacher' networks can speed up training on larger 'teacher' networks.",1
"Machine learning models are powerful but fallible. Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities. Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require full access to target model parameters, rendering them impractical for many real-world attacks. In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers. We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario.",0
"Although machine learning models are potent, they are still prone to errors. By crafting adversarial examples - inputs that are intentionally created to cause model misclassification or other types of errors - we can gain valuable insight into model assumptions and vulnerabilities. Despite the considerable efforts devoted to generating adversarial examples for image classifiers, there has been little exploration into generating these examples for text classifiers. Moreover, most existing adversarial example generation algorithms require full access to the target model parameters, which makes them impractical for real-world attacks. In this study, we propose DANCin SEQ2SEQ, a GAN-inspired algorithm that generates adversarial examples for text classifiers, even in a mostly black-box scenario. We reframe adversarial text example generation as a reinforcement learning problem and demonstrate that our algorithm takes promising initial steps towards generating semantically meaningful adversarial text examples that could be used in real-world attacks.",1
"Deep reinforcement learning (DRL) has shown incredible performance in learning various tasks to the human level. However, unlike human perception, current DRL models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input. Because of this difference, DRL needs vast amount of experience samples to learn. In this paper, we propose a Multi-focus Attention Network (MANet) which mimics human ability to spatially abstract the low-level sensory input into multiple entities and attend to them simultaneously. The proposed method first divides the low-level input into several segments which we refer to as partial states. After this segmentation, parallel attention layers attend to the partial states relevant to solving the task. Our model estimates state-action values using these attended partial states. In our experiments, MANet attains highest scores with significantly less experience samples. Additionally, the model shows higher performance compared to the Deep Q-network and the single attention model as benchmarks. Furthermore, we extend our model to attentive communication model for performing multi-agent cooperative tasks. In multi-agent cooperative task experiments, our model shows 20% faster learning than existing state-of-the-art model.",0
"Although deep reinforcement learning (DRL) has demonstrated exceptional proficiency in mastering various tasks at a human level, it differs from human perception in that DRL models connect the entire low-level sensory input to state-action values, rather than exploiting the relationships and interconnections between the entities that make up the sensory input. Consequently, vast amounts of experience samples are required for DRL to learn. This paper proposes a Multi-focus Attention Network (MANet) that replicates human ability to abstract the low-level sensory input into multiple entities and simultaneously attend to them. The proposed approach segments the low-level input into partial states, which are subsequently addressed by parallel attention layers that are relevant to solving the task, and the model estimates state-action values based on these attended partial states. In our experiments, MANet achieves the highest scores with significantly fewer experience samples than other benchmarks, including the Deep Q-network and the single attention model. Moreover, our model extends to an attentive communication model for performing multi-agent cooperative tasks, where it demonstrates a 20% faster learning rate than the current state-of-the-art model.",1
"This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$ over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.",0
"This paper aims to compare two common optimization criteria for Reinforcement Learning, namely maximizing the mean value and minimizing the Bellman residual, both theoretically and empirically. To achieve this, we adopt policy search algorithms, which are typically designed for maximizing the mean value, and develop a method for minimizing the residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$ across policies. Our theoretical analysis demonstrates the efficacy of this approach for policy optimization, surpassing its value-based counterpart. We also present experiments on randomly generated generic Markov decision processes, specifically designed to study the influence of the concentrability coefficient. These experiments reveal that minimizing the Bellman residual is generally not a good approach for policy optimization, and that directly maximizing the mean value is far superior, despite the current lack of deep theoretical analysis. This may seem self-evident, since addressing the problem directly is usually better, but given the widespread use of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this issue warrants further investigation.",1
"A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a dynamic neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.",0
"The ability to quickly learn new tasks and adapt to changes is a crucial element of human intelligence. In this study, we propose a modular continual reinforcement learning approach that draws inspiration from these abilities. Our method incorporates a visual interaction environment that can accommodate various tasks within a single framework. We also introduce a reward map prediction technique that can effectively learn new tasks in large state and action spaces. We investigate the impact of module architecture on task learning efficiency and find that a module motif with specific design principles (such as early bottlenecks, low-order polynomial nonlinearities, and symmetry) outperforms standard neural network motifs, requiring fewer training examples and neurons to achieve high performance. Finally, we present a meta-controller architecture for task switching that utilizes a dynamic neural voting scheme, which improves learning efficiency by allowing new modules to leverage information learned from previously-seen tasks.",1
"Recent improvements in deep reinforcement learning have allowed to solve problems in many 2D domains such as Atari games. However, in complex 3D environments, numerous learning episodes are required which may be too time consuming or even impossible especially in real-world scenarios. We present a new architecture to combine external knowledge and deep reinforcement learning using only visual input. A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision. We evaluate the performances of our method in a 3D partially-observable environment from the Microsoft Malmo platform. Experimental evaluation exhibits higher performance and faster learning compared to a single reinforcement learning model.",0
"Advancements in deep reinforcement learning have enabled problem-solving in multiple 2D domains, including Atari games. However, learning in complex 3D environments necessitates several episodes, which may be impractical or infeasible in real-world scenarios. To address this, we propose a novel architecture that integrates external knowledge with deep reinforcement learning, utilizing solely visual input. Our system augments image input by integrating environmental feature information and fusing two sources of decision. We assess our method's efficiency in a 3D partially-observable environment using the Microsoft Malmo platform. Experimental results demonstrate superior performance and quicker learning compared to a single reinforcement learning model.",1
"We present MINOS, a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. The simulator leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites. We use MINOS to benchmark deep-learning-based navigation methods, to analyze the influence of environmental complexity on navigation performance, and to carry out a controlled study of multimodality in sensorimotor learning. The experiments show that current deep reinforcement learning approaches fail in large realistic environments. The experiments also indicate that multimodality is beneficial in learning to navigate cluttered scenes. MINOS is released open-source to the research community at http://minosworld.org . A video that shows MINOS can be found at https://youtu.be/c0mL9K64q84",0
"Introducing MINOS, a simulator that facilitates the creation of multisensory models for navigating complex indoor environments. By utilizing extensive 3D datasets and flexible sensor suite configurations, MINOS allows for the evaluation of deep-learning-based navigation methods, analysis of environmental effects on performance, and controlled studies of sensorimotor learning with multimodality. Results reveal the inadequacy of current deep reinforcement learning approaches in large, realistic environments, while also demonstrating the benefits of multimodal learning in navigating cluttered scenes. MINOS is available as an open-source tool for the research community at http://minosworld.org , and a video showcasing its capabilities can be viewed at https://youtu.be/c0mL9K64q84.",1
"This paper proposes adversarial attacks for Reinforcement Learning (RL) and then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to parameter uncertainties with the help of these attacks. We show that even a naively engineered attack successfully degrades the performance of DRL algorithm. We further improve the attack using gradient information of an engineered loss function which leads to further degradation in performance. These attacks are then leveraged during training to improve the robustness of RL within robust control framework. We show that this adversarial training of DRL algorithms like Deep Double Q learning and Deep Deterministic Policy Gradients leads to significant increase in robustness to parameter variations for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah environment.",0
"In this paper, we introduce adversarial attacks to Reinforcement Learning (RL) and demonstrate how these attacks can enhance the robustness of Deep Reinforcement Learning algorithms (DRL) against parameter uncertainties. Our findings reveal that even a simple attack can significantly impair the performance of DRL algorithms and that incorporating gradient information of an engineered loss function can further deteriorate performance. We utilize these attacks during training to augment the robustness of RL in a robust control framework. Our results demonstrate that adversarial training of DRL algorithms, such as Deep Double Q learning and Deep Deterministic Policy Gradients, can substantially improve their resilience to parameter variations in widely-used RL benchmarks, including Cart-pole, Mountain Car, Hopper, and Half Cheetah environment.",1
"We develop a parameterized Primal-Dual $\pi$ Learning method based on deep neural networks for Markov decision process with large state space and off-policy reinforcement learning. In contrast to the popular Q-learning and actor-critic methods that are based on successive approximations to the nonlinear Bellman equation, our method makes primal-dual updates to the policy and value functions utilizing the fundamental linear Bellman duality. Naive parametrization of the primal-dual $\pi$ learning method using deep neural networks would encounter two major challenges: (1) each update requires computing a probability distribution over the state space and is intractable; (2) the iterates are unstable since the parameterized Lagrangian function is no longer linear. We address these challenges by proposing a relaxed Lagrangian formulation with a regularization penalty using the advantage function. We show that the dual policy update step in our method is equivalent to the policy gradient update in the actor-critic method in some special case, while the value updates differ substantially. The main advantage of the primal-dual $\pi$ learning method lies in that the value and policy updates are closely coupled together using the Bellman duality and therefore more informative. Experiments on a simple cart-pole problem show that the algorithm significantly outperforms the one-step temporal-difference actor-critic method, which is the most relevant benchmark method to compare with. We believe that the primal-dual updates to the value and policy functions would expedite the learning process. The proposed methods might open a door to more efficient algorithms and sharper theoretical analysis.",0
"Our approach is to create a parameterized Primal-Dual $\pi$ Learning method for off-policy reinforcement learning in Markov decision processes with large state spaces, utilizing deep neural networks. While existing methods rely on approximating the nonlinear Bellman equation, we use primal-dual updates to both policy and value functions with the linear Bellman duality. However, implementing this method with deep neural networks presents two significant challenges: the need to compute a probability distribution over the state space for each update is infeasible, and the iterates are unstable as the parameterized Lagrangian function is no longer linear. To address these challenges, we propose a relaxed Lagrangian formulation with a regularization penalty using the advantage function. Our dual policy update step is equivalent to the policy gradient update in the actor-critic method in some cases, but our value updates differ significantly. The main advantage of our method is the close coupling between value and policy updates using the Bellman duality, making them more informative. Experiments on a cart-pole problem demonstrate that our method outperforms the one-step temporal-difference actor-critic method, which is the most relevant benchmark method. We expect that our approach will accelerate the learning process and lead to more efficient algorithms and sharper theoretical analysis.",1
"Glycemic control is essential for critical care. However, it is a challenging task because there has been no study on personalized optimal strategies for glycemic control. This work aims to learn personalized optimal glycemic trajectories for severely ill septic patients by learning data-driven policies to identify optimal targeted blood glucose levels as a reference for clinicians. We encoded patient states using a sparse autoencoder and adopted a reinforcement learning paradigm using policy iteration to learn the optimal policy from data. We also estimated the expected return following the policy learned from the recorded glycemic trajectories, which yielded a function indicating the relationship between real blood glucose values and 90-day mortality rates. This suggests that the learned optimal policy could reduce the patients' estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. The result demonstrates that reinforcement learning with appropriate patient state encoding can potentially provide optimal glycemic trajectories and allow clinicians to design a personalized strategy for glycemic control in septic patients.",0
"Achieving glycemic control in critical care is crucial, but challenging due to the lack of personalized optimal strategies. This study aimed to address this issue by developing data-driven policies to identify optimal targeted blood glucose levels for severely ill septic patients. Patient states were encoded using a sparse autoencoder, and a reinforcement learning paradigm was adopted to learn the optimal policy from data. The expected return following the policy was estimated, revealing a relationship between real blood glucose values and 90-day mortality rates. The results showed that the learned optimal policy could potentially reduce the patients' estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. These findings demonstrate the potential of reinforcement learning and appropriate patient state encoding to provide optimal glycemic trajectories, allowing clinicians to design personalized strategies for glycemic control in septic patients.",1
"Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an algorithm that performs dimensionality reduction on high dimensional input signal. It extracts those subsignals that are most predictable according to a certain prediction model. We refer to these extracted signals as predictable features.   In this work we extend the notion of PFA to take supplementary information into account for improving its predictions. Such information can be a multidimensional signal like the main input to PFA, but is regarded external. That means it won't participate in the feature extraction - no features get extracted or composed of it. Features will be exclusively extracted from the main input such that they are most predictable based on themselves and the supplementary information. We refer to this enhanced PFA as PFAx (PFA extended).   Even more important than improving prediction quality is to observe the effect of supplementary information on feature selection. PFAx transparently provides insight how the supplementary information adds to prediction quality and whether it is valuable at all. Finally we show how to invert that relation and can generate the supplementary information such that it would yield a certain desired outcome of the main signal.   We apply this to a setting inspired by reinforcement learning and let the algorithm learn how to control an agent in an environment. With this method it is feasible to locally optimize the agent's state, i.e. reach a certain goal that is near enough. We are preparing a follow-up paper that extends this method such that also global optimization is feasible.",0
"The Predictable Feature Analysis (PFA) algorithm, developed by Richthofer and Wiskott in 2015, is used to reduce the dimensionality of high dimensional input signals by extracting the subsignals that are most predictable based on a specific prediction model. These extracted signals, known as predictable features, are exclusively derived from the main input, with no influence from external multidimensional signals. However, in this study, we have extended the concept of PFA to incorporate supplementary information that can potentially enhance the accuracy of predictions. While this supplementary information is not involved in feature extraction, PFAx (PFA extended) uses it to improve the predictability of features based on both the main input and the supplementary information. PFAx also provides valuable insights into the effect of supplementary information on feature selection and prediction quality. Additionally, we demonstrate in our study how the relationship between the main signal and the supplementary information can be inverted to achieve a specific desired outcome. We have applied this method to a reinforcement learning scenario to optimize the state of an agent in an environment. We are currently working on a follow-up paper that expands this method to enable global optimization.",1
"We introduce MAgent, a platform to support research and development of many-agent reinforcement learning. Unlike previous research platforms on single or multi-agent reinforcement learning, MAgent focuses on supporting the tasks and the applications that require hundreds to millions of agents. Within the interactions among a population of agents, it enables not only the study of learning algorithms for agents' optimal polices, but more importantly, the observation and understanding of individual agent's behaviors and social phenomena emerging from the AI society, including communication languages, leaderships, altruism. MAgent is highly scalable and can host up to one million agents on a single GPU server. MAgent also provides flexible configurations for AI researchers to design their customized environments and agents. In this demo, we present three environments designed on MAgent and show emerged collective intelligence by learning from scratch.",0
"MAgent is a novel platform that aims to facilitate the advancement of many-agent reinforcement learning research and development. Unlike existing research platforms that focus on single or multi-agent reinforcement learning, MAgent caters to tasks and applications that necessitate hundreds to millions of agents. By enabling the observation and comprehension of individual agent behaviors and social phenomena that arise from interactions among a population of agents, such as communication languages, leaderships, and altruism, MAgent allows for the study of learning algorithms for agents' optimal policies. MAgent is highly scalable and can support up to one million agents on a single GPU server. Additionally, MAgent offers customizable environments and agents, allowing AI researchers to tailor their configurations as per their requirements. In this demonstration, we showcase three environments that we designed on MAgent, highlighting the collective intelligence that emerged from learning from scratch.",1
"We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the UCSG algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the diameter, which is an intrinsic value related to the mixing property of SGs. If we let the opponent play an optimistic best response to the learner, UCSG finds an $\varepsilon$-maximin stationary policy with a sample complexity of $\tilde{\mathcal{O}}\left(\text{poly}(1/\varepsilon)\right)$, where $\varepsilon$ is the gap to the best policy.",0
"Our focus is on studying online reinforcement learning in average-reward stochastic games (SGs), which are models of two-player zero-sum games in a Markov environment. In these games, both state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We have developed the UCSG algorithm, which outperforms previous methods by achieving sublinear regret compared to the game value when competing with any opponent. The regret bound is dependent on the diameter, which is a value that is related to the mixing property of SGs. If the opponent plays an optimistic best response to the learner, UCSG can find an $\varepsilon$-maximin stationary policy with a sample complexity of $\tilde{\mathcal{O}}\left(\text{poly}(1/\varepsilon)\right)$. This approach is highly effective in minimizing the gap to the best policy.",1
"Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf",0
"Although model-free deep reinforcement learning algorithms are capable of learning a variety of robotic skills, they often require a large number of samples to perform well. Model-based algorithms can offer more efficient learning, but are difficult to extend to high-capacity models such as deep neural networks. This study demonstrates that medium-sized neural network models can be combined with model predictive control (MPC) to achieve excellent sample efficiency in a model-based reinforcement learning algorithm, producing stable gaits for various complex locomotion tasks. Additionally, deep neural network dynamics models can be used to initialize a model-free learner, allowing for a combination of the efficiency of model-based approaches with the performance of model-free methods. Empirical evidence shows that the pure model-based approach trained solely on random action data can follow any trajectory with excellent sample efficiency, and that the hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos of the results can be found at https://sites.google.com/view/mbmf.",1
"We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (""What color is the car?""). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question (""orange"").   This challenging task requires a range of AI skills -- active perception, language understanding, goal-driven navigation, commonsense reasoning, and grounding of language into actions. In this work, we develop the environments, end-to-end-trained reinforcement learning agents, and evaluation protocols for EmbodiedQA.",0
"A new AI task called Embodied Question Answering (EmbodiedQA) is introduced in which an agent is placed at a random location in a 3D environment and asked a question (""What color is the car?""). To provide an answer, the agent must first navigate intelligently to explore the surroundings, gather information through first-person (egocentric) vision, and then answer the question (""orange""). This task is challenging as it requires a variety of AI skills such as active perception, language comprehension, goal-driven navigation, commonsense reasoning, and language grounding into actions. The development of environments, reinforcement learning agents, and assessment protocols for EmbodiedQA is presented in this study.",1
"Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100.",0
"A framework called Learning to Optimize has been proposed, which utilizes reinforcement learning to teach optimization algorithms. Our research delves into using this framework to train shallow neural nets with an optimization algorithm. This poses challenges for existing reinforcement learning algorithms due to the high-dimensional stochastic optimization problems involved. Therefore, we have developed an extension that addresses this issue and demonstrated that the learned optimization algorithm surpasses other known optimization algorithms, even when applied to new tasks and faced with changes in gradient stochasticity and neural net architecture. Specifically, we have proven that the proposed optimization algorithm, which was trained on the MNIST problem, can be applied to training neural nets for the Toronto Faces Dataset, CIFAR-10, and CIFAR-100.",1
"In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.",0
"The process of statistical dialogue management involves the dialogue manager learning a policy that links a belief state to a system action. Effective policy optimization relies on efficient exploration. Although current deep reinforcement learning methods show promise, they use epsilon-greedy exploration, which results in the user being subjected to random action choices during learning. To improve the user experience, alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are more sample efficient, but this comes at the cost of greater computational complexity. This paper investigates ways to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We extensively benchmark different deep Bayesian methods, including Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble, and alpha-divergences, and combine them with the DQN algorithm.",1
"Learning an optimal policy from a multi-modal reward function is a challenging problem in reinforcement learning (RL). Hierarchical RL (HRL) tackles this problem by learning a hierarchical policy, where multiple option policies are in charge of different strategies corresponding to modes of a reward function and a gating policy selects the best option for a given context. Although HRL has been demonstrated to be promising, current state-of-the-art methods cannot still perform well in complex real-world problems due to the difficulty of identifying modes of the reward function. In this paper, we propose a novel method called hierarchical policy search via return-weighted density estimation (HPSDE), which can efficiently identify the modes through density estimation with return-weighted importance sampling. Our proposed method finds option policies corresponding to the modes of the return function and automatically determines the number and the location of option policies, which significantly reduces the burden of hyper-parameters tuning. Through experiments, we demonstrate that the proposed HPSDE successfully learns option policies corresponding to modes of the return function and that it can be successfully applied to a challenging motion planning problem of a redundant robotic manipulator.",0
"Reinforcement learning (RL) faces a challenging obstacle when learning an optimal policy from a multi-modal reward function. To address this problem, hierarchical RL (HRL) has been developed, which involves learning a hierarchical policy with multiple option policies assigned to different strategies based on the modes of a reward function. However, existing HRL methods struggle to perform well in complex real-world situations due to the difficulty of detecting modes of the reward function. To overcome this issue, we propose a new method called hierarchical policy search via return-weighted density estimation (HPSDE). Our approach efficiently identifies modes by utilizing density estimation with return-weighted importance sampling. HPSDE determines the number and location of option policies automatically, which significantly reduces the need for hyper-parameter tuning. Experimental results demonstrate that HPSDE successfully learns option policies corresponding to modes of the return function and can be applied to solve challenging motion planning problems for redundant robotic manipulators.",1
"We view intersection handling on autonomous vehicles as a reinforcement learning problem, and study its behavior in a transfer learning setting. We show that a network trained on one type of intersection generally is not able to generalize to other intersections. However, a network that is pre-trained on one intersection and fine-tuned on another performs better on the new task compared to training in isolation. This network also retains knowledge of the prior task, even though some forgetting occurs. Finally, we show that the benefits of fine-tuning hold when transferring simulated intersection handling knowledge to a real autonomous vehicle.",0
"The handling of intersections in autonomous vehicles is perceived as a problem of reinforcement learning, and we investigate its performance in a transfer learning context. Our findings indicate that a network trained on a specific type of intersection does not have the ability to generalize to other intersections. Nonetheless, a pre-trained network on one intersection that is further refined on another performs better on the new task than when trained in isolation. This network also retains knowledge of the previous task despite some degree of forgetting. Ultimately, we demonstrate that fine-tuning results in advantages when transferring simulated intersection handling knowledge to an actual autonomous vehicle.",1
"Safely exploring an unknown dynamical system is critical to the deployment of reinforcement learning (RL) in physical systems where failures may have catastrophic consequences. In scenarios where one knows little about the dynamics, diverse transition data covering relevant regions of state-action space is needed to apply either model-based or model-free RL. Motivated by the cooling of Google's data centers, we study how one can safely identify the parameters of a system model with a desired accuracy and confidence level. In particular, we focus on learning an unknown linear system with Gaussian noise assuming only that, initially, a nominal safe action is known. Define safety as satisfying specific linear constraints on the state space (e.g., requirements on process variable) that must hold over the span of an entire trajectory, and given a Probably Approximately Correct (PAC) style bound on the estimation error of model parameters, we show how to compute safe regions of action space by gradually growing a ball around the nominal safe action. One can apply any exploration strategy where actions are chosen from such safe regions. Experiments on a stylized model of data center cooling dynamics show how computing proper safe regions can increase the sample efficiency of safe exploration.",0
"Ensuring the safe exploration of an unfamiliar dynamical system is crucial for the successful implementation of reinforcement learning (RL) in physical systems where any failure could result in dire consequences. If one possesses limited knowledge of the dynamics, it is necessary to gather a range of transition data that covers relevant regions of the state-action space to apply either model-based or model-free RL. Our study is motivated by the cooling of Google's data centers and aims to identify the system model's parameters safely with the desired level of accuracy and confidence. Specifically, we concentrate on learning an unknown linear system with Gaussian noise, starting with knowledge of a nominal safe action. We define safety as fulfilling specific linear constraints on the state space, which must remain valid throughout an entire trajectory. Given a Probably Approximately Correct (PAC) style bound on the model parameter estimation error, we demonstrate how to calculate safe regions of action space by gradually expanding a ball around the nominal safe action. Any exploration strategy may be utilized with actions chosen from these safe regions. Our experiments on a stylized model of data center cooling dynamics show that determining appropriate safe regions can increase the sample efficiency of safe exploration.",1
